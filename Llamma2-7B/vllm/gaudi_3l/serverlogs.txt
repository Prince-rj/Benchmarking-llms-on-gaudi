Calling add_step_closure function does not have any effect. It's lazy mode only functionality. (warning logged once)
Calling mark_step function does not have any effect. It's lazy mode only functionality. (warning logged once)
Calling iter_mark_step function does not have any effect. It's lazy mode only functionality. (warning logged once)
INFO 08-09 13:55:07 [__init__.py:254] Automatically detected platform hpu.
INFO 08-09 13:55:09 [config.py:1975] Disabled the custom all-reduce kernel because it is not supported on current platform.
INFO 08-09 13:55:10 [config.py:1975] Disabled the custom all-reduce kernel because it is not supported on current platform.
INFO 08-09 13:55:11 [config.py:1975] Disabled the custom all-reduce kernel because it is not supported on current platform.
INFO 08-09 13:55:11 [config.py:1975] Disabled the custom all-reduce kernel because it is not supported on current platform.
INFO 08-09 13:55:11 [api_server.py:1289] vLLM API server version 0.6.3.dev5453+ge9c83fca1
INFO 08-09 13:55:12 [config.py:1975] Disabled the custom all-reduce kernel because it is not supported on current platform.
INFO 08-09 13:55:12 [cli_args.py:309] non-default args: {'model': '/software/data/llama_inference/Llama-2-7b-hf/', 'tensor_parallel_size': 8}
INFO 08-09 13:55:21 [config.py:822] This model supports multiple tasks: {'reward', 'classify', 'score', 'embed', 'generate'}. Defaulting to 'generate'.
WARNING 08-09 13:55:21 [config.py:3236] Your device 'hpu' doesn't support torch.float16. Falling back to torch.bfloat16 for compatibility.
WARNING 08-09 13:55:21 [config.py:3287] Casting torch.float16 to torch.bfloat16.
INFO 08-09 13:55:21 [arg_utils.py:1706] hpu is experimental on VLLM_USE_V1=1. Falling back to V0 Engine.
INFO 08-09 13:55:21 [config.py:1941] Defaulting to use mp for distributed inference
INFO 08-09 13:55:21 [config.py:1975] Disabled the custom all-reduce kernel because it is not supported on current platform.
INFO 08-09 13:55:21 [api_server.py:267] Started engine process with PID 56331
Calling add_step_closure function does not have any effect. It's lazy mode only functionality. (warning logged once)
Calling mark_step function does not have any effect. It's lazy mode only functionality. (warning logged once)
Calling iter_mark_step function does not have any effect. It's lazy mode only functionality. (warning logged once)
WARNING 08-09 13:55:24 [env_override.py:17] NCCL_CUMEM_ENABLE is set to 0, skipping override. This may increase memory overhead with cudagraph+allreduce: https://github.com/NVIDIA/nccl/issues/1234
INFO 08-09 13:55:25 [__init__.py:254] Automatically detected platform hpu.
INFO 08-09 13:55:27 [llm_engine.py:239] Initializing a V0 LLM engine (v0.6.3.dev5453+ge9c83fca1) with config: model='/software/data/llama_inference/Llama-2-7b-hf/', speculative_config=None, tokenizer='/software/data/llama_inference/Llama-2-7b-hf/', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=8, pipeline_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=hpu, decoding_config=DecodingConfig(backend='xgrammar', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=None, served_model_name=/software/data/llama_inference/Llama-2-7b-hf/, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, pooler_config=None, compilation_config={"level":0,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":[],"splitting_ops":[],"use_inductor":true,"compile_sizes":[],"inductor_compile_config":{"enable_auto_functionalized_v2":false},"inductor_passes":{},"use_cudagraph":false,"cudagraph_num_of_warmups":0,"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"max_capture_size":256,"local_cache_dir":null}, use_cached_outputs=True, 
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
WARNING 08-09 13:55:27 [multiproc_worker_utils.py:325] Reducing Torch parallelism from 112 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
WARNING 08-09 13:55:27 [rocm.py:28] Failed to import from amdsmi with ModuleNotFoundError("No module named 'amdsmi'")
WARNING 08-09 13:55:27 [rocm.py:33] Failed to import from vllm._C with ModuleNotFoundError("No module named 'vllm._C'")
WARNING 08-09 13:55:27 [rocm.py:39] Failed to import from vllm._rocm_C with ModuleNotFoundError("No module named 'vllm._rocm_C'")
WARNING 08-09 13:55:27 [utils.py:2850] Methods add_prompt_adapter,cache_config,compilation_config,current_platform,list_prompt_adapters,load_config,pin_prompt_adapter,remove_prompt_adapter,scheduler_config not implemented in <vllm.worker.hpu_worker.HPUWorker object at 0x7f95e1c8b670>
INFO 08-09 13:55:28 [runtime.py:26] Environment:
INFO 08-09 13:55:28 [runtime.py:30]     hw: gaudi3
INFO 08-09 13:55:28 [runtime.py:30]     build: 1.22.0.425
INFO 08-09 13:55:28 [runtime.py:30]     engine_version: v0
INFO 08-09 13:55:28 [runtime.py:30]     bridge_mode: eager
INFO 08-09 13:55:28 [runtime.py:30]     model_type: llama
INFO 08-09 13:55:28 [runtime.py:26] Features:
INFO 08-09 13:55:28 [runtime.py:30]     fp32_alibi_biases: True
INFO 08-09 13:55:28 [runtime.py:30]     fp32_softmax: False
INFO 08-09 13:55:28 [runtime.py:30]     fused_block_softmax_adjustment: False
INFO 08-09 13:55:28 [runtime.py:30]     fused_block_softmax: False
INFO 08-09 13:55:28 [runtime.py:30]     prompt_attn_impl: fsdpa_impl
INFO 08-09 13:55:28 [runtime.py:30]     skip_warmup: False
INFO 08-09 13:55:28 [runtime.py:30]     merged_prefill: False
INFO 08-09 13:55:28 [runtime.py:30]     use_contiguous_pa: True
INFO 08-09 13:55:28 [runtime.py:30]     use_delayed_sampling: True
INFO 08-09 13:55:28 [runtime.py:30]     use_bucketing: True
INFO 08-09 13:55:28 [runtime.py:30]     bucketing_strategy: exponential_bucketing
INFO 08-09 13:55:28 [runtime.py:26] User flags:
INFO 08-09 13:55:28 [runtime.py:30]     VLLM_USE_V1: False
WARNING 08-09 13:55:28 [hpu.py:135] Pin memory is not supported on HPU.
INFO 08-09 13:55:28 [hpu.py:60] Using HPUAttention backend.
Calling add_step_closure function does not have any effect. It's lazy mode only functionality. (warning logged once)
Calling mark_step function does not have any effect. It's lazy mode only functionality. (warning logged once)
Calling iter_mark_step function does not have any effect. It's lazy mode only functionality. (warning logged once)
Calling add_step_closure function does not have any effect. It's lazy mode only functionality. (warning logged once)
Calling mark_step function does not have any effect. It's lazy mode only functionality. (warning logged once)
Calling iter_mark_step function does not have any effect. It's lazy mode only functionality. (warning logged once)
Calling add_step_closure function does not have any effect. It's lazy mode only functionality. (warning logged once)
Calling mark_step function does not have any effect. It's lazy mode only functionality. (warning logged once)
Calling iter_mark_step function does not have any effect. It's lazy mode only functionality. (warning logged once)
Calling add_step_closure function does not have any effect. It's lazy mode only functionality. (warning logged once)
Calling mark_step function does not have any effect. It's lazy mode only functionality. (warning logged once)
Calling iter_mark_step function does not have any effect. It's lazy mode only functionality. (warning logged once)
Calling add_step_closure function does not have any effect. It's lazy mode only functionality. (warning logged once)
Calling mark_step function does not have any effect. It's lazy mode only functionality. (warning logged once)
Calling iter_mark_step function does not have any effect. It's lazy mode only functionality. (warning logged once)
Calling add_step_closure function does not have any effect. It's lazy mode only functionality. (warning logged once)
Calling mark_step function does not have any effect. It's lazy mode only functionality. (warning logged once)
Calling iter_mark_step function does not have any effect. It's lazy mode only functionality. (warning logged once)
Calling add_step_closure function does not have any effect. It's lazy mode only functionality. (warning logged once)
Calling mark_step function does not have any effect. It's lazy mode only functionality. (warning logged once)
Calling iter_mark_step function does not have any effect. It's lazy mode only functionality. (warning logged once)
WARNING 08-09 13:55:30 [env_override.py:17] NCCL_CUMEM_ENABLE is set to 0, skipping override. This may increase memory overhead with cudagraph+allreduce: https://github.com/NVIDIA/nccl/issues/1234
WARNING 08-09 13:55:30 [env_override.py:17] NCCL_CUMEM_ENABLE is set to 0, skipping override. This may increase memory overhead with cudagraph+allreduce: https://github.com/NVIDIA/nccl/issues/1234
WARNING 08-09 13:55:30 [env_override.py:17] NCCL_CUMEM_ENABLE is set to 0, skipping override. This may increase memory overhead with cudagraph+allreduce: https://github.com/NVIDIA/nccl/issues/1234
WARNING 08-09 13:55:30 [env_override.py:17] NCCL_CUMEM_ENABLE is set to 0, skipping override. This may increase memory overhead with cudagraph+allreduce: https://github.com/NVIDIA/nccl/issues/1234
WARNING 08-09 13:55:30 [env_override.py:17] NCCL_CUMEM_ENABLE is set to 0, skipping override. This may increase memory overhead with cudagraph+allreduce: https://github.com/NVIDIA/nccl/issues/1234
WARNING 08-09 13:55:30 [env_override.py:17] NCCL_CUMEM_ENABLE is set to 0, skipping override. This may increase memory overhead with cudagraph+allreduce: https://github.com/NVIDIA/nccl/issues/1234
WARNING 08-09 13:55:30 [env_override.py:17] NCCL_CUMEM_ENABLE is set to 0, skipping override. This may increase memory overhead with cudagraph+allreduce: https://github.com/NVIDIA/nccl/issues/1234
INFO 08-09 13:55:31 [__init__.py:254] Automatically detected platform hpu.
INFO 08-09 13:55:31 [__init__.py:254] Automatically detected platform hpu.
INFO 08-09 13:55:31 [__init__.py:254] Automatically detected platform hpu.
INFO 08-09 13:55:31 [__init__.py:254] Automatically detected platform hpu.
INFO 08-09 13:55:31 [__init__.py:254] Automatically detected platform hpu.
INFO 08-09 13:55:31 [__init__.py:254] Automatically detected platform hpu.
INFO 08-09 13:55:31 [__init__.py:254] Automatically detected platform hpu.
[1;36m(VllmWorkerProcess pid=56473)[0;0m INFO 08-09 13:55:33 [multiproc_worker_utils.py:228] Worker ready; awaiting tasks
[1;36m(VllmWorkerProcess pid=56467)[0;0m INFO 08-09 13:55:33 [multiproc_worker_utils.py:228] Worker ready; awaiting tasks
[1;36m(VllmWorkerProcess pid=56470)[0;0m INFO 08-09 13:55:33 [multiproc_worker_utils.py:228] Worker ready; awaiting tasks
[1;36m(VllmWorkerProcess pid=56468)[0;0m INFO 08-09 13:55:33 [multiproc_worker_utils.py:228] Worker ready; awaiting tasks
[1;36m(VllmWorkerProcess pid=56471)[0;0m INFO 08-09 13:55:33 [multiproc_worker_utils.py:228] Worker ready; awaiting tasks
[1;36m(VllmWorkerProcess pid=56469)[0;0m INFO 08-09 13:55:33 [multiproc_worker_utils.py:228] Worker ready; awaiting tasks
[1;36m(VllmWorkerProcess pid=56473)[0;0m WARNING 08-09 13:55:33 [rocm.py:28] Failed to import from amdsmi with ModuleNotFoundError("No module named 'amdsmi'")
[1;36m(VllmWorkerProcess pid=56473)[0;0m WARNING 08-09 13:55:33 [rocm.py:33] Failed to import from vllm._C with ModuleNotFoundError("No module named 'vllm._C'")
[1;36m(VllmWorkerProcess pid=56473)[0;0m WARNING 08-09 13:55:33 [rocm.py:39] Failed to import from vllm._rocm_C with ModuleNotFoundError("No module named 'vllm._rocm_C'")
[1;36m(VllmWorkerProcess pid=56473)[0;0m WARNING 08-09 13:55:33 [utils.py:2850] Methods add_prompt_adapter,cache_config,compilation_config,current_platform,list_prompt_adapters,load_config,pin_prompt_adapter,remove_prompt_adapter,scheduler_config not implemented in <vllm.worker.hpu_worker.HPUWorker object at 0x7fb8e0039e10>
[1;36m(VllmWorkerProcess pid=56467)[0;0m WARNING 08-09 13:55:33 [rocm.py:28] Failed to import from amdsmi with ModuleNotFoundError("No module named 'amdsmi'")
[1;36m(VllmWorkerProcess pid=56467)[0;0m WARNING 08-09 13:55:33 [rocm.py:33] Failed to import from vllm._C with ModuleNotFoundError("No module named 'vllm._C'")
[1;36m(VllmWorkerProcess pid=56467)[0;0m WARNING 08-09 13:55:33 [rocm.py:39] Failed to import from vllm._rocm_C with ModuleNotFoundError("No module named 'vllm._rocm_C'")
[1;36m(VllmWorkerProcess pid=56470)[0;0m WARNING 08-09 13:55:33 [rocm.py:28] Failed to import from amdsmi with ModuleNotFoundError("No module named 'amdsmi'")
[1;36m(VllmWorkerProcess pid=56470)[0;0m WARNING 08-09 13:55:33 [rocm.py:33] Failed to import from vllm._C with ModuleNotFoundError("No module named 'vllm._C'")
[1;36m(VllmWorkerProcess pid=56470)[0;0m WARNING 08-09 13:55:33 [rocm.py:39] Failed to import from vllm._rocm_C with ModuleNotFoundError("No module named 'vllm._rocm_C'")
[1;36m(VllmWorkerProcess pid=56467)[0;0m WARNING 08-09 13:55:33 [utils.py:2850] Methods add_prompt_adapter,cache_config,compilation_config,current_platform,list_prompt_adapters,load_config,pin_prompt_adapter,remove_prompt_adapter,scheduler_config not implemented in <vllm.worker.hpu_worker.HPUWorker object at 0x7f40d3f21e10>
[1;36m(VllmWorkerProcess pid=56470)[0;0m WARNING 08-09 13:55:33 [utils.py:2850] Methods add_prompt_adapter,cache_config,compilation_config,current_platform,list_prompt_adapters,load_config,pin_prompt_adapter,remove_prompt_adapter,scheduler_config not implemented in <vllm.worker.hpu_worker.HPUWorker object at 0x7f38a5935e70>
[1;36m(VllmWorkerProcess pid=56472)[0;0m INFO 08-09 13:55:33 [multiproc_worker_utils.py:228] Worker ready; awaiting tasks
[1;36m(VllmWorkerProcess pid=56471)[0;0m WARNING 08-09 13:55:33 [rocm.py:28] Failed to import from amdsmi with ModuleNotFoundError("No module named 'amdsmi'")
[1;36m(VllmWorkerProcess pid=56471)[0;0m WARNING 08-09 13:55:33 [rocm.py:33] Failed to import from vllm._C with ModuleNotFoundError("No module named 'vllm._C'")
[1;36m(VllmWorkerProcess pid=56471)[0;0m WARNING 08-09 13:55:33 [rocm.py:39] Failed to import from vllm._rocm_C with ModuleNotFoundError("No module named 'vllm._rocm_C'")
[1;36m(VllmWorkerProcess pid=56468)[0;0m WARNING 08-09 13:55:33 [rocm.py:28] Failed to import from amdsmi with ModuleNotFoundError("No module named 'amdsmi'")
[1;36m(VllmWorkerProcess pid=56468)[0;0m WARNING 08-09 13:55:33 [rocm.py:33] Failed to import from vllm._C with ModuleNotFoundError("No module named 'vllm._C'")
[1;36m(VllmWorkerProcess pid=56468)[0;0m WARNING 08-09 13:55:33 [rocm.py:39] Failed to import from vllm._rocm_C with ModuleNotFoundError("No module named 'vllm._rocm_C'")
[1;36m(VllmWorkerProcess pid=56469)[0;0m WARNING 08-09 13:55:33 [rocm.py:28] Failed to import from amdsmi with ModuleNotFoundError("No module named 'amdsmi'")
[1;36m(VllmWorkerProcess pid=56469)[0;0m WARNING 08-09 13:55:33 [rocm.py:33] Failed to import from vllm._C with ModuleNotFoundError("No module named 'vllm._C'")
[1;36m(VllmWorkerProcess pid=56469)[0;0m WARNING 08-09 13:55:33 [rocm.py:39] Failed to import from vllm._rocm_C with ModuleNotFoundError("No module named 'vllm._rocm_C'")
[1;36m(VllmWorkerProcess pid=56471)[0;0m WARNING 08-09 13:55:33 [utils.py:2850] Methods add_prompt_adapter,cache_config,compilation_config,current_platform,list_prompt_adapters,load_config,pin_prompt_adapter,remove_prompt_adapter,scheduler_config not implemented in <vllm.worker.hpu_worker.HPUWorker object at 0x7f5686aaddb0>
[1;36m(VllmWorkerProcess pid=56468)[0;0m WARNING 08-09 13:55:33 [utils.py:2850] Methods add_prompt_adapter,cache_config,compilation_config,current_platform,list_prompt_adapters,load_config,pin_prompt_adapter,remove_prompt_adapter,scheduler_config not implemented in <vllm.worker.hpu_worker.HPUWorker object at 0x7fb0c54ade10>
[1;36m(VllmWorkerProcess pid=56469)[0;0m WARNING 08-09 13:55:33 [utils.py:2850] Methods add_prompt_adapter,cache_config,compilation_config,current_platform,list_prompt_adapters,load_config,pin_prompt_adapter,remove_prompt_adapter,scheduler_config not implemented in <vllm.worker.hpu_worker.HPUWorker object at 0x7fe0f1854040>
[1;36m(VllmWorkerProcess pid=56472)[0;0m WARNING 08-09 13:55:34 [rocm.py:28] Failed to import from amdsmi with ModuleNotFoundError("No module named 'amdsmi'")
[1;36m(VllmWorkerProcess pid=56472)[0;0m WARNING 08-09 13:55:34 [rocm.py:33] Failed to import from vllm._C with ModuleNotFoundError("No module named 'vllm._C'")
[1;36m(VllmWorkerProcess pid=56472)[0;0m WARNING 08-09 13:55:34 [rocm.py:39] Failed to import from vllm._rocm_C with ModuleNotFoundError("No module named 'vllm._rocm_C'")
[1;36m(VllmWorkerProcess pid=56472)[0;0m WARNING 08-09 13:55:34 [utils.py:2850] Methods add_prompt_adapter,cache_config,compilation_config,current_platform,list_prompt_adapters,load_config,pin_prompt_adapter,remove_prompt_adapter,scheduler_config not implemented in <vllm.worker.hpu_worker.HPUWorker object at 0x7fbec0ac1e40>
[1;36m(VllmWorkerProcess pid=56473)[0;0m INFO 08-09 13:55:34 [runtime.py:26] Environment:
[1;36m(VllmWorkerProcess pid=56473)[0;0m INFO 08-09 13:55:34 [runtime.py:30]     hw: gaudi3
[1;36m(VllmWorkerProcess pid=56473)[0;0m INFO 08-09 13:55:34 [runtime.py:30]     build: 1.22.0.425
[1;36m(VllmWorkerProcess pid=56473)[0;0m INFO 08-09 13:55:34 [runtime.py:30]     engine_version: v0
[1;36m(VllmWorkerProcess pid=56473)[0;0m INFO 08-09 13:55:34 [runtime.py:30]     bridge_mode: eager
[1;36m(VllmWorkerProcess pid=56473)[0;0m INFO 08-09 13:55:34 [runtime.py:30]     model_type: llama
[1;36m(VllmWorkerProcess pid=56473)[0;0m INFO 08-09 13:55:34 [runtime.py:26] Features:
[1;36m(VllmWorkerProcess pid=56473)[0;0m INFO 08-09 13:55:34 [runtime.py:30]     fp32_alibi_biases: True
[1;36m(VllmWorkerProcess pid=56473)[0;0m INFO 08-09 13:55:34 [runtime.py:30]     fp32_softmax: False
[1;36m(VllmWorkerProcess pid=56473)[0;0m INFO 08-09 13:55:34 [runtime.py:30]     fused_block_softmax_adjustment: False
[1;36m(VllmWorkerProcess pid=56473)[0;0m INFO 08-09 13:55:34 [runtime.py:30]     fused_block_softmax: False
[1;36m(VllmWorkerProcess pid=56473)[0;0m INFO 08-09 13:55:34 [runtime.py:30]     prompt_attn_impl: fsdpa_impl
[1;36m(VllmWorkerProcess pid=56473)[0;0m INFO 08-09 13:55:34 [runtime.py:30]     skip_warmup: False
[1;36m(VllmWorkerProcess pid=56473)[0;0m INFO 08-09 13:55:34 [runtime.py:30]     merged_prefill: False
[1;36m(VllmWorkerProcess pid=56473)[0;0m INFO 08-09 13:55:34 [runtime.py:30]     use_contiguous_pa: True
[1;36m(VllmWorkerProcess pid=56473)[0;0m INFO 08-09 13:55:34 [runtime.py:30]     use_delayed_sampling: True
[1;36m(VllmWorkerProcess pid=56473)[0;0m INFO 08-09 13:55:34 [runtime.py:30]     use_bucketing: True
[1;36m(VllmWorkerProcess pid=56473)[0;0m INFO 08-09 13:55:34 [runtime.py:30]     bucketing_strategy: exponential_bucketing
[1;36m(VllmWorkerProcess pid=56473)[0;0m INFO 08-09 13:55:34 [runtime.py:26] User flags:
[1;36m(VllmWorkerProcess pid=56473)[0;0m INFO 08-09 13:55:34 [runtime.py:30]     VLLM_USE_V1: False
[1;36m(VllmWorkerProcess pid=56473)[0;0m WARNING 08-09 13:55:34 [hpu.py:135] Pin memory is not supported on HPU.
[1;36m(VllmWorkerProcess pid=56473)[0;0m INFO 08-09 13:55:34 [hpu.py:60] Using HPUAttention backend.
[1;36m(VllmWorkerProcess pid=56467)[0;0m INFO 08-09 13:55:34 [runtime.py:26] Environment:
[1;36m(VllmWorkerProcess pid=56467)[0;0m INFO 08-09 13:55:34 [runtime.py:30]     hw: gaudi3
[1;36m(VllmWorkerProcess pid=56467)[0;0m INFO 08-09 13:55:34 [runtime.py:30]     build: 1.22.0.425
[1;36m(VllmWorkerProcess pid=56467)[0;0m INFO 08-09 13:55:34 [runtime.py:30]     engine_version: v0
[1;36m(VllmWorkerProcess pid=56467)[0;0m INFO 08-09 13:55:34 [runtime.py:30]     bridge_mode: eager
[1;36m(VllmWorkerProcess pid=56467)[0;0m INFO 08-09 13:55:34 [runtime.py:30]     model_type: llama
[1;36m(VllmWorkerProcess pid=56467)[0;0m INFO 08-09 13:55:34 [runtime.py:26] Features:
[1;36m(VllmWorkerProcess pid=56467)[0;0m INFO 08-09 13:55:34 [runtime.py:30]     fp32_alibi_biases: True
[1;36m(VllmWorkerProcess pid=56467)[0;0m INFO 08-09 13:55:34 [runtime.py:30]     fp32_softmax: False
[1;36m(VllmWorkerProcess pid=56467)[0;0m INFO 08-09 13:55:34 [runtime.py:30]     fused_block_softmax_adjustment: False
[1;36m(VllmWorkerProcess pid=56467)[0;0m INFO 08-09 13:55:34 [runtime.py:30]     fused_block_softmax: False
[1;36m(VllmWorkerProcess pid=56467)[0;0m INFO 08-09 13:55:34 [runtime.py:30]     prompt_attn_impl: fsdpa_impl
[1;36m(VllmWorkerProcess pid=56467)[0;0m INFO 08-09 13:55:34 [runtime.py:30]     skip_warmup: False
[1;36m(VllmWorkerProcess pid=56467)[0;0m INFO 08-09 13:55:34 [runtime.py:30]     merged_prefill: False
[1;36m(VllmWorkerProcess pid=56467)[0;0m INFO 08-09 13:55:34 [runtime.py:30]     use_contiguous_pa: True
[1;36m(VllmWorkerProcess pid=56467)[0;0m INFO 08-09 13:55:34 [runtime.py:30]     use_delayed_sampling: True
[1;36m(VllmWorkerProcess pid=56467)[0;0m INFO 08-09 13:55:34 [runtime.py:30]     use_bucketing: True
[1;36m(VllmWorkerProcess pid=56467)[0;0m INFO 08-09 13:55:34 [runtime.py:30]     bucketing_strategy: exponential_bucketing
[1;36m(VllmWorkerProcess pid=56467)[0;0m INFO 08-09 13:55:34 [runtime.py:26] User flags:
[1;36m(VllmWorkerProcess pid=56467)[0;0m INFO 08-09 13:55:34 [runtime.py:30]     VLLM_USE_V1: False
[1;36m(VllmWorkerProcess pid=56467)[0;0m WARNING 08-09 13:55:34 [hpu.py:135] Pin memory is not supported on HPU.
[1;36m(VllmWorkerProcess pid=56467)[0;0m INFO 08-09 13:55:34 [hpu.py:60] Using HPUAttention backend.
[1;36m(VllmWorkerProcess pid=56470)[0;0m INFO 08-09 13:55:34 [runtime.py:26] Environment:
[1;36m(VllmWorkerProcess pid=56470)[0;0m INFO 08-09 13:55:34 [runtime.py:30]     hw: gaudi3
[1;36m(VllmWorkerProcess pid=56470)[0;0m INFO 08-09 13:55:34 [runtime.py:30]     build: 1.22.0.425
[1;36m(VllmWorkerProcess pid=56470)[0;0m INFO 08-09 13:55:34 [runtime.py:30]     engine_version: v0
[1;36m(VllmWorkerProcess pid=56470)[0;0m INFO 08-09 13:55:34 [runtime.py:30]     bridge_mode: eager
[1;36m(VllmWorkerProcess pid=56470)[0;0m INFO 08-09 13:55:34 [runtime.py:30]     model_type: llama
[1;36m(VllmWorkerProcess pid=56470)[0;0m INFO 08-09 13:55:34 [runtime.py:26] Features:
[1;36m(VllmWorkerProcess pid=56470)[0;0m INFO 08-09 13:55:34 [runtime.py:30]     fp32_alibi_biases: True
[1;36m(VllmWorkerProcess pid=56470)[0;0m INFO 08-09 13:55:34 [runtime.py:30]     fp32_softmax: False
[1;36m(VllmWorkerProcess pid=56470)[0;0m INFO 08-09 13:55:34 [runtime.py:30]     fused_block_softmax_adjustment: False
[1;36m(VllmWorkerProcess pid=56470)[0;0m INFO 08-09 13:55:34 [runtime.py:30]     fused_block_softmax: False
[1;36m(VllmWorkerProcess pid=56470)[0;0m INFO 08-09 13:55:34 [runtime.py:30]     prompt_attn_impl: fsdpa_impl
[1;36m(VllmWorkerProcess pid=56470)[0;0m INFO 08-09 13:55:34 [runtime.py:30]     skip_warmup: False
[1;36m(VllmWorkerProcess pid=56470)[0;0m INFO 08-09 13:55:34 [runtime.py:30]     merged_prefill: False
[1;36m(VllmWorkerProcess pid=56470)[0;0m INFO 08-09 13:55:34 [runtime.py:30]     use_contiguous_pa: True
[1;36m(VllmWorkerProcess pid=56470)[0;0m INFO 08-09 13:55:34 [runtime.py:30]     use_delayed_sampling: True
[1;36m(VllmWorkerProcess pid=56470)[0;0m INFO 08-09 13:55:34 [runtime.py:30]     use_bucketing: True
[1;36m(VllmWorkerProcess pid=56470)[0;0m INFO 08-09 13:55:34 [runtime.py:30]     bucketing_strategy: exponential_bucketing
[1;36m(VllmWorkerProcess pid=56470)[0;0m INFO 08-09 13:55:34 [runtime.py:26] User flags:
[1;36m(VllmWorkerProcess pid=56470)[0;0m INFO 08-09 13:55:34 [runtime.py:30]     VLLM_USE_V1: False
[1;36m(VllmWorkerProcess pid=56470)[0;0m WARNING 08-09 13:55:34 [hpu.py:135] Pin memory is not supported on HPU.
[1;36m(VllmWorkerProcess pid=56470)[0;0m INFO 08-09 13:55:34 [hpu.py:60] Using HPUAttention backend.
[1;36m(VllmWorkerProcess pid=56468)[0;0m INFO 08-09 13:55:34 [runtime.py:26] Environment:
[1;36m(VllmWorkerProcess pid=56468)[0;0m INFO 08-09 13:55:34 [runtime.py:30]     hw: gaudi3
[1;36m(VllmWorkerProcess pid=56468)[0;0m INFO 08-09 13:55:34 [runtime.py:30]     build: 1.22.0.425
[1;36m(VllmWorkerProcess pid=56468)[0;0m INFO 08-09 13:55:34 [runtime.py:30]     engine_version: v0
[1;36m(VllmWorkerProcess pid=56468)[0;0m INFO 08-09 13:55:34 [runtime.py:30]     bridge_mode: eager
[1;36m(VllmWorkerProcess pid=56468)[0;0m INFO 08-09 13:55:34 [runtime.py:30]     model_type: llama
[1;36m(VllmWorkerProcess pid=56468)[0;0m INFO 08-09 13:55:34 [runtime.py:26] Features:
[1;36m(VllmWorkerProcess pid=56468)[0;0m INFO 08-09 13:55:34 [runtime.py:30]     fp32_alibi_biases: True
[1;36m(VllmWorkerProcess pid=56468)[0;0m INFO 08-09 13:55:34 [runtime.py:30]     fp32_softmax: False
[1;36m(VllmWorkerProcess pid=56468)[0;0m INFO 08-09 13:55:34 [runtime.py:30]     fused_block_softmax_adjustment: False
[1;36m(VllmWorkerProcess pid=56468)[0;0m INFO 08-09 13:55:34 [runtime.py:30]     fused_block_softmax: False
[1;36m(VllmWorkerProcess pid=56468)[0;0m INFO 08-09 13:55:34 [runtime.py:30]     prompt_attn_impl: fsdpa_impl
[1;36m(VllmWorkerProcess pid=56468)[0;0m INFO 08-09 13:55:34 [runtime.py:30]     skip_warmup: False
[1;36m(VllmWorkerProcess pid=56468)[0;0m INFO 08-09 13:55:34 [runtime.py:30]     merged_prefill: False
[1;36m(VllmWorkerProcess pid=56468)[0;0m INFO 08-09 13:55:34 [runtime.py:30]     use_contiguous_pa: True
[1;36m(VllmWorkerProcess pid=56468)[0;0m INFO 08-09 13:55:34 [runtime.py:30]     use_delayed_sampling: True
[1;36m(VllmWorkerProcess pid=56468)[0;0m INFO 08-09 13:55:34 [runtime.py:30]     use_bucketing: True
[1;36m(VllmWorkerProcess pid=56468)[0;0m INFO 08-09 13:55:34 [runtime.py:30]     bucketing_strategy: exponential_bucketing
[1;36m(VllmWorkerProcess pid=56468)[0;0m INFO 08-09 13:55:34 [runtime.py:26] User flags:
[1;36m(VllmWorkerProcess pid=56468)[0;0m INFO 08-09 13:55:34 [runtime.py:30]     VLLM_USE_V1: False
[1;36m(VllmWorkerProcess pid=56468)[0;0m WARNING 08-09 13:55:34 [hpu.py:135] Pin memory is not supported on HPU.
[1;36m(VllmWorkerProcess pid=56468)[0;0m INFO 08-09 13:55:34 [hpu.py:60] Using HPUAttention backend.
[1;36m(VllmWorkerProcess pid=56471)[0;0m INFO 08-09 13:55:34 [runtime.py:26] Environment:
[1;36m(VllmWorkerProcess pid=56471)[0;0m INFO 08-09 13:55:34 [runtime.py:30]     hw: gaudi3
[1;36m(VllmWorkerProcess pid=56471)[0;0m INFO 08-09 13:55:34 [runtime.py:30]     build: 1.22.0.425
[1;36m(VllmWorkerProcess pid=56471)[0;0m INFO 08-09 13:55:34 [runtime.py:30]     engine_version: v0
[1;36m(VllmWorkerProcess pid=56471)[0;0m INFO 08-09 13:55:34 [runtime.py:30]     bridge_mode: eager
[1;36m(VllmWorkerProcess pid=56471)[0;0m INFO 08-09 13:55:34 [runtime.py:30]     model_type: llama
[1;36m(VllmWorkerProcess pid=56471)[0;0m INFO 08-09 13:55:34 [runtime.py:26] Features:
[1;36m(VllmWorkerProcess pid=56471)[0;0m INFO 08-09 13:55:34 [runtime.py:30]     fp32_alibi_biases: True
[1;36m(VllmWorkerProcess pid=56471)[0;0m INFO 08-09 13:55:34 [runtime.py:30]     fp32_softmax: False
[1;36m(VllmWorkerProcess pid=56471)[0;0m INFO 08-09 13:55:34 [runtime.py:30]     fused_block_softmax_adjustment: False
[1;36m(VllmWorkerProcess pid=56471)[0;0m INFO 08-09 13:55:34 [runtime.py:30]     fused_block_softmax: False
[1;36m(VllmWorkerProcess pid=56471)[0;0m INFO 08-09 13:55:34 [runtime.py:30]     prompt_attn_impl: fsdpa_impl
[1;36m(VllmWorkerProcess pid=56471)[0;0m INFO 08-09 13:55:34 [runtime.py:30]     skip_warmup: False
[1;36m(VllmWorkerProcess pid=56471)[0;0m INFO 08-09 13:55:34 [runtime.py:30]     merged_prefill: False
[1;36m(VllmWorkerProcess pid=56471)[0;0m INFO 08-09 13:55:34 [runtime.py:30]     use_contiguous_pa: True
[1;36m(VllmWorkerProcess pid=56471)[0;0m INFO 08-09 13:55:34 [runtime.py:30]     use_delayed_sampling: True
[1;36m(VllmWorkerProcess pid=56471)[0;0m INFO 08-09 13:55:34 [runtime.py:30]     use_bucketing: True
[1;36m(VllmWorkerProcess pid=56471)[0;0m INFO 08-09 13:55:34 [runtime.py:30]     bucketing_strategy: exponential_bucketing
[1;36m(VllmWorkerProcess pid=56471)[0;0m INFO 08-09 13:55:34 [runtime.py:26] User flags:
[1;36m(VllmWorkerProcess pid=56471)[0;0m INFO 08-09 13:55:34 [runtime.py:30]     VLLM_USE_V1: False
[1;36m(VllmWorkerProcess pid=56471)[0;0m WARNING 08-09 13:55:34 [hpu.py:135] Pin memory is not supported on HPU.
[1;36m(VllmWorkerProcess pid=56471)[0;0m INFO 08-09 13:55:34 [hpu.py:60] Using HPUAttention backend.
[1;36m(VllmWorkerProcess pid=56469)[0;0m INFO 08-09 13:55:34 [runtime.py:26] Environment:
[1;36m(VllmWorkerProcess pid=56469)[0;0m INFO 08-09 13:55:34 [runtime.py:30]     hw: gaudi3
[1;36m(VllmWorkerProcess pid=56469)[0;0m INFO 08-09 13:55:34 [runtime.py:30]     build: 1.22.0.425
[1;36m(VllmWorkerProcess pid=56469)[0;0m INFO 08-09 13:55:34 [runtime.py:30]     engine_version: v0
[1;36m(VllmWorkerProcess pid=56469)[0;0m INFO 08-09 13:55:34 [runtime.py:30]     bridge_mode: eager
[1;36m(VllmWorkerProcess pid=56469)[0;0m INFO 08-09 13:55:34 [runtime.py:30]     model_type: llama
[1;36m(VllmWorkerProcess pid=56469)[0;0m INFO 08-09 13:55:34 [runtime.py:26] Features:
[1;36m(VllmWorkerProcess pid=56469)[0;0m INFO 08-09 13:55:34 [runtime.py:30]     fp32_alibi_biases: True
[1;36m(VllmWorkerProcess pid=56469)[0;0m INFO 08-09 13:55:34 [runtime.py:30]     fp32_softmax: False
[1;36m(VllmWorkerProcess pid=56469)[0;0m INFO 08-09 13:55:34 [runtime.py:30]     fused_block_softmax_adjustment: False
[1;36m(VllmWorkerProcess pid=56469)[0;0m INFO 08-09 13:55:34 [runtime.py:30]     fused_block_softmax: False
[1;36m(VllmWorkerProcess pid=56469)[0;0m INFO 08-09 13:55:34 [runtime.py:30]     prompt_attn_impl: fsdpa_impl
[1;36m(VllmWorkerProcess pid=56469)[0;0m INFO 08-09 13:55:34 [runtime.py:30]     skip_warmup: False
[1;36m(VllmWorkerProcess pid=56469)[0;0m INFO 08-09 13:55:34 [runtime.py:30]     merged_prefill: False
[1;36m(VllmWorkerProcess pid=56469)[0;0m INFO 08-09 13:55:34 [runtime.py:30]     use_contiguous_pa: True
[1;36m(VllmWorkerProcess pid=56469)[0;0m INFO 08-09 13:55:34 [runtime.py:30]     use_delayed_sampling: True
[1;36m(VllmWorkerProcess pid=56469)[0;0m INFO 08-09 13:55:34 [runtime.py:30]     use_bucketing: True
[1;36m(VllmWorkerProcess pid=56469)[0;0m INFO 08-09 13:55:34 [runtime.py:30]     bucketing_strategy: exponential_bucketing
[1;36m(VllmWorkerProcess pid=56469)[0;0m INFO 08-09 13:55:34 [runtime.py:26] User flags:
[1;36m(VllmWorkerProcess pid=56469)[0;0m INFO 08-09 13:55:34 [runtime.py:30]     VLLM_USE_V1: False
[1;36m(VllmWorkerProcess pid=56469)[0;0m WARNING 08-09 13:55:34 [hpu.py:135] Pin memory is not supported on HPU.
[1;36m(VllmWorkerProcess pid=56469)[0;0m INFO 08-09 13:55:34 [hpu.py:60] Using HPUAttention backend.
[1;36m(VllmWorkerProcess pid=56472)[0;0m INFO 08-09 13:55:34 [runtime.py:26] Environment:
[1;36m(VllmWorkerProcess pid=56472)[0;0m INFO 08-09 13:55:34 [runtime.py:30]     hw: gaudi3
[1;36m(VllmWorkerProcess pid=56472)[0;0m INFO 08-09 13:55:34 [runtime.py:30]     build: 1.22.0.425
[1;36m(VllmWorkerProcess pid=56472)[0;0m INFO 08-09 13:55:34 [runtime.py:30]     engine_version: v0
[1;36m(VllmWorkerProcess pid=56472)[0;0m INFO 08-09 13:55:34 [runtime.py:30]     bridge_mode: eager
[1;36m(VllmWorkerProcess pid=56472)[0;0m INFO 08-09 13:55:34 [runtime.py:30]     model_type: llama
[1;36m(VllmWorkerProcess pid=56472)[0;0m INFO 08-09 13:55:34 [runtime.py:26] Features:
[1;36m(VllmWorkerProcess pid=56472)[0;0m INFO 08-09 13:55:34 [runtime.py:30]     fp32_alibi_biases: True
[1;36m(VllmWorkerProcess pid=56472)[0;0m INFO 08-09 13:55:34 [runtime.py:30]     fp32_softmax: False
[1;36m(VllmWorkerProcess pid=56472)[0;0m INFO 08-09 13:55:34 [runtime.py:30]     fused_block_softmax_adjustment: False
[1;36m(VllmWorkerProcess pid=56472)[0;0m INFO 08-09 13:55:34 [runtime.py:30]     fused_block_softmax: False
[1;36m(VllmWorkerProcess pid=56472)[0;0m INFO 08-09 13:55:34 [runtime.py:30]     prompt_attn_impl: fsdpa_impl
[1;36m(VllmWorkerProcess pid=56472)[0;0m INFO 08-09 13:55:34 [runtime.py:30]     skip_warmup: False
[1;36m(VllmWorkerProcess pid=56472)[0;0m INFO 08-09 13:55:34 [runtime.py:30]     merged_prefill: False
[1;36m(VllmWorkerProcess pid=56472)[0;0m INFO 08-09 13:55:34 [runtime.py:30]     use_contiguous_pa: True
[1;36m(VllmWorkerProcess pid=56472)[0;0m INFO 08-09 13:55:34 [runtime.py:30]     use_delayed_sampling: True
[1;36m(VllmWorkerProcess pid=56472)[0;0m INFO 08-09 13:55:34 [runtime.py:30]     use_bucketing: True
[1;36m(VllmWorkerProcess pid=56472)[0;0m INFO 08-09 13:55:34 [runtime.py:30]     bucketing_strategy: exponential_bucketing
[1;36m(VllmWorkerProcess pid=56472)[0;0m INFO 08-09 13:55:34 [runtime.py:26] User flags:
[1;36m(VllmWorkerProcess pid=56472)[0;0m INFO 08-09 13:55:34 [runtime.py:30]     VLLM_USE_V1: False
[1;36m(VllmWorkerProcess pid=56472)[0;0m WARNING 08-09 13:55:34 [hpu.py:135] Pin memory is not supported on HPU.
[1;36m(VllmWorkerProcess pid=56472)[0;0m INFO 08-09 13:55:34 [hpu.py:60] Using HPUAttention backend.
============================= HPU SW VERSION ====================================== 
 HB_BUILD_VER = 1.22.0-425
============================= HPU PT BRIDGE CONFIGURATION ON RANK = 0 ============= 
 PT_HPU_LAZY_MODE = 0
 PT_HPU_RECIPE_CACHE_CONFIG = ,false,1024,false
 PT_HPU_MAX_COMPOUND_OP_SIZE = 9223372036854775807
 PT_HPU_LAZY_ACC_PAR_MODE = 1
 PT_HPU_ENABLE_REFINE_DYNAMIC_SHAPES = 0
 PT_HPU_EAGER_PIPELINE_ENABLE = 1
 PT_HPU_EAGER_COLLECTIVE_PIPELINE_ENABLE = 1
 PT_HPU_ENABLE_LAZY_COLLECTIVES = 1
---------------------------: System Configuration :---------------------------
Num CPU Cores : 224
CPU RAM       : 1007 GB
------------------------------------------------------------------------------
============================= HPU SW VERSION ====================================== 
 HB_BUILD_VER = 1.22.0-425
============================= HPU PT BRIDGE CONFIGURATION ON RANK = 0 ============= 
 PT_HPU_LAZY_MODE = 0
 PT_HPU_RECIPE_CACHE_CONFIG = ,false,1024,false
 PT_HPU_MAX_COMPOUND_OP_SIZE = 9223372036854775807
 PT_HPU_LAZY_ACC_PAR_MODE = 1
 PT_HPU_ENABLE_REFINE_DYNAMIC_SHAPES = 0
 PT_HPU_EAGER_PIPELINE_ENABLE = 1
 PT_HPU_EAGER_COLLECTIVE_PIPELINE_ENABLE = 1
 PT_HPU_ENABLE_LAZY_COLLECTIVES = 1
---------------------------: System Configuration :---------------------------
Num CPU Cores : 224
CPU RAM       : 1007 GB
------------------------------------------------------------------------------
============================= HPU SW VERSION ====================================== 
 HB_BUILD_VER = 1.22.0-425
============================= HPU PT BRIDGE CONFIGURATION ON RANK = 0 ============= 
 PT_HPU_LAZY_MODE = 0
 PT_HPU_RECIPE_CACHE_CONFIG = ,false,1024,false
 PT_HPU_MAX_COMPOUND_OP_SIZE = 9223372036854775807
 PT_HPU_LAZY_ACC_PAR_MODE = 1
 PT_HPU_ENABLE_REFINE_DYNAMIC_SHAPES = 0
 PT_HPU_EAGER_PIPELINE_ENABLE = 1
 PT_HPU_EAGER_COLLECTIVE_PIPELINE_ENABLE = 1
 PT_HPU_ENABLE_LAZY_COLLECTIVES = 1
---------------------------: System Configuration :---------------------------
Num CPU Cores : 224
CPU RAM       : 1007 GB
------------------------------------------------------------------------------
============================= HPU SW VERSION ====================================== 
 HB_BUILD_VER = 1.22.0-425
============================= HPU PT BRIDGE CONFIGURATION ON RANK = 0 ============= 
 PT_HPU_LAZY_MODE = 0
 PT_HPU_RECIPE_CACHE_CONFIG = ,false,1024,false
 PT_HPU_MAX_COMPOUND_OP_SIZE = 9223372036854775807
 PT_HPU_LAZY_ACC_PAR_MODE = 1
 PT_HPU_ENABLE_REFINE_DYNAMIC_SHAPES = 0
 PT_HPU_EAGER_PIPELINE_ENABLE = 1
 PT_HPU_EAGER_COLLECTIVE_PIPELINE_ENABLE = 1
 PT_HPU_ENABLE_LAZY_COLLECTIVES = 1
---------------------------: System Configuration :---------------------------
Num CPU Cores : 224
CPU RAM       : 1007 GB
------------------------------------------------------------------------------
============================= HPU SW VERSION ====================================== 
 HB_BUILD_VER = 1.22.0-425
============================= HPU PT BRIDGE CONFIGURATION ON RANK = 0 ============= 
 PT_HPU_LAZY_MODE = 0
 PT_HPU_RECIPE_CACHE_CONFIG = ,false,1024,false
 PT_HPU_MAX_COMPOUND_OP_SIZE = 9223372036854775807
 PT_HPU_LAZY_ACC_PAR_MODE = 1
 PT_HPU_ENABLE_REFINE_DYNAMIC_SHAPES = 0
 PT_HPU_EAGER_PIPELINE_ENABLE = 1
 PT_HPU_EAGER_COLLECTIVE_PIPELINE_ENABLE = 1
 PT_HPU_ENABLE_LAZY_COLLECTIVES = 1
---------------------------: System Configuration :---------------------------
Num CPU Cores : 224
CPU RAM       : 1007 GB
------------------------------------------------------------------------------
============================= HPU SW VERSION ====================================== 
 HB_BUILD_VER = 1.22.0-425
============================= HPU PT BRIDGE CONFIGURATION ON RANK = 0 ============= 
 PT_HPU_LAZY_MODE = 0
 PT_HPU_RECIPE_CACHE_CONFIG = ,false,1024,false
 PT_HPU_MAX_COMPOUND_OP_SIZE = 9223372036854775807
 PT_HPU_LAZY_ACC_PAR_MODE = 1
 PT_HPU_ENABLE_REFINE_DYNAMIC_SHAPES = 0
 PT_HPU_EAGER_PIPELINE_ENABLE = 1
 PT_HPU_EAGER_COLLECTIVE_PIPELINE_ENABLE = 1
 PT_HPU_ENABLE_LAZY_COLLECTIVES = 1
---------------------------: System Configuration :---------------------------
Num CPU Cores : 224
CPU RAM       : 1007 GB
------------------------------------------------------------------------------
============================= HPU SW VERSION ====================================== 
 HB_BUILD_VER = 1.22.0-425
============================= HPU PT BRIDGE CONFIGURATION ON RANK = 0 ============= 
 PT_HPU_LAZY_MODE = 0
 PT_HPU_RECIPE_CACHE_CONFIG = ,false,1024,false
 PT_HPU_MAX_COMPOUND_OP_SIZE = 9223372036854775807
 PT_HPU_LAZY_ACC_PAR_MODE = 1
 PT_HPU_ENABLE_REFINE_DYNAMIC_SHAPES = 0
 PT_HPU_EAGER_PIPELINE_ENABLE = 1
 PT_HPU_EAGER_COLLECTIVE_PIPELINE_ENABLE = 1
 PT_HPU_ENABLE_LAZY_COLLECTIVES = 1
---------------------------: System Configuration :---------------------------
Num CPU Cores : 224
CPU RAM       : 1007 GB
------------------------------------------------------------------------------
============================= HPU SW VERSION ====================================== 
 HB_BUILD_VER = 1.22.0-425
============================= HPU PT BRIDGE CONFIGURATION ON RANK = 0 ============= 
 PT_HPU_LAZY_MODE = 0
 PT_HPU_RECIPE_CACHE_CONFIG = ,false,1024,false
 PT_HPU_MAX_COMPOUND_OP_SIZE = 9223372036854775807
 PT_HPU_LAZY_ACC_PAR_MODE = 1
 PT_HPU_ENABLE_REFINE_DYNAMIC_SHAPES = 0
 PT_HPU_EAGER_PIPELINE_ENABLE = 1
 PT_HPU_EAGER_COLLECTIVE_PIPELINE_ENABLE = 1
 PT_HPU_ENABLE_LAZY_COLLECTIVES = 1
---------------------------: System Configuration :---------------------------
Num CPU Cores : 224
CPU RAM       : 1007 GB
------------------------------------------------------------------------------
INFO 08-09 13:55:36 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[1, 2, 3, 4, 5, 6, 7], buffer_handle=(7, 4194304, 6, 'psm_9763cf38'), local_subscribe_addr='ipc:///tmp/fe4e56b7-7f75-4cfa-90d8-d9a605fd756e', remote_subscribe_addr=None, remote_addr_ipv6=False)
INFO 08-09 13:55:36 [parallel_state.py:1065] rank 0 in world size 8 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(VllmWorkerProcess pid=56468)[0;0m INFO 08-09 13:55:36 [parallel_state.py:1065] rank 2 in world size 8 is assigned as DP rank 0, PP rank 0, TP rank 2, EP rank 2
[1;36m(VllmWorkerProcess pid=56469)[0;0m INFO 08-09 13:55:36 [parallel_state.py:1065] rank 3 in world size 8 is assigned as DP rank 0, PP rank 0, TP rank 3, EP rank 3
[1;36m(VllmWorkerProcess pid=56467)[0;0m INFO 08-09 13:55:36 [parallel_state.py:1065] rank 1 in world size 8 is assigned as DP rank 0, PP rank 0, TP rank 1, EP rank 1
[1;36m(VllmWorkerProcess pid=56471)[0;0m INFO 08-09 13:55:36 [parallel_state.py:1065] rank 5 in world size 8 is assigned as DP rank 0, PP rank 0, TP rank 5, EP rank 5
[1;36m(VllmWorkerProcess pid=56470)[0;0m INFO 08-09 13:55:36 [parallel_state.py:1065] rank 4 in world size 8 is assigned as DP rank 0, PP rank 0, TP rank 4, EP rank 4
[1;36m(VllmWorkerProcess pid=56473)[0;0m INFO 08-09 13:55:36 [parallel_state.py:1065] rank 7 in world size 8 is assigned as DP rank 0, PP rank 0, TP rank 7, EP rank 7
[1;36m(VllmWorkerProcess pid=56472)[0;0m INFO 08-09 13:55:36 [parallel_state.py:1065] rank 6 in world size 8 is assigned as DP rank 0, PP rank 0, TP rank 6, EP rank 6
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  5.68it/s]
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:00<00:00,  2.77it/s]
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:00<00:00,  2.99it/s]

INFO 08-09 13:55:38 [default_loader.py:272] Loading weights took 0.76 seconds
[1;36m(VllmWorkerProcess pid=56473)[0;0m INFO 08-09 13:55:38 [default_loader.py:272] Loading weights took 0.72 seconds
[1;36m(VllmWorkerProcess pid=56471)[0;0m INFO 08-09 13:55:38 [default_loader.py:272] Loading weights took 0.82 seconds
[1;36m(VllmWorkerProcess pid=56468)[0;0m INFO 08-09 13:55:38 [default_loader.py:272] Loading weights took 0.82 seconds
[1;36m(VllmWorkerProcess pid=56472)[0;0m INFO 08-09 13:55:38 [default_loader.py:272] Loading weights took 0.81 seconds
[1;36m(VllmWorkerProcess pid=56469)[0;0m INFO 08-09 13:55:38 [default_loader.py:272] Loading weights took 0.78 seconds
[1;36m(VllmWorkerProcess pid=56467)[0;0m INFO 08-09 13:55:38 [default_loader.py:272] Loading weights took 0.88 seconds
[1;36m(VllmWorkerProcess pid=56473)[0;0m INFO 08-09 13:55:38 [hpu_model_runner.py:1184] Pre-loading model weights on hpu:0 took 1.57 GiB of device memory (1.577 GiB/126.5 GiB used) and 188.7 MiB of host memory (150.5 GiB/1007 GiB used)
INFO 08-09 13:55:38 [hpu_model_runner.py:1184] Pre-loading model weights on hpu:0 took 1.57 GiB of device memory (1.577 GiB/126.5 GiB used) and 210.3 MiB of host memory (150.5 GiB/1007 GiB used)
[1;36m(VllmWorkerProcess pid=56470)[0;0m INFO 08-09 13:55:38 [default_loader.py:272] Loading weights took 0.76 seconds
[1;36m(VllmWorkerProcess pid=56471)[0;0m INFO 08-09 13:55:38 [hpu_model_runner.py:1184] Pre-loading model weights on hpu:0 took 1.57 GiB of device memory (1.577 GiB/126.5 GiB used) and 179.3 MiB of host memory (150.5 GiB/1007 GiB used)
[1;36m(VllmWorkerProcess pid=56468)[0;0m INFO 08-09 13:55:38 [hpu_model_runner.py:1184] Pre-loading model weights on hpu:0 took 1.57 GiB of device memory (1.577 GiB/126.5 GiB used) and 179.3 MiB of host memory (150.5 GiB/1007 GiB used)
[1;36m(VllmWorkerProcess pid=56472)[0;0m INFO 08-09 13:55:38 [hpu_model_runner.py:1184] Pre-loading model weights on hpu:0 took 1.57 GiB of device memory (1.577 GiB/126.5 GiB used) and 162.6 MiB of host memory (150.5 GiB/1007 GiB used)
[1;36m(VllmWorkerProcess pid=56467)[0;0m INFO 08-09 13:55:38 [hpu_model_runner.py:1184] Pre-loading model weights on hpu:0 took 1.57 GiB of device memory (1.577 GiB/126.5 GiB used) and 164.6 MiB of host memory (150.5 GiB/1007 GiB used)
[1;36m(VllmWorkerProcess pid=56469)[0;0m INFO 08-09 13:55:38 [hpu_model_runner.py:1184] Pre-loading model weights on hpu:0 took 1.57 GiB of device memory (1.577 GiB/126.5 GiB used) and 106.9 MiB of host memory (150.5 GiB/1007 GiB used)
[1;36m(VllmWorkerProcess pid=56473)[0;0m INFO 08-09 13:55:39 [hpu_model_runner.py:1266] Wrapping in HPU Graph took 0 B of device memory (1.577 GiB/126.5 GiB used) and -620 KiB of host memory (150.5 GiB/1007 GiB used)
[1;36m(VllmWorkerProcess pid=56470)[0;0m INFO 08-09 13:55:39 [hpu_model_runner.py:1184] Pre-loading model weights on hpu:0 took 1.57 GiB of device memory (1.577 GiB/126.5 GiB used) and 37.44 MiB of host memory (150.5 GiB/1007 GiB used)
[1;36m(VllmWorkerProcess pid=56467)[0;0m INFO 08-09 13:55:39 [hpu_model_runner.py:1266] Wrapping in HPU Graph took 0 B of device memory (1.577 GiB/126.5 GiB used) and -116 KiB of host memory (150.5 GiB/1007 GiB used)
[1;36m(VllmWorkerProcess pid=56471)[0;0m INFO 08-09 13:55:39 [hpu_model_runner.py:1266] Wrapping in HPU Graph took 0 B of device memory (1.577 GiB/126.5 GiB used) and -116 KiB of host memory (150.5 GiB/1007 GiB used)
[1;36m(VllmWorkerProcess pid=56468)[0;0m INFO 08-09 13:55:39 [hpu_model_runner.py:1266] Wrapping in HPU Graph took 0 B of device memory (1.577 GiB/126.5 GiB used) and -116 KiB of host memory (150.5 GiB/1007 GiB used)
[1;36m(VllmWorkerProcess pid=56472)[0;0m INFO 08-09 13:55:39 [hpu_model_runner.py:1266] Wrapping in HPU Graph took 0 B of device memory (1.577 GiB/126.5 GiB used) and -168 KiB of host memory (150.5 GiB/1007 GiB used)
[1;36m(VllmWorkerProcess pid=56469)[0;0m INFO 08-09 13:55:39 [hpu_model_runner.py:1266] Wrapping in HPU Graph took 0 B of device memory (1.577 GiB/126.5 GiB used) and 4.746 MiB of host memory (150.5 GiB/1007 GiB used)
INFO 08-09 13:55:39 [hpu_model_runner.py:1266] Wrapping in HPU Graph took 0 B of device memory (1.577 GiB/126.5 GiB used) and 4.906 MiB of host memory (150.5 GiB/1007 GiB used)
[1;36m(VllmWorkerProcess pid=56473)[0;0m INFO 08-09 13:55:39 [hpu_model_runner.py:1270] Compiling took 0 B of device memory (1.577 GiB/126.5 GiB used) and 4.773 MiB of host memory (150.5 GiB/1007 GiB used)
[1;36m(VllmWorkerProcess pid=56473)[0;0m INFO 08-09 13:55:39 [hpu_model_runner.py:1274] Loading model weights took in total 1.57 GiB of device memory (1.577 GiB/126.5 GiB used) and 211.5 MiB of host memory (150.5 GiB/1007 GiB used)
[1;36m(VllmWorkerProcess pid=56467)[0;0m INFO 08-09 13:55:39 [hpu_model_runner.py:1270] Compiling took 0 B of device memory (1.577 GiB/126.5 GiB used) and -72 KiB of host memory (150.5 GiB/1007 GiB used)
[1;36m(VllmWorkerProcess pid=56471)[0;0m INFO 08-09 13:55:39 [hpu_model_runner.py:1270] Compiling took 0 B of device memory (1.577 GiB/126.5 GiB used) and -32 KiB of host memory (150.5 GiB/1007 GiB used)
[1;36m(VllmWorkerProcess pid=56468)[0;0m INFO 08-09 13:55:39 [hpu_model_runner.py:1270] Compiling took 0 B of device memory (1.577 GiB/126.5 GiB used) and -32 KiB of host memory (150.5 GiB/1007 GiB used)
[1;36m(VllmWorkerProcess pid=56470)[0;0m INFO 08-09 13:55:39 [hpu_model_runner.py:1266] Wrapping in HPU Graph took 0 B of device memory (1.577 GiB/126.5 GiB used) and -32 KiB of host memory (150.5 GiB/1007 GiB used)
[1;36m(VllmWorkerProcess pid=56472)[0;0m INFO 08-09 13:55:39 [hpu_model_runner.py:1270] Compiling took 0 B of device memory (1.577 GiB/126.5 GiB used) and -32 KiB of host memory (150.5 GiB/1007 GiB used)
[1;36m(VllmWorkerProcess pid=56467)[0;0m INFO 08-09 13:55:39 [hpu_model_runner.py:1274] Loading model weights took in total 1.57 GiB of device memory (1.577 GiB/126.5 GiB used) and 211.3 MiB of host memory (150.5 GiB/1007 GiB used)
[1;36m(VllmWorkerProcess pid=56471)[0;0m INFO 08-09 13:55:39 [hpu_model_runner.py:1274] Loading model weights took in total 1.57 GiB of device memory (1.577 GiB/126.5 GiB used) and 211.3 MiB of host memory (150.5 GiB/1007 GiB used)
[1;36m(VllmWorkerProcess pid=56469)[0;0m INFO 08-09 13:55:39 [hpu_model_runner.py:1270] Compiling took 0 B of device memory (1.577 GiB/126.5 GiB used) and 0 B of host memory (150.5 GiB/1007 GiB used)
[1;36m(VllmWorkerProcess pid=56468)[0;0m INFO 08-09 13:55:39 [hpu_model_runner.py:1274] Loading model weights took in total 1.57 GiB of device memory (1.577 GiB/126.5 GiB used) and 211 MiB of host memory (150.5 GiB/1007 GiB used)
[1;36m(VllmWorkerProcess pid=56472)[0;0m INFO 08-09 13:55:40 [hpu_model_runner.py:1274] Loading model weights took in total 1.57 GiB of device memory (1.577 GiB/126.5 GiB used) and 211 MiB of host memory (150.5 GiB/1007 GiB used)
INFO 08-09 13:55:40 [hpu_model_runner.py:1270] Compiling took 0 B of device memory (1.577 GiB/126.5 GiB used) and -776 KiB of host memory (150.5 GiB/1007 GiB used)
[1;36m(VllmWorkerProcess pid=56469)[0;0m INFO 08-09 13:55:40 [hpu_model_runner.py:1274] Loading model weights took in total 1.57 GiB of device memory (1.577 GiB/126.5 GiB used) and 210.5 MiB of host memory (150.5 GiB/1007 GiB used)
[1;36m(VllmWorkerProcess pid=56470)[0;0m INFO 08-09 13:55:40 [hpu_model_runner.py:1270] Compiling took 0 B of device memory (1.577 GiB/126.5 GiB used) and -1.312 MiB of host memory (150.5 GiB/1007 GiB used)
INFO 08-09 13:55:40 [hpu_model_runner.py:1274] Loading model weights took in total 1.57 GiB of device memory (1.577 GiB/126.5 GiB used) and 209.3 MiB of host memory (150.5 GiB/1007 GiB used)
[1;36m(VllmWorkerProcess pid=56470)[0;0m INFO 08-09 13:55:40 [hpu_model_runner.py:1274] Loading model weights took in total 1.57 GiB of device memory (1.577 GiB/126.5 GiB used) and 201.1 MiB of host memory (150.5 GiB/1007 GiB used)
WARNING 08-09 13:55:40 [common.py:139] Prompt bucket for (1, 4096, 0) was not prepared. Adding new bucket: (1, 4096, 0)
[1;36m(VllmWorkerProcess pid=56468)[0;0m WARNING 08-09 13:55:41 [common.py:139] Prompt bucket for (1, 4096, 0) was not prepared. Adding new bucket: (1, 4096, 0)
[1;36m(VllmWorkerProcess pid=56467)[0;0m WARNING 08-09 13:55:41 [common.py:139] Prompt bucket for (1, 4096, 0) was not prepared. Adding new bucket: (1, 4096, 0)
[1;36m(VllmWorkerProcess pid=56473)[0;0m WARNING 08-09 13:55:41 [common.py:139] Prompt bucket for (1, 4096, 0) was not prepared. Adding new bucket: (1, 4096, 0)
[1;36m(VllmWorkerProcess pid=56471)[0;0m WARNING 08-09 13:55:41 [common.py:139] Prompt bucket for (1, 4096, 0) was not prepared. Adding new bucket: (1, 4096, 0)
[1;36m(VllmWorkerProcess pid=56469)[0;0m WARNING 08-09 13:55:41 [common.py:139] Prompt bucket for (1, 4096, 0) was not prepared. Adding new bucket: (1, 4096, 0)
[1;36m(VllmWorkerProcess pid=56472)[0;0m WARNING 08-09 13:55:41 [common.py:139] Prompt bucket for (1, 4096, 0) was not prepared. Adding new bucket: (1, 4096, 0)
[1;36m(VllmWorkerProcess pid=56470)[0;0m WARNING 08-09 13:55:41 [common.py:139] Prompt bucket for (1, 4096, 0) was not prepared. Adding new bucket: (1, 4096, 0)
[1;36m(VllmWorkerProcess pid=56467)[0;0m INFO 08-09 13:55:43 [hpu_worker.py:289] Model profiling run took 98 MiB of device memory (1.673 GiB/126.5 GiB used) and 540.9 MiB of host memory (151 GiB/1007 GiB used)
[1;36m(VllmWorkerProcess pid=56467)[0;0m INFO 08-09 13:55:43 [hpu_worker.py:313] Free device memory: 124.9 GiB, 112.4 GiB usable (gpu_memory_utilization=0.9), 11.24 GiB reserved for HPUGraphs (VLLM_GRAPH_RESERVED_MEM=0.1), 101.1 GiB reserved for KV cache
[1;36m(VllmWorkerProcess pid=56471)[0;0m INFO 08-09 13:55:43 [hpu_worker.py:289] Model profiling run took 98 MiB of device memory (1.673 GiB/126.5 GiB used) and 520.5 MiB of host memory (151 GiB/1007 GiB used)
[1;36m(VllmWorkerProcess pid=56471)[0;0m INFO 08-09 13:55:43 [hpu_worker.py:313] Free device memory: 124.9 GiB, 112.4 GiB usable (gpu_memory_utilization=0.9), 11.24 GiB reserved for HPUGraphs (VLLM_GRAPH_RESERVED_MEM=0.1), 101.1 GiB reserved for KV cache
[1;36m(VllmWorkerProcess pid=56473)[0;0m INFO 08-09 13:55:43 [hpu_worker.py:289] Model profiling run took 98 MiB of device memory (1.673 GiB/126.5 GiB used) and 536.3 MiB of host memory (151 GiB/1007 GiB used)
[1;36m(VllmWorkerProcess pid=56473)[0;0m INFO 08-09 13:55:43 [hpu_worker.py:313] Free device memory: 124.9 GiB, 112.4 GiB usable (gpu_memory_utilization=0.9), 11.24 GiB reserved for HPUGraphs (VLLM_GRAPH_RESERVED_MEM=0.1), 101.1 GiB reserved for KV cache
[1;36m(VllmWorkerProcess pid=56468)[0;0m INFO 08-09 13:55:43 [hpu_worker.py:289] Model profiling run took 98 MiB of device memory (1.673 GiB/126.5 GiB used) and 558.1 MiB of host memory (151 GiB/1007 GiB used)
[1;36m(VllmWorkerProcess pid=56468)[0;0m INFO 08-09 13:55:43 [hpu_worker.py:313] Free device memory: 124.9 GiB, 112.4 GiB usable (gpu_memory_utilization=0.9), 11.24 GiB reserved for HPUGraphs (VLLM_GRAPH_RESERVED_MEM=0.1), 101.1 GiB reserved for KV cache
[1;36m(VllmWorkerProcess pid=56469)[0;0m INFO 08-09 13:55:43 [hpu_worker.py:289] Model profiling run took 98 MiB of device memory (1.673 GiB/126.5 GiB used) and 495.2 MiB of host memory (151 GiB/1007 GiB used)
[1;36m(VllmWorkerProcess pid=56469)[0;0m INFO 08-09 13:55:43 [hpu_worker.py:313] Free device memory: 124.9 GiB, 112.4 GiB usable (gpu_memory_utilization=0.9), 11.24 GiB reserved for HPUGraphs (VLLM_GRAPH_RESERVED_MEM=0.1), 101.1 GiB reserved for KV cache
[1;36m(VllmWorkerProcess pid=56472)[0;0m INFO 08-09 13:55:43 [hpu_worker.py:289] Model profiling run took 98 MiB of device memory (1.673 GiB/126.5 GiB used) and 476.5 MiB of host memory (151 GiB/1007 GiB used)
[1;36m(VllmWorkerProcess pid=56472)[0;0m INFO 08-09 13:55:43 [hpu_worker.py:313] Free device memory: 124.9 GiB, 112.4 GiB usable (gpu_memory_utilization=0.9), 11.24 GiB reserved for HPUGraphs (VLLM_GRAPH_RESERVED_MEM=0.1), 101.1 GiB reserved for KV cache
[1;36m(VllmWorkerProcess pid=56470)[0;0m INFO 08-09 13:55:43 [hpu_worker.py:289] Model profiling run took 98 MiB of device memory (1.673 GiB/126.5 GiB used) and 460.5 MiB of host memory (151 GiB/1007 GiB used)
[1;36m(VllmWorkerProcess pid=56470)[0;0m INFO 08-09 13:55:43 [hpu_worker.py:313] Free device memory: 124.9 GiB, 112.4 GiB usable (gpu_memory_utilization=0.9), 11.24 GiB reserved for HPUGraphs (VLLM_GRAPH_RESERVED_MEM=0.1), 101.1 GiB reserved for KV cache
INFO 08-09 13:55:43 [hpu_worker.py:289] Model profiling run took 98 MiB of device memory (1.673 GiB/126.5 GiB used) and 584.5 MiB of host memory (151 GiB/1007 GiB used)
INFO 08-09 13:55:43 [hpu_worker.py:313] Free device memory: 124.9 GiB, 112.4 GiB usable (gpu_memory_utilization=0.9), 11.24 GiB reserved for HPUGraphs (VLLM_GRAPH_RESERVED_MEM=0.1), 101.1 GiB reserved for KV cache
INFO 08-09 13:55:43 [executor_base.py:113] # hpu blocks: 12946, # CPU blocks: 512
INFO 08-09 13:55:43 [executor_base.py:118] Maximum concurrency for 4096 tokens per request: 404.56x
[1;36m(VllmWorkerProcess pid=56468)[0;0m INFO 08-09 13:55:43 [exponential.py:49] Prompt bucket config (min, step, max_warmup, limit) bs:[1, 2, 256, 9], seq:[128, 128, 4096, 13]
[1;36m(VllmWorkerProcess pid=56470)[0;0m INFO 08-09 13:55:43 [exponential.py:49] Prompt bucket config (min, step, max_warmup, limit) bs:[1, 2, 256, 9], seq:[128, 128, 4096, 13]
[1;36m(VllmWorkerProcess pid=56468)[0;0m INFO 08-09 13:55:43 [common.py:117] Generated 38 prompt buckets [bs, query, num_blocks]: [(1, 128, 0), (1, 256, 0), (1, 384, 0), (1, 512, 0), (1, 640, 0), (1, 768, 0), (1, 896, 0), (1, 1024, 0), (1, 1408, 0), (1, 1792, 0), (1, 2304, 0), (1, 3072, 0), (1, 4096, 0), (2, 128, 0), (2, 256, 0), (2, 384, 0), (2, 512, 0), (2, 640, 0), (2, 768, 0), (2, 896, 0), (2, 1024, 0), (2, 1408, 0), (2, 1792, 0), (4, 128, 0), (4, 256, 0), (4, 384, 0), (4, 512, 0), (4, 640, 0), (4, 768, 0), (4, 896, 0), (4, 1024, 0), (8, 128, 0), (8, 256, 0), (8, 384, 0), (8, 512, 0), (16, 128, 0), (16, 256, 0), (32, 128, 0)]
[1;36m(VllmWorkerProcess pid=56467)[0;0m INFO 08-09 13:55:43 [exponential.py:49] Prompt bucket config (min, step, max_warmup, limit) bs:[1, 2, 256, 9], seq:[128, 128, 4096, 13]
[1;36m(VllmWorkerProcess pid=56468)[0;0m INFO 08-09 13:55:43 [exponential.py:79] Decode bucket config (min, step, max_warmup, limit) bs:[1, 2, 256, 9], block:[128, 128, 12946, 15]
[1;36m(VllmWorkerProcess pid=56469)[0;0m INFO 08-09 13:55:43 [exponential.py:49] Prompt bucket config (min, step, max_warmup, limit) bs:[1, 2, 256, 9], seq:[128, 128, 4096, 13]
[1;36m(VllmWorkerProcess pid=56470)[0;0m INFO 08-09 13:55:43 [common.py:117] Generated 38 prompt buckets [bs, query, num_blocks]: [(1, 128, 0), (1, 256, 0), (1, 384, 0), (1, 512, 0), (1, 640, 0), (1, 768, 0), (1, 896, 0), (1, 1024, 0), (1, 1408, 0), (1, 1792, 0), (1, 2304, 0), (1, 3072, 0), (1, 4096, 0), (2, 128, 0), (2, 256, 0), (2, 384, 0), (2, 512, 0), (2, 640, 0), (2, 768, 0), (2, 896, 0), (2, 1024, 0), (2, 1408, 0), (2, 1792, 0), (4, 128, 0), (4, 256, 0), (4, 384, 0), (4, 512, 0), (4, 640, 0), (4, 768, 0), (4, 896, 0), (4, 1024, 0), (8, 128, 0), (8, 256, 0), (8, 384, 0), (8, 512, 0), (16, 128, 0), (16, 256, 0), (32, 128, 0)]
[1;36m(VllmWorkerProcess pid=56470)[0;0m INFO 08-09 13:55:43 [exponential.py:79] Decode bucket config (min, step, max_warmup, limit) bs:[1, 2, 256, 9], block:[128, 128, 12946, 15]
[1;36m(VllmWorkerProcess pid=56468)[0;0m INFO 08-09 13:55:43 [common.py:117] Generated 135 decode buckets [bs, query, num_blocks]: [(1, 1, 128), (1, 1, 256), (1, 1, 384), (1, 1, 512), (1, 1, 640), (1, 1, 768), (1, 1, 1024), (1, 1, 1408), (1, 1, 1792), (1, 1, 2560), (1, 1, 3584), (1, 1, 4864), (1, 1, 6784), (1, 1, 9344), (1, 1, 12946), (2, 1, 128), (2, 1, 256), (2, 1, 384), (2, 1, 512), (2, 1, 640), (2, 1, 768), (2, 1, 1024), (2, 1, 1408), (2, 1, 1792), (2, 1, 2560), (2, 1, 3584), (2, 1, 4864), (2, 1, 6784), (2, 1, 9344), (2, 1, 12946), (4, 1, 128), (4, 1, 256), (4, 1, 384), (4, 1, 512), (4, 1, 640), (4, 1, 768), (4, 1, 1024), (4, 1, 1408), (4, 1, 1792), (4, 1, 2560), (4, 1, 3584), (4, 1, 4864), (4, 1, 6784), (4, 1, 9344), (4, 1, 12946), (8, 1, 128), (8, 1, 256), (8, 1, 384), (8, 1, 512), (8, 1, 640), (8, 1, 768), (8, 1, 1024), (8, 1, 1408), (8, 1, 1792), (8, 1, 2560), (8, 1, 3584), (8, 1, 4864), (8, 1, 6784), (8, 1, 9344), (8, 1, 12946), (16, 1, 128), (16, 1, 256), (16, 1, 384), (16, 1, 512), (16, 1, 640), (16, 1, 768), (16, 1, 1024), (16, 1, 1408), (16, 1, 1792), (16, 1, 2560), (16, 1, 3584), (16, 1, 4864), (16, 1, 6784), (16, 1, 9344), (16, 1, 12946), (32, 1, 128), (32, 1, 256), (32, 1, 384), (32, 1, 512), (32, 1, 640), (32, 1, 768), (32, 1, 1024), (32, 1, 1408), (32, 1, 1792), (32, 1, 2560), (32, 1, 3584), (32, 1, 4864), (32, 1, 6784), (32, 1, 9344), (32, 1, 12946), (64, 1, 128), (64, 1, 256), (64, 1, 384), (64, 1, 512), (64, 1, 640), (64, 1, 768), (64, 1, 1024), (64, 1, 1408), (64, 1, 1792), (64, 1, 2560), (64, 1, 3584), (64, 1, 4864), (64, 1, 6784), (64, 1, 9344), (64, 1, 12946), (128, 1, 128), (128, 1, 256), (128, 1, 384), (128, 1, 512), (128, 1, 640), (128, 1, 768), (128, 1, 1024), (128, 1, 1408), (128, 1, 1792), (128, 1, 2560), (128, 1, 3584), (128, 1, 4864), (128, 1, 6784), (128, 1, 9344), (128, 1, 12946), (256, 1, 128), (256, 1, 256), (256, 1, 384), (256, 1, 512), (256, 1, 640), (256, 1, 768), (256, 1, 1024), (256, 1, 1408), (256, 1, 1792), (256, 1, 2560), (256, 1, 3584), (256, 1, 4864), (256, 1, 6784), (256, 1, 9344), (256, 1, 12946)]
[1;36m(VllmWorkerProcess pid=56467)[0;0m INFO 08-09 13:55:43 [common.py:117] Generated 38 prompt buckets [bs, query, num_blocks]: [(1, 128, 0), (1, 256, 0), (1, 384, 0), (1, 512, 0), (1, 640, 0), (1, 768, 0), (1, 896, 0), (1, 1024, 0), (1, 1408, 0), (1, 1792, 0), (1, 2304, 0), (1, 3072, 0), (1, 4096, 0), (2, 128, 0), (2, 256, 0), (2, 384, 0), (2, 512, 0), (2, 640, 0), (2, 768, 0), (2, 896, 0), (2, 1024, 0), (2, 1408, 0), (2, 1792, 0), (4, 128, 0), (4, 256, 0), (4, 384, 0), (4, 512, 0), (4, 640, 0), (4, 768, 0), (4, 896, 0), (4, 1024, 0), (8, 128, 0), (8, 256, 0), (8, 384, 0), (8, 512, 0), (16, 128, 0), (16, 256, 0), (32, 128, 0)]
[1;36m(VllmWorkerProcess pid=56467)[0;0m INFO 08-09 13:55:43 [exponential.py:79] Decode bucket config (min, step, max_warmup, limit) bs:[1, 2, 256, 9], block:[128, 128, 12946, 15]
INFO 08-09 13:55:43 [exponential.py:49] Prompt bucket config (min, step, max_warmup, limit) bs:[1, 2, 256, 9], seq:[128, 128, 4096, 13]
[1;36m(VllmWorkerProcess pid=56471)[0;0m INFO 08-09 13:55:43 [exponential.py:49] Prompt bucket config (min, step, max_warmup, limit) bs:[1, 2, 256, 9], seq:[128, 128, 4096, 13]
[1;36m(VllmWorkerProcess pid=56469)[0;0m INFO 08-09 13:55:43 [common.py:117] Generated 38 prompt buckets [bs, query, num_blocks]: [(1, 128, 0), (1, 256, 0), (1, 384, 0), (1, 512, 0), (1, 640, 0), (1, 768, 0), (1, 896, 0), (1, 1024, 0), (1, 1408, 0), (1, 1792, 0), (1, 2304, 0), (1, 3072, 0), (1, 4096, 0), (2, 128, 0), (2, 256, 0), (2, 384, 0), (2, 512, 0), (2, 640, 0), (2, 768, 0), (2, 896, 0), (2, 1024, 0), (2, 1408, 0), (2, 1792, 0), (4, 128, 0), (4, 256, 0), (4, 384, 0), (4, 512, 0), (4, 640, 0), (4, 768, 0), (4, 896, 0), (4, 1024, 0), (8, 128, 0), (8, 256, 0), (8, 384, 0), (8, 512, 0), (16, 128, 0), (16, 256, 0), (32, 128, 0)]
[1;36m(VllmWorkerProcess pid=56470)[0;0m INFO 08-09 13:55:43 [common.py:117] Generated 135 decode buckets [bs, query, num_blocks]: [(1, 1, 128), (1, 1, 256), (1, 1, 384), (1, 1, 512), (1, 1, 640), (1, 1, 768), (1, 1, 1024), (1, 1, 1408), (1, 1, 1792), (1, 1, 2560), (1, 1, 3584), (1, 1, 4864), (1, 1, 6784), (1, 1, 9344), (1, 1, 12946), (2, 1, 128), (2, 1, 256), (2, 1, 384), (2, 1, 512), (2, 1, 640), (2, 1, 768), (2, 1, 1024), (2, 1, 1408), (2, 1, 1792), (2, 1, 2560), (2, 1, 3584), (2, 1, 4864), (2, 1, 6784), (2, 1, 9344), (2, 1, 12946), (4, 1, 128), (4, 1, 256), (4, 1, 384), (4, 1, 512), (4, 1, 640), (4, 1, 768), (4, 1, 1024), (4, 1, 1408), (4, 1, 1792), (4, 1, 2560), (4, 1, 3584), (4, 1, 4864), (4, 1, 6784), (4, 1, 9344), (4, 1, 12946), (8, 1, 128), (8, 1, 256), (8, 1, 384), (8, 1, 512), (8, 1, 640), (8, 1, 768), (8, 1, 1024), (8, 1, 1408), (8, 1, 1792), (8, 1, 2560), (8, 1, 3584), (8, 1, 4864), (8, 1, 6784), (8, 1, 9344), (8, 1, 12946), (16, 1, 128), (16, 1, 256), (16, 1, 384), (16, 1, 512), (16, 1, 640), (16, 1, 768), (16, 1, 1024), (16, 1, 1408), (16, 1, 1792), (16, 1, 2560), (16, 1, 3584), (16, 1, 4864), (16, 1, 6784), (16, 1, 9344), (16, 1, 12946), (32, 1, 128), (32, 1, 256), (32, 1, 384), (32, 1, 512), (32, 1, 640), (32, 1, 768), (32, 1, 1024), (32, 1, 1408), (32, 1, 1792), (32, 1, 2560), (32, 1, 3584), (32, 1, 4864), (32, 1, 6784), (32, 1, 9344), (32, 1, 12946), (64, 1, 128), (64, 1, 256), (64, 1, 384), (64, 1, 512), (64, 1, 640), (64, 1, 768), (64, 1, 1024), (64, 1, 1408), (64, 1, 1792), (64, 1, 2560), (64, 1, 3584), (64, 1, 4864), (64, 1, 6784), (64, 1, 9344), (64, 1, 12946), (128, 1, 128), (128, 1, 256), (128, 1, 384), (128, 1, 512), (128, 1, 640), (128, 1, 768), (128, 1, 1024), (128, 1, 1408), (128, 1, 1792), (128, 1, 2560), (128, 1, 3584), (128, 1, 4864), (128, 1, 6784), (128, 1, 9344), (128, 1, 12946), (256, 1, 128), (256, 1, 256), (256, 1, 384), (256, 1, 512), (256, 1, 640), (256, 1, 768), (256, 1, 1024), (256, 1, 1408), (256, 1, 1792), (256, 1, 2560), (256, 1, 3584), (256, 1, 4864), (256, 1, 6784), (256, 1, 9344), (256, 1, 12946)]
[1;36m(VllmWorkerProcess pid=56473)[0;0m INFO 08-09 13:55:43 [exponential.py:49] Prompt bucket config (min, step, max_warmup, limit) bs:[1, 2, 256, 9], seq:[128, 128, 4096, 13]
[1;36m(VllmWorkerProcess pid=56469)[0;0m INFO 08-09 13:55:43 [exponential.py:79] Decode bucket config (min, step, max_warmup, limit) bs:[1, 2, 256, 9], block:[128, 128, 12946, 15]
[1;36m(VllmWorkerProcess pid=56467)[0;0m INFO 08-09 13:55:43 [common.py:117] Generated 135 decode buckets [bs, query, num_blocks]: [(1, 1, 128), (1, 1, 256), (1, 1, 384), (1, 1, 512), (1, 1, 640), (1, 1, 768), (1, 1, 1024), (1, 1, 1408), (1, 1, 1792), (1, 1, 2560), (1, 1, 3584), (1, 1, 4864), (1, 1, 6784), (1, 1, 9344), (1, 1, 12946), (2, 1, 128), (2, 1, 256), (2, 1, 384), (2, 1, 512), (2, 1, 640), (2, 1, 768), (2, 1, 1024), (2, 1, 1408), (2, 1, 1792), (2, 1, 2560), (2, 1, 3584), (2, 1, 4864), (2, 1, 6784), (2, 1, 9344), (2, 1, 12946), (4, 1, 128), (4, 1, 256), (4, 1, 384), (4, 1, 512), (4, 1, 640), (4, 1, 768), (4, 1, 1024), (4, 1, 1408), (4, 1, 1792), (4, 1, 2560), (4, 1, 3584), (4, 1, 4864), (4, 1, 6784), (4, 1, 9344), (4, 1, 12946), (8, 1, 128), (8, 1, 256), (8, 1, 384), (8, 1, 512), (8, 1, 640), (8, 1, 768), (8, 1, 1024), (8, 1, 1408), (8, 1, 1792), (8, 1, 2560), (8, 1, 3584), (8, 1, 4864), (8, 1, 6784), (8, 1, 9344), (8, 1, 12946), (16, 1, 128), (16, 1, 256), (16, 1, 384), (16, 1, 512), (16, 1, 640), (16, 1, 768), (16, 1, 1024), (16, 1, 1408), (16, 1, 1792), (16, 1, 2560), (16, 1, 3584), (16, 1, 4864), (16, 1, 6784), (16, 1, 9344), (16, 1, 12946), (32, 1, 128), (32, 1, 256), (32, 1, 384), (32, 1, 512), (32, 1, 640), (32, 1, 768), (32, 1, 1024), (32, 1, 1408), (32, 1, 1792), (32, 1, 2560), (32, 1, 3584), (32, 1, 4864), (32, 1, 6784), (32, 1, 9344), (32, 1, 12946), (64, 1, 128), (64, 1, 256), (64, 1, 384), (64, 1, 512), (64, 1, 640), (64, 1, 768), (64, 1, 1024), (64, 1, 1408), (64, 1, 1792), (64, 1, 2560), (64, 1, 3584), (64, 1, 4864), (64, 1, 6784), (64, 1, 9344), (64, 1, 12946), (128, 1, 128), (128, 1, 256), (128, 1, 384), (128, 1, 512), (128, 1, 640), (128, 1, 768), (128, 1, 1024), (128, 1, 1408), (128, 1, 1792), (128, 1, 2560), (128, 1, 3584), (128, 1, 4864), (128, 1, 6784), (128, 1, 9344), (128, 1, 12946), (256, 1, 128), (256, 1, 256), (256, 1, 384), (256, 1, 512), (256, 1, 640), (256, 1, 768), (256, 1, 1024), (256, 1, 1408), (256, 1, 1792), (256, 1, 2560), (256, 1, 3584), (256, 1, 4864), (256, 1, 6784), (256, 1, 9344), (256, 1, 12946)]
[1;36m(VllmWorkerProcess pid=56472)[0;0m INFO 08-09 13:55:43 [exponential.py:49] Prompt bucket config (min, step, max_warmup, limit) bs:[1, 2, 256, 9], seq:[128, 128, 4096, 13]
[1;36m(VllmWorkerProcess pid=56471)[0;0m INFO 08-09 13:55:43 [common.py:117] Generated 38 prompt buckets [bs, query, num_blocks]: [(1, 128, 0), (1, 256, 0), (1, 384, 0), (1, 512, 0), (1, 640, 0), (1, 768, 0), (1, 896, 0), (1, 1024, 0), (1, 1408, 0), (1, 1792, 0), (1, 2304, 0), (1, 3072, 0), (1, 4096, 0), (2, 128, 0), (2, 256, 0), (2, 384, 0), (2, 512, 0), (2, 640, 0), (2, 768, 0), (2, 896, 0), (2, 1024, 0), (2, 1408, 0), (2, 1792, 0), (4, 128, 0), (4, 256, 0), (4, 384, 0), (4, 512, 0), (4, 640, 0), (4, 768, 0), (4, 896, 0), (4, 1024, 0), (8, 128, 0), (8, 256, 0), (8, 384, 0), (8, 512, 0), (16, 128, 0), (16, 256, 0), (32, 128, 0)]
[1;36m(VllmWorkerProcess pid=56471)[0;0m INFO 08-09 13:55:43 [exponential.py:79] Decode bucket config (min, step, max_warmup, limit) bs:[1, 2, 256, 9], block:[128, 128, 12946, 15]
INFO 08-09 13:55:43 [common.py:117] Generated 38 prompt buckets [bs, query, num_blocks]: [(1, 128, 0), (1, 256, 0), (1, 384, 0), (1, 512, 0), (1, 640, 0), (1, 768, 0), (1, 896, 0), (1, 1024, 0), (1, 1408, 0), (1, 1792, 0), (1, 2304, 0), (1, 3072, 0), (1, 4096, 0), (2, 128, 0), (2, 256, 0), (2, 384, 0), (2, 512, 0), (2, 640, 0), (2, 768, 0), (2, 896, 0), (2, 1024, 0), (2, 1408, 0), (2, 1792, 0), (4, 128, 0), (4, 256, 0), (4, 384, 0), (4, 512, 0), (4, 640, 0), (4, 768, 0), (4, 896, 0), (4, 1024, 0), (8, 128, 0), (8, 256, 0), (8, 384, 0), (8, 512, 0), (16, 128, 0), (16, 256, 0), (32, 128, 0)]
[1;36m(VllmWorkerProcess pid=56469)[0;0m INFO 08-09 13:55:43 [common.py:117] Generated 135 decode buckets [bs, query, num_blocks]: [(1, 1, 128), (1, 1, 256), (1, 1, 384), (1, 1, 512), (1, 1, 640), (1, 1, 768), (1, 1, 1024), (1, 1, 1408), (1, 1, 1792), (1, 1, 2560), (1, 1, 3584), (1, 1, 4864), (1, 1, 6784), (1, 1, 9344), (1, 1, 12946), (2, 1, 128), (2, 1, 256), (2, 1, 384), (2, 1, 512), (2, 1, 640), (2, 1, 768), (2, 1, 1024), (2, 1, 1408), (2, 1, 1792), (2, 1, 2560), (2, 1, 3584), (2, 1, 4864), (2, 1, 6784), (2, 1, 9344), (2, 1, 12946), (4, 1, 128), (4, 1, 256), (4, 1, 384), (4, 1, 512), (4, 1, 640), (4, 1, 768), (4, 1, 1024), (4, 1, 1408), (4, 1, 1792), (4, 1, 2560), (4, 1, 3584), (4, 1, 4864), (4, 1, 6784), (4, 1, 9344), (4, 1, 12946), (8, 1, 128), (8, 1, 256), (8, 1, 384), (8, 1, 512), (8, 1, 640), (8, 1, 768), (8, 1, 1024), (8, 1, 1408), (8, 1, 1792), (8, 1, 2560), (8, 1, 3584), (8, 1, 4864), (8, 1, 6784), (8, 1, 9344), (8, 1, 12946), (16, 1, 128), (16, 1, 256), (16, 1, 384), (16, 1, 512), (16, 1, 640), (16, 1, 768), (16, 1, 1024), (16, 1, 1408), (16, 1, 1792), (16, 1, 2560), (16, 1, 3584), (16, 1, 4864), (16, 1, 6784), (16, 1, 9344), (16, 1, 12946), (32, 1, 128), (32, 1, 256), (32, 1, 384), (32, 1, 512), (32, 1, 640), (32, 1, 768), (32, 1, 1024), (32, 1, 1408), (32, 1, 1792), (32, 1, 2560), (32, 1, 3584), (32, 1, 4864), (32, 1, 6784), (32, 1, 9344), (32, 1, 12946), (64, 1, 128), (64, 1, 256), (64, 1, 384), (64, 1, 512), (64, 1, 640), (64, 1, 768), (64, 1, 1024), (64, 1, 1408), (64, 1, 1792), (64, 1, 2560), (64, 1, 3584), (64, 1, 4864), (64, 1, 6784), (64, 1, 9344), (64, 1, 12946), (128, 1, 128), (128, 1, 256), (128, 1, 384), (128, 1, 512), (128, 1, 640), (128, 1, 768), (128, 1, 1024), (128, 1, 1408), (128, 1, 1792), (128, 1, 2560), (128, 1, 3584), (128, 1, 4864), (128, 1, 6784), (128, 1, 9344), (128, 1, 12946), (256, 1, 128), (256, 1, 256), (256, 1, 384), (256, 1, 512), (256, 1, 640), (256, 1, 768), (256, 1, 1024), (256, 1, 1408), (256, 1, 1792), (256, 1, 2560), (256, 1, 3584), (256, 1, 4864), (256, 1, 6784), (256, 1, 9344), (256, 1, 12946)]
[1;36m(VllmWorkerProcess pid=56473)[0;0m INFO 08-09 13:55:43 [common.py:117] Generated 38 prompt buckets [bs, query, num_blocks]: [(1, 128, 0), (1, 256, 0), (1, 384, 0), (1, 512, 0), (1, 640, 0), (1, 768, 0), (1, 896, 0), (1, 1024, 0), (1, 1408, 0), (1, 1792, 0), (1, 2304, 0), (1, 3072, 0), (1, 4096, 0), (2, 128, 0), (2, 256, 0), (2, 384, 0), (2, 512, 0), (2, 640, 0), (2, 768, 0), (2, 896, 0), (2, 1024, 0), (2, 1408, 0), (2, 1792, 0), (4, 128, 0), (4, 256, 0), (4, 384, 0), (4, 512, 0), (4, 640, 0), (4, 768, 0), (4, 896, 0), (4, 1024, 0), (8, 128, 0), (8, 256, 0), (8, 384, 0), (8, 512, 0), (16, 128, 0), (16, 256, 0), (32, 128, 0)]
INFO 08-09 13:55:43 [exponential.py:79] Decode bucket config (min, step, max_warmup, limit) bs:[1, 2, 256, 9], block:[128, 128, 12946, 15]
[1;36m(VllmWorkerProcess pid=56473)[0;0m INFO 08-09 13:55:43 [exponential.py:79] Decode bucket config (min, step, max_warmup, limit) bs:[1, 2, 256, 9], block:[128, 128, 12946, 15]
[1;36m(VllmWorkerProcess pid=56472)[0;0m INFO 08-09 13:55:43 [common.py:117] Generated 38 prompt buckets [bs, query, num_blocks]: [(1, 128, 0), (1, 256, 0), (1, 384, 0), (1, 512, 0), (1, 640, 0), (1, 768, 0), (1, 896, 0), (1, 1024, 0), (1, 1408, 0), (1, 1792, 0), (1, 2304, 0), (1, 3072, 0), (1, 4096, 0), (2, 128, 0), (2, 256, 0), (2, 384, 0), (2, 512, 0), (2, 640, 0), (2, 768, 0), (2, 896, 0), (2, 1024, 0), (2, 1408, 0), (2, 1792, 0), (4, 128, 0), (4, 256, 0), (4, 384, 0), (4, 512, 0), (4, 640, 0), (4, 768, 0), (4, 896, 0), (4, 1024, 0), (8, 128, 0), (8, 256, 0), (8, 384, 0), (8, 512, 0), (16, 128, 0), (16, 256, 0), (32, 128, 0)]
[1;36m(VllmWorkerProcess pid=56472)[0;0m INFO 08-09 13:55:43 [exponential.py:79] Decode bucket config (min, step, max_warmup, limit) bs:[1, 2, 256, 9], block:[128, 128, 12946, 15]
[1;36m(VllmWorkerProcess pid=56471)[0;0m INFO 08-09 13:55:43 [common.py:117] Generated 135 decode buckets [bs, query, num_blocks]: [(1, 1, 128), (1, 1, 256), (1, 1, 384), (1, 1, 512), (1, 1, 640), (1, 1, 768), (1, 1, 1024), (1, 1, 1408), (1, 1, 1792), (1, 1, 2560), (1, 1, 3584), (1, 1, 4864), (1, 1, 6784), (1, 1, 9344), (1, 1, 12946), (2, 1, 128), (2, 1, 256), (2, 1, 384), (2, 1, 512), (2, 1, 640), (2, 1, 768), (2, 1, 1024), (2, 1, 1408), (2, 1, 1792), (2, 1, 2560), (2, 1, 3584), (2, 1, 4864), (2, 1, 6784), (2, 1, 9344), (2, 1, 12946), (4, 1, 128), (4, 1, 256), (4, 1, 384), (4, 1, 512), (4, 1, 640), (4, 1, 768), (4, 1, 1024), (4, 1, 1408), (4, 1, 1792), (4, 1, 2560), (4, 1, 3584), (4, 1, 4864), (4, 1, 6784), (4, 1, 9344), (4, 1, 12946), (8, 1, 128), (8, 1, 256), (8, 1, 384), (8, 1, 512), (8, 1, 640), (8, 1, 768), (8, 1, 1024), (8, 1, 1408), (8, 1, 1792), (8, 1, 2560), (8, 1, 3584), (8, 1, 4864), (8, 1, 6784), (8, 1, 9344), (8, 1, 12946), (16, 1, 128), (16, 1, 256), (16, 1, 384), (16, 1, 512), (16, 1, 640), (16, 1, 768), (16, 1, 1024), (16, 1, 1408), (16, 1, 1792), (16, 1, 2560), (16, 1, 3584), (16, 1, 4864), (16, 1, 6784), (16, 1, 9344), (16, 1, 12946), (32, 1, 128), (32, 1, 256), (32, 1, 384), (32, 1, 512), (32, 1, 640), (32, 1, 768), (32, 1, 1024), (32, 1, 1408), (32, 1, 1792), (32, 1, 2560), (32, 1, 3584), (32, 1, 4864), (32, 1, 6784), (32, 1, 9344), (32, 1, 12946), (64, 1, 128), (64, 1, 256), (64, 1, 384), (64, 1, 512), (64, 1, 640), (64, 1, 768), (64, 1, 1024), (64, 1, 1408), (64, 1, 1792), (64, 1, 2560), (64, 1, 3584), (64, 1, 4864), (64, 1, 6784), (64, 1, 9344), (64, 1, 12946), (128, 1, 128), (128, 1, 256), (128, 1, 384), (128, 1, 512), (128, 1, 640), (128, 1, 768), (128, 1, 1024), (128, 1, 1408), (128, 1, 1792), (128, 1, 2560), (128, 1, 3584), (128, 1, 4864), (128, 1, 6784), (128, 1, 9344), (128, 1, 12946), (256, 1, 128), (256, 1, 256), (256, 1, 384), (256, 1, 512), (256, 1, 640), (256, 1, 768), (256, 1, 1024), (256, 1, 1408), (256, 1, 1792), (256, 1, 2560), (256, 1, 3584), (256, 1, 4864), (256, 1, 6784), (256, 1, 9344), (256, 1, 12946)]
[1;36m(VllmWorkerProcess pid=56473)[0;0m INFO 08-09 13:55:43 [common.py:117] Generated 135 decode buckets [bs, query, num_blocks]: [(1, 1, 128), (1, 1, 256), (1, 1, 384), (1, 1, 512), (1, 1, 640), (1, 1, 768), (1, 1, 1024), (1, 1, 1408), (1, 1, 1792), (1, 1, 2560), (1, 1, 3584), (1, 1, 4864), (1, 1, 6784), (1, 1, 9344), (1, 1, 12946), (2, 1, 128), (2, 1, 256), (2, 1, 384), (2, 1, 512), (2, 1, 640), (2, 1, 768), (2, 1, 1024), (2, 1, 1408), (2, 1, 1792), (2, 1, 2560), (2, 1, 3584), (2, 1, 4864), (2, 1, 6784), (2, 1, 9344), (2, 1, 12946), (4, 1, 128), (4, 1, 256), (4, 1, 384), (4, 1, 512), (4, 1, 640), (4, 1, 768), (4, 1, 1024), (4, 1, 1408), (4, 1, 1792), (4, 1, 2560), (4, 1, 3584), (4, 1, 4864), (4, 1, 6784), (4, 1, 9344), (4, 1, 12946), (8, 1, 128), (8, 1, 256), (8, 1, 384), (8, 1, 512), (8, 1, 640), (8, 1, 768), (8, 1, 1024), (8, 1, 1408), (8, 1, 1792), (8, 1, 2560), (8, 1, 3584), (8, 1, 4864), (8, 1, 6784), (8, 1, 9344), (8, 1, 12946), (16, 1, 128), (16, 1, 256), (16, 1, 384), (16, 1, 512), (16, 1, 640), (16, 1, 768), (16, 1, 1024), (16, 1, 1408), (16, 1, 1792), (16, 1, 2560), (16, 1, 3584), (16, 1, 4864), (16, 1, 6784), (16, 1, 9344), (16, 1, 12946), (32, 1, 128), (32, 1, 256), (32, 1, 384), (32, 1, 512), (32, 1, 640), (32, 1, 768), (32, 1, 1024), (32, 1, 1408), (32, 1, 1792), (32, 1, 2560), (32, 1, 3584), (32, 1, 4864), (32, 1, 6784), (32, 1, 9344), (32, 1, 12946), (64, 1, 128), (64, 1, 256), (64, 1, 384), (64, 1, 512), (64, 1, 640), (64, 1, 768), (64, 1, 1024), (64, 1, 1408), (64, 1, 1792), (64, 1, 2560), (64, 1, 3584), (64, 1, 4864), (64, 1, 6784), (64, 1, 9344), (64, 1, 12946), (128, 1, 128), (128, 1, 256), (128, 1, 384), (128, 1, 512), (128, 1, 640), (128, 1, 768), (128, 1, 1024), (128, 1, 1408), (128, 1, 1792), (128, 1, 2560), (128, 1, 3584), (128, 1, 4864), (128, 1, 6784), (128, 1, 9344), (128, 1, 12946), (256, 1, 128), (256, 1, 256), (256, 1, 384), (256, 1, 512), (256, 1, 640), (256, 1, 768), (256, 1, 1024), (256, 1, 1408), (256, 1, 1792), (256, 1, 2560), (256, 1, 3584), (256, 1, 4864), (256, 1, 6784), (256, 1, 9344), (256, 1, 12946)]
[1;36m(VllmWorkerProcess pid=56472)[0;0m INFO 08-09 13:55:43 [common.py:117] Generated 135 decode buckets [bs, query, num_blocks]: [(1, 1, 128), (1, 1, 256), (1, 1, 384), (1, 1, 512), (1, 1, 640), (1, 1, 768), (1, 1, 1024), (1, 1, 1408), (1, 1, 1792), (1, 1, 2560), (1, 1, 3584), (1, 1, 4864), (1, 1, 6784), (1, 1, 9344), (1, 1, 12946), (2, 1, 128), (2, 1, 256), (2, 1, 384), (2, 1, 512), (2, 1, 640), (2, 1, 768), (2, 1, 1024), (2, 1, 1408), (2, 1, 1792), (2, 1, 2560), (2, 1, 3584), (2, 1, 4864), (2, 1, 6784), (2, 1, 9344), (2, 1, 12946), (4, 1, 128), (4, 1, 256), (4, 1, 384), (4, 1, 512), (4, 1, 640), (4, 1, 768), (4, 1, 1024), (4, 1, 1408), (4, 1, 1792), (4, 1, 2560), (4, 1, 3584), (4, 1, 4864), (4, 1, 6784), (4, 1, 9344), (4, 1, 12946), (8, 1, 128), (8, 1, 256), (8, 1, 384), (8, 1, 512), (8, 1, 640), (8, 1, 768), (8, 1, 1024), (8, 1, 1408), (8, 1, 1792), (8, 1, 2560), (8, 1, 3584), (8, 1, 4864), (8, 1, 6784), (8, 1, 9344), (8, 1, 12946), (16, 1, 128), (16, 1, 256), (16, 1, 384), (16, 1, 512), (16, 1, 640), (16, 1, 768), (16, 1, 1024), (16, 1, 1408), (16, 1, 1792), (16, 1, 2560), (16, 1, 3584), (16, 1, 4864), (16, 1, 6784), (16, 1, 9344), (16, 1, 12946), (32, 1, 128), (32, 1, 256), (32, 1, 384), (32, 1, 512), (32, 1, 640), (32, 1, 768), (32, 1, 1024), (32, 1, 1408), (32, 1, 1792), (32, 1, 2560), (32, 1, 3584), (32, 1, 4864), (32, 1, 6784), (32, 1, 9344), (32, 1, 12946), (64, 1, 128), (64, 1, 256), (64, 1, 384), (64, 1, 512), (64, 1, 640), (64, 1, 768), (64, 1, 1024), (64, 1, 1408), (64, 1, 1792), (64, 1, 2560), (64, 1, 3584), (64, 1, 4864), (64, 1, 6784), (64, 1, 9344), (64, 1, 12946), (128, 1, 128), (128, 1, 256), (128, 1, 384), (128, 1, 512), (128, 1, 640), (128, 1, 768), (128, 1, 1024), (128, 1, 1408), (128, 1, 1792), (128, 1, 2560), (128, 1, 3584), (128, 1, 4864), (128, 1, 6784), (128, 1, 9344), (128, 1, 12946), (256, 1, 128), (256, 1, 256), (256, 1, 384), (256, 1, 512), (256, 1, 640), (256, 1, 768), (256, 1, 1024), (256, 1, 1408), (256, 1, 1792), (256, 1, 2560), (256, 1, 3584), (256, 1, 4864), (256, 1, 6784), (256, 1, 9344), (256, 1, 12946)]
INFO 08-09 13:55:43 [common.py:117] Generated 135 decode buckets [bs, query, num_blocks]: [(1, 1, 128), (1, 1, 256), (1, 1, 384), (1, 1, 512), (1, 1, 640), (1, 1, 768), (1, 1, 1024), (1, 1, 1408), (1, 1, 1792), (1, 1, 2560), (1, 1, 3584), (1, 1, 4864), (1, 1, 6784), (1, 1, 9344), (1, 1, 12946), (2, 1, 128), (2, 1, 256), (2, 1, 384), (2, 1, 512), (2, 1, 640), (2, 1, 768), (2, 1, 1024), (2, 1, 1408), (2, 1, 1792), (2, 1, 2560), (2, 1, 3584), (2, 1, 4864), (2, 1, 6784), (2, 1, 9344), (2, 1, 12946), (4, 1, 128), (4, 1, 256), (4, 1, 384), (4, 1, 512), (4, 1, 640), (4, 1, 768), (4, 1, 1024), (4, 1, 1408), (4, 1, 1792), (4, 1, 2560), (4, 1, 3584), (4, 1, 4864), (4, 1, 6784), (4, 1, 9344), (4, 1, 12946), (8, 1, 128), (8, 1, 256), (8, 1, 384), (8, 1, 512), (8, 1, 640), (8, 1, 768), (8, 1, 1024), (8, 1, 1408), (8, 1, 1792), (8, 1, 2560), (8, 1, 3584), (8, 1, 4864), (8, 1, 6784), (8, 1, 9344), (8, 1, 12946), (16, 1, 128), (16, 1, 256), (16, 1, 384), (16, 1, 512), (16, 1, 640), (16, 1, 768), (16, 1, 1024), (16, 1, 1408), (16, 1, 1792), (16, 1, 2560), (16, 1, 3584), (16, 1, 4864), (16, 1, 6784), (16, 1, 9344), (16, 1, 12946), (32, 1, 128), (32, 1, 256), (32, 1, 384), (32, 1, 512), (32, 1, 640), (32, 1, 768), (32, 1, 1024), (32, 1, 1408), (32, 1, 1792), (32, 1, 2560), (32, 1, 3584), (32, 1, 4864), (32, 1, 6784), (32, 1, 9344), (32, 1, 12946), (64, 1, 128), (64, 1, 256), (64, 1, 384), (64, 1, 512), (64, 1, 640), (64, 1, 768), (64, 1, 1024), (64, 1, 1408), (64, 1, 1792), (64, 1, 2560), (64, 1, 3584), (64, 1, 4864), (64, 1, 6784), (64, 1, 9344), (64, 1, 12946), (128, 1, 128), (128, 1, 256), (128, 1, 384), (128, 1, 512), (128, 1, 640), (128, 1, 768), (128, 1, 1024), (128, 1, 1408), (128, 1, 1792), (128, 1, 2560), (128, 1, 3584), (128, 1, 4864), (128, 1, 6784), (128, 1, 9344), (128, 1, 12946), (256, 1, 128), (256, 1, 256), (256, 1, 384), (256, 1, 512), (256, 1, 640), (256, 1, 768), (256, 1, 1024), (256, 1, 1408), (256, 1, 1792), (256, 1, 2560), (256, 1, 3584), (256, 1, 4864), (256, 1, 6784), (256, 1, 9344), (256, 1, 12946)]
[1;36m(VllmWorkerProcess pid=56467)[0;0m INFO 08-09 13:55:45 [hpu_worker.py:350] Initializing cache engine took 101.1 GiB of device memory (102.8 GiB/126.5 GiB used) and 32.01 GiB of host memory (183.1 GiB/1007 GiB used)
[1;36m(VllmWorkerProcess pid=56471)[0;0m INFO 08-09 13:55:45 [hpu_worker.py:350] Initializing cache engine took 101.1 GiB of device memory (102.8 GiB/126.5 GiB used) and 32 GiB of host memory (183.1 GiB/1007 GiB used)
[1;36m(VllmWorkerProcess pid=56468)[0;0m INFO 08-09 13:55:45 [hpu_worker.py:350] Initializing cache engine took 101.1 GiB of device memory (102.8 GiB/126.5 GiB used) and 32.09 GiB of host memory (183.1 GiB/1007 GiB used)
[1;36m(VllmWorkerProcess pid=56473)[0;0m INFO 08-09 13:55:45 [hpu_worker.py:350] Initializing cache engine took 101.1 GiB of device memory (102.8 GiB/126.5 GiB used) and 31.99 GiB of host memory (183.1 GiB/1007 GiB used)
[1;36m(VllmWorkerProcess pid=56469)[0;0m INFO 08-09 13:55:45 [hpu_worker.py:350] Initializing cache engine took 101.1 GiB of device memory (102.8 GiB/126.5 GiB used) and 31.41 GiB of host memory (183.1 GiB/1007 GiB used)
[1;36m(VllmWorkerProcess pid=56472)[0;0m INFO 08-09 13:55:45 [hpu_worker.py:350] Initializing cache engine took 101.1 GiB of device memory (102.8 GiB/126.5 GiB used) and 30.35 GiB of host memory (183.1 GiB/1007 GiB used)
INFO 08-09 13:55:45 [hpu_worker.py:350] Initializing cache engine took 101.1 GiB of device memory (102.8 GiB/126.5 GiB used) and 30.91 GiB of host memory (183.1 GiB/1007 GiB used)
[1;36m(VllmWorkerProcess pid=56470)[0;0m INFO 08-09 13:55:45 [hpu_worker.py:350] Initializing cache engine took 101.1 GiB of device memory (102.8 GiB/126.5 GiB used) and 30.73 GiB of host memory (183.1 GiB/1007 GiB used)
INFO 08-09 13:55:45 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][1/38] batch_size:32 query_len:128 num_blocks:0 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56472)[0;0m INFO 08-09 13:55:45 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][1/38] batch_size:32 query_len:128 num_blocks:0 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56470)[0;0m INFO 08-09 13:55:45 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][1/38] batch_size:32 query_len:128 num_blocks:0 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56473)[0;0m INFO 08-09 13:55:45 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][1/38] batch_size:32 query_len:128 num_blocks:0 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56468)[0;0m INFO 08-09 13:55:45 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][1/38] batch_size:32 query_len:128 num_blocks:0 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56471)[0;0m INFO 08-09 13:55:45 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][1/38] batch_size:32 query_len:128 num_blocks:0 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56469)[0;0m INFO 08-09 13:55:45 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][1/38] batch_size:32 query_len:128 num_blocks:0 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56467)[0;0m INFO 08-09 13:55:45 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][1/38] batch_size:32 query_len:128 num_blocks:0 free_mem:23.73 GiB
INFO 08-09 13:55:48 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][2/38] batch_size:16 query_len:256 num_blocks:0 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56473)[0;0m INFO 08-09 13:55:48 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][2/38] batch_size:16 query_len:256 num_blocks:0 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56472)[0;0m INFO 08-09 13:55:48 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][2/38] batch_size:16 query_len:256 num_blocks:0 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56471)[0;0m INFO 08-09 13:55:48 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][2/38] batch_size:16 query_len:256 num_blocks:0 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56470)[0;0m INFO 08-09 13:55:48 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][2/38] batch_size:16 query_len:256 num_blocks:0 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56468)[0;0m INFO 08-09 13:55:48 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][2/38] batch_size:16 query_len:256 num_blocks:0 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56469)[0;0m INFO 08-09 13:55:48 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][2/38] batch_size:16 query_len:256 num_blocks:0 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56467)[0;0m INFO 08-09 13:55:48 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][2/38] batch_size:16 query_len:256 num_blocks:0 free_mem:23.73 GiB
INFO 08-09 13:55:51 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][3/38] batch_size:16 query_len:128 num_blocks:0 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56473)[0;0m INFO 08-09 13:55:51 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][3/38] batch_size:16 query_len:128 num_blocks:0 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56472)[0;0m INFO 08-09 13:55:51 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][3/38] batch_size:16 query_len:128 num_blocks:0 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56471)[0;0m INFO 08-09 13:55:51 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][3/38] batch_size:16 query_len:128 num_blocks:0 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56468)[0;0m INFO 08-09 13:55:51 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][3/38] batch_size:16 query_len:128 num_blocks:0 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56470)[0;0m INFO 08-09 13:55:51 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][3/38] batch_size:16 query_len:128 num_blocks:0 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56467)[0;0m INFO 08-09 13:55:51 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][3/38] batch_size:16 query_len:128 num_blocks:0 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56469)[0;0m INFO 08-09 13:55:51 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][3/38] batch_size:16 query_len:128 num_blocks:0 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56472)[0;0m INFO 08-09 13:55:53 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][4/38] batch_size:8 query_len:512 num_blocks:0 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56473)[0;0m INFO 08-09 13:55:53 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][4/38] batch_size:8 query_len:512 num_blocks:0 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56471)[0;0m INFO 08-09 13:55:53 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][4/38] batch_size:8 query_len:512 num_blocks:0 free_mem:23.73 GiB
INFO 08-09 13:55:53 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][4/38] batch_size:8 query_len:512 num_blocks:0 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56470)[0;0m INFO 08-09 13:55:53 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][4/38] batch_size:8 query_len:512 num_blocks:0 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56468)[0;0m INFO 08-09 13:55:53 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][4/38] batch_size:8 query_len:512 num_blocks:0 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56467)[0;0m INFO 08-09 13:55:53 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][4/38] batch_size:8 query_len:512 num_blocks:0 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56469)[0;0m INFO 08-09 13:55:53 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][4/38] batch_size:8 query_len:512 num_blocks:0 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56472)[0;0m INFO 08-09 13:55:56 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][5/38] batch_size:8 query_len:384 num_blocks:0 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56470)[0;0m INFO 08-09 13:55:56 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][5/38] batch_size:8 query_len:384 num_blocks:0 free_mem:23.73 GiB
INFO 08-09 13:55:56 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][5/38] batch_size:8 query_len:384 num_blocks:0 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56471)[0;0m INFO 08-09 13:55:56 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][5/38] batch_size:8 query_len:384 num_blocks:0 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56473)[0;0m INFO 08-09 13:55:56 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][5/38] batch_size:8 query_len:384 num_blocks:0 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56469)[0;0m INFO 08-09 13:55:56 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][5/38] batch_size:8 query_len:384 num_blocks:0 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56468)[0;0m INFO 08-09 13:55:56 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][5/38] batch_size:8 query_len:384 num_blocks:0 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56467)[0;0m INFO 08-09 13:55:56 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][5/38] batch_size:8 query_len:384 num_blocks:0 free_mem:23.73 GiB
INFO 08-09 13:55:58 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][6/38] batch_size:8 query_len:256 num_blocks:0 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56473)[0;0m INFO 08-09 13:55:58 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][6/38] batch_size:8 query_len:256 num_blocks:0 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56472)[0;0m INFO 08-09 13:55:58 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][6/38] batch_size:8 query_len:256 num_blocks:0 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56470)[0;0m INFO 08-09 13:55:58 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][6/38] batch_size:8 query_len:256 num_blocks:0 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56468)[0;0m INFO 08-09 13:55:58 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][6/38] batch_size:8 query_len:256 num_blocks:0 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56471)[0;0m INFO 08-09 13:55:58 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][6/38] batch_size:8 query_len:256 num_blocks:0 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56467)[0;0m INFO 08-09 13:55:58 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][6/38] batch_size:8 query_len:256 num_blocks:0 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56469)[0;0m INFO 08-09 13:55:58 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][6/38] batch_size:8 query_len:256 num_blocks:0 free_mem:23.73 GiB
INFO 08-09 13:56:01 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][7/38] batch_size:8 query_len:128 num_blocks:0 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56473)[0;0m INFO 08-09 13:56:01 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][7/38] batch_size:8 query_len:128 num_blocks:0 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56472)[0;0m INFO 08-09 13:56:01 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][7/38] batch_size:8 query_len:128 num_blocks:0 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56471)[0;0m INFO 08-09 13:56:01 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][7/38] batch_size:8 query_len:128 num_blocks:0 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56470)[0;0m INFO 08-09 13:56:01 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][7/38] batch_size:8 query_len:128 num_blocks:0 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56468)[0;0m INFO 08-09 13:56:01 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][7/38] batch_size:8 query_len:128 num_blocks:0 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56467)[0;0m INFO 08-09 13:56:01 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][7/38] batch_size:8 query_len:128 num_blocks:0 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56469)[0;0m INFO 08-09 13:56:01 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][7/38] batch_size:8 query_len:128 num_blocks:0 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56473)[0;0m INFO 08-09 13:56:03 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][8/38] batch_size:4 query_len:1024 num_blocks:0 free_mem:23.73 GiB
INFO 08-09 13:56:03 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][8/38] batch_size:4 query_len:1024 num_blocks:0 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56472)[0;0m INFO 08-09 13:56:03 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][8/38] batch_size:4 query_len:1024 num_blocks:0 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56471)[0;0m INFO 08-09 13:56:03 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][8/38] batch_size:4 query_len:1024 num_blocks:0 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56470)[0;0m INFO 08-09 13:56:03 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][8/38] batch_size:4 query_len:1024 num_blocks:0 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56468)[0;0m INFO 08-09 13:56:03 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][8/38] batch_size:4 query_len:1024 num_blocks:0 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56467)[0;0m INFO 08-09 13:56:03 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][8/38] batch_size:4 query_len:1024 num_blocks:0 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56469)[0;0m INFO 08-09 13:56:03 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][8/38] batch_size:4 query_len:1024 num_blocks:0 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56472)[0;0m INFO 08-09 13:56:06 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][9/38] batch_size:4 query_len:896 num_blocks:0 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56473)[0;0m INFO 08-09 13:56:06 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][9/38] batch_size:4 query_len:896 num_blocks:0 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56471)[0;0m INFO 08-09 13:56:06 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][9/38] batch_size:4 query_len:896 num_blocks:0 free_mem:23.73 GiB
INFO 08-09 13:56:06 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][9/38] batch_size:4 query_len:896 num_blocks:0 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56470)[0;0m INFO 08-09 13:56:06 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][9/38] batch_size:4 query_len:896 num_blocks:0 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56469)[0;0m INFO 08-09 13:56:06 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][9/38] batch_size:4 query_len:896 num_blocks:0 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56468)[0;0m INFO 08-09 13:56:06 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][9/38] batch_size:4 query_len:896 num_blocks:0 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56467)[0;0m INFO 08-09 13:56:06 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][9/38] batch_size:4 query_len:896 num_blocks:0 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56472)[0;0m INFO 08-09 13:56:09 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][10/38] batch_size:4 query_len:768 num_blocks:0 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56473)[0;0m INFO 08-09 13:56:09 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][10/38] batch_size:4 query_len:768 num_blocks:0 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56471)[0;0m INFO 08-09 13:56:09 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][10/38] batch_size:4 query_len:768 num_blocks:0 free_mem:23.73 GiB
INFO 08-09 13:56:09 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][10/38] batch_size:4 query_len:768 num_blocks:0 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56470)[0;0m INFO 08-09 13:56:09 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][10/38] batch_size:4 query_len:768 num_blocks:0 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56468)[0;0m INFO 08-09 13:56:09 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][10/38] batch_size:4 query_len:768 num_blocks:0 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56469)[0;0m INFO 08-09 13:56:09 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][10/38] batch_size:4 query_len:768 num_blocks:0 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56467)[0;0m INFO 08-09 13:56:09 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][10/38] batch_size:4 query_len:768 num_blocks:0 free_mem:23.73 GiB
INFO 08-09 13:56:11 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][11/38] batch_size:4 query_len:640 num_blocks:0 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56472)[0;0m INFO 08-09 13:56:11 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][11/38] batch_size:4 query_len:640 num_blocks:0 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56473)[0;0m INFO 08-09 13:56:11 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][11/38] batch_size:4 query_len:640 num_blocks:0 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56470)[0;0m INFO 08-09 13:56:11 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][11/38] batch_size:4 query_len:640 num_blocks:0 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56471)[0;0m INFO 08-09 13:56:11 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][11/38] batch_size:4 query_len:640 num_blocks:0 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56469)[0;0m INFO 08-09 13:56:11 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][11/38] batch_size:4 query_len:640 num_blocks:0 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56468)[0;0m INFO 08-09 13:56:11 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][11/38] batch_size:4 query_len:640 num_blocks:0 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56467)[0;0m INFO 08-09 13:56:11 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][11/38] batch_size:4 query_len:640 num_blocks:0 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56473)[0;0m INFO 08-09 13:56:14 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][12/38] batch_size:4 query_len:512 num_blocks:0 free_mem:23.73 GiB
INFO 08-09 13:56:14 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][12/38] batch_size:4 query_len:512 num_blocks:0 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56472)[0;0m INFO 08-09 13:56:14 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][12/38] batch_size:4 query_len:512 num_blocks:0 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56471)[0;0m INFO 08-09 13:56:14 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][12/38] batch_size:4 query_len:512 num_blocks:0 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56470)[0;0m INFO 08-09 13:56:14 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][12/38] batch_size:4 query_len:512 num_blocks:0 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56469)[0;0m INFO 08-09 13:56:14 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][12/38] batch_size:4 query_len:512 num_blocks:0 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56467)[0;0m INFO 08-09 13:56:14 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][12/38] batch_size:4 query_len:512 num_blocks:0 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56468)[0;0m INFO 08-09 13:56:14 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][12/38] batch_size:4 query_len:512 num_blocks:0 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56472)[0;0m INFO 08-09 13:56:17 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][13/38] batch_size:4 query_len:384 num_blocks:0 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56473)[0;0m INFO 08-09 13:56:17 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][13/38] batch_size:4 query_len:384 num_blocks:0 free_mem:23.73 GiB
INFO 08-09 13:56:17 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][13/38] batch_size:4 query_len:384 num_blocks:0 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56471)[0;0m INFO 08-09 13:56:17 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][13/38] batch_size:4 query_len:384 num_blocks:0 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56470)[0;0m INFO 08-09 13:56:17 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][13/38] batch_size:4 query_len:384 num_blocks:0 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56467)[0;0m INFO 08-09 13:56:17 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][13/38] batch_size:4 query_len:384 num_blocks:0 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56468)[0;0m INFO 08-09 13:56:17 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][13/38] batch_size:4 query_len:384 num_blocks:0 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56469)[0;0m INFO 08-09 13:56:17 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][13/38] batch_size:4 query_len:384 num_blocks:0 free_mem:23.73 GiB
INFO 08-09 13:56:19 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][14/38] batch_size:4 query_len:256 num_blocks:0 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56473)[0;0m INFO 08-09 13:56:19 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][14/38] batch_size:4 query_len:256 num_blocks:0 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56472)[0;0m INFO 08-09 13:56:19 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][14/38] batch_size:4 query_len:256 num_blocks:0 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56471)[0;0m INFO 08-09 13:56:19 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][14/38] batch_size:4 query_len:256 num_blocks:0 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56470)[0;0m INFO 08-09 13:56:19 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][14/38] batch_size:4 query_len:256 num_blocks:0 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56469)[0;0m INFO 08-09 13:56:19 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][14/38] batch_size:4 query_len:256 num_blocks:0 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56467)[0;0m INFO 08-09 13:56:19 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][14/38] batch_size:4 query_len:256 num_blocks:0 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56468)[0;0m INFO 08-09 13:56:19 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][14/38] batch_size:4 query_len:256 num_blocks:0 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56473)[0;0m INFO 08-09 13:56:22 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][15/38] batch_size:4 query_len:128 num_blocks:0 free_mem:23.73 GiB
INFO 08-09 13:56:22 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][15/38] batch_size:4 query_len:128 num_blocks:0 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56472)[0;0m INFO 08-09 13:56:22 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][15/38] batch_size:4 query_len:128 num_blocks:0 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56471)[0;0m INFO 08-09 13:56:22 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][15/38] batch_size:4 query_len:128 num_blocks:0 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56470)[0;0m INFO 08-09 13:56:22 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][15/38] batch_size:4 query_len:128 num_blocks:0 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56468)[0;0m INFO 08-09 13:56:22 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][15/38] batch_size:4 query_len:128 num_blocks:0 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56467)[0;0m INFO 08-09 13:56:22 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][15/38] batch_size:4 query_len:128 num_blocks:0 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56469)[0;0m INFO 08-09 13:56:22 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][15/38] batch_size:4 query_len:128 num_blocks:0 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56473)[0;0m INFO 08-09 13:56:25 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][16/38] batch_size:2 query_len:1792 num_blocks:0 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56472)[0;0m INFO 08-09 13:56:25 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][16/38] batch_size:2 query_len:1792 num_blocks:0 free_mem:23.73 GiB
INFO 08-09 13:56:25 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][16/38] batch_size:2 query_len:1792 num_blocks:0 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56470)[0;0m INFO 08-09 13:56:25 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][16/38] batch_size:2 query_len:1792 num_blocks:0 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56471)[0;0m INFO 08-09 13:56:25 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][16/38] batch_size:2 query_len:1792 num_blocks:0 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56469)[0;0m INFO 08-09 13:56:25 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][16/38] batch_size:2 query_len:1792 num_blocks:0 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56468)[0;0m INFO 08-09 13:56:25 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][16/38] batch_size:2 query_len:1792 num_blocks:0 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56467)[0;0m INFO 08-09 13:56:25 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][16/38] batch_size:2 query_len:1792 num_blocks:0 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56473)[0;0m INFO 08-09 13:56:28 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][17/38] batch_size:2 query_len:1408 num_blocks:0 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56472)[0;0m INFO 08-09 13:56:28 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][17/38] batch_size:2 query_len:1408 num_blocks:0 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56470)[0;0m INFO 08-09 13:56:28 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][17/38] batch_size:2 query_len:1408 num_blocks:0 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56471)[0;0m INFO 08-09 13:56:28 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][17/38] batch_size:2 query_len:1408 num_blocks:0 free_mem:23.73 GiB
INFO 08-09 13:56:28 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][17/38] batch_size:2 query_len:1408 num_blocks:0 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56469)[0;0m INFO 08-09 13:56:28 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][17/38] batch_size:2 query_len:1408 num_blocks:0 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56468)[0;0m INFO 08-09 13:56:28 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][17/38] batch_size:2 query_len:1408 num_blocks:0 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56467)[0;0m INFO 08-09 13:56:28 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][17/38] batch_size:2 query_len:1408 num_blocks:0 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56472)[0;0m INFO 08-09 13:56:31 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][18/38] batch_size:2 query_len:1024 num_blocks:0 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56473)[0;0m INFO 08-09 13:56:31 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][18/38] batch_size:2 query_len:1024 num_blocks:0 free_mem:23.73 GiB
INFO 08-09 13:56:31 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][18/38] batch_size:2 query_len:1024 num_blocks:0 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56471)[0;0m INFO 08-09 13:56:31 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][18/38] batch_size:2 query_len:1024 num_blocks:0 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56467)[0;0m INFO 08-09 13:56:31 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][18/38] batch_size:2 query_len:1024 num_blocks:0 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56469)[0;0m INFO 08-09 13:56:31 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][18/38] batch_size:2 query_len:1024 num_blocks:0 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56468)[0;0m INFO 08-09 13:56:31 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][18/38] batch_size:2 query_len:1024 num_blocks:0 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56470)[0;0m INFO 08-09 13:56:31 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][18/38] batch_size:2 query_len:1024 num_blocks:0 free_mem:23.73 GiB
INFO 08-09 13:56:34 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][19/38] batch_size:2 query_len:896 num_blocks:0 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56472)[0;0m INFO 08-09 13:56:34 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][19/38] batch_size:2 query_len:896 num_blocks:0 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56471)[0;0m INFO 08-09 13:56:34 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][19/38] batch_size:2 query_len:896 num_blocks:0 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56470)[0;0m INFO 08-09 13:56:34 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][19/38] batch_size:2 query_len:896 num_blocks:0 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56473)[0;0m INFO 08-09 13:56:34 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][19/38] batch_size:2 query_len:896 num_blocks:0 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56469)[0;0m INFO 08-09 13:56:34 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][19/38] batch_size:2 query_len:896 num_blocks:0 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56468)[0;0m INFO 08-09 13:56:34 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][19/38] batch_size:2 query_len:896 num_blocks:0 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56467)[0;0m INFO 08-09 13:56:34 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][19/38] batch_size:2 query_len:896 num_blocks:0 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56473)[0;0m INFO 08-09 13:56:37 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][20/38] batch_size:2 query_len:768 num_blocks:0 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56472)[0;0m INFO 08-09 13:56:37 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][20/38] batch_size:2 query_len:768 num_blocks:0 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56471)[0;0m INFO 08-09 13:56:37 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][20/38] batch_size:2 query_len:768 num_blocks:0 free_mem:23.73 GiB
INFO 08-09 13:56:37 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][20/38] batch_size:2 query_len:768 num_blocks:0 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56470)[0;0m INFO 08-09 13:56:37 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][20/38] batch_size:2 query_len:768 num_blocks:0 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56469)[0;0m INFO 08-09 13:56:37 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][20/38] batch_size:2 query_len:768 num_blocks:0 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56467)[0;0m INFO 08-09 13:56:37 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][20/38] batch_size:2 query_len:768 num_blocks:0 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56468)[0;0m INFO 08-09 13:56:37 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][20/38] batch_size:2 query_len:768 num_blocks:0 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56472)[0;0m INFO 08-09 13:56:39 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][21/38] batch_size:2 query_len:640 num_blocks:0 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56469)[0;0m INFO 08-09 13:56:39 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][21/38] batch_size:2 query_len:640 num_blocks:0 free_mem:23.73 GiB
INFO 08-09 13:56:39 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][21/38] batch_size:2 query_len:640 num_blocks:0 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56471)[0;0m INFO 08-09 13:56:39 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][21/38] batch_size:2 query_len:640 num_blocks:0 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56473)[0;0m INFO 08-09 13:56:39 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][21/38] batch_size:2 query_len:640 num_blocks:0 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56470)[0;0m INFO 08-09 13:56:39 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][21/38] batch_size:2 query_len:640 num_blocks:0 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56468)[0;0m INFO 08-09 13:56:39 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][21/38] batch_size:2 query_len:640 num_blocks:0 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56467)[0;0m INFO 08-09 13:56:39 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][21/38] batch_size:2 query_len:640 num_blocks:0 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56473)[0;0m INFO 08-09 13:56:42 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][22/38] batch_size:2 query_len:512 num_blocks:0 free_mem:23.73 GiB
INFO 08-09 13:56:42 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][22/38] batch_size:2 query_len:512 num_blocks:0 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56472)[0;0m INFO 08-09 13:56:42 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][22/38] batch_size:2 query_len:512 num_blocks:0 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56471)[0;0m INFO 08-09 13:56:42 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][22/38] batch_size:2 query_len:512 num_blocks:0 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56470)[0;0m INFO 08-09 13:56:42 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][22/38] batch_size:2 query_len:512 num_blocks:0 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56467)[0;0m INFO 08-09 13:56:42 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][22/38] batch_size:2 query_len:512 num_blocks:0 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56468)[0;0m INFO 08-09 13:56:42 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][22/38] batch_size:2 query_len:512 num_blocks:0 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56469)[0;0m INFO 08-09 13:56:42 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][22/38] batch_size:2 query_len:512 num_blocks:0 free_mem:23.73 GiB
INFO 08-09 13:56:45 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][23/38] batch_size:2 query_len:384 num_blocks:0 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56472)[0;0m INFO 08-09 13:56:45 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][23/38] batch_size:2 query_len:384 num_blocks:0 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56471)[0;0m INFO 08-09 13:56:45 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][23/38] batch_size:2 query_len:384 num_blocks:0 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56470)[0;0m INFO 08-09 13:56:45 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][23/38] batch_size:2 query_len:384 num_blocks:0 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56473)[0;0m INFO 08-09 13:56:45 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][23/38] batch_size:2 query_len:384 num_blocks:0 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56469)[0;0m INFO 08-09 13:56:45 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][23/38] batch_size:2 query_len:384 num_blocks:0 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56467)[0;0m INFO 08-09 13:56:45 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][23/38] batch_size:2 query_len:384 num_blocks:0 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56468)[0;0m INFO 08-09 13:56:45 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][23/38] batch_size:2 query_len:384 num_blocks:0 free_mem:23.73 GiB
INFO 08-09 13:56:48 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][24/38] batch_size:2 query_len:256 num_blocks:0 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56472)[0;0m INFO 08-09 13:56:48 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][24/38] batch_size:2 query_len:256 num_blocks:0 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56470)[0;0m INFO 08-09 13:56:48 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][24/38] batch_size:2 query_len:256 num_blocks:0 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56471)[0;0m INFO 08-09 13:56:48 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][24/38] batch_size:2 query_len:256 num_blocks:0 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56469)[0;0m INFO 08-09 13:56:48 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][24/38] batch_size:2 query_len:256 num_blocks:0 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56467)[0;0m INFO 08-09 13:56:48 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][24/38] batch_size:2 query_len:256 num_blocks:0 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56473)[0;0m INFO 08-09 13:56:48 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][24/38] batch_size:2 query_len:256 num_blocks:0 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56468)[0;0m INFO 08-09 13:56:48 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][24/38] batch_size:2 query_len:256 num_blocks:0 free_mem:23.73 GiB
INFO 08-09 13:56:51 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][25/38] batch_size:2 query_len:128 num_blocks:0 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56473)[0;0m INFO 08-09 13:56:51 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][25/38] batch_size:2 query_len:128 num_blocks:0 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56472)[0;0m INFO 08-09 13:56:51 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][25/38] batch_size:2 query_len:128 num_blocks:0 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56471)[0;0m INFO 08-09 13:56:51 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][25/38] batch_size:2 query_len:128 num_blocks:0 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56470)[0;0m INFO 08-09 13:56:51 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][25/38] batch_size:2 query_len:128 num_blocks:0 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56469)[0;0m INFO 08-09 13:56:51 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][25/38] batch_size:2 query_len:128 num_blocks:0 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56467)[0;0m INFO 08-09 13:56:51 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][25/38] batch_size:2 query_len:128 num_blocks:0 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56468)[0;0m INFO 08-09 13:56:51 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][25/38] batch_size:2 query_len:128 num_blocks:0 free_mem:23.73 GiB
INFO 08-09 13:56:54 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][26/38] batch_size:1 query_len:4096 num_blocks:0 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56473)[0;0m INFO 08-09 13:56:54 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][26/38] batch_size:1 query_len:4096 num_blocks:0 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56472)[0;0m INFO 08-09 13:56:54 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][26/38] batch_size:1 query_len:4096 num_blocks:0 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56471)[0;0m INFO 08-09 13:56:54 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][26/38] batch_size:1 query_len:4096 num_blocks:0 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56470)[0;0m INFO 08-09 13:56:54 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][26/38] batch_size:1 query_len:4096 num_blocks:0 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56469)[0;0m INFO 08-09 13:56:54 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][26/38] batch_size:1 query_len:4096 num_blocks:0 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56468)[0;0m INFO 08-09 13:56:54 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][26/38] batch_size:1 query_len:4096 num_blocks:0 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56467)[0;0m INFO 08-09 13:56:54 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][26/38] batch_size:1 query_len:4096 num_blocks:0 free_mem:23.73 GiB
INFO 08-09 13:56:57 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][27/38] batch_size:1 query_len:3072 num_blocks:0 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56473)[0;0m INFO 08-09 13:56:57 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][27/38] batch_size:1 query_len:3072 num_blocks:0 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56472)[0;0m INFO 08-09 13:56:57 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][27/38] batch_size:1 query_len:3072 num_blocks:0 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56471)[0;0m INFO 08-09 13:56:57 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][27/38] batch_size:1 query_len:3072 num_blocks:0 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56470)[0;0m INFO 08-09 13:56:57 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][27/38] batch_size:1 query_len:3072 num_blocks:0 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56469)[0;0m INFO 08-09 13:56:57 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][27/38] batch_size:1 query_len:3072 num_blocks:0 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56468)[0;0m INFO 08-09 13:56:57 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][27/38] batch_size:1 query_len:3072 num_blocks:0 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56467)[0;0m INFO 08-09 13:56:57 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][27/38] batch_size:1 query_len:3072 num_blocks:0 free_mem:23.73 GiB
INFO 08-09 13:57:00 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][28/38] batch_size:1 query_len:2304 num_blocks:0 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56472)[0;0m INFO 08-09 13:57:00 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][28/38] batch_size:1 query_len:2304 num_blocks:0 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56471)[0;0m INFO 08-09 13:57:00 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][28/38] batch_size:1 query_len:2304 num_blocks:0 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56473)[0;0m INFO 08-09 13:57:00 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][28/38] batch_size:1 query_len:2304 num_blocks:0 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56468)[0;0m INFO 08-09 13:57:00 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][28/38] batch_size:1 query_len:2304 num_blocks:0 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56470)[0;0m INFO 08-09 13:57:00 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][28/38] batch_size:1 query_len:2304 num_blocks:0 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56467)[0;0m INFO 08-09 13:57:00 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][28/38] batch_size:1 query_len:2304 num_blocks:0 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56469)[0;0m INFO 08-09 13:57:00 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][28/38] batch_size:1 query_len:2304 num_blocks:0 free_mem:23.73 GiB
INFO 08-09 13:57:04 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][29/38] batch_size:1 query_len:1792 num_blocks:0 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56473)[0;0m INFO 08-09 13:57:04 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][29/38] batch_size:1 query_len:1792 num_blocks:0 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56472)[0;0m INFO 08-09 13:57:04 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][29/38] batch_size:1 query_len:1792 num_blocks:0 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56471)[0;0m INFO 08-09 13:57:04 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][29/38] batch_size:1 query_len:1792 num_blocks:0 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56470)[0;0m INFO 08-09 13:57:04 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][29/38] batch_size:1 query_len:1792 num_blocks:0 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56469)[0;0m INFO 08-09 13:57:04 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][29/38] batch_size:1 query_len:1792 num_blocks:0 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56468)[0;0m INFO 08-09 13:57:04 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][29/38] batch_size:1 query_len:1792 num_blocks:0 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56467)[0;0m INFO 08-09 13:57:04 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][29/38] batch_size:1 query_len:1792 num_blocks:0 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56472)[0;0m INFO 08-09 13:57:07 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][30/38] batch_size:1 query_len:1408 num_blocks:0 free_mem:23.73 GiB
INFO 08-09 13:57:07 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][30/38] batch_size:1 query_len:1408 num_blocks:0 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56471)[0;0m INFO 08-09 13:57:07 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][30/38] batch_size:1 query_len:1408 num_blocks:0 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56470)[0;0m INFO 08-09 13:57:07 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][30/38] batch_size:1 query_len:1408 num_blocks:0 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56473)[0;0m INFO 08-09 13:57:07 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][30/38] batch_size:1 query_len:1408 num_blocks:0 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56469)[0;0m INFO 08-09 13:57:07 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][30/38] batch_size:1 query_len:1408 num_blocks:0 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56468)[0;0m INFO 08-09 13:57:07 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][30/38] batch_size:1 query_len:1408 num_blocks:0 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56467)[0;0m INFO 08-09 13:57:07 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][30/38] batch_size:1 query_len:1408 num_blocks:0 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56473)[0;0m INFO 08-09 13:57:10 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][31/38] batch_size:1 query_len:1024 num_blocks:0 free_mem:23.73 GiB
INFO 08-09 13:57:10 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][31/38] batch_size:1 query_len:1024 num_blocks:0 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56472)[0;0m INFO 08-09 13:57:10 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][31/38] batch_size:1 query_len:1024 num_blocks:0 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56471)[0;0m INFO 08-09 13:57:10 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][31/38] batch_size:1 query_len:1024 num_blocks:0 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56470)[0;0m INFO 08-09 13:57:10 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][31/38] batch_size:1 query_len:1024 num_blocks:0 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56469)[0;0m INFO 08-09 13:57:10 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][31/38] batch_size:1 query_len:1024 num_blocks:0 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56468)[0;0m INFO 08-09 13:57:10 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][31/38] batch_size:1 query_len:1024 num_blocks:0 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56467)[0;0m INFO 08-09 13:57:10 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][31/38] batch_size:1 query_len:1024 num_blocks:0 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56472)[0;0m INFO 08-09 13:57:14 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][32/38] batch_size:1 query_len:896 num_blocks:0 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56471)[0;0m INFO 08-09 13:57:14 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][32/38] batch_size:1 query_len:896 num_blocks:0 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56470)[0;0m INFO 08-09 13:57:14 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][32/38] batch_size:1 query_len:896 num_blocks:0 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56468)[0;0m INFO 08-09 13:57:14 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][32/38] batch_size:1 query_len:896 num_blocks:0 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56473)[0;0m INFO 08-09 13:57:14 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][32/38] batch_size:1 query_len:896 num_blocks:0 free_mem:23.73 GiB
INFO 08-09 13:57:14 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][32/38] batch_size:1 query_len:896 num_blocks:0 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56467)[0;0m INFO 08-09 13:57:14 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][32/38] batch_size:1 query_len:896 num_blocks:0 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56469)[0;0m INFO 08-09 13:57:14 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][32/38] batch_size:1 query_len:896 num_blocks:0 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56472)[0;0m INFO 08-09 13:57:17 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][33/38] batch_size:1 query_len:768 num_blocks:0 free_mem:23.73 GiB
INFO 08-09 13:57:17 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][33/38] batch_size:1 query_len:768 num_blocks:0 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56473)[0;0m INFO 08-09 13:57:17 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][33/38] batch_size:1 query_len:768 num_blocks:0 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56468)[0;0m INFO 08-09 13:57:17 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][33/38] batch_size:1 query_len:768 num_blocks:0 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56471)[0;0m INFO 08-09 13:57:17 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][33/38] batch_size:1 query_len:768 num_blocks:0 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56470)[0;0m INFO 08-09 13:57:17 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][33/38] batch_size:1 query_len:768 num_blocks:0 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56469)[0;0m INFO 08-09 13:57:17 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][33/38] batch_size:1 query_len:768 num_blocks:0 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56467)[0;0m INFO 08-09 13:57:17 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][33/38] batch_size:1 query_len:768 num_blocks:0 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56473)[0;0m INFO 08-09 13:57:21 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][34/38] batch_size:1 query_len:640 num_blocks:0 free_mem:23.73 GiB
INFO 08-09 13:57:21 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][34/38] batch_size:1 query_len:640 num_blocks:0 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56471)[0;0m INFO 08-09 13:57:21 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][34/38] batch_size:1 query_len:640 num_blocks:0 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56468)[0;0m INFO 08-09 13:57:21 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][34/38] batch_size:1 query_len:640 num_blocks:0 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56470)[0;0m INFO 08-09 13:57:21 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][34/38] batch_size:1 query_len:640 num_blocks:0 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56469)[0;0m INFO 08-09 13:57:21 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][34/38] batch_size:1 query_len:640 num_blocks:0 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56472)[0;0m INFO 08-09 13:57:21 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][34/38] batch_size:1 query_len:640 num_blocks:0 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56467)[0;0m INFO 08-09 13:57:21 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][34/38] batch_size:1 query_len:640 num_blocks:0 free_mem:23.73 GiB
INFO 08-09 13:57:24 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][35/38] batch_size:1 query_len:512 num_blocks:0 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56473)[0;0m INFO 08-09 13:57:24 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][35/38] batch_size:1 query_len:512 num_blocks:0 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56472)[0;0m INFO 08-09 13:57:24 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][35/38] batch_size:1 query_len:512 num_blocks:0 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56471)[0;0m INFO 08-09 13:57:24 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][35/38] batch_size:1 query_len:512 num_blocks:0 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56468)[0;0m INFO 08-09 13:57:24 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][35/38] batch_size:1 query_len:512 num_blocks:0 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56470)[0;0m INFO 08-09 13:57:24 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][35/38] batch_size:1 query_len:512 num_blocks:0 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56467)[0;0m INFO 08-09 13:57:24 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][35/38] batch_size:1 query_len:512 num_blocks:0 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56469)[0;0m INFO 08-09 13:57:24 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][35/38] batch_size:1 query_len:512 num_blocks:0 free_mem:23.73 GiB
INFO 08-09 13:57:27 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][36/38] batch_size:1 query_len:384 num_blocks:0 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56473)[0;0m INFO 08-09 13:57:27 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][36/38] batch_size:1 query_len:384 num_blocks:0 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56472)[0;0m INFO 08-09 13:57:27 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][36/38] batch_size:1 query_len:384 num_blocks:0 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56471)[0;0m INFO 08-09 13:57:27 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][36/38] batch_size:1 query_len:384 num_blocks:0 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56470)[0;0m INFO 08-09 13:57:27 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][36/38] batch_size:1 query_len:384 num_blocks:0 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56468)[0;0m INFO 08-09 13:57:27 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][36/38] batch_size:1 query_len:384 num_blocks:0 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56469)[0;0m INFO 08-09 13:57:27 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][36/38] batch_size:1 query_len:384 num_blocks:0 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56467)[0;0m INFO 08-09 13:57:27 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][36/38] batch_size:1 query_len:384 num_blocks:0 free_mem:23.73 GiB
INFO 08-09 13:57:31 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][37/38] batch_size:1 query_len:256 num_blocks:0 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56472)[0;0m INFO 08-09 13:57:31 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][37/38] batch_size:1 query_len:256 num_blocks:0 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56471)[0;0m INFO 08-09 13:57:31 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][37/38] batch_size:1 query_len:256 num_blocks:0 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56473)[0;0m INFO 08-09 13:57:31 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][37/38] batch_size:1 query_len:256 num_blocks:0 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56470)[0;0m INFO 08-09 13:57:31 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][37/38] batch_size:1 query_len:256 num_blocks:0 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56467)[0;0m INFO 08-09 13:57:31 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][37/38] batch_size:1 query_len:256 num_blocks:0 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56468)[0;0m INFO 08-09 13:57:31 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][37/38] batch_size:1 query_len:256 num_blocks:0 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56469)[0;0m INFO 08-09 13:57:31 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][37/38] batch_size:1 query_len:256 num_blocks:0 free_mem:23.73 GiB
INFO 08-09 13:57:34 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][38/38] batch_size:1 query_len:128 num_blocks:0 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56472)[0;0m INFO 08-09 13:57:34 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][38/38] batch_size:1 query_len:128 num_blocks:0 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56473)[0;0m INFO 08-09 13:57:34 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][38/38] batch_size:1 query_len:128 num_blocks:0 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56471)[0;0m INFO 08-09 13:57:34 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][38/38] batch_size:1 query_len:128 num_blocks:0 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56470)[0;0m INFO 08-09 13:57:34 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][38/38] batch_size:1 query_len:128 num_blocks:0 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56468)[0;0m INFO 08-09 13:57:34 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][38/38] batch_size:1 query_len:128 num_blocks:0 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56469)[0;0m INFO 08-09 13:57:34 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][38/38] batch_size:1 query_len:128 num_blocks:0 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56467)[0;0m INFO 08-09 13:57:34 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][38/38] batch_size:1 query_len:128 num_blocks:0 free_mem:23.73 GiB
INFO 08-09 13:57:38 [hpu_model_runner.py:3029] [Warmup][Graph/decode][1/135] batch_size:256 query_len:1 num_blocks:12946 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56473)[0;0m INFO 08-09 13:57:38 [hpu_model_runner.py:3029] [Warmup][Graph/decode][1/135] batch_size:256 query_len:1 num_blocks:12946 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56472)[0;0m INFO 08-09 13:57:38 [hpu_model_runner.py:3029] [Warmup][Graph/decode][1/135] batch_size:256 query_len:1 num_blocks:12946 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56471)[0;0m INFO 08-09 13:57:38 [hpu_model_runner.py:3029] [Warmup][Graph/decode][1/135] batch_size:256 query_len:1 num_blocks:12946 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56470)[0;0m INFO 08-09 13:57:38 [hpu_model_runner.py:3029] [Warmup][Graph/decode][1/135] batch_size:256 query_len:1 num_blocks:12946 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56469)[0;0m INFO 08-09 13:57:38 [hpu_model_runner.py:3029] [Warmup][Graph/decode][1/135] batch_size:256 query_len:1 num_blocks:12946 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56468)[0;0m INFO 08-09 13:57:38 [hpu_model_runner.py:3029] [Warmup][Graph/decode][1/135] batch_size:256 query_len:1 num_blocks:12946 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56467)[0;0m INFO 08-09 13:57:38 [hpu_model_runner.py:3029] [Warmup][Graph/decode][1/135] batch_size:256 query_len:1 num_blocks:12946 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56472)[0;0m INFO 08-09 13:57:42 [hpu_model_runner.py:3029] [Warmup][Graph/decode][2/135] batch_size:256 query_len:1 num_blocks:9344 free_mem:23.73 GiB
INFO 08-09 13:57:42 [hpu_model_runner.py:3029] [Warmup][Graph/decode][2/135] batch_size:256 query_len:1 num_blocks:9344 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56473)[0;0m INFO 08-09 13:57:42 [hpu_model_runner.py:3029] [Warmup][Graph/decode][2/135] batch_size:256 query_len:1 num_blocks:9344 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56471)[0;0m INFO 08-09 13:57:42 [hpu_model_runner.py:3029] [Warmup][Graph/decode][2/135] batch_size:256 query_len:1 num_blocks:9344 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56470)[0;0m INFO 08-09 13:57:42 [hpu_model_runner.py:3029] [Warmup][Graph/decode][2/135] batch_size:256 query_len:1 num_blocks:9344 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56468)[0;0m INFO 08-09 13:57:42 [hpu_model_runner.py:3029] [Warmup][Graph/decode][2/135] batch_size:256 query_len:1 num_blocks:9344 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56469)[0;0m INFO 08-09 13:57:42 [hpu_model_runner.py:3029] [Warmup][Graph/decode][2/135] batch_size:256 query_len:1 num_blocks:9344 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56467)[0;0m INFO 08-09 13:57:42 [hpu_model_runner.py:3029] [Warmup][Graph/decode][2/135] batch_size:256 query_len:1 num_blocks:9344 free_mem:23.73 GiB
INFO 08-09 13:57:46 [hpu_model_runner.py:3029] [Warmup][Graph/decode][3/135] batch_size:256 query_len:1 num_blocks:6784 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56472)[0;0m INFO 08-09 13:57:46 [hpu_model_runner.py:3029] [Warmup][Graph/decode][3/135] batch_size:256 query_len:1 num_blocks:6784 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56473)[0;0m INFO 08-09 13:57:46 [hpu_model_runner.py:3029] [Warmup][Graph/decode][3/135] batch_size:256 query_len:1 num_blocks:6784 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56471)[0;0m INFO 08-09 13:57:46 [hpu_model_runner.py:3029] [Warmup][Graph/decode][3/135] batch_size:256 query_len:1 num_blocks:6784 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56468)[0;0m INFO 08-09 13:57:46 [hpu_model_runner.py:3029] [Warmup][Graph/decode][3/135] batch_size:256 query_len:1 num_blocks:6784 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56470)[0;0m INFO 08-09 13:57:46 [hpu_model_runner.py:3029] [Warmup][Graph/decode][3/135] batch_size:256 query_len:1 num_blocks:6784 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56469)[0;0m INFO 08-09 13:57:46 [hpu_model_runner.py:3029] [Warmup][Graph/decode][3/135] batch_size:256 query_len:1 num_blocks:6784 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56467)[0;0m INFO 08-09 13:57:46 [hpu_model_runner.py:3029] [Warmup][Graph/decode][3/135] batch_size:256 query_len:1 num_blocks:6784 free_mem:23.73 GiB
INFO 08-09 13:57:50 [hpu_model_runner.py:3029] [Warmup][Graph/decode][4/135] batch_size:256 query_len:1 num_blocks:4864 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56472)[0;0m INFO 08-09 13:57:50 [hpu_model_runner.py:3029] [Warmup][Graph/decode][4/135] batch_size:256 query_len:1 num_blocks:4864 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56473)[0;0m INFO 08-09 13:57:50 [hpu_model_runner.py:3029] [Warmup][Graph/decode][4/135] batch_size:256 query_len:1 num_blocks:4864 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56471)[0;0m INFO 08-09 13:57:50 [hpu_model_runner.py:3029] [Warmup][Graph/decode][4/135] batch_size:256 query_len:1 num_blocks:4864 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56470)[0;0m INFO 08-09 13:57:50 [hpu_model_runner.py:3029] [Warmup][Graph/decode][4/135] batch_size:256 query_len:1 num_blocks:4864 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56467)[0;0m INFO 08-09 13:57:50 [hpu_model_runner.py:3029] [Warmup][Graph/decode][4/135] batch_size:256 query_len:1 num_blocks:4864 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56469)[0;0m INFO 08-09 13:57:50 [hpu_model_runner.py:3029] [Warmup][Graph/decode][4/135] batch_size:256 query_len:1 num_blocks:4864 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56468)[0;0m INFO 08-09 13:57:50 [hpu_model_runner.py:3029] [Warmup][Graph/decode][4/135] batch_size:256 query_len:1 num_blocks:4864 free_mem:23.73 GiB
INFO 08-09 13:57:54 [hpu_model_runner.py:3029] [Warmup][Graph/decode][5/135] batch_size:256 query_len:1 num_blocks:3584 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56473)[0;0m INFO 08-09 13:57:54 [hpu_model_runner.py:3029] [Warmup][Graph/decode][5/135] batch_size:256 query_len:1 num_blocks:3584 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56472)[0;0m INFO 08-09 13:57:54 [hpu_model_runner.py:3029] [Warmup][Graph/decode][5/135] batch_size:256 query_len:1 num_blocks:3584 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56471)[0;0m INFO 08-09 13:57:54 [hpu_model_runner.py:3029] [Warmup][Graph/decode][5/135] batch_size:256 query_len:1 num_blocks:3584 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56470)[0;0m INFO 08-09 13:57:54 [hpu_model_runner.py:3029] [Warmup][Graph/decode][5/135] batch_size:256 query_len:1 num_blocks:3584 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56469)[0;0m INFO 08-09 13:57:54 [hpu_model_runner.py:3029] [Warmup][Graph/decode][5/135] batch_size:256 query_len:1 num_blocks:3584 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56468)[0;0m INFO 08-09 13:57:54 [hpu_model_runner.py:3029] [Warmup][Graph/decode][5/135] batch_size:256 query_len:1 num_blocks:3584 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56467)[0;0m INFO 08-09 13:57:54 [hpu_model_runner.py:3029] [Warmup][Graph/decode][5/135] batch_size:256 query_len:1 num_blocks:3584 free_mem:23.73 GiB
INFO 08-09 13:57:58 [hpu_model_runner.py:3029] [Warmup][Graph/decode][6/135] batch_size:256 query_len:1 num_blocks:2560 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56472)[0;0m INFO 08-09 13:57:58 [hpu_model_runner.py:3029] [Warmup][Graph/decode][6/135] batch_size:256 query_len:1 num_blocks:2560 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56471)[0;0m INFO 08-09 13:57:58 [hpu_model_runner.py:3029] [Warmup][Graph/decode][6/135] batch_size:256 query_len:1 num_blocks:2560 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56473)[0;0m INFO 08-09 13:57:58 [hpu_model_runner.py:3029] [Warmup][Graph/decode][6/135] batch_size:256 query_len:1 num_blocks:2560 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56470)[0;0m INFO 08-09 13:57:58 [hpu_model_runner.py:3029] [Warmup][Graph/decode][6/135] batch_size:256 query_len:1 num_blocks:2560 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56468)[0;0m INFO 08-09 13:57:58 [hpu_model_runner.py:3029] [Warmup][Graph/decode][6/135] batch_size:256 query_len:1 num_blocks:2560 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56469)[0;0m INFO 08-09 13:57:58 [hpu_model_runner.py:3029] [Warmup][Graph/decode][6/135] batch_size:256 query_len:1 num_blocks:2560 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56467)[0;0m INFO 08-09 13:57:58 [hpu_model_runner.py:3029] [Warmup][Graph/decode][6/135] batch_size:256 query_len:1 num_blocks:2560 free_mem:23.73 GiB
INFO 08-09 13:58:02 [hpu_model_runner.py:3029] [Warmup][Graph/decode][7/135] batch_size:256 query_len:1 num_blocks:1792 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56472)[0;0m INFO 08-09 13:58:02 [hpu_model_runner.py:3029] [Warmup][Graph/decode][7/135] batch_size:256 query_len:1 num_blocks:1792 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56471)[0;0m INFO 08-09 13:58:02 [hpu_model_runner.py:3029] [Warmup][Graph/decode][7/135] batch_size:256 query_len:1 num_blocks:1792 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56473)[0;0m INFO 08-09 13:58:02 [hpu_model_runner.py:3029] [Warmup][Graph/decode][7/135] batch_size:256 query_len:1 num_blocks:1792 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56470)[0;0m INFO 08-09 13:58:02 [hpu_model_runner.py:3029] [Warmup][Graph/decode][7/135] batch_size:256 query_len:1 num_blocks:1792 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56467)[0;0m INFO 08-09 13:58:02 [hpu_model_runner.py:3029] [Warmup][Graph/decode][7/135] batch_size:256 query_len:1 num_blocks:1792 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56469)[0;0m INFO 08-09 13:58:02 [hpu_model_runner.py:3029] [Warmup][Graph/decode][7/135] batch_size:256 query_len:1 num_blocks:1792 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56468)[0;0m INFO 08-09 13:58:02 [hpu_model_runner.py:3029] [Warmup][Graph/decode][7/135] batch_size:256 query_len:1 num_blocks:1792 free_mem:23.73 GiB
INFO 08-09 13:58:06 [hpu_model_runner.py:3029] [Warmup][Graph/decode][8/135] batch_size:256 query_len:1 num_blocks:1408 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56472)[0;0m INFO 08-09 13:58:06 [hpu_model_runner.py:3029] [Warmup][Graph/decode][8/135] batch_size:256 query_len:1 num_blocks:1408 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56473)[0;0m INFO 08-09 13:58:06 [hpu_model_runner.py:3029] [Warmup][Graph/decode][8/135] batch_size:256 query_len:1 num_blocks:1408 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56471)[0;0m INFO 08-09 13:58:06 [hpu_model_runner.py:3029] [Warmup][Graph/decode][8/135] batch_size:256 query_len:1 num_blocks:1408 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56470)[0;0m INFO 08-09 13:58:06 [hpu_model_runner.py:3029] [Warmup][Graph/decode][8/135] batch_size:256 query_len:1 num_blocks:1408 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56468)[0;0m INFO 08-09 13:58:06 [hpu_model_runner.py:3029] [Warmup][Graph/decode][8/135] batch_size:256 query_len:1 num_blocks:1408 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56467)[0;0m INFO 08-09 13:58:06 [hpu_model_runner.py:3029] [Warmup][Graph/decode][8/135] batch_size:256 query_len:1 num_blocks:1408 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56469)[0;0m INFO 08-09 13:58:06 [hpu_model_runner.py:3029] [Warmup][Graph/decode][8/135] batch_size:256 query_len:1 num_blocks:1408 free_mem:23.73 GiB
INFO 08-09 13:58:10 [hpu_model_runner.py:3029] [Warmup][Graph/decode][9/135] batch_size:256 query_len:1 num_blocks:1024 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56473)[0;0m INFO 08-09 13:58:10 [hpu_model_runner.py:3029] [Warmup][Graph/decode][9/135] batch_size:256 query_len:1 num_blocks:1024 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56472)[0;0m INFO 08-09 13:58:10 [hpu_model_runner.py:3029] [Warmup][Graph/decode][9/135] batch_size:256 query_len:1 num_blocks:1024 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56471)[0;0m INFO 08-09 13:58:10 [hpu_model_runner.py:3029] [Warmup][Graph/decode][9/135] batch_size:256 query_len:1 num_blocks:1024 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56470)[0;0m INFO 08-09 13:58:10 [hpu_model_runner.py:3029] [Warmup][Graph/decode][9/135] batch_size:256 query_len:1 num_blocks:1024 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56469)[0;0m INFO 08-09 13:58:10 [hpu_model_runner.py:3029] [Warmup][Graph/decode][9/135] batch_size:256 query_len:1 num_blocks:1024 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56467)[0;0m INFO 08-09 13:58:10 [hpu_model_runner.py:3029] [Warmup][Graph/decode][9/135] batch_size:256 query_len:1 num_blocks:1024 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56468)[0;0m INFO 08-09 13:58:10 [hpu_model_runner.py:3029] [Warmup][Graph/decode][9/135] batch_size:256 query_len:1 num_blocks:1024 free_mem:23.73 GiB
INFO 08-09 13:58:14 [hpu_model_runner.py:3029] [Warmup][Graph/decode][10/135] batch_size:256 query_len:1 num_blocks:768 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56473)[0;0m INFO 08-09 13:58:14 [hpu_model_runner.py:3029] [Warmup][Graph/decode][10/135] batch_size:256 query_len:1 num_blocks:768 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56472)[0;0m INFO 08-09 13:58:14 [hpu_model_runner.py:3029] [Warmup][Graph/decode][10/135] batch_size:256 query_len:1 num_blocks:768 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56471)[0;0m INFO 08-09 13:58:14 [hpu_model_runner.py:3029] [Warmup][Graph/decode][10/135] batch_size:256 query_len:1 num_blocks:768 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56470)[0;0m INFO 08-09 13:58:14 [hpu_model_runner.py:3029] [Warmup][Graph/decode][10/135] batch_size:256 query_len:1 num_blocks:768 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56468)[0;0m INFO 08-09 13:58:14 [hpu_model_runner.py:3029] [Warmup][Graph/decode][10/135] batch_size:256 query_len:1 num_blocks:768 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56469)[0;0m INFO 08-09 13:58:14 [hpu_model_runner.py:3029] [Warmup][Graph/decode][10/135] batch_size:256 query_len:1 num_blocks:768 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56467)[0;0m INFO 08-09 13:58:14 [hpu_model_runner.py:3029] [Warmup][Graph/decode][10/135] batch_size:256 query_len:1 num_blocks:768 free_mem:23.73 GiB
INFO 08-09 13:58:18 [hpu_model_runner.py:3029] [Warmup][Graph/decode][11/135] batch_size:256 query_len:1 num_blocks:640 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56472)[0;0m INFO 08-09 13:58:18 [hpu_model_runner.py:3029] [Warmup][Graph/decode][11/135] batch_size:256 query_len:1 num_blocks:640 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56468)[0;0m INFO 08-09 13:58:18 [hpu_model_runner.py:3029] [Warmup][Graph/decode][11/135] batch_size:256 query_len:1 num_blocks:640 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56473)[0;0m INFO 08-09 13:58:18 [hpu_model_runner.py:3029] [Warmup][Graph/decode][11/135] batch_size:256 query_len:1 num_blocks:640 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56471)[0;0m INFO 08-09 13:58:18 [hpu_model_runner.py:3029] [Warmup][Graph/decode][11/135] batch_size:256 query_len:1 num_blocks:640 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56470)[0;0m INFO 08-09 13:58:18 [hpu_model_runner.py:3029] [Warmup][Graph/decode][11/135] batch_size:256 query_len:1 num_blocks:640 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56469)[0;0m INFO 08-09 13:58:18 [hpu_model_runner.py:3029] [Warmup][Graph/decode][11/135] batch_size:256 query_len:1 num_blocks:640 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56467)[0;0m INFO 08-09 13:58:18 [hpu_model_runner.py:3029] [Warmup][Graph/decode][11/135] batch_size:256 query_len:1 num_blocks:640 free_mem:23.73 GiB
INFO 08-09 13:58:22 [hpu_model_runner.py:3029] [Warmup][Graph/decode][12/135] batch_size:256 query_len:1 num_blocks:512 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56473)[0;0m INFO 08-09 13:58:22 [hpu_model_runner.py:3029] [Warmup][Graph/decode][12/135] batch_size:256 query_len:1 num_blocks:512 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56472)[0;0m INFO 08-09 13:58:22 [hpu_model_runner.py:3029] [Warmup][Graph/decode][12/135] batch_size:256 query_len:1 num_blocks:512 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56471)[0;0m INFO 08-09 13:58:22 [hpu_model_runner.py:3029] [Warmup][Graph/decode][12/135] batch_size:256 query_len:1 num_blocks:512 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56470)[0;0m INFO 08-09 13:58:22 [hpu_model_runner.py:3029] [Warmup][Graph/decode][12/135] batch_size:256 query_len:1 num_blocks:512 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56468)[0;0m INFO 08-09 13:58:22 [hpu_model_runner.py:3029] [Warmup][Graph/decode][12/135] batch_size:256 query_len:1 num_blocks:512 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56469)[0;0m INFO 08-09 13:58:22 [hpu_model_runner.py:3029] [Warmup][Graph/decode][12/135] batch_size:256 query_len:1 num_blocks:512 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56467)[0;0m INFO 08-09 13:58:22 [hpu_model_runner.py:3029] [Warmup][Graph/decode][12/135] batch_size:256 query_len:1 num_blocks:512 free_mem:23.73 GiB
INFO 08-09 13:58:26 [hpu_model_runner.py:3029] [Warmup][Graph/decode][13/135] batch_size:256 query_len:1 num_blocks:384 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56473)[0;0m INFO 08-09 13:58:26 [hpu_model_runner.py:3029] [Warmup][Graph/decode][13/135] batch_size:256 query_len:1 num_blocks:384 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56472)[0;0m INFO 08-09 13:58:26 [hpu_model_runner.py:3029] [Warmup][Graph/decode][13/135] batch_size:256 query_len:1 num_blocks:384 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56471)[0;0m INFO 08-09 13:58:26 [hpu_model_runner.py:3029] [Warmup][Graph/decode][13/135] batch_size:256 query_len:1 num_blocks:384 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56470)[0;0m INFO 08-09 13:58:26 [hpu_model_runner.py:3029] [Warmup][Graph/decode][13/135] batch_size:256 query_len:1 num_blocks:384 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56469)[0;0m INFO 08-09 13:58:26 [hpu_model_runner.py:3029] [Warmup][Graph/decode][13/135] batch_size:256 query_len:1 num_blocks:384 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56468)[0;0m INFO 08-09 13:58:26 [hpu_model_runner.py:3029] [Warmup][Graph/decode][13/135] batch_size:256 query_len:1 num_blocks:384 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56467)[0;0m INFO 08-09 13:58:26 [hpu_model_runner.py:3029] [Warmup][Graph/decode][13/135] batch_size:256 query_len:1 num_blocks:384 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56473)[0;0m INFO 08-09 13:58:30 [hpu_model_runner.py:3029] [Warmup][Graph/decode][14/135] batch_size:256 query_len:1 num_blocks:256 free_mem:23.73 GiB
INFO 08-09 13:58:30 [hpu_model_runner.py:3029] [Warmup][Graph/decode][14/135] batch_size:256 query_len:1 num_blocks:256 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56472)[0;0m INFO 08-09 13:58:30 [hpu_model_runner.py:3029] [Warmup][Graph/decode][14/135] batch_size:256 query_len:1 num_blocks:256 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56471)[0;0m INFO 08-09 13:58:30 [hpu_model_runner.py:3029] [Warmup][Graph/decode][14/135] batch_size:256 query_len:1 num_blocks:256 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56470)[0;0m INFO 08-09 13:58:30 [hpu_model_runner.py:3029] [Warmup][Graph/decode][14/135] batch_size:256 query_len:1 num_blocks:256 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56468)[0;0m INFO 08-09 13:58:30 [hpu_model_runner.py:3029] [Warmup][Graph/decode][14/135] batch_size:256 query_len:1 num_blocks:256 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56469)[0;0m INFO 08-09 13:58:30 [hpu_model_runner.py:3029] [Warmup][Graph/decode][14/135] batch_size:256 query_len:1 num_blocks:256 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56467)[0;0m INFO 08-09 13:58:30 [hpu_model_runner.py:3029] [Warmup][Graph/decode][14/135] batch_size:256 query_len:1 num_blocks:256 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56473)[0;0m INFO 08-09 13:58:35 [hpu_model_runner.py:3029] [Warmup][Graph/decode][15/135] batch_size:256 query_len:1 num_blocks:128 free_mem:23.73 GiB
INFO 08-09 13:58:35 [hpu_model_runner.py:3029] [Warmup][Graph/decode][15/135] batch_size:256 query_len:1 num_blocks:128 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56471)[0;0m INFO 08-09 13:58:35 [hpu_model_runner.py:3029] [Warmup][Graph/decode][15/135] batch_size:256 query_len:1 num_blocks:128 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56472)[0;0m INFO 08-09 13:58:35 [hpu_model_runner.py:3029] [Warmup][Graph/decode][15/135] batch_size:256 query_len:1 num_blocks:128 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56469)[0;0m INFO 08-09 13:58:35 [hpu_model_runner.py:3029] [Warmup][Graph/decode][15/135] batch_size:256 query_len:1 num_blocks:128 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56470)[0;0m INFO 08-09 13:58:35 [hpu_model_runner.py:3029] [Warmup][Graph/decode][15/135] batch_size:256 query_len:1 num_blocks:128 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56468)[0;0m INFO 08-09 13:58:35 [hpu_model_runner.py:3029] [Warmup][Graph/decode][15/135] batch_size:256 query_len:1 num_blocks:128 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56467)[0;0m INFO 08-09 13:58:35 [hpu_model_runner.py:3029] [Warmup][Graph/decode][15/135] batch_size:256 query_len:1 num_blocks:128 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56473)[0;0m INFO 08-09 13:58:39 [hpu_model_runner.py:3029] [Warmup][Graph/decode][16/135] batch_size:128 query_len:1 num_blocks:12946 free_mem:23.73 GiB
INFO 08-09 13:58:39 [hpu_model_runner.py:3029] [Warmup][Graph/decode][16/135] batch_size:128 query_len:1 num_blocks:12946 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56472)[0;0m INFO 08-09 13:58:39 [hpu_model_runner.py:3029] [Warmup][Graph/decode][16/135] batch_size:128 query_len:1 num_blocks:12946 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56471)[0;0m INFO 08-09 13:58:39 [hpu_model_runner.py:3029] [Warmup][Graph/decode][16/135] batch_size:128 query_len:1 num_blocks:12946 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56470)[0;0m INFO 08-09 13:58:39 [hpu_model_runner.py:3029] [Warmup][Graph/decode][16/135] batch_size:128 query_len:1 num_blocks:12946 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56468)[0;0m INFO 08-09 13:58:39 [hpu_model_runner.py:3029] [Warmup][Graph/decode][16/135] batch_size:128 query_len:1 num_blocks:12946 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56469)[0;0m INFO 08-09 13:58:39 [hpu_model_runner.py:3029] [Warmup][Graph/decode][16/135] batch_size:128 query_len:1 num_blocks:12946 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56467)[0;0m INFO 08-09 13:58:39 [hpu_model_runner.py:3029] [Warmup][Graph/decode][16/135] batch_size:128 query_len:1 num_blocks:12946 free_mem:23.73 GiB
INFO 08-09 13:58:44 [hpu_model_runner.py:3029] [Warmup][Graph/decode][17/135] batch_size:128 query_len:1 num_blocks:9344 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56473)[0;0m INFO 08-09 13:58:44 [hpu_model_runner.py:3029] [Warmup][Graph/decode][17/135] batch_size:128 query_len:1 num_blocks:9344 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56472)[0;0m INFO 08-09 13:58:44 [hpu_model_runner.py:3029] [Warmup][Graph/decode][17/135] batch_size:128 query_len:1 num_blocks:9344 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56471)[0;0m INFO 08-09 13:58:44 [hpu_model_runner.py:3029] [Warmup][Graph/decode][17/135] batch_size:128 query_len:1 num_blocks:9344 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56470)[0;0m INFO 08-09 13:58:44 [hpu_model_runner.py:3029] [Warmup][Graph/decode][17/135] batch_size:128 query_len:1 num_blocks:9344 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56469)[0;0m INFO 08-09 13:58:44 [hpu_model_runner.py:3029] [Warmup][Graph/decode][17/135] batch_size:128 query_len:1 num_blocks:9344 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56467)[0;0m INFO 08-09 13:58:44 [hpu_model_runner.py:3029] [Warmup][Graph/decode][17/135] batch_size:128 query_len:1 num_blocks:9344 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56468)[0;0m INFO 08-09 13:58:44 [hpu_model_runner.py:3029] [Warmup][Graph/decode][17/135] batch_size:128 query_len:1 num_blocks:9344 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56473)[0;0m INFO 08-09 13:58:48 [hpu_model_runner.py:3029] [Warmup][Graph/decode][18/135] batch_size:128 query_len:1 num_blocks:6784 free_mem:23.73 GiB
INFO 08-09 13:58:48 [hpu_model_runner.py:3029] [Warmup][Graph/decode][18/135] batch_size:128 query_len:1 num_blocks:6784 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56471)[0;0m INFO 08-09 13:58:48 [hpu_model_runner.py:3029] [Warmup][Graph/decode][18/135] batch_size:128 query_len:1 num_blocks:6784 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56472)[0;0m INFO 08-09 13:58:48 [hpu_model_runner.py:3029] [Warmup][Graph/decode][18/135] batch_size:128 query_len:1 num_blocks:6784 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56470)[0;0m INFO 08-09 13:58:48 [hpu_model_runner.py:3029] [Warmup][Graph/decode][18/135] batch_size:128 query_len:1 num_blocks:6784 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56469)[0;0m INFO 08-09 13:58:48 [hpu_model_runner.py:3029] [Warmup][Graph/decode][18/135] batch_size:128 query_len:1 num_blocks:6784 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56468)[0;0m INFO 08-09 13:58:48 [hpu_model_runner.py:3029] [Warmup][Graph/decode][18/135] batch_size:128 query_len:1 num_blocks:6784 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56467)[0;0m INFO 08-09 13:58:48 [hpu_model_runner.py:3029] [Warmup][Graph/decode][18/135] batch_size:128 query_len:1 num_blocks:6784 free_mem:23.73 GiB
INFO 08-09 13:58:53 [hpu_model_runner.py:3029] [Warmup][Graph/decode][19/135] batch_size:128 query_len:1 num_blocks:4864 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56473)[0;0m INFO 08-09 13:58:53 [hpu_model_runner.py:3029] [Warmup][Graph/decode][19/135] batch_size:128 query_len:1 num_blocks:4864 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56471)[0;0m INFO 08-09 13:58:53 [hpu_model_runner.py:3029] [Warmup][Graph/decode][19/135] batch_size:128 query_len:1 num_blocks:4864 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56472)[0;0m INFO 08-09 13:58:53 [hpu_model_runner.py:3029] [Warmup][Graph/decode][19/135] batch_size:128 query_len:1 num_blocks:4864 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56470)[0;0m INFO 08-09 13:58:53 [hpu_model_runner.py:3029] [Warmup][Graph/decode][19/135] batch_size:128 query_len:1 num_blocks:4864 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56469)[0;0m INFO 08-09 13:58:53 [hpu_model_runner.py:3029] [Warmup][Graph/decode][19/135] batch_size:128 query_len:1 num_blocks:4864 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56468)[0;0m INFO 08-09 13:58:53 [hpu_model_runner.py:3029] [Warmup][Graph/decode][19/135] batch_size:128 query_len:1 num_blocks:4864 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56467)[0;0m INFO 08-09 13:58:53 [hpu_model_runner.py:3029] [Warmup][Graph/decode][19/135] batch_size:128 query_len:1 num_blocks:4864 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56473)[0;0m INFO 08-09 13:58:57 [hpu_model_runner.py:3029] [Warmup][Graph/decode][20/135] batch_size:128 query_len:1 num_blocks:3584 free_mem:23.73 GiB
INFO 08-09 13:58:57 [hpu_model_runner.py:3029] [Warmup][Graph/decode][20/135] batch_size:128 query_len:1 num_blocks:3584 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56472)[0;0m INFO 08-09 13:58:57 [hpu_model_runner.py:3029] [Warmup][Graph/decode][20/135] batch_size:128 query_len:1 num_blocks:3584 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56471)[0;0m INFO 08-09 13:58:57 [hpu_model_runner.py:3029] [Warmup][Graph/decode][20/135] batch_size:128 query_len:1 num_blocks:3584 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56470)[0;0m INFO 08-09 13:58:57 [hpu_model_runner.py:3029] [Warmup][Graph/decode][20/135] batch_size:128 query_len:1 num_blocks:3584 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56469)[0;0m INFO 08-09 13:58:57 [hpu_model_runner.py:3029] [Warmup][Graph/decode][20/135] batch_size:128 query_len:1 num_blocks:3584 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56468)[0;0m INFO 08-09 13:58:57 [hpu_model_runner.py:3029] [Warmup][Graph/decode][20/135] batch_size:128 query_len:1 num_blocks:3584 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56467)[0;0m INFO 08-09 13:58:57 [hpu_model_runner.py:3029] [Warmup][Graph/decode][20/135] batch_size:128 query_len:1 num_blocks:3584 free_mem:23.73 GiB
INFO 08-09 13:59:02 [hpu_model_runner.py:3029] [Warmup][Graph/decode][21/135] batch_size:128 query_len:1 num_blocks:2560 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56473)[0;0m INFO 08-09 13:59:02 [hpu_model_runner.py:3029] [Warmup][Graph/decode][21/135] batch_size:128 query_len:1 num_blocks:2560 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56471)[0;0m INFO 08-09 13:59:02 [hpu_model_runner.py:3029] [Warmup][Graph/decode][21/135] batch_size:128 query_len:1 num_blocks:2560 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56470)[0;0m INFO 08-09 13:59:02 [hpu_model_runner.py:3029] [Warmup][Graph/decode][21/135] batch_size:128 query_len:1 num_blocks:2560 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56469)[0;0m INFO 08-09 13:59:02 [hpu_model_runner.py:3029] [Warmup][Graph/decode][21/135] batch_size:128 query_len:1 num_blocks:2560 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56468)[0;0m INFO 08-09 13:59:02 [hpu_model_runner.py:3029] [Warmup][Graph/decode][21/135] batch_size:128 query_len:1 num_blocks:2560 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56472)[0;0m INFO 08-09 13:59:02 [hpu_model_runner.py:3029] [Warmup][Graph/decode][21/135] batch_size:128 query_len:1 num_blocks:2560 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56467)[0;0m INFO 08-09 13:59:02 [hpu_model_runner.py:3029] [Warmup][Graph/decode][21/135] batch_size:128 query_len:1 num_blocks:2560 free_mem:23.73 GiB
INFO 08-09 13:59:06 [hpu_model_runner.py:3029] [Warmup][Graph/decode][22/135] batch_size:128 query_len:1 num_blocks:1792 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56473)[0;0m INFO 08-09 13:59:06 [hpu_model_runner.py:3029] [Warmup][Graph/decode][22/135] batch_size:128 query_len:1 num_blocks:1792 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56472)[0;0m INFO 08-09 13:59:06 [hpu_model_runner.py:3029] [Warmup][Graph/decode][22/135] batch_size:128 query_len:1 num_blocks:1792 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56471)[0;0m INFO 08-09 13:59:06 [hpu_model_runner.py:3029] [Warmup][Graph/decode][22/135] batch_size:128 query_len:1 num_blocks:1792 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56470)[0;0m INFO 08-09 13:59:06 [hpu_model_runner.py:3029] [Warmup][Graph/decode][22/135] batch_size:128 query_len:1 num_blocks:1792 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56469)[0;0m INFO 08-09 13:59:06 [hpu_model_runner.py:3029] [Warmup][Graph/decode][22/135] batch_size:128 query_len:1 num_blocks:1792 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56467)[0;0m INFO 08-09 13:59:06 [hpu_model_runner.py:3029] [Warmup][Graph/decode][22/135] batch_size:128 query_len:1 num_blocks:1792 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56468)[0;0m INFO 08-09 13:59:06 [hpu_model_runner.py:3029] [Warmup][Graph/decode][22/135] batch_size:128 query_len:1 num_blocks:1792 free_mem:23.73 GiB
INFO 08-09 13:59:11 [hpu_model_runner.py:3029] [Warmup][Graph/decode][23/135] batch_size:128 query_len:1 num_blocks:1408 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56473)[0;0m INFO 08-09 13:59:11 [hpu_model_runner.py:3029] [Warmup][Graph/decode][23/135] batch_size:128 query_len:1 num_blocks:1408 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56471)[0;0m INFO 08-09 13:59:11 [hpu_model_runner.py:3029] [Warmup][Graph/decode][23/135] batch_size:128 query_len:1 num_blocks:1408 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56470)[0;0m INFO 08-09 13:59:11 [hpu_model_runner.py:3029] [Warmup][Graph/decode][23/135] batch_size:128 query_len:1 num_blocks:1408 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56469)[0;0m INFO 08-09 13:59:11 [hpu_model_runner.py:3029] [Warmup][Graph/decode][23/135] batch_size:128 query_len:1 num_blocks:1408 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56472)[0;0m INFO 08-09 13:59:11 [hpu_model_runner.py:3029] [Warmup][Graph/decode][23/135] batch_size:128 query_len:1 num_blocks:1408 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56467)[0;0m INFO 08-09 13:59:11 [hpu_model_runner.py:3029] [Warmup][Graph/decode][23/135] batch_size:128 query_len:1 num_blocks:1408 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56468)[0;0m INFO 08-09 13:59:11 [hpu_model_runner.py:3029] [Warmup][Graph/decode][23/135] batch_size:128 query_len:1 num_blocks:1408 free_mem:23.73 GiB
INFO 08-09 13:59:15 [hpu_model_runner.py:3029] [Warmup][Graph/decode][24/135] batch_size:128 query_len:1 num_blocks:1024 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56473)[0;0m INFO 08-09 13:59:15 [hpu_model_runner.py:3029] [Warmup][Graph/decode][24/135] batch_size:128 query_len:1 num_blocks:1024 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56472)[0;0m INFO 08-09 13:59:15 [hpu_model_runner.py:3029] [Warmup][Graph/decode][24/135] batch_size:128 query_len:1 num_blocks:1024 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56471)[0;0m INFO 08-09 13:59:15 [hpu_model_runner.py:3029] [Warmup][Graph/decode][24/135] batch_size:128 query_len:1 num_blocks:1024 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56469)[0;0m INFO 08-09 13:59:15 [hpu_model_runner.py:3029] [Warmup][Graph/decode][24/135] batch_size:128 query_len:1 num_blocks:1024 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56470)[0;0m INFO 08-09 13:59:15 [hpu_model_runner.py:3029] [Warmup][Graph/decode][24/135] batch_size:128 query_len:1 num_blocks:1024 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56467)[0;0m INFO 08-09 13:59:15 [hpu_model_runner.py:3029] [Warmup][Graph/decode][24/135] batch_size:128 query_len:1 num_blocks:1024 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56468)[0;0m INFO 08-09 13:59:15 [hpu_model_runner.py:3029] [Warmup][Graph/decode][24/135] batch_size:128 query_len:1 num_blocks:1024 free_mem:23.73 GiB
INFO 08-09 13:59:20 [hpu_model_runner.py:3029] [Warmup][Graph/decode][25/135] batch_size:128 query_len:1 num_blocks:768 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56473)[0;0m INFO 08-09 13:59:20 [hpu_model_runner.py:3029] [Warmup][Graph/decode][25/135] batch_size:128 query_len:1 num_blocks:768 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56471)[0;0m INFO 08-09 13:59:20 [hpu_model_runner.py:3029] [Warmup][Graph/decode][25/135] batch_size:128 query_len:1 num_blocks:768 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56472)[0;0m INFO 08-09 13:59:20 [hpu_model_runner.py:3029] [Warmup][Graph/decode][25/135] batch_size:128 query_len:1 num_blocks:768 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56470)[0;0m INFO 08-09 13:59:20 [hpu_model_runner.py:3029] [Warmup][Graph/decode][25/135] batch_size:128 query_len:1 num_blocks:768 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56469)[0;0m INFO 08-09 13:59:20 [hpu_model_runner.py:3029] [Warmup][Graph/decode][25/135] batch_size:128 query_len:1 num_blocks:768 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56468)[0;0m INFO 08-09 13:59:20 [hpu_model_runner.py:3029] [Warmup][Graph/decode][25/135] batch_size:128 query_len:1 num_blocks:768 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56467)[0;0m INFO 08-09 13:59:20 [hpu_model_runner.py:3029] [Warmup][Graph/decode][25/135] batch_size:128 query_len:1 num_blocks:768 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56473)[0;0m INFO 08-09 13:59:24 [hpu_model_runner.py:3029] [Warmup][Graph/decode][26/135] batch_size:128 query_len:1 num_blocks:640 free_mem:23.73 GiB
INFO 08-09 13:59:24 [hpu_model_runner.py:3029] [Warmup][Graph/decode][26/135] batch_size:128 query_len:1 num_blocks:640 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56471)[0;0m INFO 08-09 13:59:24 [hpu_model_runner.py:3029] [Warmup][Graph/decode][26/135] batch_size:128 query_len:1 num_blocks:640 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56472)[0;0m INFO 08-09 13:59:24 [hpu_model_runner.py:3029] [Warmup][Graph/decode][26/135] batch_size:128 query_len:1 num_blocks:640 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56470)[0;0m INFO 08-09 13:59:24 [hpu_model_runner.py:3029] [Warmup][Graph/decode][26/135] batch_size:128 query_len:1 num_blocks:640 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56469)[0;0m INFO 08-09 13:59:24 [hpu_model_runner.py:3029] [Warmup][Graph/decode][26/135] batch_size:128 query_len:1 num_blocks:640 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56468)[0;0m INFO 08-09 13:59:24 [hpu_model_runner.py:3029] [Warmup][Graph/decode][26/135] batch_size:128 query_len:1 num_blocks:640 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56467)[0;0m INFO 08-09 13:59:24 [hpu_model_runner.py:3029] [Warmup][Graph/decode][26/135] batch_size:128 query_len:1 num_blocks:640 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56473)[0;0m INFO 08-09 13:59:29 [hpu_model_runner.py:3029] [Warmup][Graph/decode][27/135] batch_size:128 query_len:1 num_blocks:512 free_mem:23.73 GiB
INFO 08-09 13:59:29 [hpu_model_runner.py:3029] [Warmup][Graph/decode][27/135] batch_size:128 query_len:1 num_blocks:512 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56472)[0;0m INFO 08-09 13:59:29 [hpu_model_runner.py:3029] [Warmup][Graph/decode][27/135] batch_size:128 query_len:1 num_blocks:512 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56471)[0;0m INFO 08-09 13:59:29 [hpu_model_runner.py:3029] [Warmup][Graph/decode][27/135] batch_size:128 query_len:1 num_blocks:512 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56470)[0;0m INFO 08-09 13:59:29 [hpu_model_runner.py:3029] [Warmup][Graph/decode][27/135] batch_size:128 query_len:1 num_blocks:512 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56469)[0;0m INFO 08-09 13:59:29 [hpu_model_runner.py:3029] [Warmup][Graph/decode][27/135] batch_size:128 query_len:1 num_blocks:512 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56468)[0;0m INFO 08-09 13:59:29 [hpu_model_runner.py:3029] [Warmup][Graph/decode][27/135] batch_size:128 query_len:1 num_blocks:512 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56467)[0;0m INFO 08-09 13:59:29 [hpu_model_runner.py:3029] [Warmup][Graph/decode][27/135] batch_size:128 query_len:1 num_blocks:512 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56473)[0;0m INFO 08-09 13:59:33 [hpu_model_runner.py:3029] [Warmup][Graph/decode][28/135] batch_size:128 query_len:1 num_blocks:384 free_mem:23.73 GiB
INFO 08-09 13:59:33 [hpu_model_runner.py:3029] [Warmup][Graph/decode][28/135] batch_size:128 query_len:1 num_blocks:384 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56472)[0;0m INFO 08-09 13:59:33 [hpu_model_runner.py:3029] [Warmup][Graph/decode][28/135] batch_size:128 query_len:1 num_blocks:384 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56471)[0;0m INFO 08-09 13:59:33 [hpu_model_runner.py:3029] [Warmup][Graph/decode][28/135] batch_size:128 query_len:1 num_blocks:384 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56470)[0;0m INFO 08-09 13:59:33 [hpu_model_runner.py:3029] [Warmup][Graph/decode][28/135] batch_size:128 query_len:1 num_blocks:384 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56469)[0;0m INFO 08-09 13:59:33 [hpu_model_runner.py:3029] [Warmup][Graph/decode][28/135] batch_size:128 query_len:1 num_blocks:384 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56468)[0;0m INFO 08-09 13:59:33 [hpu_model_runner.py:3029] [Warmup][Graph/decode][28/135] batch_size:128 query_len:1 num_blocks:384 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56467)[0;0m INFO 08-09 13:59:33 [hpu_model_runner.py:3029] [Warmup][Graph/decode][28/135] batch_size:128 query_len:1 num_blocks:384 free_mem:23.73 GiB
INFO 08-09 13:59:38 [hpu_model_runner.py:3029] [Warmup][Graph/decode][29/135] batch_size:128 query_len:1 num_blocks:256 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56472)[0;0m INFO 08-09 13:59:38 [hpu_model_runner.py:3029] [Warmup][Graph/decode][29/135] batch_size:128 query_len:1 num_blocks:256 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56473)[0;0m INFO 08-09 13:59:38 [hpu_model_runner.py:3029] [Warmup][Graph/decode][29/135] batch_size:128 query_len:1 num_blocks:256 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56471)[0;0m INFO 08-09 13:59:38 [hpu_model_runner.py:3029] [Warmup][Graph/decode][29/135] batch_size:128 query_len:1 num_blocks:256 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56470)[0;0m INFO 08-09 13:59:38 [hpu_model_runner.py:3029] [Warmup][Graph/decode][29/135] batch_size:128 query_len:1 num_blocks:256 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56469)[0;0m INFO 08-09 13:59:38 [hpu_model_runner.py:3029] [Warmup][Graph/decode][29/135] batch_size:128 query_len:1 num_blocks:256 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56467)[0;0m INFO 08-09 13:59:38 [hpu_model_runner.py:3029] [Warmup][Graph/decode][29/135] batch_size:128 query_len:1 num_blocks:256 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56468)[0;0m INFO 08-09 13:59:38 [hpu_model_runner.py:3029] [Warmup][Graph/decode][29/135] batch_size:128 query_len:1 num_blocks:256 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56473)[0;0m INFO 08-09 13:59:43 [hpu_model_runner.py:3029] [Warmup][Graph/decode][30/135] batch_size:128 query_len:1 num_blocks:128 free_mem:23.73 GiB
INFO 08-09 13:59:43 [hpu_model_runner.py:3029] [Warmup][Graph/decode][30/135] batch_size:128 query_len:1 num_blocks:128 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56471)[0;0m INFO 08-09 13:59:43 [hpu_model_runner.py:3029] [Warmup][Graph/decode][30/135] batch_size:128 query_len:1 num_blocks:128 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56470)[0;0m INFO 08-09 13:59:43 [hpu_model_runner.py:3029] [Warmup][Graph/decode][30/135] batch_size:128 query_len:1 num_blocks:128 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56469)[0;0m INFO 08-09 13:59:43 [hpu_model_runner.py:3029] [Warmup][Graph/decode][30/135] batch_size:128 query_len:1 num_blocks:128 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56468)[0;0m INFO 08-09 13:59:43 [hpu_model_runner.py:3029] [Warmup][Graph/decode][30/135] batch_size:128 query_len:1 num_blocks:128 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56472)[0;0m INFO 08-09 13:59:43 [hpu_model_runner.py:3029] [Warmup][Graph/decode][30/135] batch_size:128 query_len:1 num_blocks:128 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56467)[0;0m INFO 08-09 13:59:43 [hpu_model_runner.py:3029] [Warmup][Graph/decode][30/135] batch_size:128 query_len:1 num_blocks:128 free_mem:23.73 GiB
INFO 08-09 13:59:47 [hpu_model_runner.py:3029] [Warmup][Graph/decode][31/135] batch_size:64 query_len:1 num_blocks:12946 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56472)[0;0m INFO 08-09 13:59:47 [hpu_model_runner.py:3029] [Warmup][Graph/decode][31/135] batch_size:64 query_len:1 num_blocks:12946 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56473)[0;0m INFO 08-09 13:59:47 [hpu_model_runner.py:3029] [Warmup][Graph/decode][31/135] batch_size:64 query_len:1 num_blocks:12946 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56471)[0;0m INFO 08-09 13:59:47 [hpu_model_runner.py:3029] [Warmup][Graph/decode][31/135] batch_size:64 query_len:1 num_blocks:12946 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56469)[0;0m INFO 08-09 13:59:47 [hpu_model_runner.py:3029] [Warmup][Graph/decode][31/135] batch_size:64 query_len:1 num_blocks:12946 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56470)[0;0m INFO 08-09 13:59:47 [hpu_model_runner.py:3029] [Warmup][Graph/decode][31/135] batch_size:64 query_len:1 num_blocks:12946 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56468)[0;0m INFO 08-09 13:59:47 [hpu_model_runner.py:3029] [Warmup][Graph/decode][31/135] batch_size:64 query_len:1 num_blocks:12946 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56467)[0;0m INFO 08-09 13:59:47 [hpu_model_runner.py:3029] [Warmup][Graph/decode][31/135] batch_size:64 query_len:1 num_blocks:12946 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56473)[0;0m INFO 08-09 13:59:52 [hpu_model_runner.py:3029] [Warmup][Graph/decode][32/135] batch_size:64 query_len:1 num_blocks:9344 free_mem:23.73 GiB
INFO 08-09 13:59:52 [hpu_model_runner.py:3029] [Warmup][Graph/decode][32/135] batch_size:64 query_len:1 num_blocks:9344 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56472)[0;0m INFO 08-09 13:59:52 [hpu_model_runner.py:3029] [Warmup][Graph/decode][32/135] batch_size:64 query_len:1 num_blocks:9344 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56471)[0;0m INFO 08-09 13:59:52 [hpu_model_runner.py:3029] [Warmup][Graph/decode][32/135] batch_size:64 query_len:1 num_blocks:9344 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56470)[0;0m INFO 08-09 13:59:52 [hpu_model_runner.py:3029] [Warmup][Graph/decode][32/135] batch_size:64 query_len:1 num_blocks:9344 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56469)[0;0m INFO 08-09 13:59:52 [hpu_model_runner.py:3029] [Warmup][Graph/decode][32/135] batch_size:64 query_len:1 num_blocks:9344 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56468)[0;0m INFO 08-09 13:59:52 [hpu_model_runner.py:3029] [Warmup][Graph/decode][32/135] batch_size:64 query_len:1 num_blocks:9344 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56467)[0;0m INFO 08-09 13:59:52 [hpu_model_runner.py:3029] [Warmup][Graph/decode][32/135] batch_size:64 query_len:1 num_blocks:9344 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56473)[0;0m INFO 08-09 13:59:57 [hpu_model_runner.py:3029] [Warmup][Graph/decode][33/135] batch_size:64 query_len:1 num_blocks:6784 free_mem:23.73 GiB
INFO 08-09 13:59:57 [hpu_model_runner.py:3029] [Warmup][Graph/decode][33/135] batch_size:64 query_len:1 num_blocks:6784 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56472)[0;0m INFO 08-09 13:59:57 [hpu_model_runner.py:3029] [Warmup][Graph/decode][33/135] batch_size:64 query_len:1 num_blocks:6784 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56471)[0;0m INFO 08-09 13:59:57 [hpu_model_runner.py:3029] [Warmup][Graph/decode][33/135] batch_size:64 query_len:1 num_blocks:6784 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56470)[0;0m INFO 08-09 13:59:57 [hpu_model_runner.py:3029] [Warmup][Graph/decode][33/135] batch_size:64 query_len:1 num_blocks:6784 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56469)[0;0m INFO 08-09 13:59:57 [hpu_model_runner.py:3029] [Warmup][Graph/decode][33/135] batch_size:64 query_len:1 num_blocks:6784 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56467)[0;0m INFO 08-09 13:59:57 [hpu_model_runner.py:3029] [Warmup][Graph/decode][33/135] batch_size:64 query_len:1 num_blocks:6784 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56468)[0;0m INFO 08-09 13:59:57 [hpu_model_runner.py:3029] [Warmup][Graph/decode][33/135] batch_size:64 query_len:1 num_blocks:6784 free_mem:23.73 GiB
INFO 08-09 14:00:02 [hpu_model_runner.py:3029] [Warmup][Graph/decode][34/135] batch_size:64 query_len:1 num_blocks:4864 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56473)[0;0m INFO 08-09 14:00:02 [hpu_model_runner.py:3029] [Warmup][Graph/decode][34/135] batch_size:64 query_len:1 num_blocks:4864 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56471)[0;0m INFO 08-09 14:00:02 [hpu_model_runner.py:3029] [Warmup][Graph/decode][34/135] batch_size:64 query_len:1 num_blocks:4864 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56472)[0;0m INFO 08-09 14:00:02 [hpu_model_runner.py:3029] [Warmup][Graph/decode][34/135] batch_size:64 query_len:1 num_blocks:4864 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56470)[0;0m INFO 08-09 14:00:02 [hpu_model_runner.py:3029] [Warmup][Graph/decode][34/135] batch_size:64 query_len:1 num_blocks:4864 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56469)[0;0m INFO 08-09 14:00:02 [hpu_model_runner.py:3029] [Warmup][Graph/decode][34/135] batch_size:64 query_len:1 num_blocks:4864 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56467)[0;0m INFO 08-09 14:00:02 [hpu_model_runner.py:3029] [Warmup][Graph/decode][34/135] batch_size:64 query_len:1 num_blocks:4864 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56468)[0;0m INFO 08-09 14:00:02 [hpu_model_runner.py:3029] [Warmup][Graph/decode][34/135] batch_size:64 query_len:1 num_blocks:4864 free_mem:23.73 GiB
INFO 08-09 14:00:07 [hpu_model_runner.py:3029] [Warmup][Graph/decode][35/135] batch_size:64 query_len:1 num_blocks:3584 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56473)[0;0m INFO 08-09 14:00:07 [hpu_model_runner.py:3029] [Warmup][Graph/decode][35/135] batch_size:64 query_len:1 num_blocks:3584 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56472)[0;0m INFO 08-09 14:00:07 [hpu_model_runner.py:3029] [Warmup][Graph/decode][35/135] batch_size:64 query_len:1 num_blocks:3584 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56471)[0;0m INFO 08-09 14:00:07 [hpu_model_runner.py:3029] [Warmup][Graph/decode][35/135] batch_size:64 query_len:1 num_blocks:3584 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56470)[0;0m INFO 08-09 14:00:07 [hpu_model_runner.py:3029] [Warmup][Graph/decode][35/135] batch_size:64 query_len:1 num_blocks:3584 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56469)[0;0m INFO 08-09 14:00:07 [hpu_model_runner.py:3029] [Warmup][Graph/decode][35/135] batch_size:64 query_len:1 num_blocks:3584 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56468)[0;0m INFO 08-09 14:00:07 [hpu_model_runner.py:3029] [Warmup][Graph/decode][35/135] batch_size:64 query_len:1 num_blocks:3584 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56467)[0;0m INFO 08-09 14:00:07 [hpu_model_runner.py:3029] [Warmup][Graph/decode][35/135] batch_size:64 query_len:1 num_blocks:3584 free_mem:23.73 GiB
INFO 08-09 14:00:12 [hpu_model_runner.py:3029] [Warmup][Graph/decode][36/135] batch_size:64 query_len:1 num_blocks:2560 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56473)[0;0m INFO 08-09 14:00:12 [hpu_model_runner.py:3029] [Warmup][Graph/decode][36/135] batch_size:64 query_len:1 num_blocks:2560 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56471)[0;0m INFO 08-09 14:00:12 [hpu_model_runner.py:3029] [Warmup][Graph/decode][36/135] batch_size:64 query_len:1 num_blocks:2560 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56472)[0;0m INFO 08-09 14:00:12 [hpu_model_runner.py:3029] [Warmup][Graph/decode][36/135] batch_size:64 query_len:1 num_blocks:2560 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56470)[0;0m INFO 08-09 14:00:12 [hpu_model_runner.py:3029] [Warmup][Graph/decode][36/135] batch_size:64 query_len:1 num_blocks:2560 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56469)[0;0m INFO 08-09 14:00:12 [hpu_model_runner.py:3029] [Warmup][Graph/decode][36/135] batch_size:64 query_len:1 num_blocks:2560 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56468)[0;0m INFO 08-09 14:00:12 [hpu_model_runner.py:3029] [Warmup][Graph/decode][36/135] batch_size:64 query_len:1 num_blocks:2560 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56467)[0;0m INFO 08-09 14:00:12 [hpu_model_runner.py:3029] [Warmup][Graph/decode][36/135] batch_size:64 query_len:1 num_blocks:2560 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56473)[0;0m INFO 08-09 14:00:17 [hpu_model_runner.py:3029] [Warmup][Graph/decode][37/135] batch_size:64 query_len:1 num_blocks:1792 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56472)[0;0m INFO 08-09 14:00:17 [hpu_model_runner.py:3029] [Warmup][Graph/decode][37/135] batch_size:64 query_len:1 num_blocks:1792 free_mem:23.73 GiB
INFO 08-09 14:00:17 [hpu_model_runner.py:3029] [Warmup][Graph/decode][37/135] batch_size:64 query_len:1 num_blocks:1792 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56471)[0;0m INFO 08-09 14:00:17 [hpu_model_runner.py:3029] [Warmup][Graph/decode][37/135] batch_size:64 query_len:1 num_blocks:1792 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56469)[0;0m INFO 08-09 14:00:17 [hpu_model_runner.py:3029] [Warmup][Graph/decode][37/135] batch_size:64 query_len:1 num_blocks:1792 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56470)[0;0m INFO 08-09 14:00:17 [hpu_model_runner.py:3029] [Warmup][Graph/decode][37/135] batch_size:64 query_len:1 num_blocks:1792 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56467)[0;0m INFO 08-09 14:00:17 [hpu_model_runner.py:3029] [Warmup][Graph/decode][37/135] batch_size:64 query_len:1 num_blocks:1792 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56468)[0;0m INFO 08-09 14:00:17 [hpu_model_runner.py:3029] [Warmup][Graph/decode][37/135] batch_size:64 query_len:1 num_blocks:1792 free_mem:23.73 GiB
INFO 08-09 14:00:22 [hpu_model_runner.py:3029] [Warmup][Graph/decode][38/135] batch_size:64 query_len:1 num_blocks:1408 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56473)[0;0m INFO 08-09 14:00:22 [hpu_model_runner.py:3029] [Warmup][Graph/decode][38/135] batch_size:64 query_len:1 num_blocks:1408 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56472)[0;0m INFO 08-09 14:00:22 [hpu_model_runner.py:3029] [Warmup][Graph/decode][38/135] batch_size:64 query_len:1 num_blocks:1408 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56471)[0;0m INFO 08-09 14:00:22 [hpu_model_runner.py:3029] [Warmup][Graph/decode][38/135] batch_size:64 query_len:1 num_blocks:1408 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56470)[0;0m INFO 08-09 14:00:22 [hpu_model_runner.py:3029] [Warmup][Graph/decode][38/135] batch_size:64 query_len:1 num_blocks:1408 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56467)[0;0m INFO 08-09 14:00:22 [hpu_model_runner.py:3029] [Warmup][Graph/decode][38/135] batch_size:64 query_len:1 num_blocks:1408 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56468)[0;0m INFO 08-09 14:00:22 [hpu_model_runner.py:3029] [Warmup][Graph/decode][38/135] batch_size:64 query_len:1 num_blocks:1408 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56469)[0;0m INFO 08-09 14:00:22 [hpu_model_runner.py:3029] [Warmup][Graph/decode][38/135] batch_size:64 query_len:1 num_blocks:1408 free_mem:23.73 GiB
INFO 08-09 14:00:27 [hpu_model_runner.py:3029] [Warmup][Graph/decode][39/135] batch_size:64 query_len:1 num_blocks:1024 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56473)[0;0m INFO 08-09 14:00:27 [hpu_model_runner.py:3029] [Warmup][Graph/decode][39/135] batch_size:64 query_len:1 num_blocks:1024 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56472)[0;0m INFO 08-09 14:00:27 [hpu_model_runner.py:3029] [Warmup][Graph/decode][39/135] batch_size:64 query_len:1 num_blocks:1024 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56471)[0;0m INFO 08-09 14:00:27 [hpu_model_runner.py:3029] [Warmup][Graph/decode][39/135] batch_size:64 query_len:1 num_blocks:1024 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56470)[0;0m INFO 08-09 14:00:27 [hpu_model_runner.py:3029] [Warmup][Graph/decode][39/135] batch_size:64 query_len:1 num_blocks:1024 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56468)[0;0m INFO 08-09 14:00:27 [hpu_model_runner.py:3029] [Warmup][Graph/decode][39/135] batch_size:64 query_len:1 num_blocks:1024 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56469)[0;0m INFO 08-09 14:00:27 [hpu_model_runner.py:3029] [Warmup][Graph/decode][39/135] batch_size:64 query_len:1 num_blocks:1024 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56467)[0;0m INFO 08-09 14:00:27 [hpu_model_runner.py:3029] [Warmup][Graph/decode][39/135] batch_size:64 query_len:1 num_blocks:1024 free_mem:23.73 GiB
INFO 08-09 14:00:32 [hpu_model_runner.py:3029] [Warmup][Graph/decode][40/135] batch_size:64 query_len:1 num_blocks:768 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56473)[0;0m INFO 08-09 14:00:32 [hpu_model_runner.py:3029] [Warmup][Graph/decode][40/135] batch_size:64 query_len:1 num_blocks:768 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56471)[0;0m INFO 08-09 14:00:32 [hpu_model_runner.py:3029] [Warmup][Graph/decode][40/135] batch_size:64 query_len:1 num_blocks:768 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56470)[0;0m INFO 08-09 14:00:32 [hpu_model_runner.py:3029] [Warmup][Graph/decode][40/135] batch_size:64 query_len:1 num_blocks:768 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56472)[0;0m INFO 08-09 14:00:32 [hpu_model_runner.py:3029] [Warmup][Graph/decode][40/135] batch_size:64 query_len:1 num_blocks:768 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56468)[0;0m INFO 08-09 14:00:32 [hpu_model_runner.py:3029] [Warmup][Graph/decode][40/135] batch_size:64 query_len:1 num_blocks:768 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56467)[0;0m INFO 08-09 14:00:32 [hpu_model_runner.py:3029] [Warmup][Graph/decode][40/135] batch_size:64 query_len:1 num_blocks:768 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56469)[0;0m INFO 08-09 14:00:32 [hpu_model_runner.py:3029] [Warmup][Graph/decode][40/135] batch_size:64 query_len:1 num_blocks:768 free_mem:23.73 GiB
INFO 08-09 14:00:37 [hpu_model_runner.py:3029] [Warmup][Graph/decode][41/135] batch_size:64 query_len:1 num_blocks:640 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56473)[0;0m INFO 08-09 14:00:37 [hpu_model_runner.py:3029] [Warmup][Graph/decode][41/135] batch_size:64 query_len:1 num_blocks:640 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56472)[0;0m INFO 08-09 14:00:37 [hpu_model_runner.py:3029] [Warmup][Graph/decode][41/135] batch_size:64 query_len:1 num_blocks:640 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56471)[0;0m INFO 08-09 14:00:37 [hpu_model_runner.py:3029] [Warmup][Graph/decode][41/135] batch_size:64 query_len:1 num_blocks:640 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56470)[0;0m INFO 08-09 14:00:37 [hpu_model_runner.py:3029] [Warmup][Graph/decode][41/135] batch_size:64 query_len:1 num_blocks:640 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56467)[0;0m INFO 08-09 14:00:37 [hpu_model_runner.py:3029] [Warmup][Graph/decode][41/135] batch_size:64 query_len:1 num_blocks:640 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56468)[0;0m INFO 08-09 14:00:37 [hpu_model_runner.py:3029] [Warmup][Graph/decode][41/135] batch_size:64 query_len:1 num_blocks:640 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56469)[0;0m INFO 08-09 14:00:37 [hpu_model_runner.py:3029] [Warmup][Graph/decode][41/135] batch_size:64 query_len:1 num_blocks:640 free_mem:23.73 GiB
INFO 08-09 14:00:42 [hpu_model_runner.py:3029] [Warmup][Graph/decode][42/135] batch_size:64 query_len:1 num_blocks:512 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56473)[0;0m INFO 08-09 14:00:42 [hpu_model_runner.py:3029] [Warmup][Graph/decode][42/135] batch_size:64 query_len:1 num_blocks:512 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56472)[0;0m INFO 08-09 14:00:42 [hpu_model_runner.py:3029] [Warmup][Graph/decode][42/135] batch_size:64 query_len:1 num_blocks:512 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56471)[0;0m INFO 08-09 14:00:42 [hpu_model_runner.py:3029] [Warmup][Graph/decode][42/135] batch_size:64 query_len:1 num_blocks:512 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56470)[0;0m INFO 08-09 14:00:42 [hpu_model_runner.py:3029] [Warmup][Graph/decode][42/135] batch_size:64 query_len:1 num_blocks:512 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56467)[0;0m INFO 08-09 14:00:42 [hpu_model_runner.py:3029] [Warmup][Graph/decode][42/135] batch_size:64 query_len:1 num_blocks:512 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56468)[0;0m INFO 08-09 14:00:42 [hpu_model_runner.py:3029] [Warmup][Graph/decode][42/135] batch_size:64 query_len:1 num_blocks:512 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56469)[0;0m INFO 08-09 14:00:42 [hpu_model_runner.py:3029] [Warmup][Graph/decode][42/135] batch_size:64 query_len:1 num_blocks:512 free_mem:23.73 GiB
INFO 08-09 14:00:47 [hpu_model_runner.py:3029] [Warmup][Graph/decode][43/135] batch_size:64 query_len:1 num_blocks:384 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56473)[0;0m INFO 08-09 14:00:47 [hpu_model_runner.py:3029] [Warmup][Graph/decode][43/135] batch_size:64 query_len:1 num_blocks:384 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56472)[0;0m INFO 08-09 14:00:47 [hpu_model_runner.py:3029] [Warmup][Graph/decode][43/135] batch_size:64 query_len:1 num_blocks:384 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56471)[0;0m INFO 08-09 14:00:47 [hpu_model_runner.py:3029] [Warmup][Graph/decode][43/135] batch_size:64 query_len:1 num_blocks:384 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56470)[0;0m INFO 08-09 14:00:47 [hpu_model_runner.py:3029] [Warmup][Graph/decode][43/135] batch_size:64 query_len:1 num_blocks:384 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56468)[0;0m INFO 08-09 14:00:47 [hpu_model_runner.py:3029] [Warmup][Graph/decode][43/135] batch_size:64 query_len:1 num_blocks:384 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56469)[0;0m INFO 08-09 14:00:47 [hpu_model_runner.py:3029] [Warmup][Graph/decode][43/135] batch_size:64 query_len:1 num_blocks:384 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56467)[0;0m INFO 08-09 14:00:47 [hpu_model_runner.py:3029] [Warmup][Graph/decode][43/135] batch_size:64 query_len:1 num_blocks:384 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56473)[0;0m INFO 08-09 14:00:52 [hpu_model_runner.py:3029] [Warmup][Graph/decode][44/135] batch_size:64 query_len:1 num_blocks:256 free_mem:23.73 GiB
INFO 08-09 14:00:52 [hpu_model_runner.py:3029] [Warmup][Graph/decode][44/135] batch_size:64 query_len:1 num_blocks:256 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56471)[0;0m INFO 08-09 14:00:52 [hpu_model_runner.py:3029] [Warmup][Graph/decode][44/135] batch_size:64 query_len:1 num_blocks:256 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56470)[0;0m INFO 08-09 14:00:52 [hpu_model_runner.py:3029] [Warmup][Graph/decode][44/135] batch_size:64 query_len:1 num_blocks:256 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56472)[0;0m INFO 08-09 14:00:52 [hpu_model_runner.py:3029] [Warmup][Graph/decode][44/135] batch_size:64 query_len:1 num_blocks:256 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56468)[0;0m INFO 08-09 14:00:52 [hpu_model_runner.py:3029] [Warmup][Graph/decode][44/135] batch_size:64 query_len:1 num_blocks:256 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56467)[0;0m INFO 08-09 14:00:52 [hpu_model_runner.py:3029] [Warmup][Graph/decode][44/135] batch_size:64 query_len:1 num_blocks:256 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56469)[0;0m INFO 08-09 14:00:52 [hpu_model_runner.py:3029] [Warmup][Graph/decode][44/135] batch_size:64 query_len:1 num_blocks:256 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56473)[0;0m INFO 08-09 14:00:58 [hpu_model_runner.py:3029] [Warmup][Graph/decode][45/135] batch_size:64 query_len:1 num_blocks:128 free_mem:23.73 GiB
INFO 08-09 14:00:58 [hpu_model_runner.py:3029] [Warmup][Graph/decode][45/135] batch_size:64 query_len:1 num_blocks:128 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56472)[0;0m INFO 08-09 14:00:58 [hpu_model_runner.py:3029] [Warmup][Graph/decode][45/135] batch_size:64 query_len:1 num_blocks:128 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56471)[0;0m INFO 08-09 14:00:58 [hpu_model_runner.py:3029] [Warmup][Graph/decode][45/135] batch_size:64 query_len:1 num_blocks:128 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56470)[0;0m INFO 08-09 14:00:58 [hpu_model_runner.py:3029] [Warmup][Graph/decode][45/135] batch_size:64 query_len:1 num_blocks:128 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56469)[0;0m INFO 08-09 14:00:58 [hpu_model_runner.py:3029] [Warmup][Graph/decode][45/135] batch_size:64 query_len:1 num_blocks:128 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56468)[0;0m INFO 08-09 14:00:58 [hpu_model_runner.py:3029] [Warmup][Graph/decode][45/135] batch_size:64 query_len:1 num_blocks:128 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56467)[0;0m INFO 08-09 14:00:58 [hpu_model_runner.py:3029] [Warmup][Graph/decode][45/135] batch_size:64 query_len:1 num_blocks:128 free_mem:23.73 GiB
INFO 08-09 14:01:03 [hpu_model_runner.py:3029] [Warmup][Graph/decode][46/135] batch_size:32 query_len:1 num_blocks:12946 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56473)[0;0m INFO 08-09 14:01:03 [hpu_model_runner.py:3029] [Warmup][Graph/decode][46/135] batch_size:32 query_len:1 num_blocks:12946 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56472)[0;0m INFO 08-09 14:01:03 [hpu_model_runner.py:3029] [Warmup][Graph/decode][46/135] batch_size:32 query_len:1 num_blocks:12946 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56468)[0;0m INFO 08-09 14:01:03 [hpu_model_runner.py:3029] [Warmup][Graph/decode][46/135] batch_size:32 query_len:1 num_blocks:12946 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56471)[0;0m INFO 08-09 14:01:03 [hpu_model_runner.py:3029] [Warmup][Graph/decode][46/135] batch_size:32 query_len:1 num_blocks:12946 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56469)[0;0m INFO 08-09 14:01:03 [hpu_model_runner.py:3029] [Warmup][Graph/decode][46/135] batch_size:32 query_len:1 num_blocks:12946 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56467)[0;0m INFO 08-09 14:01:03 [hpu_model_runner.py:3029] [Warmup][Graph/decode][46/135] batch_size:32 query_len:1 num_blocks:12946 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56470)[0;0m INFO 08-09 14:01:03 [hpu_model_runner.py:3029] [Warmup][Graph/decode][46/135] batch_size:32 query_len:1 num_blocks:12946 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56473)[0;0m INFO 08-09 14:01:08 [hpu_model_runner.py:3029] [Warmup][Graph/decode][47/135] batch_size:32 query_len:1 num_blocks:9344 free_mem:23.73 GiB
INFO 08-09 14:01:08 [hpu_model_runner.py:3029] [Warmup][Graph/decode][47/135] batch_size:32 query_len:1 num_blocks:9344 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56471)[0;0m INFO 08-09 14:01:08 [hpu_model_runner.py:3029] [Warmup][Graph/decode][47/135] batch_size:32 query_len:1 num_blocks:9344 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56472)[0;0m INFO 08-09 14:01:08 [hpu_model_runner.py:3029] [Warmup][Graph/decode][47/135] batch_size:32 query_len:1 num_blocks:9344 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56470)[0;0m INFO 08-09 14:01:08 [hpu_model_runner.py:3029] [Warmup][Graph/decode][47/135] batch_size:32 query_len:1 num_blocks:9344 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56468)[0;0m INFO 08-09 14:01:08 [hpu_model_runner.py:3029] [Warmup][Graph/decode][47/135] batch_size:32 query_len:1 num_blocks:9344 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56469)[0;0m INFO 08-09 14:01:08 [hpu_model_runner.py:3029] [Warmup][Graph/decode][47/135] batch_size:32 query_len:1 num_blocks:9344 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56467)[0;0m INFO 08-09 14:01:08 [hpu_model_runner.py:3029] [Warmup][Graph/decode][47/135] batch_size:32 query_len:1 num_blocks:9344 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56473)[0;0m INFO 08-09 14:01:14 [hpu_model_runner.py:3029] [Warmup][Graph/decode][48/135] batch_size:32 query_len:1 num_blocks:6784 free_mem:23.73 GiB
INFO 08-09 14:01:14 [hpu_model_runner.py:3029] [Warmup][Graph/decode][48/135] batch_size:32 query_len:1 num_blocks:6784 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56472)[0;0m INFO 08-09 14:01:14 [hpu_model_runner.py:3029] [Warmup][Graph/decode][48/135] batch_size:32 query_len:1 num_blocks:6784 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56471)[0;0m INFO 08-09 14:01:14 [hpu_model_runner.py:3029] [Warmup][Graph/decode][48/135] batch_size:32 query_len:1 num_blocks:6784 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56470)[0;0m INFO 08-09 14:01:14 [hpu_model_runner.py:3029] [Warmup][Graph/decode][48/135] batch_size:32 query_len:1 num_blocks:6784 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56468)[0;0m INFO 08-09 14:01:14 [hpu_model_runner.py:3029] [Warmup][Graph/decode][48/135] batch_size:32 query_len:1 num_blocks:6784 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56469)[0;0m INFO 08-09 14:01:14 [hpu_model_runner.py:3029] [Warmup][Graph/decode][48/135] batch_size:32 query_len:1 num_blocks:6784 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56467)[0;0m INFO 08-09 14:01:14 [hpu_model_runner.py:3029] [Warmup][Graph/decode][48/135] batch_size:32 query_len:1 num_blocks:6784 free_mem:23.73 GiB
INFO 08-09 14:01:19 [hpu_model_runner.py:3029] [Warmup][Graph/decode][49/135] batch_size:32 query_len:1 num_blocks:4864 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56473)[0;0m INFO 08-09 14:01:19 [hpu_model_runner.py:3029] [Warmup][Graph/decode][49/135] batch_size:32 query_len:1 num_blocks:4864 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56471)[0;0m INFO 08-09 14:01:19 [hpu_model_runner.py:3029] [Warmup][Graph/decode][49/135] batch_size:32 query_len:1 num_blocks:4864 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56472)[0;0m INFO 08-09 14:01:19 [hpu_model_runner.py:3029] [Warmup][Graph/decode][49/135] batch_size:32 query_len:1 num_blocks:4864 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56470)[0;0m INFO 08-09 14:01:19 [hpu_model_runner.py:3029] [Warmup][Graph/decode][49/135] batch_size:32 query_len:1 num_blocks:4864 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56467)[0;0m INFO 08-09 14:01:19 [hpu_model_runner.py:3029] [Warmup][Graph/decode][49/135] batch_size:32 query_len:1 num_blocks:4864 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56468)[0;0m INFO 08-09 14:01:19 [hpu_model_runner.py:3029] [Warmup][Graph/decode][49/135] batch_size:32 query_len:1 num_blocks:4864 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56469)[0;0m INFO 08-09 14:01:19 [hpu_model_runner.py:3029] [Warmup][Graph/decode][49/135] batch_size:32 query_len:1 num_blocks:4864 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56473)[0;0m INFO 08-09 14:01:24 [hpu_model_runner.py:3029] [Warmup][Graph/decode][50/135] batch_size:32 query_len:1 num_blocks:3584 free_mem:23.73 GiB
INFO 08-09 14:01:24 [hpu_model_runner.py:3029] [Warmup][Graph/decode][50/135] batch_size:32 query_len:1 num_blocks:3584 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56472)[0;0m INFO 08-09 14:01:24 [hpu_model_runner.py:3029] [Warmup][Graph/decode][50/135] batch_size:32 query_len:1 num_blocks:3584 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56471)[0;0m INFO 08-09 14:01:24 [hpu_model_runner.py:3029] [Warmup][Graph/decode][50/135] batch_size:32 query_len:1 num_blocks:3584 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56470)[0;0m INFO 08-09 14:01:24 [hpu_model_runner.py:3029] [Warmup][Graph/decode][50/135] batch_size:32 query_len:1 num_blocks:3584 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56468)[0;0m INFO 08-09 14:01:24 [hpu_model_runner.py:3029] [Warmup][Graph/decode][50/135] batch_size:32 query_len:1 num_blocks:3584 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56469)[0;0m INFO 08-09 14:01:24 [hpu_model_runner.py:3029] [Warmup][Graph/decode][50/135] batch_size:32 query_len:1 num_blocks:3584 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56467)[0;0m INFO 08-09 14:01:24 [hpu_model_runner.py:3029] [Warmup][Graph/decode][50/135] batch_size:32 query_len:1 num_blocks:3584 free_mem:23.73 GiB
INFO 08-09 14:01:30 [hpu_model_runner.py:3029] [Warmup][Graph/decode][51/135] batch_size:32 query_len:1 num_blocks:2560 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56473)[0;0m INFO 08-09 14:01:30 [hpu_model_runner.py:3029] [Warmup][Graph/decode][51/135] batch_size:32 query_len:1 num_blocks:2560 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56472)[0;0m INFO 08-09 14:01:30 [hpu_model_runner.py:3029] [Warmup][Graph/decode][51/135] batch_size:32 query_len:1 num_blocks:2560 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56471)[0;0m INFO 08-09 14:01:30 [hpu_model_runner.py:3029] [Warmup][Graph/decode][51/135] batch_size:32 query_len:1 num_blocks:2560 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56470)[0;0m INFO 08-09 14:01:30 [hpu_model_runner.py:3029] [Warmup][Graph/decode][51/135] batch_size:32 query_len:1 num_blocks:2560 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56469)[0;0m INFO 08-09 14:01:30 [hpu_model_runner.py:3029] [Warmup][Graph/decode][51/135] batch_size:32 query_len:1 num_blocks:2560 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56468)[0;0m INFO 08-09 14:01:30 [hpu_model_runner.py:3029] [Warmup][Graph/decode][51/135] batch_size:32 query_len:1 num_blocks:2560 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56467)[0;0m INFO 08-09 14:01:30 [hpu_model_runner.py:3029] [Warmup][Graph/decode][51/135] batch_size:32 query_len:1 num_blocks:2560 free_mem:23.73 GiB
INFO 08-09 14:01:35 [hpu_model_runner.py:3029] [Warmup][Graph/decode][52/135] batch_size:32 query_len:1 num_blocks:1792 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56473)[0;0m INFO 08-09 14:01:35 [hpu_model_runner.py:3029] [Warmup][Graph/decode][52/135] batch_size:32 query_len:1 num_blocks:1792 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56472)[0;0m INFO 08-09 14:01:35 [hpu_model_runner.py:3029] [Warmup][Graph/decode][52/135] batch_size:32 query_len:1 num_blocks:1792 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56471)[0;0m INFO 08-09 14:01:35 [hpu_model_runner.py:3029] [Warmup][Graph/decode][52/135] batch_size:32 query_len:1 num_blocks:1792 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56470)[0;0m INFO 08-09 14:01:35 [hpu_model_runner.py:3029] [Warmup][Graph/decode][52/135] batch_size:32 query_len:1 num_blocks:1792 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56468)[0;0m INFO 08-09 14:01:35 [hpu_model_runner.py:3029] [Warmup][Graph/decode][52/135] batch_size:32 query_len:1 num_blocks:1792 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56467)[0;0m INFO 08-09 14:01:35 [hpu_model_runner.py:3029] [Warmup][Graph/decode][52/135] batch_size:32 query_len:1 num_blocks:1792 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56469)[0;0m INFO 08-09 14:01:35 [hpu_model_runner.py:3029] [Warmup][Graph/decode][52/135] batch_size:32 query_len:1 num_blocks:1792 free_mem:23.73 GiB
INFO 08-09 14:01:41 [hpu_model_runner.py:3029] [Warmup][Graph/decode][53/135] batch_size:32 query_len:1 num_blocks:1408 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56473)[0;0m INFO 08-09 14:01:41 [hpu_model_runner.py:3029] [Warmup][Graph/decode][53/135] batch_size:32 query_len:1 num_blocks:1408 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56472)[0;0m INFO 08-09 14:01:41 [hpu_model_runner.py:3029] [Warmup][Graph/decode][53/135] batch_size:32 query_len:1 num_blocks:1408 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56471)[0;0m INFO 08-09 14:01:41 [hpu_model_runner.py:3029] [Warmup][Graph/decode][53/135] batch_size:32 query_len:1 num_blocks:1408 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56470)[0;0m INFO 08-09 14:01:41 [hpu_model_runner.py:3029] [Warmup][Graph/decode][53/135] batch_size:32 query_len:1 num_blocks:1408 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56468)[0;0m INFO 08-09 14:01:41 [hpu_model_runner.py:3029] [Warmup][Graph/decode][53/135] batch_size:32 query_len:1 num_blocks:1408 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56467)[0;0m INFO 08-09 14:01:41 [hpu_model_runner.py:3029] [Warmup][Graph/decode][53/135] batch_size:32 query_len:1 num_blocks:1408 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56469)[0;0m INFO 08-09 14:01:41 [hpu_model_runner.py:3029] [Warmup][Graph/decode][53/135] batch_size:32 query_len:1 num_blocks:1408 free_mem:23.73 GiB
INFO 08-09 14:01:46 [hpu_model_runner.py:3029] [Warmup][Graph/decode][54/135] batch_size:32 query_len:1 num_blocks:1024 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56473)[0;0m INFO 08-09 14:01:46 [hpu_model_runner.py:3029] [Warmup][Graph/decode][54/135] batch_size:32 query_len:1 num_blocks:1024 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56472)[0;0m INFO 08-09 14:01:46 [hpu_model_runner.py:3029] [Warmup][Graph/decode][54/135] batch_size:32 query_len:1 num_blocks:1024 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56470)[0;0m INFO 08-09 14:01:46 [hpu_model_runner.py:3029] [Warmup][Graph/decode][54/135] batch_size:32 query_len:1 num_blocks:1024 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56469)[0;0m INFO 08-09 14:01:46 [hpu_model_runner.py:3029] [Warmup][Graph/decode][54/135] batch_size:32 query_len:1 num_blocks:1024 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56468)[0;0m INFO 08-09 14:01:46 [hpu_model_runner.py:3029] [Warmup][Graph/decode][54/135] batch_size:32 query_len:1 num_blocks:1024 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56471)[0;0m INFO 08-09 14:01:46 [hpu_model_runner.py:3029] [Warmup][Graph/decode][54/135] batch_size:32 query_len:1 num_blocks:1024 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56467)[0;0m INFO 08-09 14:01:46 [hpu_model_runner.py:3029] [Warmup][Graph/decode][54/135] batch_size:32 query_len:1 num_blocks:1024 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56473)[0;0m INFO 08-09 14:01:52 [hpu_model_runner.py:3029] [Warmup][Graph/decode][55/135] batch_size:32 query_len:1 num_blocks:768 free_mem:23.73 GiB
INFO 08-09 14:01:52 [hpu_model_runner.py:3029] [Warmup][Graph/decode][55/135] batch_size:32 query_len:1 num_blocks:768 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56472)[0;0m INFO 08-09 14:01:52 [hpu_model_runner.py:3029] [Warmup][Graph/decode][55/135] batch_size:32 query_len:1 num_blocks:768 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56471)[0;0m INFO 08-09 14:01:52 [hpu_model_runner.py:3029] [Warmup][Graph/decode][55/135] batch_size:32 query_len:1 num_blocks:768 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56470)[0;0m INFO 08-09 14:01:52 [hpu_model_runner.py:3029] [Warmup][Graph/decode][55/135] batch_size:32 query_len:1 num_blocks:768 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56469)[0;0m INFO 08-09 14:01:52 [hpu_model_runner.py:3029] [Warmup][Graph/decode][55/135] batch_size:32 query_len:1 num_blocks:768 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56468)[0;0m INFO 08-09 14:01:52 [hpu_model_runner.py:3029] [Warmup][Graph/decode][55/135] batch_size:32 query_len:1 num_blocks:768 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56467)[0;0m INFO 08-09 14:01:52 [hpu_model_runner.py:3029] [Warmup][Graph/decode][55/135] batch_size:32 query_len:1 num_blocks:768 free_mem:23.73 GiB
INFO 08-09 14:01:57 [hpu_model_runner.py:3029] [Warmup][Graph/decode][56/135] batch_size:32 query_len:1 num_blocks:640 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56473)[0;0m INFO 08-09 14:01:57 [hpu_model_runner.py:3029] [Warmup][Graph/decode][56/135] batch_size:32 query_len:1 num_blocks:640 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56472)[0;0m INFO 08-09 14:01:57 [hpu_model_runner.py:3029] [Warmup][Graph/decode][56/135] batch_size:32 query_len:1 num_blocks:640 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56471)[0;0m INFO 08-09 14:01:57 [hpu_model_runner.py:3029] [Warmup][Graph/decode][56/135] batch_size:32 query_len:1 num_blocks:640 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56468)[0;0m INFO 08-09 14:01:57 [hpu_model_runner.py:3029] [Warmup][Graph/decode][56/135] batch_size:32 query_len:1 num_blocks:640 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56470)[0;0m INFO 08-09 14:01:57 [hpu_model_runner.py:3029] [Warmup][Graph/decode][56/135] batch_size:32 query_len:1 num_blocks:640 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56469)[0;0m INFO 08-09 14:01:57 [hpu_model_runner.py:3029] [Warmup][Graph/decode][56/135] batch_size:32 query_len:1 num_blocks:640 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56467)[0;0m INFO 08-09 14:01:57 [hpu_model_runner.py:3029] [Warmup][Graph/decode][56/135] batch_size:32 query_len:1 num_blocks:640 free_mem:23.73 GiB
INFO 08-09 14:02:02 [hpu_model_runner.py:3029] [Warmup][Graph/decode][57/135] batch_size:32 query_len:1 num_blocks:512 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56473)[0;0m INFO 08-09 14:02:02 [hpu_model_runner.py:3029] [Warmup][Graph/decode][57/135] batch_size:32 query_len:1 num_blocks:512 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56472)[0;0m INFO 08-09 14:02:02 [hpu_model_runner.py:3029] [Warmup][Graph/decode][57/135] batch_size:32 query_len:1 num_blocks:512 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56471)[0;0m INFO 08-09 14:02:02 [hpu_model_runner.py:3029] [Warmup][Graph/decode][57/135] batch_size:32 query_len:1 num_blocks:512 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56470)[0;0m INFO 08-09 14:02:02 [hpu_model_runner.py:3029] [Warmup][Graph/decode][57/135] batch_size:32 query_len:1 num_blocks:512 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56468)[0;0m INFO 08-09 14:02:02 [hpu_model_runner.py:3029] [Warmup][Graph/decode][57/135] batch_size:32 query_len:1 num_blocks:512 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56469)[0;0m INFO 08-09 14:02:02 [hpu_model_runner.py:3029] [Warmup][Graph/decode][57/135] batch_size:32 query_len:1 num_blocks:512 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56467)[0;0m INFO 08-09 14:02:02 [hpu_model_runner.py:3029] [Warmup][Graph/decode][57/135] batch_size:32 query_len:1 num_blocks:512 free_mem:23.73 GiB
INFO 08-09 14:02:08 [hpu_model_runner.py:3029] [Warmup][Graph/decode][58/135] batch_size:32 query_len:1 num_blocks:384 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56473)[0;0m INFO 08-09 14:02:08 [hpu_model_runner.py:3029] [Warmup][Graph/decode][58/135] batch_size:32 query_len:1 num_blocks:384 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56471)[0;0m INFO 08-09 14:02:08 [hpu_model_runner.py:3029] [Warmup][Graph/decode][58/135] batch_size:32 query_len:1 num_blocks:384 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56470)[0;0m INFO 08-09 14:02:08 [hpu_model_runner.py:3029] [Warmup][Graph/decode][58/135] batch_size:32 query_len:1 num_blocks:384 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56472)[0;0m INFO 08-09 14:02:08 [hpu_model_runner.py:3029] [Warmup][Graph/decode][58/135] batch_size:32 query_len:1 num_blocks:384 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56468)[0;0m INFO 08-09 14:02:08 [hpu_model_runner.py:3029] [Warmup][Graph/decode][58/135] batch_size:32 query_len:1 num_blocks:384 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56469)[0;0m INFO 08-09 14:02:08 [hpu_model_runner.py:3029] [Warmup][Graph/decode][58/135] batch_size:32 query_len:1 num_blocks:384 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56467)[0;0m INFO 08-09 14:02:08 [hpu_model_runner.py:3029] [Warmup][Graph/decode][58/135] batch_size:32 query_len:1 num_blocks:384 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56473)[0;0m INFO 08-09 14:02:13 [hpu_model_runner.py:3029] [Warmup][Graph/decode][59/135] batch_size:32 query_len:1 num_blocks:256 free_mem:23.73 GiB
INFO 08-09 14:02:13 [hpu_model_runner.py:3029] [Warmup][Graph/decode][59/135] batch_size:32 query_len:1 num_blocks:256 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56472)[0;0m INFO 08-09 14:02:13 [hpu_model_runner.py:3029] [Warmup][Graph/decode][59/135] batch_size:32 query_len:1 num_blocks:256 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56471)[0;0m INFO 08-09 14:02:13 [hpu_model_runner.py:3029] [Warmup][Graph/decode][59/135] batch_size:32 query_len:1 num_blocks:256 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56467)[0;0m INFO 08-09 14:02:13 [hpu_model_runner.py:3029] [Warmup][Graph/decode][59/135] batch_size:32 query_len:1 num_blocks:256 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56470)[0;0m INFO 08-09 14:02:13 [hpu_model_runner.py:3029] [Warmup][Graph/decode][59/135] batch_size:32 query_len:1 num_blocks:256 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56468)[0;0m INFO 08-09 14:02:13 [hpu_model_runner.py:3029] [Warmup][Graph/decode][59/135] batch_size:32 query_len:1 num_blocks:256 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56469)[0;0m INFO 08-09 14:02:13 [hpu_model_runner.py:3029] [Warmup][Graph/decode][59/135] batch_size:32 query_len:1 num_blocks:256 free_mem:23.73 GiB
INFO 08-09 14:02:19 [hpu_model_runner.py:3029] [Warmup][Graph/decode][60/135] batch_size:32 query_len:1 num_blocks:128 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56473)[0;0m INFO 08-09 14:02:19 [hpu_model_runner.py:3029] [Warmup][Graph/decode][60/135] batch_size:32 query_len:1 num_blocks:128 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56471)[0;0m INFO 08-09 14:02:19 [hpu_model_runner.py:3029] [Warmup][Graph/decode][60/135] batch_size:32 query_len:1 num_blocks:128 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56472)[0;0m INFO 08-09 14:02:19 [hpu_model_runner.py:3029] [Warmup][Graph/decode][60/135] batch_size:32 query_len:1 num_blocks:128 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56468)[0;0m INFO 08-09 14:02:19 [hpu_model_runner.py:3029] [Warmup][Graph/decode][60/135] batch_size:32 query_len:1 num_blocks:128 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56469)[0;0m INFO 08-09 14:02:19 [hpu_model_runner.py:3029] [Warmup][Graph/decode][60/135] batch_size:32 query_len:1 num_blocks:128 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56467)[0;0m INFO 08-09 14:02:19 [hpu_model_runner.py:3029] [Warmup][Graph/decode][60/135] batch_size:32 query_len:1 num_blocks:128 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56470)[0;0m INFO 08-09 14:02:19 [hpu_model_runner.py:3029] [Warmup][Graph/decode][60/135] batch_size:32 query_len:1 num_blocks:128 free_mem:23.73 GiB
INFO 08-09 14:02:24 [hpu_model_runner.py:3029] [Warmup][Graph/decode][61/135] batch_size:16 query_len:1 num_blocks:12946 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56473)[0;0m INFO 08-09 14:02:24 [hpu_model_runner.py:3029] [Warmup][Graph/decode][61/135] batch_size:16 query_len:1 num_blocks:12946 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56471)[0;0m INFO 08-09 14:02:24 [hpu_model_runner.py:3029] [Warmup][Graph/decode][61/135] batch_size:16 query_len:1 num_blocks:12946 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56472)[0;0m INFO 08-09 14:02:24 [hpu_model_runner.py:3029] [Warmup][Graph/decode][61/135] batch_size:16 query_len:1 num_blocks:12946 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56470)[0;0m INFO 08-09 14:02:24 [hpu_model_runner.py:3029] [Warmup][Graph/decode][61/135] batch_size:16 query_len:1 num_blocks:12946 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56468)[0;0m INFO 08-09 14:02:24 [hpu_model_runner.py:3029] [Warmup][Graph/decode][61/135] batch_size:16 query_len:1 num_blocks:12946 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56467)[0;0m INFO 08-09 14:02:24 [hpu_model_runner.py:3029] [Warmup][Graph/decode][61/135] batch_size:16 query_len:1 num_blocks:12946 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56469)[0;0m INFO 08-09 14:02:24 [hpu_model_runner.py:3029] [Warmup][Graph/decode][61/135] batch_size:16 query_len:1 num_blocks:12946 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56473)[0;0m INFO 08-09 14:02:30 [hpu_model_runner.py:3029] [Warmup][Graph/decode][62/135] batch_size:16 query_len:1 num_blocks:9344 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56472)[0;0m INFO 08-09 14:02:30 [hpu_model_runner.py:3029] [Warmup][Graph/decode][62/135] batch_size:16 query_len:1 num_blocks:9344 free_mem:23.73 GiB
INFO 08-09 14:02:30 [hpu_model_runner.py:3029] [Warmup][Graph/decode][62/135] batch_size:16 query_len:1 num_blocks:9344 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56471)[0;0m INFO 08-09 14:02:30 [hpu_model_runner.py:3029] [Warmup][Graph/decode][62/135] batch_size:16 query_len:1 num_blocks:9344 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56470)[0;0m INFO 08-09 14:02:30 [hpu_model_runner.py:3029] [Warmup][Graph/decode][62/135] batch_size:16 query_len:1 num_blocks:9344 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56468)[0;0m INFO 08-09 14:02:30 [hpu_model_runner.py:3029] [Warmup][Graph/decode][62/135] batch_size:16 query_len:1 num_blocks:9344 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56467)[0;0m INFO 08-09 14:02:30 [hpu_model_runner.py:3029] [Warmup][Graph/decode][62/135] batch_size:16 query_len:1 num_blocks:9344 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56469)[0;0m INFO 08-09 14:02:30 [hpu_model_runner.py:3029] [Warmup][Graph/decode][62/135] batch_size:16 query_len:1 num_blocks:9344 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56473)[0;0m INFO 08-09 14:02:36 [hpu_model_runner.py:3029] [Warmup][Graph/decode][63/135] batch_size:16 query_len:1 num_blocks:6784 free_mem:23.73 GiB
INFO 08-09 14:02:36 [hpu_model_runner.py:3029] [Warmup][Graph/decode][63/135] batch_size:16 query_len:1 num_blocks:6784 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56472)[0;0m INFO 08-09 14:02:36 [hpu_model_runner.py:3029] [Warmup][Graph/decode][63/135] batch_size:16 query_len:1 num_blocks:6784 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56471)[0;0m INFO 08-09 14:02:36 [hpu_model_runner.py:3029] [Warmup][Graph/decode][63/135] batch_size:16 query_len:1 num_blocks:6784 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56468)[0;0m INFO 08-09 14:02:36 [hpu_model_runner.py:3029] [Warmup][Graph/decode][63/135] batch_size:16 query_len:1 num_blocks:6784 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56470)[0;0m INFO 08-09 14:02:36 [hpu_model_runner.py:3029] [Warmup][Graph/decode][63/135] batch_size:16 query_len:1 num_blocks:6784 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56469)[0;0m INFO 08-09 14:02:36 [hpu_model_runner.py:3029] [Warmup][Graph/decode][63/135] batch_size:16 query_len:1 num_blocks:6784 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56467)[0;0m INFO 08-09 14:02:36 [hpu_model_runner.py:3029] [Warmup][Graph/decode][63/135] batch_size:16 query_len:1 num_blocks:6784 free_mem:23.73 GiB
INFO 08-09 14:02:42 [hpu_model_runner.py:3029] [Warmup][Graph/decode][64/135] batch_size:16 query_len:1 num_blocks:4864 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56473)[0;0m INFO 08-09 14:02:42 [hpu_model_runner.py:3029] [Warmup][Graph/decode][64/135] batch_size:16 query_len:1 num_blocks:4864 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56472)[0;0m INFO 08-09 14:02:42 [hpu_model_runner.py:3029] [Warmup][Graph/decode][64/135] batch_size:16 query_len:1 num_blocks:4864 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56468)[0;0m INFO 08-09 14:02:42 [hpu_model_runner.py:3029] [Warmup][Graph/decode][64/135] batch_size:16 query_len:1 num_blocks:4864 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56470)[0;0m INFO 08-09 14:02:42 [hpu_model_runner.py:3029] [Warmup][Graph/decode][64/135] batch_size:16 query_len:1 num_blocks:4864 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56469)[0;0m INFO 08-09 14:02:42 [hpu_model_runner.py:3029] [Warmup][Graph/decode][64/135] batch_size:16 query_len:1 num_blocks:4864 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56471)[0;0m INFO 08-09 14:02:42 [hpu_model_runner.py:3029] [Warmup][Graph/decode][64/135] batch_size:16 query_len:1 num_blocks:4864 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56467)[0;0m INFO 08-09 14:02:42 [hpu_model_runner.py:3029] [Warmup][Graph/decode][64/135] batch_size:16 query_len:1 num_blocks:4864 free_mem:23.73 GiB
INFO 08-09 14:02:48 [hpu_model_runner.py:3029] [Warmup][Graph/decode][65/135] batch_size:16 query_len:1 num_blocks:3584 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56473)[0;0m INFO 08-09 14:02:48 [hpu_model_runner.py:3029] [Warmup][Graph/decode][65/135] batch_size:16 query_len:1 num_blocks:3584 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56467)[0;0m INFO 08-09 14:02:48 [hpu_model_runner.py:3029] [Warmup][Graph/decode][65/135] batch_size:16 query_len:1 num_blocks:3584 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56470)[0;0m INFO 08-09 14:02:48 [hpu_model_runner.py:3029] [Warmup][Graph/decode][65/135] batch_size:16 query_len:1 num_blocks:3584 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56468)[0;0m INFO 08-09 14:02:48 [hpu_model_runner.py:3029] [Warmup][Graph/decode][65/135] batch_size:16 query_len:1 num_blocks:3584 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56471)[0;0m INFO 08-09 14:02:48 [hpu_model_runner.py:3029] [Warmup][Graph/decode][65/135] batch_size:16 query_len:1 num_blocks:3584 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56469)[0;0m INFO 08-09 14:02:48 [hpu_model_runner.py:3029] [Warmup][Graph/decode][65/135] batch_size:16 query_len:1 num_blocks:3584 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56472)[0;0m INFO 08-09 14:02:48 [hpu_model_runner.py:3029] [Warmup][Graph/decode][65/135] batch_size:16 query_len:1 num_blocks:3584 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56473)[0;0m INFO 08-09 14:02:54 [hpu_model_runner.py:3029] [Warmup][Graph/decode][66/135] batch_size:16 query_len:1 num_blocks:2560 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56472)[0;0m INFO 08-09 14:02:54 [hpu_model_runner.py:3029] [Warmup][Graph/decode][66/135] batch_size:16 query_len:1 num_blocks:2560 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56471)[0;0m INFO 08-09 14:02:54 [hpu_model_runner.py:3029] [Warmup][Graph/decode][66/135] batch_size:16 query_len:1 num_blocks:2560 free_mem:23.73 GiB
INFO 08-09 14:02:54 [hpu_model_runner.py:3029] [Warmup][Graph/decode][66/135] batch_size:16 query_len:1 num_blocks:2560 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56470)[0;0m INFO 08-09 14:02:54 [hpu_model_runner.py:3029] [Warmup][Graph/decode][66/135] batch_size:16 query_len:1 num_blocks:2560 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56468)[0;0m INFO 08-09 14:02:54 [hpu_model_runner.py:3029] [Warmup][Graph/decode][66/135] batch_size:16 query_len:1 num_blocks:2560 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56469)[0;0m INFO 08-09 14:02:54 [hpu_model_runner.py:3029] [Warmup][Graph/decode][66/135] batch_size:16 query_len:1 num_blocks:2560 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56467)[0;0m INFO 08-09 14:02:54 [hpu_model_runner.py:3029] [Warmup][Graph/decode][66/135] batch_size:16 query_len:1 num_blocks:2560 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56473)[0;0m INFO 08-09 14:02:59 [hpu_model_runner.py:3029] [Warmup][Graph/decode][67/135] batch_size:16 query_len:1 num_blocks:1792 free_mem:23.73 GiB
INFO 08-09 14:02:59 [hpu_model_runner.py:3029] [Warmup][Graph/decode][67/135] batch_size:16 query_len:1 num_blocks:1792 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56472)[0;0m INFO 08-09 14:02:59 [hpu_model_runner.py:3029] [Warmup][Graph/decode][67/135] batch_size:16 query_len:1 num_blocks:1792 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56470)[0;0m INFO 08-09 14:02:59 [hpu_model_runner.py:3029] [Warmup][Graph/decode][67/135] batch_size:16 query_len:1 num_blocks:1792 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56471)[0;0m INFO 08-09 14:02:59 [hpu_model_runner.py:3029] [Warmup][Graph/decode][67/135] batch_size:16 query_len:1 num_blocks:1792 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56468)[0;0m INFO 08-09 14:02:59 [hpu_model_runner.py:3029] [Warmup][Graph/decode][67/135] batch_size:16 query_len:1 num_blocks:1792 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56467)[0;0m INFO 08-09 14:02:59 [hpu_model_runner.py:3029] [Warmup][Graph/decode][67/135] batch_size:16 query_len:1 num_blocks:1792 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56469)[0;0m INFO 08-09 14:02:59 [hpu_model_runner.py:3029] [Warmup][Graph/decode][67/135] batch_size:16 query_len:1 num_blocks:1792 free_mem:23.73 GiB
INFO 08-09 14:03:05 [hpu_model_runner.py:3029] [Warmup][Graph/decode][68/135] batch_size:16 query_len:1 num_blocks:1408 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56473)[0;0m INFO 08-09 14:03:05 [hpu_model_runner.py:3029] [Warmup][Graph/decode][68/135] batch_size:16 query_len:1 num_blocks:1408 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56471)[0;0m INFO 08-09 14:03:05 [hpu_model_runner.py:3029] [Warmup][Graph/decode][68/135] batch_size:16 query_len:1 num_blocks:1408 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56470)[0;0m INFO 08-09 14:03:05 [hpu_model_runner.py:3029] [Warmup][Graph/decode][68/135] batch_size:16 query_len:1 num_blocks:1408 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56467)[0;0m INFO 08-09 14:03:05 [hpu_model_runner.py:3029] [Warmup][Graph/decode][68/135] batch_size:16 query_len:1 num_blocks:1408 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56472)[0;0m INFO 08-09 14:03:05 [hpu_model_runner.py:3029] [Warmup][Graph/decode][68/135] batch_size:16 query_len:1 num_blocks:1408 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56468)[0;0m INFO 08-09 14:03:05 [hpu_model_runner.py:3029] [Warmup][Graph/decode][68/135] batch_size:16 query_len:1 num_blocks:1408 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56469)[0;0m INFO 08-09 14:03:05 [hpu_model_runner.py:3029] [Warmup][Graph/decode][68/135] batch_size:16 query_len:1 num_blocks:1408 free_mem:23.73 GiB
INFO 08-09 14:03:11 [hpu_model_runner.py:3029] [Warmup][Graph/decode][69/135] batch_size:16 query_len:1 num_blocks:1024 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56473)[0;0m INFO 08-09 14:03:11 [hpu_model_runner.py:3029] [Warmup][Graph/decode][69/135] batch_size:16 query_len:1 num_blocks:1024 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56471)[0;0m INFO 08-09 14:03:11 [hpu_model_runner.py:3029] [Warmup][Graph/decode][69/135] batch_size:16 query_len:1 num_blocks:1024 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56472)[0;0m INFO 08-09 14:03:11 [hpu_model_runner.py:3029] [Warmup][Graph/decode][69/135] batch_size:16 query_len:1 num_blocks:1024 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56470)[0;0m INFO 08-09 14:03:11 [hpu_model_runner.py:3029] [Warmup][Graph/decode][69/135] batch_size:16 query_len:1 num_blocks:1024 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56468)[0;0m INFO 08-09 14:03:11 [hpu_model_runner.py:3029] [Warmup][Graph/decode][69/135] batch_size:16 query_len:1 num_blocks:1024 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56467)[0;0m INFO 08-09 14:03:11 [hpu_model_runner.py:3029] [Warmup][Graph/decode][69/135] batch_size:16 query_len:1 num_blocks:1024 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56469)[0;0m INFO 08-09 14:03:11 [hpu_model_runner.py:3029] [Warmup][Graph/decode][69/135] batch_size:16 query_len:1 num_blocks:1024 free_mem:23.73 GiB
INFO 08-09 14:03:17 [hpu_model_runner.py:3029] [Warmup][Graph/decode][70/135] batch_size:16 query_len:1 num_blocks:768 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56473)[0;0m INFO 08-09 14:03:17 [hpu_model_runner.py:3029] [Warmup][Graph/decode][70/135] batch_size:16 query_len:1 num_blocks:768 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56471)[0;0m INFO 08-09 14:03:17 [hpu_model_runner.py:3029] [Warmup][Graph/decode][70/135] batch_size:16 query_len:1 num_blocks:768 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56472)[0;0m INFO 08-09 14:03:17 [hpu_model_runner.py:3029] [Warmup][Graph/decode][70/135] batch_size:16 query_len:1 num_blocks:768 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56468)[0;0m INFO 08-09 14:03:17 [hpu_model_runner.py:3029] [Warmup][Graph/decode][70/135] batch_size:16 query_len:1 num_blocks:768 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56469)[0;0m INFO 08-09 14:03:17 [hpu_model_runner.py:3029] [Warmup][Graph/decode][70/135] batch_size:16 query_len:1 num_blocks:768 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56470)[0;0m INFO 08-09 14:03:17 [hpu_model_runner.py:3029] [Warmup][Graph/decode][70/135] batch_size:16 query_len:1 num_blocks:768 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56467)[0;0m INFO 08-09 14:03:17 [hpu_model_runner.py:3029] [Warmup][Graph/decode][70/135] batch_size:16 query_len:1 num_blocks:768 free_mem:23.73 GiB
INFO 08-09 14:03:23 [hpu_model_runner.py:3029] [Warmup][Graph/decode][71/135] batch_size:16 query_len:1 num_blocks:640 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56472)[0;0m INFO 08-09 14:03:23 [hpu_model_runner.py:3029] [Warmup][Graph/decode][71/135] batch_size:16 query_len:1 num_blocks:640 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56473)[0;0m INFO 08-09 14:03:23 [hpu_model_runner.py:3029] [Warmup][Graph/decode][71/135] batch_size:16 query_len:1 num_blocks:640 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56470)[0;0m INFO 08-09 14:03:23 [hpu_model_runner.py:3029] [Warmup][Graph/decode][71/135] batch_size:16 query_len:1 num_blocks:640 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56468)[0;0m INFO 08-09 14:03:23 [hpu_model_runner.py:3029] [Warmup][Graph/decode][71/135] batch_size:16 query_len:1 num_blocks:640 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56471)[0;0m INFO 08-09 14:03:23 [hpu_model_runner.py:3029] [Warmup][Graph/decode][71/135] batch_size:16 query_len:1 num_blocks:640 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56469)[0;0m INFO 08-09 14:03:23 [hpu_model_runner.py:3029] [Warmup][Graph/decode][71/135] batch_size:16 query_len:1 num_blocks:640 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56467)[0;0m INFO 08-09 14:03:23 [hpu_model_runner.py:3029] [Warmup][Graph/decode][71/135] batch_size:16 query_len:1 num_blocks:640 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56473)[0;0m INFO 08-09 14:03:28 [hpu_model_runner.py:3029] [Warmup][Graph/decode][72/135] batch_size:16 query_len:1 num_blocks:512 free_mem:23.73 GiB
INFO 08-09 14:03:28 [hpu_model_runner.py:3029] [Warmup][Graph/decode][72/135] batch_size:16 query_len:1 num_blocks:512 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56472)[0;0m INFO 08-09 14:03:28 [hpu_model_runner.py:3029] [Warmup][Graph/decode][72/135] batch_size:16 query_len:1 num_blocks:512 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56470)[0;0m INFO 08-09 14:03:28 [hpu_model_runner.py:3029] [Warmup][Graph/decode][72/135] batch_size:16 query_len:1 num_blocks:512 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56471)[0;0m INFO 08-09 14:03:28 [hpu_model_runner.py:3029] [Warmup][Graph/decode][72/135] batch_size:16 query_len:1 num_blocks:512 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56468)[0;0m INFO 08-09 14:03:28 [hpu_model_runner.py:3029] [Warmup][Graph/decode][72/135] batch_size:16 query_len:1 num_blocks:512 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56469)[0;0m INFO 08-09 14:03:28 [hpu_model_runner.py:3029] [Warmup][Graph/decode][72/135] batch_size:16 query_len:1 num_blocks:512 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56467)[0;0m INFO 08-09 14:03:28 [hpu_model_runner.py:3029] [Warmup][Graph/decode][72/135] batch_size:16 query_len:1 num_blocks:512 free_mem:23.73 GiB
INFO 08-09 14:03:34 [hpu_model_runner.py:3029] [Warmup][Graph/decode][73/135] batch_size:16 query_len:1 num_blocks:384 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56473)[0;0m INFO 08-09 14:03:34 [hpu_model_runner.py:3029] [Warmup][Graph/decode][73/135] batch_size:16 query_len:1 num_blocks:384 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56472)[0;0m INFO 08-09 14:03:34 [hpu_model_runner.py:3029] [Warmup][Graph/decode][73/135] batch_size:16 query_len:1 num_blocks:384 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56471)[0;0m INFO 08-09 14:03:34 [hpu_model_runner.py:3029] [Warmup][Graph/decode][73/135] batch_size:16 query_len:1 num_blocks:384 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56470)[0;0m INFO 08-09 14:03:34 [hpu_model_runner.py:3029] [Warmup][Graph/decode][73/135] batch_size:16 query_len:1 num_blocks:384 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56468)[0;0m INFO 08-09 14:03:34 [hpu_model_runner.py:3029] [Warmup][Graph/decode][73/135] batch_size:16 query_len:1 num_blocks:384 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56469)[0;0m INFO 08-09 14:03:34 [hpu_model_runner.py:3029] [Warmup][Graph/decode][73/135] batch_size:16 query_len:1 num_blocks:384 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56467)[0;0m INFO 08-09 14:03:34 [hpu_model_runner.py:3029] [Warmup][Graph/decode][73/135] batch_size:16 query_len:1 num_blocks:384 free_mem:23.73 GiB
INFO 08-09 14:03:40 [hpu_model_runner.py:3029] [Warmup][Graph/decode][74/135] batch_size:16 query_len:1 num_blocks:256 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56473)[0;0m INFO 08-09 14:03:40 [hpu_model_runner.py:3029] [Warmup][Graph/decode][74/135] batch_size:16 query_len:1 num_blocks:256 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56472)[0;0m INFO 08-09 14:03:40 [hpu_model_runner.py:3029] [Warmup][Graph/decode][74/135] batch_size:16 query_len:1 num_blocks:256 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56470)[0;0m INFO 08-09 14:03:40 [hpu_model_runner.py:3029] [Warmup][Graph/decode][74/135] batch_size:16 query_len:1 num_blocks:256 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56471)[0;0m INFO 08-09 14:03:40 [hpu_model_runner.py:3029] [Warmup][Graph/decode][74/135] batch_size:16 query_len:1 num_blocks:256 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56469)[0;0m INFO 08-09 14:03:40 [hpu_model_runner.py:3029] [Warmup][Graph/decode][74/135] batch_size:16 query_len:1 num_blocks:256 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56468)[0;0m INFO 08-09 14:03:40 [hpu_model_runner.py:3029] [Warmup][Graph/decode][74/135] batch_size:16 query_len:1 num_blocks:256 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56467)[0;0m INFO 08-09 14:03:40 [hpu_model_runner.py:3029] [Warmup][Graph/decode][74/135] batch_size:16 query_len:1 num_blocks:256 free_mem:23.73 GiB
INFO 08-09 14:03:46 [hpu_model_runner.py:3029] [Warmup][Graph/decode][75/135] batch_size:16 query_len:1 num_blocks:128 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56471)[0;0m INFO 08-09 14:03:46 [hpu_model_runner.py:3029] [Warmup][Graph/decode][75/135] batch_size:16 query_len:1 num_blocks:128 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56470)[0;0m INFO 08-09 14:03:46 [hpu_model_runner.py:3029] [Warmup][Graph/decode][75/135] batch_size:16 query_len:1 num_blocks:128 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56473)[0;0m INFO 08-09 14:03:46 [hpu_model_runner.py:3029] [Warmup][Graph/decode][75/135] batch_size:16 query_len:1 num_blocks:128 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56472)[0;0m INFO 08-09 14:03:46 [hpu_model_runner.py:3029] [Warmup][Graph/decode][75/135] batch_size:16 query_len:1 num_blocks:128 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56468)[0;0m INFO 08-09 14:03:46 [hpu_model_runner.py:3029] [Warmup][Graph/decode][75/135] batch_size:16 query_len:1 num_blocks:128 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56467)[0;0m INFO 08-09 14:03:46 [hpu_model_runner.py:3029] [Warmup][Graph/decode][75/135] batch_size:16 query_len:1 num_blocks:128 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56469)[0;0m INFO 08-09 14:03:46 [hpu_model_runner.py:3029] [Warmup][Graph/decode][75/135] batch_size:16 query_len:1 num_blocks:128 free_mem:23.73 GiB
INFO 08-09 14:03:52 [hpu_model_runner.py:3029] [Warmup][Graph/decode][76/135] batch_size:8 query_len:1 num_blocks:12946 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56473)[0;0m INFO 08-09 14:03:52 [hpu_model_runner.py:3029] [Warmup][Graph/decode][76/135] batch_size:8 query_len:1 num_blocks:12946 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56472)[0;0m INFO 08-09 14:03:52 [hpu_model_runner.py:3029] [Warmup][Graph/decode][76/135] batch_size:8 query_len:1 num_blocks:12946 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56471)[0;0m INFO 08-09 14:03:52 [hpu_model_runner.py:3029] [Warmup][Graph/decode][76/135] batch_size:8 query_len:1 num_blocks:12946 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56468)[0;0m INFO 08-09 14:03:52 [hpu_model_runner.py:3029] [Warmup][Graph/decode][76/135] batch_size:8 query_len:1 num_blocks:12946 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56467)[0;0m INFO 08-09 14:03:52 [hpu_model_runner.py:3029] [Warmup][Graph/decode][76/135] batch_size:8 query_len:1 num_blocks:12946 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56469)[0;0m INFO 08-09 14:03:52 [hpu_model_runner.py:3029] [Warmup][Graph/decode][76/135] batch_size:8 query_len:1 num_blocks:12946 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56470)[0;0m INFO 08-09 14:03:52 [hpu_model_runner.py:3029] [Warmup][Graph/decode][76/135] batch_size:8 query_len:1 num_blocks:12946 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56473)[0;0m INFO 08-09 14:03:58 [hpu_model_runner.py:3029] [Warmup][Graph/decode][77/135] batch_size:8 query_len:1 num_blocks:9344 free_mem:23.73 GiB
INFO 08-09 14:03:58 [hpu_model_runner.py:3029] [Warmup][Graph/decode][77/135] batch_size:8 query_len:1 num_blocks:9344 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56471)[0;0m INFO 08-09 14:03:58 [hpu_model_runner.py:3029] [Warmup][Graph/decode][77/135] batch_size:8 query_len:1 num_blocks:9344 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56472)[0;0m INFO 08-09 14:03:58 [hpu_model_runner.py:3029] [Warmup][Graph/decode][77/135] batch_size:8 query_len:1 num_blocks:9344 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56470)[0;0m INFO 08-09 14:03:58 [hpu_model_runner.py:3029] [Warmup][Graph/decode][77/135] batch_size:8 query_len:1 num_blocks:9344 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56468)[0;0m INFO 08-09 14:03:58 [hpu_model_runner.py:3029] [Warmup][Graph/decode][77/135] batch_size:8 query_len:1 num_blocks:9344 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56469)[0;0m INFO 08-09 14:03:58 [hpu_model_runner.py:3029] [Warmup][Graph/decode][77/135] batch_size:8 query_len:1 num_blocks:9344 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56467)[0;0m INFO 08-09 14:03:58 [hpu_model_runner.py:3029] [Warmup][Graph/decode][77/135] batch_size:8 query_len:1 num_blocks:9344 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56473)[0;0m INFO 08-09 14:04:04 [hpu_model_runner.py:3029] [Warmup][Graph/decode][78/135] batch_size:8 query_len:1 num_blocks:6784 free_mem:23.73 GiB
INFO 08-09 14:04:04 [hpu_model_runner.py:3029] [Warmup][Graph/decode][78/135] batch_size:8 query_len:1 num_blocks:6784 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56472)[0;0m INFO 08-09 14:04:04 [hpu_model_runner.py:3029] [Warmup][Graph/decode][78/135] batch_size:8 query_len:1 num_blocks:6784 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56471)[0;0m INFO 08-09 14:04:04 [hpu_model_runner.py:3029] [Warmup][Graph/decode][78/135] batch_size:8 query_len:1 num_blocks:6784 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56470)[0;0m INFO 08-09 14:04:04 [hpu_model_runner.py:3029] [Warmup][Graph/decode][78/135] batch_size:8 query_len:1 num_blocks:6784 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56467)[0;0m INFO 08-09 14:04:04 [hpu_model_runner.py:3029] [Warmup][Graph/decode][78/135] batch_size:8 query_len:1 num_blocks:6784 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56468)[0;0m INFO 08-09 14:04:04 [hpu_model_runner.py:3029] [Warmup][Graph/decode][78/135] batch_size:8 query_len:1 num_blocks:6784 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56469)[0;0m INFO 08-09 14:04:04 [hpu_model_runner.py:3029] [Warmup][Graph/decode][78/135] batch_size:8 query_len:1 num_blocks:6784 free_mem:23.73 GiB
INFO 08-09 14:04:10 [hpu_model_runner.py:3029] [Warmup][Graph/decode][79/135] batch_size:8 query_len:1 num_blocks:4864 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56473)[0;0m INFO 08-09 14:04:10 [hpu_model_runner.py:3029] [Warmup][Graph/decode][79/135] batch_size:8 query_len:1 num_blocks:4864 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56472)[0;0m INFO 08-09 14:04:10 [hpu_model_runner.py:3029] [Warmup][Graph/decode][79/135] batch_size:8 query_len:1 num_blocks:4864 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56471)[0;0m INFO 08-09 14:04:10 [hpu_model_runner.py:3029] [Warmup][Graph/decode][79/135] batch_size:8 query_len:1 num_blocks:4864 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56470)[0;0m INFO 08-09 14:04:10 [hpu_model_runner.py:3029] [Warmup][Graph/decode][79/135] batch_size:8 query_len:1 num_blocks:4864 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56468)[0;0m INFO 08-09 14:04:10 [hpu_model_runner.py:3029] [Warmup][Graph/decode][79/135] batch_size:8 query_len:1 num_blocks:4864 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56467)[0;0m INFO 08-09 14:04:10 [hpu_model_runner.py:3029] [Warmup][Graph/decode][79/135] batch_size:8 query_len:1 num_blocks:4864 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56469)[0;0m INFO 08-09 14:04:10 [hpu_model_runner.py:3029] [Warmup][Graph/decode][79/135] batch_size:8 query_len:1 num_blocks:4864 free_mem:23.73 GiB
INFO 08-09 14:04:16 [hpu_model_runner.py:3029] [Warmup][Graph/decode][80/135] batch_size:8 query_len:1 num_blocks:3584 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56473)[0;0m INFO 08-09 14:04:16 [hpu_model_runner.py:3029] [Warmup][Graph/decode][80/135] batch_size:8 query_len:1 num_blocks:3584 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56471)[0;0m INFO 08-09 14:04:16 [hpu_model_runner.py:3029] [Warmup][Graph/decode][80/135] batch_size:8 query_len:1 num_blocks:3584 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56470)[0;0m INFO 08-09 14:04:16 [hpu_model_runner.py:3029] [Warmup][Graph/decode][80/135] batch_size:8 query_len:1 num_blocks:3584 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56472)[0;0m INFO 08-09 14:04:16 [hpu_model_runner.py:3029] [Warmup][Graph/decode][80/135] batch_size:8 query_len:1 num_blocks:3584 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56468)[0;0m INFO 08-09 14:04:16 [hpu_model_runner.py:3029] [Warmup][Graph/decode][80/135] batch_size:8 query_len:1 num_blocks:3584 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56469)[0;0m INFO 08-09 14:04:16 [hpu_model_runner.py:3029] [Warmup][Graph/decode][80/135] batch_size:8 query_len:1 num_blocks:3584 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56467)[0;0m INFO 08-09 14:04:16 [hpu_model_runner.py:3029] [Warmup][Graph/decode][80/135] batch_size:8 query_len:1 num_blocks:3584 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56473)[0;0m INFO 08-09 14:04:23 [hpu_model_runner.py:3029] [Warmup][Graph/decode][81/135] batch_size:8 query_len:1 num_blocks:2560 free_mem:23.73 GiB
INFO 08-09 14:04:23 [hpu_model_runner.py:3029] [Warmup][Graph/decode][81/135] batch_size:8 query_len:1 num_blocks:2560 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56472)[0;0m INFO 08-09 14:04:23 [hpu_model_runner.py:3029] [Warmup][Graph/decode][81/135] batch_size:8 query_len:1 num_blocks:2560 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56471)[0;0m INFO 08-09 14:04:23 [hpu_model_runner.py:3029] [Warmup][Graph/decode][81/135] batch_size:8 query_len:1 num_blocks:2560 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56470)[0;0m INFO 08-09 14:04:23 [hpu_model_runner.py:3029] [Warmup][Graph/decode][81/135] batch_size:8 query_len:1 num_blocks:2560 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56469)[0;0m INFO 08-09 14:04:23 [hpu_model_runner.py:3029] [Warmup][Graph/decode][81/135] batch_size:8 query_len:1 num_blocks:2560 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56467)[0;0m INFO 08-09 14:04:23 [hpu_model_runner.py:3029] [Warmup][Graph/decode][81/135] batch_size:8 query_len:1 num_blocks:2560 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56468)[0;0m INFO 08-09 14:04:23 [hpu_model_runner.py:3029] [Warmup][Graph/decode][81/135] batch_size:8 query_len:1 num_blocks:2560 free_mem:23.73 GiB
INFO 08-09 14:04:29 [hpu_model_runner.py:3029] [Warmup][Graph/decode][82/135] batch_size:8 query_len:1 num_blocks:1792 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56468)[0;0m INFO 08-09 14:04:29 [hpu_model_runner.py:3029] [Warmup][Graph/decode][82/135] batch_size:8 query_len:1 num_blocks:1792 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56472)[0;0m INFO 08-09 14:04:29 [hpu_model_runner.py:3029] [Warmup][Graph/decode][82/135] batch_size:8 query_len:1 num_blocks:1792 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56473)[0;0m INFO 08-09 14:04:29 [hpu_model_runner.py:3029] [Warmup][Graph/decode][82/135] batch_size:8 query_len:1 num_blocks:1792 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56471)[0;0m INFO 08-09 14:04:29 [hpu_model_runner.py:3029] [Warmup][Graph/decode][82/135] batch_size:8 query_len:1 num_blocks:1792 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56470)[0;0m INFO 08-09 14:04:29 [hpu_model_runner.py:3029] [Warmup][Graph/decode][82/135] batch_size:8 query_len:1 num_blocks:1792 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56467)[0;0m INFO 08-09 14:04:29 [hpu_model_runner.py:3029] [Warmup][Graph/decode][82/135] batch_size:8 query_len:1 num_blocks:1792 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56469)[0;0m INFO 08-09 14:04:29 [hpu_model_runner.py:3029] [Warmup][Graph/decode][82/135] batch_size:8 query_len:1 num_blocks:1792 free_mem:23.73 GiB
INFO 08-09 14:04:35 [hpu_model_runner.py:3029] [Warmup][Graph/decode][83/135] batch_size:8 query_len:1 num_blocks:1408 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56473)[0;0m INFO 08-09 14:04:35 [hpu_model_runner.py:3029] [Warmup][Graph/decode][83/135] batch_size:8 query_len:1 num_blocks:1408 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56472)[0;0m INFO 08-09 14:04:35 [hpu_model_runner.py:3029] [Warmup][Graph/decode][83/135] batch_size:8 query_len:1 num_blocks:1408 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56468)[0;0m INFO 08-09 14:04:35 [hpu_model_runner.py:3029] [Warmup][Graph/decode][83/135] batch_size:8 query_len:1 num_blocks:1408 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56471)[0;0m INFO 08-09 14:04:35 [hpu_model_runner.py:3029] [Warmup][Graph/decode][83/135] batch_size:8 query_len:1 num_blocks:1408 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56470)[0;0m INFO 08-09 14:04:35 [hpu_model_runner.py:3029] [Warmup][Graph/decode][83/135] batch_size:8 query_len:1 num_blocks:1408 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56467)[0;0m INFO 08-09 14:04:35 [hpu_model_runner.py:3029] [Warmup][Graph/decode][83/135] batch_size:8 query_len:1 num_blocks:1408 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56469)[0;0m INFO 08-09 14:04:35 [hpu_model_runner.py:3029] [Warmup][Graph/decode][83/135] batch_size:8 query_len:1 num_blocks:1408 free_mem:23.73 GiB
INFO 08-09 14:04:41 [hpu_model_runner.py:3029] [Warmup][Graph/decode][84/135] batch_size:8 query_len:1 num_blocks:1024 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56467)[0;0m INFO 08-09 14:04:41 [hpu_model_runner.py:3029] [Warmup][Graph/decode][84/135] batch_size:8 query_len:1 num_blocks:1024 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56473)[0;0m INFO 08-09 14:04:41 [hpu_model_runner.py:3029] [Warmup][Graph/decode][84/135] batch_size:8 query_len:1 num_blocks:1024 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56470)[0;0m INFO 08-09 14:04:41 [hpu_model_runner.py:3029] [Warmup][Graph/decode][84/135] batch_size:8 query_len:1 num_blocks:1024 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56468)[0;0m INFO 08-09 14:04:41 [hpu_model_runner.py:3029] [Warmup][Graph/decode][84/135] batch_size:8 query_len:1 num_blocks:1024 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56472)[0;0m INFO 08-09 14:04:41 [hpu_model_runner.py:3029] [Warmup][Graph/decode][84/135] batch_size:8 query_len:1 num_blocks:1024 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56469)[0;0m INFO 08-09 14:04:41 [hpu_model_runner.py:3029] [Warmup][Graph/decode][84/135] batch_size:8 query_len:1 num_blocks:1024 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56471)[0;0m INFO 08-09 14:04:41 [hpu_model_runner.py:3029] [Warmup][Graph/decode][84/135] batch_size:8 query_len:1 num_blocks:1024 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56473)[0;0m INFO 08-09 14:04:47 [hpu_model_runner.py:3029] [Warmup][Graph/decode][85/135] batch_size:8 query_len:1 num_blocks:768 free_mem:23.73 GiB
INFO 08-09 14:04:47 [hpu_model_runner.py:3029] [Warmup][Graph/decode][85/135] batch_size:8 query_len:1 num_blocks:768 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56471)[0;0m INFO 08-09 14:04:47 [hpu_model_runner.py:3029] [Warmup][Graph/decode][85/135] batch_size:8 query_len:1 num_blocks:768 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56472)[0;0m INFO 08-09 14:04:47 [hpu_model_runner.py:3029] [Warmup][Graph/decode][85/135] batch_size:8 query_len:1 num_blocks:768 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56470)[0;0m INFO 08-09 14:04:47 [hpu_model_runner.py:3029] [Warmup][Graph/decode][85/135] batch_size:8 query_len:1 num_blocks:768 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56467)[0;0m INFO 08-09 14:04:47 [hpu_model_runner.py:3029] [Warmup][Graph/decode][85/135] batch_size:8 query_len:1 num_blocks:768 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56468)[0;0m INFO 08-09 14:04:47 [hpu_model_runner.py:3029] [Warmup][Graph/decode][85/135] batch_size:8 query_len:1 num_blocks:768 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56469)[0;0m INFO 08-09 14:04:47 [hpu_model_runner.py:3029] [Warmup][Graph/decode][85/135] batch_size:8 query_len:1 num_blocks:768 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56473)[0;0m INFO 08-09 14:04:53 [hpu_model_runner.py:3029] [Warmup][Graph/decode][86/135] batch_size:8 query_len:1 num_blocks:640 free_mem:23.73 GiB
INFO 08-09 14:04:53 [hpu_model_runner.py:3029] [Warmup][Graph/decode][86/135] batch_size:8 query_len:1 num_blocks:640 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56472)[0;0m INFO 08-09 14:04:53 [hpu_model_runner.py:3029] [Warmup][Graph/decode][86/135] batch_size:8 query_len:1 num_blocks:640 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56471)[0;0m INFO 08-09 14:04:53 [hpu_model_runner.py:3029] [Warmup][Graph/decode][86/135] batch_size:8 query_len:1 num_blocks:640 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56470)[0;0m INFO 08-09 14:04:53 [hpu_model_runner.py:3029] [Warmup][Graph/decode][86/135] batch_size:8 query_len:1 num_blocks:640 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56468)[0;0m INFO 08-09 14:04:53 [hpu_model_runner.py:3029] [Warmup][Graph/decode][86/135] batch_size:8 query_len:1 num_blocks:640 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56467)[0;0m INFO 08-09 14:04:53 [hpu_model_runner.py:3029] [Warmup][Graph/decode][86/135] batch_size:8 query_len:1 num_blocks:640 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56469)[0;0m INFO 08-09 14:04:53 [hpu_model_runner.py:3029] [Warmup][Graph/decode][86/135] batch_size:8 query_len:1 num_blocks:640 free_mem:23.73 GiB
INFO 08-09 14:05:00 [hpu_model_runner.py:3029] [Warmup][Graph/decode][87/135] batch_size:8 query_len:1 num_blocks:512 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56473)[0;0m INFO 08-09 14:05:00 [hpu_model_runner.py:3029] [Warmup][Graph/decode][87/135] batch_size:8 query_len:1 num_blocks:512 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56472)[0;0m INFO 08-09 14:05:00 [hpu_model_runner.py:3029] [Warmup][Graph/decode][87/135] batch_size:8 query_len:1 num_blocks:512 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56471)[0;0m INFO 08-09 14:05:00 [hpu_model_runner.py:3029] [Warmup][Graph/decode][87/135] batch_size:8 query_len:1 num_blocks:512 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56470)[0;0m INFO 08-09 14:05:00 [hpu_model_runner.py:3029] [Warmup][Graph/decode][87/135] batch_size:8 query_len:1 num_blocks:512 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56468)[0;0m INFO 08-09 14:05:00 [hpu_model_runner.py:3029] [Warmup][Graph/decode][87/135] batch_size:8 query_len:1 num_blocks:512 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56469)[0;0m INFO 08-09 14:05:00 [hpu_model_runner.py:3029] [Warmup][Graph/decode][87/135] batch_size:8 query_len:1 num_blocks:512 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56467)[0;0m INFO 08-09 14:05:00 [hpu_model_runner.py:3029] [Warmup][Graph/decode][87/135] batch_size:8 query_len:1 num_blocks:512 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56473)[0;0m INFO 08-09 14:05:06 [hpu_model_runner.py:3029] [Warmup][Graph/decode][88/135] batch_size:8 query_len:1 num_blocks:384 free_mem:23.73 GiB
INFO 08-09 14:05:06 [hpu_model_runner.py:3029] [Warmup][Graph/decode][88/135] batch_size:8 query_len:1 num_blocks:384 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56472)[0;0m INFO 08-09 14:05:06 [hpu_model_runner.py:3029] [Warmup][Graph/decode][88/135] batch_size:8 query_len:1 num_blocks:384 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56471)[0;0m INFO 08-09 14:05:06 [hpu_model_runner.py:3029] [Warmup][Graph/decode][88/135] batch_size:8 query_len:1 num_blocks:384 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56470)[0;0m INFO 08-09 14:05:06 [hpu_model_runner.py:3029] [Warmup][Graph/decode][88/135] batch_size:8 query_len:1 num_blocks:384 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56468)[0;0m INFO 08-09 14:05:06 [hpu_model_runner.py:3029] [Warmup][Graph/decode][88/135] batch_size:8 query_len:1 num_blocks:384 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56467)[0;0m INFO 08-09 14:05:06 [hpu_model_runner.py:3029] [Warmup][Graph/decode][88/135] batch_size:8 query_len:1 num_blocks:384 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56469)[0;0m INFO 08-09 14:05:06 [hpu_model_runner.py:3029] [Warmup][Graph/decode][88/135] batch_size:8 query_len:1 num_blocks:384 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56473)[0;0m INFO 08-09 14:05:12 [hpu_model_runner.py:3029] [Warmup][Graph/decode][89/135] batch_size:8 query_len:1 num_blocks:256 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56471)[0;0m INFO 08-09 14:05:12 [hpu_model_runner.py:3029] [Warmup][Graph/decode][89/135] batch_size:8 query_len:1 num_blocks:256 free_mem:23.73 GiB
INFO 08-09 14:05:12 [hpu_model_runner.py:3029] [Warmup][Graph/decode][89/135] batch_size:8 query_len:1 num_blocks:256 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56470)[0;0m INFO 08-09 14:05:12 [hpu_model_runner.py:3029] [Warmup][Graph/decode][89/135] batch_size:8 query_len:1 num_blocks:256 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56468)[0;0m INFO 08-09 14:05:12 [hpu_model_runner.py:3029] [Warmup][Graph/decode][89/135] batch_size:8 query_len:1 num_blocks:256 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56472)[0;0m INFO 08-09 14:05:12 [hpu_model_runner.py:3029] [Warmup][Graph/decode][89/135] batch_size:8 query_len:1 num_blocks:256 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56469)[0;0m INFO 08-09 14:05:12 [hpu_model_runner.py:3029] [Warmup][Graph/decode][89/135] batch_size:8 query_len:1 num_blocks:256 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56467)[0;0m INFO 08-09 14:05:12 [hpu_model_runner.py:3029] [Warmup][Graph/decode][89/135] batch_size:8 query_len:1 num_blocks:256 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56473)[0;0m INFO 08-09 14:05:18 [hpu_model_runner.py:3029] [Warmup][Graph/decode][90/135] batch_size:8 query_len:1 num_blocks:128 free_mem:23.73 GiB
INFO 08-09 14:05:18 [hpu_model_runner.py:3029] [Warmup][Graph/decode][90/135] batch_size:8 query_len:1 num_blocks:128 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56471)[0;0m INFO 08-09 14:05:18 [hpu_model_runner.py:3029] [Warmup][Graph/decode][90/135] batch_size:8 query_len:1 num_blocks:128 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56472)[0;0m INFO 08-09 14:05:18 [hpu_model_runner.py:3029] [Warmup][Graph/decode][90/135] batch_size:8 query_len:1 num_blocks:128 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56470)[0;0m INFO 08-09 14:05:18 [hpu_model_runner.py:3029] [Warmup][Graph/decode][90/135] batch_size:8 query_len:1 num_blocks:128 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56467)[0;0m INFO 08-09 14:05:18 [hpu_model_runner.py:3029] [Warmup][Graph/decode][90/135] batch_size:8 query_len:1 num_blocks:128 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56468)[0;0m INFO 08-09 14:05:18 [hpu_model_runner.py:3029] [Warmup][Graph/decode][90/135] batch_size:8 query_len:1 num_blocks:128 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56469)[0;0m INFO 08-09 14:05:18 [hpu_model_runner.py:3029] [Warmup][Graph/decode][90/135] batch_size:8 query_len:1 num_blocks:128 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56473)[0;0m INFO 08-09 14:05:24 [hpu_model_runner.py:3029] [Warmup][Graph/decode][91/135] batch_size:4 query_len:1 num_blocks:12946 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56471)[0;0m INFO 08-09 14:05:24 [hpu_model_runner.py:3029] [Warmup][Graph/decode][91/135] batch_size:4 query_len:1 num_blocks:12946 free_mem:23.73 GiB
INFO 08-09 14:05:24 [hpu_model_runner.py:3029] [Warmup][Graph/decode][91/135] batch_size:4 query_len:1 num_blocks:12946 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56467)[0;0m INFO 08-09 14:05:24 [hpu_model_runner.py:3029] [Warmup][Graph/decode][91/135] batch_size:4 query_len:1 num_blocks:12946 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56468)[0;0m INFO 08-09 14:05:24 [hpu_model_runner.py:3029] [Warmup][Graph/decode][91/135] batch_size:4 query_len:1 num_blocks:12946 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56469)[0;0m INFO 08-09 14:05:24 [hpu_model_runner.py:3029] [Warmup][Graph/decode][91/135] batch_size:4 query_len:1 num_blocks:12946 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56470)[0;0m INFO 08-09 14:05:24 [hpu_model_runner.py:3029] [Warmup][Graph/decode][91/135] batch_size:4 query_len:1 num_blocks:12946 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56472)[0;0m INFO 08-09 14:05:24 [hpu_model_runner.py:3029] [Warmup][Graph/decode][91/135] batch_size:4 query_len:1 num_blocks:12946 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56473)[0;0m INFO 08-09 14:05:31 [hpu_model_runner.py:3029] [Warmup][Graph/decode][92/135] batch_size:4 query_len:1 num_blocks:9344 free_mem:23.73 GiB
INFO 08-09 14:05:31 [hpu_model_runner.py:3029] [Warmup][Graph/decode][92/135] batch_size:4 query_len:1 num_blocks:9344 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56467)[0;0m INFO 08-09 14:05:31 [hpu_model_runner.py:3029] [Warmup][Graph/decode][92/135] batch_size:4 query_len:1 num_blocks:9344 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56470)[0;0m INFO 08-09 14:05:31 [hpu_model_runner.py:3029] [Warmup][Graph/decode][92/135] batch_size:4 query_len:1 num_blocks:9344 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56471)[0;0m INFO 08-09 14:05:31 [hpu_model_runner.py:3029] [Warmup][Graph/decode][92/135] batch_size:4 query_len:1 num_blocks:9344 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56468)[0;0m INFO 08-09 14:05:31 [hpu_model_runner.py:3029] [Warmup][Graph/decode][92/135] batch_size:4 query_len:1 num_blocks:9344 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56472)[0;0m INFO 08-09 14:05:31 [hpu_model_runner.py:3029] [Warmup][Graph/decode][92/135] batch_size:4 query_len:1 num_blocks:9344 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56469)[0;0m INFO 08-09 14:05:31 [hpu_model_runner.py:3029] [Warmup][Graph/decode][92/135] batch_size:4 query_len:1 num_blocks:9344 free_mem:23.73 GiB
INFO 08-09 14:05:38 [hpu_model_runner.py:3029] [Warmup][Graph/decode][93/135] batch_size:4 query_len:1 num_blocks:6784 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56473)[0;0m INFO 08-09 14:05:38 [hpu_model_runner.py:3029] [Warmup][Graph/decode][93/135] batch_size:4 query_len:1 num_blocks:6784 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56471)[0;0m INFO 08-09 14:05:38 [hpu_model_runner.py:3029] [Warmup][Graph/decode][93/135] batch_size:4 query_len:1 num_blocks:6784 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56470)[0;0m INFO 08-09 14:05:38 [hpu_model_runner.py:3029] [Warmup][Graph/decode][93/135] batch_size:4 query_len:1 num_blocks:6784 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56472)[0;0m INFO 08-09 14:05:38 [hpu_model_runner.py:3029] [Warmup][Graph/decode][93/135] batch_size:4 query_len:1 num_blocks:6784 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56467)[0;0m INFO 08-09 14:05:38 [hpu_model_runner.py:3029] [Warmup][Graph/decode][93/135] batch_size:4 query_len:1 num_blocks:6784 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56468)[0;0m INFO 08-09 14:05:38 [hpu_model_runner.py:3029] [Warmup][Graph/decode][93/135] batch_size:4 query_len:1 num_blocks:6784 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56469)[0;0m INFO 08-09 14:05:38 [hpu_model_runner.py:3029] [Warmup][Graph/decode][93/135] batch_size:4 query_len:1 num_blocks:6784 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56473)[0;0m INFO 08-09 14:05:44 [hpu_model_runner.py:3029] [Warmup][Graph/decode][94/135] batch_size:4 query_len:1 num_blocks:4864 free_mem:23.73 GiB
INFO 08-09 14:05:44 [hpu_model_runner.py:3029] [Warmup][Graph/decode][94/135] batch_size:4 query_len:1 num_blocks:4864 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56471)[0;0m INFO 08-09 14:05:44 [hpu_model_runner.py:3029] [Warmup][Graph/decode][94/135] batch_size:4 query_len:1 num_blocks:4864 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56472)[0;0m INFO 08-09 14:05:44 [hpu_model_runner.py:3029] [Warmup][Graph/decode][94/135] batch_size:4 query_len:1 num_blocks:4864 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56469)[0;0m INFO 08-09 14:05:44 [hpu_model_runner.py:3029] [Warmup][Graph/decode][94/135] batch_size:4 query_len:1 num_blocks:4864 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56467)[0;0m INFO 08-09 14:05:44 [hpu_model_runner.py:3029] [Warmup][Graph/decode][94/135] batch_size:4 query_len:1 num_blocks:4864 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56470)[0;0m INFO 08-09 14:05:44 [hpu_model_runner.py:3029] [Warmup][Graph/decode][94/135] batch_size:4 query_len:1 num_blocks:4864 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56468)[0;0m INFO 08-09 14:05:44 [hpu_model_runner.py:3029] [Warmup][Graph/decode][94/135] batch_size:4 query_len:1 num_blocks:4864 free_mem:23.73 GiB
INFO 08-09 14:05:51 [hpu_model_runner.py:3029] [Warmup][Graph/decode][95/135] batch_size:4 query_len:1 num_blocks:3584 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56473)[0;0m INFO 08-09 14:05:51 [hpu_model_runner.py:3029] [Warmup][Graph/decode][95/135] batch_size:4 query_len:1 num_blocks:3584 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56472)[0;0m INFO 08-09 14:05:51 [hpu_model_runner.py:3029] [Warmup][Graph/decode][95/135] batch_size:4 query_len:1 num_blocks:3584 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56471)[0;0m INFO 08-09 14:05:51 [hpu_model_runner.py:3029] [Warmup][Graph/decode][95/135] batch_size:4 query_len:1 num_blocks:3584 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56470)[0;0m INFO 08-09 14:05:51 [hpu_model_runner.py:3029] [Warmup][Graph/decode][95/135] batch_size:4 query_len:1 num_blocks:3584 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56469)[0;0m INFO 08-09 14:05:51 [hpu_model_runner.py:3029] [Warmup][Graph/decode][95/135] batch_size:4 query_len:1 num_blocks:3584 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56467)[0;0m INFO 08-09 14:05:51 [hpu_model_runner.py:3029] [Warmup][Graph/decode][95/135] batch_size:4 query_len:1 num_blocks:3584 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56468)[0;0m INFO 08-09 14:05:51 [hpu_model_runner.py:3029] [Warmup][Graph/decode][95/135] batch_size:4 query_len:1 num_blocks:3584 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56473)[0;0m INFO 08-09 14:05:57 [hpu_model_runner.py:3029] [Warmup][Graph/decode][96/135] batch_size:4 query_len:1 num_blocks:2560 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56472)[0;0m INFO 08-09 14:05:57 [hpu_model_runner.py:3029] [Warmup][Graph/decode][96/135] batch_size:4 query_len:1 num_blocks:2560 free_mem:23.73 GiB
INFO 08-09 14:05:57 [hpu_model_runner.py:3029] [Warmup][Graph/decode][96/135] batch_size:4 query_len:1 num_blocks:2560 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56471)[0;0m INFO 08-09 14:05:57 [hpu_model_runner.py:3029] [Warmup][Graph/decode][96/135] batch_size:4 query_len:1 num_blocks:2560 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56470)[0;0m INFO 08-09 14:05:57 [hpu_model_runner.py:3029] [Warmup][Graph/decode][96/135] batch_size:4 query_len:1 num_blocks:2560 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56467)[0;0m INFO 08-09 14:05:57 [hpu_model_runner.py:3029] [Warmup][Graph/decode][96/135] batch_size:4 query_len:1 num_blocks:2560 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56468)[0;0m INFO 08-09 14:05:57 [hpu_model_runner.py:3029] [Warmup][Graph/decode][96/135] batch_size:4 query_len:1 num_blocks:2560 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56469)[0;0m INFO 08-09 14:05:57 [hpu_model_runner.py:3029] [Warmup][Graph/decode][96/135] batch_size:4 query_len:1 num_blocks:2560 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56473)[0;0m INFO 08-09 14:06:03 [hpu_model_runner.py:3029] [Warmup][Graph/decode][97/135] batch_size:4 query_len:1 num_blocks:1792 free_mem:23.73 GiB
INFO 08-09 14:06:03 [hpu_model_runner.py:3029] [Warmup][Graph/decode][97/135] batch_size:4 query_len:1 num_blocks:1792 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56471)[0;0m INFO 08-09 14:06:03 [hpu_model_runner.py:3029] [Warmup][Graph/decode][97/135] batch_size:4 query_len:1 num_blocks:1792 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56472)[0;0m INFO 08-09 14:06:03 [hpu_model_runner.py:3029] [Warmup][Graph/decode][97/135] batch_size:4 query_len:1 num_blocks:1792 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56468)[0;0m INFO 08-09 14:06:03 [hpu_model_runner.py:3029] [Warmup][Graph/decode][97/135] batch_size:4 query_len:1 num_blocks:1792 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56470)[0;0m INFO 08-09 14:06:03 [hpu_model_runner.py:3029] [Warmup][Graph/decode][97/135] batch_size:4 query_len:1 num_blocks:1792 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56469)[0;0m INFO 08-09 14:06:03 [hpu_model_runner.py:3029] [Warmup][Graph/decode][97/135] batch_size:4 query_len:1 num_blocks:1792 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56467)[0;0m INFO 08-09 14:06:03 [hpu_model_runner.py:3029] [Warmup][Graph/decode][97/135] batch_size:4 query_len:1 num_blocks:1792 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56473)[0;0m INFO 08-09 14:06:10 [hpu_model_runner.py:3029] [Warmup][Graph/decode][98/135] batch_size:4 query_len:1 num_blocks:1408 free_mem:23.73 GiB
INFO 08-09 14:06:10 [hpu_model_runner.py:3029] [Warmup][Graph/decode][98/135] batch_size:4 query_len:1 num_blocks:1408 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56471)[0;0m INFO 08-09 14:06:10 [hpu_model_runner.py:3029] [Warmup][Graph/decode][98/135] batch_size:4 query_len:1 num_blocks:1408 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56472)[0;0m INFO 08-09 14:06:10 [hpu_model_runner.py:3029] [Warmup][Graph/decode][98/135] batch_size:4 query_len:1 num_blocks:1408 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56470)[0;0m INFO 08-09 14:06:10 [hpu_model_runner.py:3029] [Warmup][Graph/decode][98/135] batch_size:4 query_len:1 num_blocks:1408 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56468)[0;0m INFO 08-09 14:06:10 [hpu_model_runner.py:3029] [Warmup][Graph/decode][98/135] batch_size:4 query_len:1 num_blocks:1408 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56467)[0;0m INFO 08-09 14:06:10 [hpu_model_runner.py:3029] [Warmup][Graph/decode][98/135] batch_size:4 query_len:1 num_blocks:1408 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56469)[0;0m INFO 08-09 14:06:10 [hpu_model_runner.py:3029] [Warmup][Graph/decode][98/135] batch_size:4 query_len:1 num_blocks:1408 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56473)[0;0m INFO 08-09 14:06:16 [hpu_model_runner.py:3029] [Warmup][Graph/decode][99/135] batch_size:4 query_len:1 num_blocks:1024 free_mem:23.73 GiB
INFO 08-09 14:06:16 [hpu_model_runner.py:3029] [Warmup][Graph/decode][99/135] batch_size:4 query_len:1 num_blocks:1024 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56472)[0;0m INFO 08-09 14:06:16 [hpu_model_runner.py:3029] [Warmup][Graph/decode][99/135] batch_size:4 query_len:1 num_blocks:1024 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56468)[0;0m INFO 08-09 14:06:16 [hpu_model_runner.py:3029] [Warmup][Graph/decode][99/135] batch_size:4 query_len:1 num_blocks:1024 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56470)[0;0m INFO 08-09 14:06:16 [hpu_model_runner.py:3029] [Warmup][Graph/decode][99/135] batch_size:4 query_len:1 num_blocks:1024 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56471)[0;0m INFO 08-09 14:06:16 [hpu_model_runner.py:3029] [Warmup][Graph/decode][99/135] batch_size:4 query_len:1 num_blocks:1024 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56469)[0;0m INFO 08-09 14:06:16 [hpu_model_runner.py:3029] [Warmup][Graph/decode][99/135] batch_size:4 query_len:1 num_blocks:1024 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56467)[0;0m INFO 08-09 14:06:16 [hpu_model_runner.py:3029] [Warmup][Graph/decode][99/135] batch_size:4 query_len:1 num_blocks:1024 free_mem:23.73 GiB
INFO 08-09 14:06:23 [hpu_model_runner.py:3029] [Warmup][Graph/decode][100/135] batch_size:4 query_len:1 num_blocks:768 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56473)[0;0m INFO 08-09 14:06:23 [hpu_model_runner.py:3029] [Warmup][Graph/decode][100/135] batch_size:4 query_len:1 num_blocks:768 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56471)[0;0m INFO 08-09 14:06:23 [hpu_model_runner.py:3029] [Warmup][Graph/decode][100/135] batch_size:4 query_len:1 num_blocks:768 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56470)[0;0m INFO 08-09 14:06:23 [hpu_model_runner.py:3029] [Warmup][Graph/decode][100/135] batch_size:4 query_len:1 num_blocks:768 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56467)[0;0m INFO 08-09 14:06:23 [hpu_model_runner.py:3029] [Warmup][Graph/decode][100/135] batch_size:4 query_len:1 num_blocks:768 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56468)[0;0m INFO 08-09 14:06:23 [hpu_model_runner.py:3029] [Warmup][Graph/decode][100/135] batch_size:4 query_len:1 num_blocks:768 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56472)[0;0m INFO 08-09 14:06:23 [hpu_model_runner.py:3029] [Warmup][Graph/decode][100/135] batch_size:4 query_len:1 num_blocks:768 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56469)[0;0m INFO 08-09 14:06:23 [hpu_model_runner.py:3029] [Warmup][Graph/decode][100/135] batch_size:4 query_len:1 num_blocks:768 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56473)[0;0m INFO 08-09 14:06:30 [hpu_model_runner.py:3029] [Warmup][Graph/decode][101/135] batch_size:4 query_len:1 num_blocks:640 free_mem:23.73 GiB
INFO 08-09 14:06:30 [hpu_model_runner.py:3029] [Warmup][Graph/decode][101/135] batch_size:4 query_len:1 num_blocks:640 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56471)[0;0m INFO 08-09 14:06:30 [hpu_model_runner.py:3029] [Warmup][Graph/decode][101/135] batch_size:4 query_len:1 num_blocks:640 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56472)[0;0m INFO 08-09 14:06:30 [hpu_model_runner.py:3029] [Warmup][Graph/decode][101/135] batch_size:4 query_len:1 num_blocks:640 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56468)[0;0m INFO 08-09 14:06:30 [hpu_model_runner.py:3029] [Warmup][Graph/decode][101/135] batch_size:4 query_len:1 num_blocks:640 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56469)[0;0m INFO 08-09 14:06:30 [hpu_model_runner.py:3029] [Warmup][Graph/decode][101/135] batch_size:4 query_len:1 num_blocks:640 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56470)[0;0m INFO 08-09 14:06:30 [hpu_model_runner.py:3029] [Warmup][Graph/decode][101/135] batch_size:4 query_len:1 num_blocks:640 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56467)[0;0m INFO 08-09 14:06:30 [hpu_model_runner.py:3029] [Warmup][Graph/decode][101/135] batch_size:4 query_len:1 num_blocks:640 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56473)[0;0m INFO 08-09 14:06:36 [hpu_model_runner.py:3029] [Warmup][Graph/decode][102/135] batch_size:4 query_len:1 num_blocks:512 free_mem:23.73 GiB
INFO 08-09 14:06:36 [hpu_model_runner.py:3029] [Warmup][Graph/decode][102/135] batch_size:4 query_len:1 num_blocks:512 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56472)[0;0m INFO 08-09 14:06:36 [hpu_model_runner.py:3029] [Warmup][Graph/decode][102/135] batch_size:4 query_len:1 num_blocks:512 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56471)[0;0m INFO 08-09 14:06:36 [hpu_model_runner.py:3029] [Warmup][Graph/decode][102/135] batch_size:4 query_len:1 num_blocks:512 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56470)[0;0m INFO 08-09 14:06:36 [hpu_model_runner.py:3029] [Warmup][Graph/decode][102/135] batch_size:4 query_len:1 num_blocks:512 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56468)[0;0m INFO 08-09 14:06:36 [hpu_model_runner.py:3029] [Warmup][Graph/decode][102/135] batch_size:4 query_len:1 num_blocks:512 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56467)[0;0m INFO 08-09 14:06:36 [hpu_model_runner.py:3029] [Warmup][Graph/decode][102/135] batch_size:4 query_len:1 num_blocks:512 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56469)[0;0m INFO 08-09 14:06:36 [hpu_model_runner.py:3029] [Warmup][Graph/decode][102/135] batch_size:4 query_len:1 num_blocks:512 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56473)[0;0m INFO 08-09 14:06:43 [hpu_model_runner.py:3029] [Warmup][Graph/decode][103/135] batch_size:4 query_len:1 num_blocks:384 free_mem:23.73 GiB
INFO 08-09 14:06:43 [hpu_model_runner.py:3029] [Warmup][Graph/decode][103/135] batch_size:4 query_len:1 num_blocks:384 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56471)[0;0m INFO 08-09 14:06:43 [hpu_model_runner.py:3029] [Warmup][Graph/decode][103/135] batch_size:4 query_len:1 num_blocks:384 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56468)[0;0m INFO 08-09 14:06:43 [hpu_model_runner.py:3029] [Warmup][Graph/decode][103/135] batch_size:4 query_len:1 num_blocks:384 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56470)[0;0m INFO 08-09 14:06:43 [hpu_model_runner.py:3029] [Warmup][Graph/decode][103/135] batch_size:4 query_len:1 num_blocks:384 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56472)[0;0m INFO 08-09 14:06:43 [hpu_model_runner.py:3029] [Warmup][Graph/decode][103/135] batch_size:4 query_len:1 num_blocks:384 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56467)[0;0m INFO 08-09 14:06:43 [hpu_model_runner.py:3029] [Warmup][Graph/decode][103/135] batch_size:4 query_len:1 num_blocks:384 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56469)[0;0m INFO 08-09 14:06:43 [hpu_model_runner.py:3029] [Warmup][Graph/decode][103/135] batch_size:4 query_len:1 num_blocks:384 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56473)[0;0m INFO 08-09 14:06:49 [hpu_model_runner.py:3029] [Warmup][Graph/decode][104/135] batch_size:4 query_len:1 num_blocks:256 free_mem:23.73 GiB
INFO 08-09 14:06:49 [hpu_model_runner.py:3029] [Warmup][Graph/decode][104/135] batch_size:4 query_len:1 num_blocks:256 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56470)[0;0m INFO 08-09 14:06:49 [hpu_model_runner.py:3029] [Warmup][Graph/decode][104/135] batch_size:4 query_len:1 num_blocks:256 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56471)[0;0m INFO 08-09 14:06:49 [hpu_model_runner.py:3029] [Warmup][Graph/decode][104/135] batch_size:4 query_len:1 num_blocks:256 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56472)[0;0m INFO 08-09 14:06:49 [hpu_model_runner.py:3029] [Warmup][Graph/decode][104/135] batch_size:4 query_len:1 num_blocks:256 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56468)[0;0m INFO 08-09 14:06:49 [hpu_model_runner.py:3029] [Warmup][Graph/decode][104/135] batch_size:4 query_len:1 num_blocks:256 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56467)[0;0m INFO 08-09 14:06:49 [hpu_model_runner.py:3029] [Warmup][Graph/decode][104/135] batch_size:4 query_len:1 num_blocks:256 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56469)[0;0m INFO 08-09 14:06:49 [hpu_model_runner.py:3029] [Warmup][Graph/decode][104/135] batch_size:4 query_len:1 num_blocks:256 free_mem:23.73 GiB
INFO 08-09 14:06:56 [hpu_model_runner.py:3029] [Warmup][Graph/decode][105/135] batch_size:4 query_len:1 num_blocks:128 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56473)[0;0m INFO 08-09 14:06:56 [hpu_model_runner.py:3029] [Warmup][Graph/decode][105/135] batch_size:4 query_len:1 num_blocks:128 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56472)[0;0m INFO 08-09 14:06:56 [hpu_model_runner.py:3029] [Warmup][Graph/decode][105/135] batch_size:4 query_len:1 num_blocks:128 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56469)[0;0m INFO 08-09 14:06:56 [hpu_model_runner.py:3029] [Warmup][Graph/decode][105/135] batch_size:4 query_len:1 num_blocks:128 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56471)[0;0m INFO 08-09 14:06:56 [hpu_model_runner.py:3029] [Warmup][Graph/decode][105/135] batch_size:4 query_len:1 num_blocks:128 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56470)[0;0m INFO 08-09 14:06:56 [hpu_model_runner.py:3029] [Warmup][Graph/decode][105/135] batch_size:4 query_len:1 num_blocks:128 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56467)[0;0m INFO 08-09 14:06:56 [hpu_model_runner.py:3029] [Warmup][Graph/decode][105/135] batch_size:4 query_len:1 num_blocks:128 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56468)[0;0m INFO 08-09 14:06:56 [hpu_model_runner.py:3029] [Warmup][Graph/decode][105/135] batch_size:4 query_len:1 num_blocks:128 free_mem:23.73 GiB
INFO 08-09 14:07:02 [hpu_model_runner.py:3029] [Warmup][Graph/decode][106/135] batch_size:2 query_len:1 num_blocks:12946 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56473)[0;0m INFO 08-09 14:07:02 [hpu_model_runner.py:3029] [Warmup][Graph/decode][106/135] batch_size:2 query_len:1 num_blocks:12946 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56472)[0;0m INFO 08-09 14:07:02 [hpu_model_runner.py:3029] [Warmup][Graph/decode][106/135] batch_size:2 query_len:1 num_blocks:12946 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56470)[0;0m INFO 08-09 14:07:02 [hpu_model_runner.py:3029] [Warmup][Graph/decode][106/135] batch_size:2 query_len:1 num_blocks:12946 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56471)[0;0m INFO 08-09 14:07:02 [hpu_model_runner.py:3029] [Warmup][Graph/decode][106/135] batch_size:2 query_len:1 num_blocks:12946 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56469)[0;0m INFO 08-09 14:07:02 [hpu_model_runner.py:3029] [Warmup][Graph/decode][106/135] batch_size:2 query_len:1 num_blocks:12946 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56467)[0;0m INFO 08-09 14:07:02 [hpu_model_runner.py:3029] [Warmup][Graph/decode][106/135] batch_size:2 query_len:1 num_blocks:12946 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56468)[0;0m INFO 08-09 14:07:02 [hpu_model_runner.py:3029] [Warmup][Graph/decode][106/135] batch_size:2 query_len:1 num_blocks:12946 free_mem:23.73 GiB
INFO 08-09 14:07:10 [hpu_model_runner.py:3029] [Warmup][Graph/decode][107/135] batch_size:2 query_len:1 num_blocks:9344 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56473)[0;0m INFO 08-09 14:07:10 [hpu_model_runner.py:3029] [Warmup][Graph/decode][107/135] batch_size:2 query_len:1 num_blocks:9344 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56472)[0;0m INFO 08-09 14:07:10 [hpu_model_runner.py:3029] [Warmup][Graph/decode][107/135] batch_size:2 query_len:1 num_blocks:9344 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56471)[0;0m INFO 08-09 14:07:10 [hpu_model_runner.py:3029] [Warmup][Graph/decode][107/135] batch_size:2 query_len:1 num_blocks:9344 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56470)[0;0m INFO 08-09 14:07:10 [hpu_model_runner.py:3029] [Warmup][Graph/decode][107/135] batch_size:2 query_len:1 num_blocks:9344 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56468)[0;0m INFO 08-09 14:07:10 [hpu_model_runner.py:3029] [Warmup][Graph/decode][107/135] batch_size:2 query_len:1 num_blocks:9344 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56467)[0;0m INFO 08-09 14:07:10 [hpu_model_runner.py:3029] [Warmup][Graph/decode][107/135] batch_size:2 query_len:1 num_blocks:9344 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56469)[0;0m INFO 08-09 14:07:10 [hpu_model_runner.py:3029] [Warmup][Graph/decode][107/135] batch_size:2 query_len:1 num_blocks:9344 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56469)[0;0m INFO 08-09 14:07:16 [hpu_model_runner.py:3029] [Warmup][Graph/decode][108/135] batch_size:2 query_len:1 num_blocks:6784 free_mem:23.73 GiB
INFO 08-09 14:07:16 [hpu_model_runner.py:3029] [Warmup][Graph/decode][108/135] batch_size:2 query_len:1 num_blocks:6784 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56471)[0;0m INFO 08-09 14:07:16 [hpu_model_runner.py:3029] [Warmup][Graph/decode][108/135] batch_size:2 query_len:1 num_blocks:6784 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56473)[0;0m INFO 08-09 14:07:16 [hpu_model_runner.py:3029] [Warmup][Graph/decode][108/135] batch_size:2 query_len:1 num_blocks:6784 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56472)[0;0m INFO 08-09 14:07:16 [hpu_model_runner.py:3029] [Warmup][Graph/decode][108/135] batch_size:2 query_len:1 num_blocks:6784 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56467)[0;0m INFO 08-09 14:07:16 [hpu_model_runner.py:3029] [Warmup][Graph/decode][108/135] batch_size:2 query_len:1 num_blocks:6784 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56468)[0;0m INFO 08-09 14:07:16 [hpu_model_runner.py:3029] [Warmup][Graph/decode][108/135] batch_size:2 query_len:1 num_blocks:6784 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56470)[0;0m INFO 08-09 14:07:16 [hpu_model_runner.py:3029] [Warmup][Graph/decode][108/135] batch_size:2 query_len:1 num_blocks:6784 free_mem:23.73 GiB
INFO 08-09 14:07:23 [hpu_model_runner.py:3029] [Warmup][Graph/decode][109/135] batch_size:2 query_len:1 num_blocks:4864 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56473)[0;0m INFO 08-09 14:07:23 [hpu_model_runner.py:3029] [Warmup][Graph/decode][109/135] batch_size:2 query_len:1 num_blocks:4864 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56471)[0;0m INFO 08-09 14:07:23 [hpu_model_runner.py:3029] [Warmup][Graph/decode][109/135] batch_size:2 query_len:1 num_blocks:4864 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56470)[0;0m INFO 08-09 14:07:23 [hpu_model_runner.py:3029] [Warmup][Graph/decode][109/135] batch_size:2 query_len:1 num_blocks:4864 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56468)[0;0m INFO 08-09 14:07:23 [hpu_model_runner.py:3029] [Warmup][Graph/decode][109/135] batch_size:2 query_len:1 num_blocks:4864 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56467)[0;0m INFO 08-09 14:07:23 [hpu_model_runner.py:3029] [Warmup][Graph/decode][109/135] batch_size:2 query_len:1 num_blocks:4864 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56472)[0;0m INFO 08-09 14:07:23 [hpu_model_runner.py:3029] [Warmup][Graph/decode][109/135] batch_size:2 query_len:1 num_blocks:4864 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56469)[0;0m INFO 08-09 14:07:23 [hpu_model_runner.py:3029] [Warmup][Graph/decode][109/135] batch_size:2 query_len:1 num_blocks:4864 free_mem:23.73 GiB
INFO 08-09 14:07:30 [hpu_model_runner.py:3029] [Warmup][Graph/decode][110/135] batch_size:2 query_len:1 num_blocks:3584 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56473)[0;0m INFO 08-09 14:07:30 [hpu_model_runner.py:3029] [Warmup][Graph/decode][110/135] batch_size:2 query_len:1 num_blocks:3584 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56471)[0;0m INFO 08-09 14:07:30 [hpu_model_runner.py:3029] [Warmup][Graph/decode][110/135] batch_size:2 query_len:1 num_blocks:3584 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56472)[0;0m INFO 08-09 14:07:30 [hpu_model_runner.py:3029] [Warmup][Graph/decode][110/135] batch_size:2 query_len:1 num_blocks:3584 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56469)[0;0m INFO 08-09 14:07:30 [hpu_model_runner.py:3029] [Warmup][Graph/decode][110/135] batch_size:2 query_len:1 num_blocks:3584 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56468)[0;0m INFO 08-09 14:07:30 [hpu_model_runner.py:3029] [Warmup][Graph/decode][110/135] batch_size:2 query_len:1 num_blocks:3584 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56470)[0;0m INFO 08-09 14:07:30 [hpu_model_runner.py:3029] [Warmup][Graph/decode][110/135] batch_size:2 query_len:1 num_blocks:3584 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56467)[0;0m INFO 08-09 14:07:30 [hpu_model_runner.py:3029] [Warmup][Graph/decode][110/135] batch_size:2 query_len:1 num_blocks:3584 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56473)[0;0m INFO 08-09 14:07:37 [hpu_model_runner.py:3029] [Warmup][Graph/decode][111/135] batch_size:2 query_len:1 num_blocks:2560 free_mem:23.73 GiB
INFO 08-09 14:07:37 [hpu_model_runner.py:3029] [Warmup][Graph/decode][111/135] batch_size:2 query_len:1 num_blocks:2560 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56472)[0;0m INFO 08-09 14:07:37 [hpu_model_runner.py:3029] [Warmup][Graph/decode][111/135] batch_size:2 query_len:1 num_blocks:2560 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56467)[0;0m INFO 08-09 14:07:37 [hpu_model_runner.py:3029] [Warmup][Graph/decode][111/135] batch_size:2 query_len:1 num_blocks:2560 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56468)[0;0m INFO 08-09 14:07:37 [hpu_model_runner.py:3029] [Warmup][Graph/decode][111/135] batch_size:2 query_len:1 num_blocks:2560 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56471)[0;0m INFO 08-09 14:07:37 [hpu_model_runner.py:3029] [Warmup][Graph/decode][111/135] batch_size:2 query_len:1 num_blocks:2560 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56470)[0;0m INFO 08-09 14:07:37 [hpu_model_runner.py:3029] [Warmup][Graph/decode][111/135] batch_size:2 query_len:1 num_blocks:2560 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56469)[0;0m INFO 08-09 14:07:37 [hpu_model_runner.py:3029] [Warmup][Graph/decode][111/135] batch_size:2 query_len:1 num_blocks:2560 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56473)[0;0m INFO 08-09 14:07:44 [hpu_model_runner.py:3029] [Warmup][Graph/decode][112/135] batch_size:2 query_len:1 num_blocks:1792 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56472)[0;0m INFO 08-09 14:07:44 [hpu_model_runner.py:3029] [Warmup][Graph/decode][112/135] batch_size:2 query_len:1 num_blocks:1792 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56471)[0;0m INFO 08-09 14:07:44 [hpu_model_runner.py:3029] [Warmup][Graph/decode][112/135] batch_size:2 query_len:1 num_blocks:1792 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56470)[0;0m INFO 08-09 14:07:44 [hpu_model_runner.py:3029] [Warmup][Graph/decode][112/135] batch_size:2 query_len:1 num_blocks:1792 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56468)[0;0m INFO 08-09 14:07:44 [hpu_model_runner.py:3029] [Warmup][Graph/decode][112/135] batch_size:2 query_len:1 num_blocks:1792 free_mem:23.73 GiB
INFO 08-09 14:07:44 [hpu_model_runner.py:3029] [Warmup][Graph/decode][112/135] batch_size:2 query_len:1 num_blocks:1792 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56469)[0;0m INFO 08-09 14:07:44 [hpu_model_runner.py:3029] [Warmup][Graph/decode][112/135] batch_size:2 query_len:1 num_blocks:1792 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56467)[0;0m INFO 08-09 14:07:44 [hpu_model_runner.py:3029] [Warmup][Graph/decode][112/135] batch_size:2 query_len:1 num_blocks:1792 free_mem:23.73 GiB
INFO 08-09 14:07:51 [hpu_model_runner.py:3029] [Warmup][Graph/decode][113/135] batch_size:2 query_len:1 num_blocks:1408 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56473)[0;0m INFO 08-09 14:07:51 [hpu_model_runner.py:3029] [Warmup][Graph/decode][113/135] batch_size:2 query_len:1 num_blocks:1408 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56472)[0;0m INFO 08-09 14:07:51 [hpu_model_runner.py:3029] [Warmup][Graph/decode][113/135] batch_size:2 query_len:1 num_blocks:1408 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56471)[0;0m INFO 08-09 14:07:51 [hpu_model_runner.py:3029] [Warmup][Graph/decode][113/135] batch_size:2 query_len:1 num_blocks:1408 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56470)[0;0m INFO 08-09 14:07:51 [hpu_model_runner.py:3029] [Warmup][Graph/decode][113/135] batch_size:2 query_len:1 num_blocks:1408 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56469)[0;0m INFO 08-09 14:07:51 [hpu_model_runner.py:3029] [Warmup][Graph/decode][113/135] batch_size:2 query_len:1 num_blocks:1408 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56468)[0;0m INFO 08-09 14:07:51 [hpu_model_runner.py:3029] [Warmup][Graph/decode][113/135] batch_size:2 query_len:1 num_blocks:1408 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56467)[0;0m INFO 08-09 14:07:51 [hpu_model_runner.py:3029] [Warmup][Graph/decode][113/135] batch_size:2 query_len:1 num_blocks:1408 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56473)[0;0m INFO 08-09 14:07:58 [hpu_model_runner.py:3029] [Warmup][Graph/decode][114/135] batch_size:2 query_len:1 num_blocks:1024 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56472)[0;0m INFO 08-09 14:07:58 [hpu_model_runner.py:3029] [Warmup][Graph/decode][114/135] batch_size:2 query_len:1 num_blocks:1024 free_mem:23.73 GiB
INFO 08-09 14:07:58 [hpu_model_runner.py:3029] [Warmup][Graph/decode][114/135] batch_size:2 query_len:1 num_blocks:1024 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56471)[0;0m INFO 08-09 14:07:58 [hpu_model_runner.py:3029] [Warmup][Graph/decode][114/135] batch_size:2 query_len:1 num_blocks:1024 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56470)[0;0m INFO 08-09 14:07:58 [hpu_model_runner.py:3029] [Warmup][Graph/decode][114/135] batch_size:2 query_len:1 num_blocks:1024 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56467)[0;0m INFO 08-09 14:07:58 [hpu_model_runner.py:3029] [Warmup][Graph/decode][114/135] batch_size:2 query_len:1 num_blocks:1024 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56468)[0;0m INFO 08-09 14:07:58 [hpu_model_runner.py:3029] [Warmup][Graph/decode][114/135] batch_size:2 query_len:1 num_blocks:1024 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56469)[0;0m INFO 08-09 14:07:58 [hpu_model_runner.py:3029] [Warmup][Graph/decode][114/135] batch_size:2 query_len:1 num_blocks:1024 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56473)[0;0m INFO 08-09 14:08:05 [hpu_model_runner.py:3029] [Warmup][Graph/decode][115/135] batch_size:2 query_len:1 num_blocks:768 free_mem:23.73 GiB
INFO 08-09 14:08:05 [hpu_model_runner.py:3029] [Warmup][Graph/decode][115/135] batch_size:2 query_len:1 num_blocks:768 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56471)[0;0m INFO 08-09 14:08:05 [hpu_model_runner.py:3029] [Warmup][Graph/decode][115/135] batch_size:2 query_len:1 num_blocks:768 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56472)[0;0m INFO 08-09 14:08:05 [hpu_model_runner.py:3029] [Warmup][Graph/decode][115/135] batch_size:2 query_len:1 num_blocks:768 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56469)[0;0m INFO 08-09 14:08:05 [hpu_model_runner.py:3029] [Warmup][Graph/decode][115/135] batch_size:2 query_len:1 num_blocks:768 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56470)[0;0m INFO 08-09 14:08:05 [hpu_model_runner.py:3029] [Warmup][Graph/decode][115/135] batch_size:2 query_len:1 num_blocks:768 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56468)[0;0m INFO 08-09 14:08:05 [hpu_model_runner.py:3029] [Warmup][Graph/decode][115/135] batch_size:2 query_len:1 num_blocks:768 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56467)[0;0m INFO 08-09 14:08:05 [hpu_model_runner.py:3029] [Warmup][Graph/decode][115/135] batch_size:2 query_len:1 num_blocks:768 free_mem:23.73 GiB
INFO 08-09 14:08:12 [hpu_model_runner.py:3029] [Warmup][Graph/decode][116/135] batch_size:2 query_len:1 num_blocks:640 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56470)[0;0m INFO 08-09 14:08:12 [hpu_model_runner.py:3029] [Warmup][Graph/decode][116/135] batch_size:2 query_len:1 num_blocks:640 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56473)[0;0m INFO 08-09 14:08:12 [hpu_model_runner.py:3029] [Warmup][Graph/decode][116/135] batch_size:2 query_len:1 num_blocks:640 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56469)[0;0m INFO 08-09 14:08:12 [hpu_model_runner.py:3029] [Warmup][Graph/decode][116/135] batch_size:2 query_len:1 num_blocks:640 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56471)[0;0m INFO 08-09 14:08:12 [hpu_model_runner.py:3029] [Warmup][Graph/decode][116/135] batch_size:2 query_len:1 num_blocks:640 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56468)[0;0m INFO 08-09 14:08:12 [hpu_model_runner.py:3029] [Warmup][Graph/decode][116/135] batch_size:2 query_len:1 num_blocks:640 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56472)[0;0m INFO 08-09 14:08:12 [hpu_model_runner.py:3029] [Warmup][Graph/decode][116/135] batch_size:2 query_len:1 num_blocks:640 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56467)[0;0m INFO 08-09 14:08:12 [hpu_model_runner.py:3029] [Warmup][Graph/decode][116/135] batch_size:2 query_len:1 num_blocks:640 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56473)[0;0m INFO 08-09 14:08:19 [hpu_model_runner.py:3029] [Warmup][Graph/decode][117/135] batch_size:2 query_len:1 num_blocks:512 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56472)[0;0m INFO 08-09 14:08:19 [hpu_model_runner.py:3029] [Warmup][Graph/decode][117/135] batch_size:2 query_len:1 num_blocks:512 free_mem:23.73 GiB
INFO 08-09 14:08:19 [hpu_model_runner.py:3029] [Warmup][Graph/decode][117/135] batch_size:2 query_len:1 num_blocks:512 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56470)[0;0m INFO 08-09 14:08:19 [hpu_model_runner.py:3029] [Warmup][Graph/decode][117/135] batch_size:2 query_len:1 num_blocks:512 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56471)[0;0m INFO 08-09 14:08:19 [hpu_model_runner.py:3029] [Warmup][Graph/decode][117/135] batch_size:2 query_len:1 num_blocks:512 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56468)[0;0m INFO 08-09 14:08:19 [hpu_model_runner.py:3029] [Warmup][Graph/decode][117/135] batch_size:2 query_len:1 num_blocks:512 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56467)[0;0m INFO 08-09 14:08:19 [hpu_model_runner.py:3029] [Warmup][Graph/decode][117/135] batch_size:2 query_len:1 num_blocks:512 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56469)[0;0m INFO 08-09 14:08:19 [hpu_model_runner.py:3029] [Warmup][Graph/decode][117/135] batch_size:2 query_len:1 num_blocks:512 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56473)[0;0m INFO 08-09 14:08:26 [hpu_model_runner.py:3029] [Warmup][Graph/decode][118/135] batch_size:2 query_len:1 num_blocks:384 free_mem:23.73 GiB
INFO 08-09 14:08:26 [hpu_model_runner.py:3029] [Warmup][Graph/decode][118/135] batch_size:2 query_len:1 num_blocks:384 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56472)[0;0m INFO 08-09 14:08:26 [hpu_model_runner.py:3029] [Warmup][Graph/decode][118/135] batch_size:2 query_len:1 num_blocks:384 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56471)[0;0m INFO 08-09 14:08:26 [hpu_model_runner.py:3029] [Warmup][Graph/decode][118/135] batch_size:2 query_len:1 num_blocks:384 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56470)[0;0m INFO 08-09 14:08:26 [hpu_model_runner.py:3029] [Warmup][Graph/decode][118/135] batch_size:2 query_len:1 num_blocks:384 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56468)[0;0m INFO 08-09 14:08:26 [hpu_model_runner.py:3029] [Warmup][Graph/decode][118/135] batch_size:2 query_len:1 num_blocks:384 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56467)[0;0m INFO 08-09 14:08:26 [hpu_model_runner.py:3029] [Warmup][Graph/decode][118/135] batch_size:2 query_len:1 num_blocks:384 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56469)[0;0m INFO 08-09 14:08:26 [hpu_model_runner.py:3029] [Warmup][Graph/decode][118/135] batch_size:2 query_len:1 num_blocks:384 free_mem:23.73 GiB
INFO 08-09 14:08:33 [hpu_model_runner.py:3029] [Warmup][Graph/decode][119/135] batch_size:2 query_len:1 num_blocks:256 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56473)[0;0m INFO 08-09 14:08:33 [hpu_model_runner.py:3029] [Warmup][Graph/decode][119/135] batch_size:2 query_len:1 num_blocks:256 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56471)[0;0m INFO 08-09 14:08:33 [hpu_model_runner.py:3029] [Warmup][Graph/decode][119/135] batch_size:2 query_len:1 num_blocks:256 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56472)[0;0m INFO 08-09 14:08:33 [hpu_model_runner.py:3029] [Warmup][Graph/decode][119/135] batch_size:2 query_len:1 num_blocks:256 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56470)[0;0m INFO 08-09 14:08:33 [hpu_model_runner.py:3029] [Warmup][Graph/decode][119/135] batch_size:2 query_len:1 num_blocks:256 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56468)[0;0m INFO 08-09 14:08:33 [hpu_model_runner.py:3029] [Warmup][Graph/decode][119/135] batch_size:2 query_len:1 num_blocks:256 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56467)[0;0m INFO 08-09 14:08:33 [hpu_model_runner.py:3029] [Warmup][Graph/decode][119/135] batch_size:2 query_len:1 num_blocks:256 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56469)[0;0m INFO 08-09 14:08:33 [hpu_model_runner.py:3029] [Warmup][Graph/decode][119/135] batch_size:2 query_len:1 num_blocks:256 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56473)[0;0m INFO 08-09 14:08:40 [hpu_model_runner.py:3029] [Warmup][Graph/decode][120/135] batch_size:2 query_len:1 num_blocks:128 free_mem:23.73 GiB
INFO 08-09 14:08:40 [hpu_model_runner.py:3029] [Warmup][Graph/decode][120/135] batch_size:2 query_len:1 num_blocks:128 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56472)[0;0m INFO 08-09 14:08:40 [hpu_model_runner.py:3029] [Warmup][Graph/decode][120/135] batch_size:2 query_len:1 num_blocks:128 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56469)[0;0m INFO 08-09 14:08:40 [hpu_model_runner.py:3029] [Warmup][Graph/decode][120/135] batch_size:2 query_len:1 num_blocks:128 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56471)[0;0m INFO 08-09 14:08:40 [hpu_model_runner.py:3029] [Warmup][Graph/decode][120/135] batch_size:2 query_len:1 num_blocks:128 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56470)[0;0m INFO 08-09 14:08:40 [hpu_model_runner.py:3029] [Warmup][Graph/decode][120/135] batch_size:2 query_len:1 num_blocks:128 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56468)[0;0m INFO 08-09 14:08:40 [hpu_model_runner.py:3029] [Warmup][Graph/decode][120/135] batch_size:2 query_len:1 num_blocks:128 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56467)[0;0m INFO 08-09 14:08:40 [hpu_model_runner.py:3029] [Warmup][Graph/decode][120/135] batch_size:2 query_len:1 num_blocks:128 free_mem:23.73 GiB
INFO 08-09 14:08:46 [hpu_model_runner.py:3029] [Warmup][Graph/decode][121/135] batch_size:1 query_len:1 num_blocks:12946 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56473)[0;0m INFO 08-09 14:08:46 [hpu_model_runner.py:3029] [Warmup][Graph/decode][121/135] batch_size:1 query_len:1 num_blocks:12946 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56472)[0;0m INFO 08-09 14:08:46 [hpu_model_runner.py:3029] [Warmup][Graph/decode][121/135] batch_size:1 query_len:1 num_blocks:12946 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56471)[0;0m INFO 08-09 14:08:46 [hpu_model_runner.py:3029] [Warmup][Graph/decode][121/135] batch_size:1 query_len:1 num_blocks:12946 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56469)[0;0m INFO 08-09 14:08:46 [hpu_model_runner.py:3029] [Warmup][Graph/decode][121/135] batch_size:1 query_len:1 num_blocks:12946 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56468)[0;0m INFO 08-09 14:08:46 [hpu_model_runner.py:3029] [Warmup][Graph/decode][121/135] batch_size:1 query_len:1 num_blocks:12946 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56470)[0;0m INFO 08-09 14:08:46 [hpu_model_runner.py:3029] [Warmup][Graph/decode][121/135] batch_size:1 query_len:1 num_blocks:12946 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56467)[0;0m INFO 08-09 14:08:46 [hpu_model_runner.py:3029] [Warmup][Graph/decode][121/135] batch_size:1 query_len:1 num_blocks:12946 free_mem:23.73 GiB
INFO 08-09 14:08:54 [hpu_model_runner.py:3029] [Warmup][Graph/decode][122/135] batch_size:1 query_len:1 num_blocks:9344 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56473)[0;0m INFO 08-09 14:08:54 [hpu_model_runner.py:3029] [Warmup][Graph/decode][122/135] batch_size:1 query_len:1 num_blocks:9344 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56472)[0;0m INFO 08-09 14:08:54 [hpu_model_runner.py:3029] [Warmup][Graph/decode][122/135] batch_size:1 query_len:1 num_blocks:9344 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56471)[0;0m INFO 08-09 14:08:54 [hpu_model_runner.py:3029] [Warmup][Graph/decode][122/135] batch_size:1 query_len:1 num_blocks:9344 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56467)[0;0m INFO 08-09 14:08:54 [hpu_model_runner.py:3029] [Warmup][Graph/decode][122/135] batch_size:1 query_len:1 num_blocks:9344 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56469)[0;0m INFO 08-09 14:08:54 [hpu_model_runner.py:3029] [Warmup][Graph/decode][122/135] batch_size:1 query_len:1 num_blocks:9344 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56468)[0;0m INFO 08-09 14:08:54 [hpu_model_runner.py:3029] [Warmup][Graph/decode][122/135] batch_size:1 query_len:1 num_blocks:9344 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56470)[0;0m INFO 08-09 14:08:54 [hpu_model_runner.py:3029] [Warmup][Graph/decode][122/135] batch_size:1 query_len:1 num_blocks:9344 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56473)[0;0m INFO 08-09 14:09:01 [hpu_model_runner.py:3029] [Warmup][Graph/decode][123/135] batch_size:1 query_len:1 num_blocks:6784 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56472)[0;0m INFO 08-09 14:09:01 [hpu_model_runner.py:3029] [Warmup][Graph/decode][123/135] batch_size:1 query_len:1 num_blocks:6784 free_mem:23.73 GiB
INFO 08-09 14:09:01 [hpu_model_runner.py:3029] [Warmup][Graph/decode][123/135] batch_size:1 query_len:1 num_blocks:6784 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56471)[0;0m INFO 08-09 14:09:01 [hpu_model_runner.py:3029] [Warmup][Graph/decode][123/135] batch_size:1 query_len:1 num_blocks:6784 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56470)[0;0m INFO 08-09 14:09:01 [hpu_model_runner.py:3029] [Warmup][Graph/decode][123/135] batch_size:1 query_len:1 num_blocks:6784 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56469)[0;0m INFO 08-09 14:09:01 [hpu_model_runner.py:3029] [Warmup][Graph/decode][123/135] batch_size:1 query_len:1 num_blocks:6784 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56467)[0;0m INFO 08-09 14:09:01 [hpu_model_runner.py:3029] [Warmup][Graph/decode][123/135] batch_size:1 query_len:1 num_blocks:6784 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56468)[0;0m INFO 08-09 14:09:01 [hpu_model_runner.py:3029] [Warmup][Graph/decode][123/135] batch_size:1 query_len:1 num_blocks:6784 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56473)[0;0m INFO 08-09 14:09:09 [hpu_model_runner.py:3029] [Warmup][Graph/decode][124/135] batch_size:1 query_len:1 num_blocks:4864 free_mem:23.73 GiB
INFO 08-09 14:09:09 [hpu_model_runner.py:3029] [Warmup][Graph/decode][124/135] batch_size:1 query_len:1 num_blocks:4864 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56472)[0;0m INFO 08-09 14:09:09 [hpu_model_runner.py:3029] [Warmup][Graph/decode][124/135] batch_size:1 query_len:1 num_blocks:4864 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56471)[0;0m INFO 08-09 14:09:09 [hpu_model_runner.py:3029] [Warmup][Graph/decode][124/135] batch_size:1 query_len:1 num_blocks:4864 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56470)[0;0m INFO 08-09 14:09:09 [hpu_model_runner.py:3029] [Warmup][Graph/decode][124/135] batch_size:1 query_len:1 num_blocks:4864 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56468)[0;0m INFO 08-09 14:09:09 [hpu_model_runner.py:3029] [Warmup][Graph/decode][124/135] batch_size:1 query_len:1 num_blocks:4864 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56469)[0;0m INFO 08-09 14:09:09 [hpu_model_runner.py:3029] [Warmup][Graph/decode][124/135] batch_size:1 query_len:1 num_blocks:4864 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56467)[0;0m INFO 08-09 14:09:09 [hpu_model_runner.py:3029] [Warmup][Graph/decode][124/135] batch_size:1 query_len:1 num_blocks:4864 free_mem:23.73 GiB
INFO 08-09 14:09:16 [hpu_model_runner.py:3029] [Warmup][Graph/decode][125/135] batch_size:1 query_len:1 num_blocks:3584 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56471)[0;0m INFO 08-09 14:09:16 [hpu_model_runner.py:3029] [Warmup][Graph/decode][125/135] batch_size:1 query_len:1 num_blocks:3584 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56473)[0;0m INFO 08-09 14:09:16 [hpu_model_runner.py:3029] [Warmup][Graph/decode][125/135] batch_size:1 query_len:1 num_blocks:3584 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56472)[0;0m INFO 08-09 14:09:16 [hpu_model_runner.py:3029] [Warmup][Graph/decode][125/135] batch_size:1 query_len:1 num_blocks:3584 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56469)[0;0m INFO 08-09 14:09:16 [hpu_model_runner.py:3029] [Warmup][Graph/decode][125/135] batch_size:1 query_len:1 num_blocks:3584 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56467)[0;0m INFO 08-09 14:09:16 [hpu_model_runner.py:3029] [Warmup][Graph/decode][125/135] batch_size:1 query_len:1 num_blocks:3584 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56470)[0;0m INFO 08-09 14:09:16 [hpu_model_runner.py:3029] [Warmup][Graph/decode][125/135] batch_size:1 query_len:1 num_blocks:3584 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56468)[0;0m INFO 08-09 14:09:16 [hpu_model_runner.py:3029] [Warmup][Graph/decode][125/135] batch_size:1 query_len:1 num_blocks:3584 free_mem:23.73 GiB
INFO 08-09 14:09:24 [hpu_model_runner.py:3029] [Warmup][Graph/decode][126/135] batch_size:1 query_len:1 num_blocks:2560 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56472)[0;0m INFO 08-09 14:09:24 [hpu_model_runner.py:3029] [Warmup][Graph/decode][126/135] batch_size:1 query_len:1 num_blocks:2560 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56473)[0;0m INFO 08-09 14:09:24 [hpu_model_runner.py:3029] [Warmup][Graph/decode][126/135] batch_size:1 query_len:1 num_blocks:2560 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56469)[0;0m INFO 08-09 14:09:24 [hpu_model_runner.py:3029] [Warmup][Graph/decode][126/135] batch_size:1 query_len:1 num_blocks:2560 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56471)[0;0m INFO 08-09 14:09:24 [hpu_model_runner.py:3029] [Warmup][Graph/decode][126/135] batch_size:1 query_len:1 num_blocks:2560 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56468)[0;0m INFO 08-09 14:09:24 [hpu_model_runner.py:3029] [Warmup][Graph/decode][126/135] batch_size:1 query_len:1 num_blocks:2560 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56470)[0;0m INFO 08-09 14:09:24 [hpu_model_runner.py:3029] [Warmup][Graph/decode][126/135] batch_size:1 query_len:1 num_blocks:2560 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56467)[0;0m INFO 08-09 14:09:24 [hpu_model_runner.py:3029] [Warmup][Graph/decode][126/135] batch_size:1 query_len:1 num_blocks:2560 free_mem:23.73 GiB
INFO 08-09 14:09:31 [hpu_model_runner.py:3029] [Warmup][Graph/decode][127/135] batch_size:1 query_len:1 num_blocks:1792 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56471)[0;0m INFO 08-09 14:09:31 [hpu_model_runner.py:3029] [Warmup][Graph/decode][127/135] batch_size:1 query_len:1 num_blocks:1792 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56473)[0;0m INFO 08-09 14:09:31 [hpu_model_runner.py:3029] [Warmup][Graph/decode][127/135] batch_size:1 query_len:1 num_blocks:1792 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56469)[0;0m INFO 08-09 14:09:31 [hpu_model_runner.py:3029] [Warmup][Graph/decode][127/135] batch_size:1 query_len:1 num_blocks:1792 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56470)[0;0m INFO 08-09 14:09:31 [hpu_model_runner.py:3029] [Warmup][Graph/decode][127/135] batch_size:1 query_len:1 num_blocks:1792 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56472)[0;0m INFO 08-09 14:09:31 [hpu_model_runner.py:3029] [Warmup][Graph/decode][127/135] batch_size:1 query_len:1 num_blocks:1792 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56468)[0;0m INFO 08-09 14:09:31 [hpu_model_runner.py:3029] [Warmup][Graph/decode][127/135] batch_size:1 query_len:1 num_blocks:1792 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56467)[0;0m INFO 08-09 14:09:31 [hpu_model_runner.py:3029] [Warmup][Graph/decode][127/135] batch_size:1 query_len:1 num_blocks:1792 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56473)[0;0m INFO 08-09 14:09:38 [hpu_model_runner.py:3029] [Warmup][Graph/decode][128/135] batch_size:1 query_len:1 num_blocks:1408 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56472)[0;0m INFO 08-09 14:09:38 [hpu_model_runner.py:3029] [Warmup][Graph/decode][128/135] batch_size:1 query_len:1 num_blocks:1408 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56471)[0;0m INFO 08-09 14:09:38 [hpu_model_runner.py:3029] [Warmup][Graph/decode][128/135] batch_size:1 query_len:1 num_blocks:1408 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56469)[0;0m INFO 08-09 14:09:38 [hpu_model_runner.py:3029] [Warmup][Graph/decode][128/135] batch_size:1 query_len:1 num_blocks:1408 free_mem:23.73 GiB
INFO 08-09 14:09:38 [hpu_model_runner.py:3029] [Warmup][Graph/decode][128/135] batch_size:1 query_len:1 num_blocks:1408 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56470)[0;0m INFO 08-09 14:09:38 [hpu_model_runner.py:3029] [Warmup][Graph/decode][128/135] batch_size:1 query_len:1 num_blocks:1408 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56468)[0;0m INFO 08-09 14:09:38 [hpu_model_runner.py:3029] [Warmup][Graph/decode][128/135] batch_size:1 query_len:1 num_blocks:1408 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56467)[0;0m INFO 08-09 14:09:38 [hpu_model_runner.py:3029] [Warmup][Graph/decode][128/135] batch_size:1 query_len:1 num_blocks:1408 free_mem:23.73 GiB
INFO 08-09 14:09:46 [hpu_model_runner.py:3029] [Warmup][Graph/decode][129/135] batch_size:1 query_len:1 num_blocks:1024 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56473)[0;0m INFO 08-09 14:09:46 [hpu_model_runner.py:3029] [Warmup][Graph/decode][129/135] batch_size:1 query_len:1 num_blocks:1024 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56472)[0;0m INFO 08-09 14:09:46 [hpu_model_runner.py:3029] [Warmup][Graph/decode][129/135] batch_size:1 query_len:1 num_blocks:1024 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56470)[0;0m INFO 08-09 14:09:46 [hpu_model_runner.py:3029] [Warmup][Graph/decode][129/135] batch_size:1 query_len:1 num_blocks:1024 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56471)[0;0m INFO 08-09 14:09:46 [hpu_model_runner.py:3029] [Warmup][Graph/decode][129/135] batch_size:1 query_len:1 num_blocks:1024 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56468)[0;0m INFO 08-09 14:09:46 [hpu_model_runner.py:3029] [Warmup][Graph/decode][129/135] batch_size:1 query_len:1 num_blocks:1024 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56469)[0;0m INFO 08-09 14:09:46 [hpu_model_runner.py:3029] [Warmup][Graph/decode][129/135] batch_size:1 query_len:1 num_blocks:1024 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56467)[0;0m INFO 08-09 14:09:46 [hpu_model_runner.py:3029] [Warmup][Graph/decode][129/135] batch_size:1 query_len:1 num_blocks:1024 free_mem:23.73 GiB
INFO 08-09 14:09:53 [hpu_model_runner.py:3029] [Warmup][Graph/decode][130/135] batch_size:1 query_len:1 num_blocks:768 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56471)[0;0m INFO 08-09 14:09:53 [hpu_model_runner.py:3029] [Warmup][Graph/decode][130/135] batch_size:1 query_len:1 num_blocks:768 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56470)[0;0m INFO 08-09 14:09:53 [hpu_model_runner.py:3029] [Warmup][Graph/decode][130/135] batch_size:1 query_len:1 num_blocks:768 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56473)[0;0m INFO 08-09 14:09:53 [hpu_model_runner.py:3029] [Warmup][Graph/decode][130/135] batch_size:1 query_len:1 num_blocks:768 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56469)[0;0m INFO 08-09 14:09:53 [hpu_model_runner.py:3029] [Warmup][Graph/decode][130/135] batch_size:1 query_len:1 num_blocks:768 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56468)[0;0m INFO 08-09 14:09:53 [hpu_model_runner.py:3029] [Warmup][Graph/decode][130/135] batch_size:1 query_len:1 num_blocks:768 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56472)[0;0m INFO 08-09 14:09:53 [hpu_model_runner.py:3029] [Warmup][Graph/decode][130/135] batch_size:1 query_len:1 num_blocks:768 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56467)[0;0m INFO 08-09 14:09:53 [hpu_model_runner.py:3029] [Warmup][Graph/decode][130/135] batch_size:1 query_len:1 num_blocks:768 free_mem:23.73 GiB
INFO 08-09 14:10:00 [hpu_model_runner.py:3029] [Warmup][Graph/decode][131/135] batch_size:1 query_len:1 num_blocks:640 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56473)[0;0m INFO 08-09 14:10:00 [hpu_model_runner.py:3029] [Warmup][Graph/decode][131/135] batch_size:1 query_len:1 num_blocks:640 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56471)[0;0m INFO 08-09 14:10:00 [hpu_model_runner.py:3029] [Warmup][Graph/decode][131/135] batch_size:1 query_len:1 num_blocks:640 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56472)[0;0m INFO 08-09 14:10:00 [hpu_model_runner.py:3029] [Warmup][Graph/decode][131/135] batch_size:1 query_len:1 num_blocks:640 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56469)[0;0m INFO 08-09 14:10:00 [hpu_model_runner.py:3029] [Warmup][Graph/decode][131/135] batch_size:1 query_len:1 num_blocks:640 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56470)[0;0m INFO 08-09 14:10:00 [hpu_model_runner.py:3029] [Warmup][Graph/decode][131/135] batch_size:1 query_len:1 num_blocks:640 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56468)[0;0m INFO 08-09 14:10:00 [hpu_model_runner.py:3029] [Warmup][Graph/decode][131/135] batch_size:1 query_len:1 num_blocks:640 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56467)[0;0m INFO 08-09 14:10:00 [hpu_model_runner.py:3029] [Warmup][Graph/decode][131/135] batch_size:1 query_len:1 num_blocks:640 free_mem:23.73 GiB
INFO 08-09 14:10:08 [hpu_model_runner.py:3029] [Warmup][Graph/decode][132/135] batch_size:1 query_len:1 num_blocks:512 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56473)[0;0m INFO 08-09 14:10:08 [hpu_model_runner.py:3029] [Warmup][Graph/decode][132/135] batch_size:1 query_len:1 num_blocks:512 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56472)[0;0m INFO 08-09 14:10:08 [hpu_model_runner.py:3029] [Warmup][Graph/decode][132/135] batch_size:1 query_len:1 num_blocks:512 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56471)[0;0m INFO 08-09 14:10:08 [hpu_model_runner.py:3029] [Warmup][Graph/decode][132/135] batch_size:1 query_len:1 num_blocks:512 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56469)[0;0m INFO 08-09 14:10:08 [hpu_model_runner.py:3029] [Warmup][Graph/decode][132/135] batch_size:1 query_len:1 num_blocks:512 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56468)[0;0m INFO 08-09 14:10:08 [hpu_model_runner.py:3029] [Warmup][Graph/decode][132/135] batch_size:1 query_len:1 num_blocks:512 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56470)[0;0m INFO 08-09 14:10:08 [hpu_model_runner.py:3029] [Warmup][Graph/decode][132/135] batch_size:1 query_len:1 num_blocks:512 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56467)[0;0m INFO 08-09 14:10:08 [hpu_model_runner.py:3029] [Warmup][Graph/decode][132/135] batch_size:1 query_len:1 num_blocks:512 free_mem:23.73 GiB
INFO 08-09 14:10:15 [hpu_model_runner.py:3029] [Warmup][Graph/decode][133/135] batch_size:1 query_len:1 num_blocks:384 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56473)[0;0m INFO 08-09 14:10:15 [hpu_model_runner.py:3029] [Warmup][Graph/decode][133/135] batch_size:1 query_len:1 num_blocks:384 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56470)[0;0m INFO 08-09 14:10:15 [hpu_model_runner.py:3029] [Warmup][Graph/decode][133/135] batch_size:1 query_len:1 num_blocks:384 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56471)[0;0m INFO 08-09 14:10:15 [hpu_model_runner.py:3029] [Warmup][Graph/decode][133/135] batch_size:1 query_len:1 num_blocks:384 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56469)[0;0m INFO 08-09 14:10:15 [hpu_model_runner.py:3029] [Warmup][Graph/decode][133/135] batch_size:1 query_len:1 num_blocks:384 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56468)[0;0m INFO 08-09 14:10:15 [hpu_model_runner.py:3029] [Warmup][Graph/decode][133/135] batch_size:1 query_len:1 num_blocks:384 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56467)[0;0m INFO 08-09 14:10:15 [hpu_model_runner.py:3029] [Warmup][Graph/decode][133/135] batch_size:1 query_len:1 num_blocks:384 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56472)[0;0m INFO 08-09 14:10:15 [hpu_model_runner.py:3029] [Warmup][Graph/decode][133/135] batch_size:1 query_len:1 num_blocks:384 free_mem:23.73 GiB
INFO 08-09 14:10:22 [hpu_model_runner.py:3029] [Warmup][Graph/decode][134/135] batch_size:1 query_len:1 num_blocks:256 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56472)[0;0m INFO 08-09 14:10:22 [hpu_model_runner.py:3029] [Warmup][Graph/decode][134/135] batch_size:1 query_len:1 num_blocks:256 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56473)[0;0m INFO 08-09 14:10:22 [hpu_model_runner.py:3029] [Warmup][Graph/decode][134/135] batch_size:1 query_len:1 num_blocks:256 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56467)[0;0m INFO 08-09 14:10:22 [hpu_model_runner.py:3029] [Warmup][Graph/decode][134/135] batch_size:1 query_len:1 num_blocks:256 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56471)[0;0m INFO 08-09 14:10:22 [hpu_model_runner.py:3029] [Warmup][Graph/decode][134/135] batch_size:1 query_len:1 num_blocks:256 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56470)[0;0m INFO 08-09 14:10:22 [hpu_model_runner.py:3029] [Warmup][Graph/decode][134/135] batch_size:1 query_len:1 num_blocks:256 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56468)[0;0m INFO 08-09 14:10:22 [hpu_model_runner.py:3029] [Warmup][Graph/decode][134/135] batch_size:1 query_len:1 num_blocks:256 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56469)[0;0m INFO 08-09 14:10:22 [hpu_model_runner.py:3029] [Warmup][Graph/decode][134/135] batch_size:1 query_len:1 num_blocks:256 free_mem:23.73 GiB
INFO 08-09 14:10:30 [hpu_model_runner.py:3029] [Warmup][Graph/decode][135/135] batch_size:1 query_len:1 num_blocks:128 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56473)[0;0m INFO 08-09 14:10:30 [hpu_model_runner.py:3029] [Warmup][Graph/decode][135/135] batch_size:1 query_len:1 num_blocks:128 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56471)[0;0m INFO 08-09 14:10:30 [hpu_model_runner.py:3029] [Warmup][Graph/decode][135/135] batch_size:1 query_len:1 num_blocks:128 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56470)[0;0m INFO 08-09 14:10:30 [hpu_model_runner.py:3029] [Warmup][Graph/decode][135/135] batch_size:1 query_len:1 num_blocks:128 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56469)[0;0m INFO 08-09 14:10:30 [hpu_model_runner.py:3029] [Warmup][Graph/decode][135/135] batch_size:1 query_len:1 num_blocks:128 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56472)[0;0m INFO 08-09 14:10:30 [hpu_model_runner.py:3029] [Warmup][Graph/decode][135/135] batch_size:1 query_len:1 num_blocks:128 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56467)[0;0m INFO 08-09 14:10:30 [hpu_model_runner.py:3029] [Warmup][Graph/decode][135/135] batch_size:1 query_len:1 num_blocks:128 free_mem:23.73 GiB
[1;36m(VllmWorkerProcess pid=56468)[0;0m INFO 08-09 14:10:30 [hpu_model_runner.py:3029] [Warmup][Graph/decode][135/135] batch_size:1 query_len:1 num_blocks:128 free_mem:23.73 GiB
INFO 08-09 14:10:38 [hpu_model_runner.py:3152] Prompt captured:38 (100.0%) used_mem:0 B
INFO 08-09 14:10:38 [hpu_model_runner.py:3152] Decode captured:135 (100.0%) used_mem:4 KiB
[1;36m(VllmWorkerProcess pid=56473)[0;0m INFO 08-09 14:10:38 [hpu_model_runner.py:3152] Prompt captured:38 (100.0%) used_mem:0 B
[1;36m(VllmWorkerProcess pid=56472)[0;0m INFO 08-09 14:10:38 [hpu_model_runner.py:3152] Prompt captured:38 (100.0%) used_mem:0 B
INFO 08-09 14:10:38 [hpu_model_runner.py:3280] Warmup finished in 892 secs, allocated 0 B of device memory
[1;36m(VllmWorkerProcess pid=56472)[0;0m INFO 08-09 14:10:38 [hpu_model_runner.py:3152] Decode captured:135 (100.0%) used_mem:4 KiB
[1;36m(VllmWorkerProcess pid=56473)[0;0m INFO 08-09 14:10:38 [hpu_model_runner.py:3152] Decode captured:135 (100.0%) used_mem:4 KiB
[1;36m(VllmWorkerProcess pid=56471)[0;0m INFO 08-09 14:10:38 [hpu_model_runner.py:3152] Prompt captured:38 (100.0%) used_mem:0 B
[1;36m(VllmWorkerProcess pid=56473)[0;0m INFO 08-09 14:10:38 [hpu_model_runner.py:3280] Warmup finished in 892 secs, allocated 0 B of device memory
[1;36m(VllmWorkerProcess pid=56472)[0;0m INFO 08-09 14:10:38 [hpu_model_runner.py:3280] Warmup finished in 892 secs, allocated 0 B of device memory
[1;36m(VllmWorkerProcess pid=56470)[0;0m INFO 08-09 14:10:38 [hpu_model_runner.py:3152] Prompt captured:38 (100.0%) used_mem:0 B
[1;36m(VllmWorkerProcess pid=56471)[0;0m INFO 08-09 14:10:38 [hpu_model_runner.py:3152] Decode captured:135 (100.0%) used_mem:4 KiB
[1;36m(VllmWorkerProcess pid=56468)[0;0m INFO 08-09 14:10:38 [hpu_model_runner.py:3152] Prompt captured:38 (100.0%) used_mem:0 B
[1;36m(VllmWorkerProcess pid=56467)[0;0m INFO 08-09 14:10:38 [hpu_model_runner.py:3152] Prompt captured:38 (100.0%) used_mem:0 B
[1;36m(VllmWorkerProcess pid=56470)[0;0m INFO 08-09 14:10:38 [hpu_model_runner.py:3152] Decode captured:135 (100.0%) used_mem:4 KiB
[1;36m(VllmWorkerProcess pid=56468)[0;0m INFO 08-09 14:10:38 [hpu_model_runner.py:3152] Decode captured:135 (100.0%) used_mem:4 KiB
[1;36m(VllmWorkerProcess pid=56467)[0;0m INFO 08-09 14:10:38 [hpu_model_runner.py:3152] Decode captured:135 (100.0%) used_mem:4 KiB
[1;36m(VllmWorkerProcess pid=56471)[0;0m INFO 08-09 14:10:38 [hpu_model_runner.py:3280] Warmup finished in 893 secs, allocated 0 B of device memory
[1;36m(VllmWorkerProcess pid=56469)[0;0m INFO 08-09 14:10:38 [hpu_model_runner.py:3152] Prompt captured:38 (100.0%) used_mem:0 B
[1;36m(VllmWorkerProcess pid=56470)[0;0m INFO 08-09 14:10:38 [hpu_model_runner.py:3280] Warmup finished in 892 secs, allocated 0 B of device memory
[1;36m(VllmWorkerProcess pid=56468)[0;0m INFO 08-09 14:10:38 [hpu_model_runner.py:3280] Warmup finished in 893 secs, allocated 0 B of device memory
[1;36m(VllmWorkerProcess pid=56467)[0;0m INFO 08-09 14:10:38 [hpu_model_runner.py:3280] Warmup finished in 893 secs, allocated 0 B of device memory
[1;36m(VllmWorkerProcess pid=56469)[0;0m INFO 08-09 14:10:38 [hpu_model_runner.py:3152] Decode captured:135 (100.0%) used_mem:4 KiB
[1;36m(VllmWorkerProcess pid=56469)[0;0m INFO 08-09 14:10:38 [hpu_model_runner.py:3280] Warmup finished in 892 secs, allocated 0 B of device memory
INFO 08-09 14:10:38 [llm_engine.py:431] init engine (profile, create kv cache, warmup model) took 897.61 seconds
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
WARNING 08-09 14:10:38 [config.py:1364] Default sampling parameters have been overridden by the model's Hugging Face generation config recommended from the model creator. If this is not intended, please relaunch vLLM instance with `--generation-config vllm`.
INFO 08-09 14:10:38 [serving_chat.py:118] Using default chat sampling params from model: {'temperature': 0.9, 'top_p': 0.6}
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
INFO 08-09 14:10:38 [serving_completion.py:66] Using default completion sampling params from model: {'temperature': 0.9, 'top_p': 0.6}
INFO 08-09 14:10:38 [api_server.py:1351] Starting vLLM API server 0 on http://0.0.0.0:8000
INFO 08-09 14:10:38 [launcher.py:29] Available routes are:
INFO 08-09 14:10:38 [launcher.py:37] Route: /openapi.json, Methods: HEAD, GET
INFO 08-09 14:10:38 [launcher.py:37] Route: /docs, Methods: HEAD, GET
INFO 08-09 14:10:38 [launcher.py:37] Route: /docs/oauth2-redirect, Methods: HEAD, GET
INFO 08-09 14:10:38 [launcher.py:37] Route: /redoc, Methods: HEAD, GET
INFO 08-09 14:10:38 [launcher.py:37] Route: /health, Methods: GET
INFO 08-09 14:10:38 [launcher.py:37] Route: /load, Methods: GET
INFO 08-09 14:10:38 [launcher.py:37] Route: /ping, Methods: POST
INFO 08-09 14:10:38 [launcher.py:37] Route: /ping, Methods: GET
INFO 08-09 14:10:38 [launcher.py:37] Route: /tokenize, Methods: POST
INFO 08-09 14:10:38 [launcher.py:37] Route: /detokenize, Methods: POST
INFO 08-09 14:10:38 [launcher.py:37] Route: /v1/models, Methods: GET
INFO 08-09 14:10:38 [launcher.py:37] Route: /version, Methods: GET
INFO 08-09 14:10:38 [launcher.py:37] Route: /v1/chat/completions, Methods: POST
INFO 08-09 14:10:38 [launcher.py:37] Route: /v1/completions, Methods: POST
INFO 08-09 14:10:38 [launcher.py:37] Route: /v1/embeddings, Methods: POST
INFO 08-09 14:10:38 [launcher.py:37] Route: /pooling, Methods: POST
INFO 08-09 14:10:38 [launcher.py:37] Route: /classify, Methods: POST
INFO 08-09 14:10:38 [launcher.py:37] Route: /score, Methods: POST
INFO 08-09 14:10:38 [launcher.py:37] Route: /v1/score, Methods: POST
INFO 08-09 14:10:38 [launcher.py:37] Route: /v1/audio/transcriptions, Methods: POST
INFO 08-09 14:10:38 [launcher.py:37] Route: /rerank, Methods: POST
INFO 08-09 14:10:38 [launcher.py:37] Route: /v1/rerank, Methods: POST
INFO 08-09 14:10:38 [launcher.py:37] Route: /v2/rerank, Methods: POST
INFO 08-09 14:10:38 [launcher.py:37] Route: /invocations, Methods: POST
INFO 08-09 14:10:38 [launcher.py:37] Route: /metrics, Methods: GET
INFO:     Started server process [56065]
INFO:     Waiting for application startup.
INFO:     Application startup complete.
INFO 08-09 14:11:19 [logger.py:43] Received request cmpl-25ac2ab107f54723824e9a8be76c763a-0: prompt: 'What is LLaMA 2 and how does it work?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 365, 5661, 1529, 29871, 29906, 322, 920, 947, 372, 664, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:42666 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:11:19 [engine.py:331] Added request cmpl-25ac2ab107f54723824e9a8be76c763a-0.
INFO 08-09 14:11:23 [metrics.py:417] Avg prompt throughput: 2.8 tokens/s, Avg generation throughput: 18.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 14:11:28 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 107.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:11:31 [logger.py:43] Received request cmpl-4c66a33864904457a8f0c3876fa27c41-0: prompt: 'Explain the architecture of a transformer model.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 12027, 7420, 278, 11258, 310, 263, 4327, 261, 1904, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:56772 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:11:31 [engine.py:331] Added request cmpl-4c66a33864904457a8f0c3876fa27c41-0.
INFO 08-09 14:11:33 [metrics.py:417] Avg prompt throughput: 2.2 tokens/s, Avg generation throughput: 106.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 14:11:38 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 108.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:11:41 [logger.py:43] Received request cmpl-5652b34281094a4986770783f3bd3b1b-0: prompt: 'What are the core differences between CNN and RNN?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 526, 278, 7136, 12651, 1546, 29696, 322, 390, 10262, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:49340 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:11:41 [engine.py:331] Added request cmpl-5652b34281094a4986770783f3bd3b1b-0.
INFO 08-09 14:11:43 [metrics.py:417] Avg prompt throughput: 2.4 tokens/s, Avg generation throughput: 106.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 14:11:48 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 106.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:11:50 [logger.py:43] Received request cmpl-5ab5fca810094fe19e2ab69aadcbf47c-0: prompt: 'Describe how attention mechanism improves translation models.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 20355, 915, 920, 8570, 13336, 4857, 1960, 13962, 4733, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:35378 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:11:50 [engine.py:331] Added request cmpl-5ab5fca810094fe19e2ab69aadcbf47c-0.
INFO 08-09 14:11:53 [metrics.py:417] Avg prompt throughput: 2.2 tokens/s, Avg generation throughput: 109.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 14:11:58 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 105.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:11:59 [logger.py:43] Received request cmpl-74b6ab9019f845e8aceea9f58ff651e5-0: prompt: 'What is the role of positional encoding in transformers?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 278, 6297, 310, 2602, 284, 8025, 297, 4327, 414, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:44702 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:11:59 [engine.py:331] Added request cmpl-74b6ab9019f845e8aceea9f58ff651e5-0.
INFO 08-09 14:12:03 [metrics.py:417] Avg prompt throughput: 2.6 tokens/s, Avg generation throughput: 103.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 14:12:08 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 101.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:12:09 [logger.py:43] Received request cmpl-c0d425ff055c48aab6401975311bca15-0: prompt: 'How does self-attention differ from cross-attention?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 947, 1583, 29899, 1131, 2509, 1163, 515, 4891, 29899, 1131, 2509, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:49142 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:12:09 [engine.py:331] Added request cmpl-c0d425ff055c48aab6401975311bca15-0.
INFO 08-09 14:12:13 [metrics.py:417] Avg prompt throughput: 2.8 tokens/s, Avg generation throughput: 105.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 14:12:17 [logger.py:43] Received request cmpl-48e51ed7a16a4627919dab3c3f9e15ae-0: prompt: 'Explain the concept of tokenization in NLP.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 12027, 7420, 278, 6964, 310, 5993, 2133, 297, 405, 13208, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:52400 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:12:17 [engine.py:331] Added request cmpl-48e51ed7a16a4627919dab3c3f9e15ae-0.
INFO 08-09 14:12:18 [metrics.py:417] Avg prompt throughput: 2.4 tokens/s, Avg generation throughput: 103.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 14:12:23 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 100.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 14:12:27 [logger.py:43] Received request cmpl-809cdef6835a404899e7ad34589346a4-0: prompt: 'What is the vanishing gradient problem and how to mitigate it?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 278, 1109, 14424, 16030, 1108, 322, 920, 304, 1380, 335, 403, 372, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:52590 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:12:27 [engine.py:331] Added request cmpl-809cdef6835a404899e7ad34589346a4-0.
INFO 08-09 14:12:28 [metrics.py:417] Avg prompt throughput: 3.2 tokens/s, Avg generation throughput: 106.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 14:12:33 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 101.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 14:12:37 [logger.py:43] Received request cmpl-66229851a50c4a4fa4cae0fafd7ec01d-0: prompt: 'Describe how Gaudi architecture differs from NVIDIA GPUs.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 20355, 915, 920, 10415, 4749, 11258, 2923, 414, 515, 405, 13044, 10764, 22796, 29879, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:47910 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:12:37 [engine.py:331] Added request cmpl-66229851a50c4a4fa4cae0fafd7ec01d-0.
INFO 08-09 14:12:38 [metrics.py:417] Avg prompt throughput: 3.2 tokens/s, Avg generation throughput: 104.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 14:12:43 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 98.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:12:47 [logger.py:43] Received request cmpl-8eb6d934d4294859a3a0e8538c845267-0: prompt: 'What is model parallelism and how does it help scale large models?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 1904, 8943, 1608, 322, 920, 947, 372, 1371, 6287, 2919, 4733, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:43070 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:12:47 [engine.py:331] Added request cmpl-8eb6d934d4294859a3a0e8538c845267-0.
INFO 08-09 14:12:48 [metrics.py:417] Avg prompt throughput: 3.0 tokens/s, Avg generation throughput: 106.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 14:12:53 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 110.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:12:56 [logger.py:43] Received request cmpl-08d397dc29e74d70a78495d2967ce6ab-0: prompt: 'Explain the difference between GPT and BERT architectures.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 12027, 7420, 278, 4328, 1546, 402, 7982, 322, 350, 20161, 6956, 1973, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:38012 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:12:56 [engine.py:331] Added request cmpl-08d397dc29e74d70a78495d2967ce6ab-0.
INFO 08-09 14:12:58 [metrics.py:417] Avg prompt throughput: 2.8 tokens/s, Avg generation throughput: 105.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 14:13:03 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 107.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:13:05 [logger.py:43] Received request cmpl-e6aae6003e14434bb4171dcff55b3fb1-0: prompt: 'How does prompt engineering affect language model outputs?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 947, 9508, 21639, 6602, 4086, 1904, 14391, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:59304 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:13:05 [engine.py:331] Added request cmpl-e6aae6003e14434bb4171dcff55b3fb1-0.
INFO 08-09 14:13:08 [metrics.py:417] Avg prompt throughput: 2.0 tokens/s, Avg generation throughput: 109.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 14:13:13 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 109.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:13:14 [logger.py:43] Received request cmpl-97ea3bf89c024ff9945e1ab5541528d0-0: prompt: 'What is the function of layer normalization in deep networks?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 278, 740, 310, 7546, 4226, 2133, 297, 6483, 14379, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:59320 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:13:14 [engine.py:331] Added request cmpl-97ea3bf89c024ff9945e1ab5541528d0-0.
INFO 08-09 14:13:18 [metrics.py:417] Avg prompt throughput: 2.6 tokens/s, Avg generation throughput: 109.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 14:13:23 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 105.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:13:23 [logger.py:43] Received request cmpl-00636b1f7d134d4d8e6d76c430591703-0: prompt: 'Explain causal language modeling with examples.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 12027, 7420, 3269, 284, 4086, 1904, 292, 411, 6455, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:54246 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:13:23 [engine.py:331] Added request cmpl-00636b1f7d134d4d8e6d76c430591703-0.
INFO 08-09 14:13:28 [metrics.py:417] Avg prompt throughput: 2.2 tokens/s, Avg generation throughput: 100.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 14:13:33 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 102.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:13:33 [logger.py:43] Received request cmpl-dfc5ca87409244ffbdcea23b77e0e68f-0: prompt: 'What is quantization and how does it improve inference performance?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 4323, 2133, 322, 920, 947, 372, 11157, 27262, 4180, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:34864 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:13:33 [engine.py:331] Added request cmpl-dfc5ca87409244ffbdcea23b77e0e68f-0.
INFO 08-09 14:13:38 [metrics.py:417] Avg prompt throughput: 2.6 tokens/s, Avg generation throughput: 99.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 14:13:43 [logger.py:43] Received request cmpl-76a10087f9da4df48ef975664141b479-0: prompt: 'Describe how fine-tuning works for large language models.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 20355, 915, 920, 2691, 29899, 29873, 27964, 1736, 363, 2919, 4086, 4733, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:53186 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:13:43 [engine.py:331] Added request cmpl-76a10087f9da4df48ef975664141b479-0.
INFO 08-09 14:13:43 [metrics.py:417] Avg prompt throughput: 2.8 tokens/s, Avg generation throughput: 105.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 14:13:48 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 104.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 14:13:52 [logger.py:43] Received request cmpl-b7e2f103719f42228bf0dda8eef3a56e-0: prompt: 'What are LoRA adapters and how do they help in parameter-efficient training?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 526, 4309, 4717, 594, 481, 2153, 322, 920, 437, 896, 1371, 297, 3443, 29899, 8462, 6694, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:49020 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:13:53 [engine.py:331] Added request cmpl-b7e2f103719f42228bf0dda8eef3a56e-0.
INFO 08-09 14:13:53 [metrics.py:417] Avg prompt throughput: 3.8 tokens/s, Avg generation throughput: 107.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 14:13:58 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 105.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 14:14:02 [logger.py:43] Received request cmpl-cfc72dc108e94671a85424b5d9e9cacb-0: prompt: 'Explain the purpose of rotary positional embeddings.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 12027, 7420, 278, 6437, 310, 5731, 653, 2602, 284, 8297, 29881, 886, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:46862 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:14:02 [engine.py:331] Added request cmpl-cfc72dc108e94671a85424b5d9e9cacb-0.
INFO 08-09 14:14:03 [metrics.py:417] Avg prompt throughput: 2.8 tokens/s, Avg generation throughput: 107.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 14:14:08 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 111.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:14:11 [logger.py:43] Received request cmpl-eb171f9b84c94c62b12d7e02c5b2bb00-0: prompt: 'How does DeepSpeed improve model training efficiency?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 947, 21784, 26539, 11157, 1904, 6694, 19201, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:46232 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:14:11 [engine.py:331] Added request cmpl-eb171f9b84c94c62b12d7e02c5b2bb00-0.
INFO 08-09 14:14:13 [metrics.py:417] Avg prompt throughput: 2.0 tokens/s, Avg generation throughput: 107.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 14:14:18 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 110.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:14:20 [logger.py:43] Received request cmpl-098ae2a39f524f3faf849572bf9670ce-0: prompt: 'Compare data parallelism and pipeline parallelism.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 3831, 598, 848, 8943, 1608, 322, 16439, 8943, 1608, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:41618 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:14:20 [engine.py:331] Added request cmpl-098ae2a39f524f3faf849572bf9670ce-0.
INFO 08-09 14:14:23 [metrics.py:417] Avg prompt throughput: 2.2 tokens/s, Avg generation throughput: 108.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 14:14:28 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 107.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:14:29 [logger.py:43] Received request cmpl-fb4f34190faf48ccb3e81fdb86028d77-0: prompt: 'What is ALiBi positional encoding?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 14445, 29875, 20517, 2602, 284, 8025, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:42928 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:14:29 [engine.py:331] Added request cmpl-fb4f34190faf48ccb3e81fdb86028d77-0.
INFO 08-09 14:14:33 [metrics.py:417] Avg prompt throughput: 2.0 tokens/s, Avg generation throughput: 102.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 14:14:38 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 110.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:14:39 [logger.py:43] Received request cmpl-b5e9c6480bea4e048e39a3443f3b62d2-0: prompt: 'Describe the purpose of HF Transformers library.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 20355, 915, 278, 6437, 310, 379, 29943, 4103, 689, 414, 3489, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:54222 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:14:39 [engine.py:331] Added request cmpl-b5e9c6480bea4e048e39a3443f3b62d2-0.
INFO 08-09 14:14:43 [metrics.py:417] Avg prompt throughput: 2.6 tokens/s, Avg generation throughput: 110.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 14:14:48 [logger.py:43] Received request cmpl-50561b3748bf419982580509ef95789a-0: prompt: 'How do attention heads capture contextual meaning?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 437, 8570, 15883, 10446, 3030, 950, 6593, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:37554 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:14:48 [engine.py:331] Added request cmpl-50561b3748bf419982580509ef95789a-0.
INFO 08-09 14:14:48 [metrics.py:417] Avg prompt throughput: 2.0 tokens/s, Avg generation throughput: 110.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 14:14:53 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 110.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 14:14:57 [logger.py:43] Received request cmpl-738466ec54054b12b8c122ccde8c2a86-0: prompt: 'What is dynamic batching in inference servers?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 7343, 9853, 292, 297, 27262, 12424, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:44638 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:14:57 [engine.py:331] Added request cmpl-738466ec54054b12b8c122ccde8c2a86-0.
INFO 08-09 14:14:58 [metrics.py:417] Avg prompt throughput: 2.0 tokens/s, Avg generation throughput: 113.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 14:15:03 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 109.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:15:06 [logger.py:43] Received request cmpl-8f9e23c467ce4226989b510f3cb6c821-0: prompt: 'Explain greedy decoding vs beam search vs sampling.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 12027, 7420, 1395, 7584, 1602, 3689, 7186, 22913, 2740, 7186, 23460, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:55854 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:15:06 [engine.py:331] Added request cmpl-8f9e23c467ce4226989b510f3cb6c821-0.
INFO 08-09 14:15:08 [metrics.py:417] Avg prompt throughput: 2.6 tokens/s, Avg generation throughput: 111.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 14:15:13 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 100.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:15:16 [logger.py:43] Received request cmpl-c243b990680147be97f6ce70471c3f0e-0: prompt: 'What is perplexity in language models and why is it important?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 639, 10709, 537, 297, 4086, 4733, 322, 2020, 338, 372, 4100, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:33436 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:15:16 [engine.py:331] Added request cmpl-c243b990680147be97f6ce70471c3f0e-0.
INFO 08-09 14:15:18 [metrics.py:417] Avg prompt throughput: 3.0 tokens/s, Avg generation throughput: 97.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 14:15:23 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 109.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:15:25 [logger.py:43] Received request cmpl-5366f3bd255145629ec21ac5d6ba6a77-0: prompt: 'How does mixed precision training benefit model training?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 947, 12849, 16716, 6694, 14169, 1904, 6694, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:47504 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:15:25 [engine.py:331] Added request cmpl-5366f3bd255145629ec21ac5d6ba6a77-0.
INFO 08-09 14:15:28 [metrics.py:417] Avg prompt throughput: 2.0 tokens/s, Avg generation throughput: 109.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 14:15:33 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 111.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:15:34 [logger.py:43] Received request cmpl-b861e1ea1a15447db50baf1071398c46-0: prompt: 'What is the function of the softmax layer in transformers?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 278, 740, 310, 278, 4964, 3317, 7546, 297, 4327, 414, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:47506 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:15:34 [engine.py:331] Added request cmpl-b861e1ea1a15447db50baf1071398c46-0.
INFO 08-09 14:15:38 [metrics.py:417] Avg prompt throughput: 2.8 tokens/s, Avg generation throughput: 111.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 14:15:43 [logger.py:43] Received request cmpl-44c30c63c86844ba9a40de984a07bcee-0: prompt: "Explain embeddings and how they're used in NLP models.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 12027, 7420, 8297, 29881, 886, 322, 920, 896, 29915, 276, 1304, 297, 405, 13208, 4733, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:52690 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:15:43 [engine.py:331] Added request cmpl-44c30c63c86844ba9a40de984a07bcee-0.
INFO 08-09 14:15:43 [metrics.py:417] Avg prompt throughput: 3.4 tokens/s, Avg generation throughput: 106.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 14:15:48 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 104.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 14:15:53 [logger.py:43] Received request cmpl-e8fafde1366349d6838fb85913545e14-0: prompt: "Describe the function of the tokenizer's vocabulary.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 20355, 915, 278, 740, 310, 278, 5993, 3950, 29915, 29879, 7931, 370, 352, 653, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:44682 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:15:53 [engine.py:331] Added request cmpl-e8fafde1366349d6838fb85913545e14-0.
INFO 08-09 14:15:53 [metrics.py:417] Avg prompt throughput: 3.2 tokens/s, Avg generation throughput: 102.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 14:15:58 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 105.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 14:16:02 [logger.py:43] Received request cmpl-53a48a6148934e97afb7967a8008970b-0: prompt: 'What is causal masking in transformer decoders?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 3269, 284, 11105, 292, 297, 4327, 261, 1602, 397, 414, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:37274 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:16:02 [engine.py:331] Added request cmpl-53a48a6148934e97afb7967a8008970b-0.
INFO 08-09 14:16:03 [metrics.py:417] Avg prompt throughput: 2.8 tokens/s, Avg generation throughput: 104.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 14:16:08 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 103.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 14:16:12 [logger.py:43] Received request cmpl-78d7e431d98d4ba19e944c7d00502be7-0: prompt: 'How does gradient checkpointing reduce memory usage?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 947, 16030, 1423, 3149, 292, 10032, 3370, 8744, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:42984 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:16:12 [engine.py:331] Added request cmpl-78d7e431d98d4ba19e944c7d00502be7-0.
INFO 08-09 14:16:13 [metrics.py:417] Avg prompt throughput: 2.2 tokens/s, Avg generation throughput: 106.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 14:16:18 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 105.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:16:21 [logger.py:43] Received request cmpl-14777f8bea7941b38e38e307362c5087-0: prompt: 'Explain the difference between encoder-decoder and decoder-only models.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 12027, 7420, 278, 4328, 1546, 2094, 6119, 29899, 7099, 6119, 322, 1602, 6119, 29899, 6194, 4733, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:54358 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:16:21 [engine.py:331] Added request cmpl-14777f8bea7941b38e38e307362c5087-0.
INFO 08-09 14:16:23 [metrics.py:417] Avg prompt throughput: 3.6 tokens/s, Avg generation throughput: 105.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 14:16:28 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 110.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:16:31 [logger.py:43] Received request cmpl-328903d80811431f85f1626f684036c9-0: prompt: 'What is zero-shot vs few-shot vs fine-tuned inference?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 5225, 29899, 8962, 7186, 2846, 29899, 8962, 7186, 2691, 29899, 29873, 348, 287, 27262, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:46776 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:16:31 [engine.py:331] Added request cmpl-328903d80811431f85f1626f684036c9-0.
INFO 08-09 14:16:33 [metrics.py:417] Avg prompt throughput: 3.6 tokens/s, Avg generation throughput: 106.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 14:16:38 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 110.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:16:40 [logger.py:43] Received request cmpl-c5f06cd9375d453ab63cbb8ec164bd70-0: prompt: 'What are prompt templates and why do they matter?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 526, 9508, 17475, 322, 2020, 437, 896, 4383, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:35834 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:16:40 [engine.py:331] Added request cmpl-c5f06cd9375d453ab63cbb8ec164bd70-0.
INFO 08-09 14:16:43 [metrics.py:417] Avg prompt throughput: 2.2 tokens/s, Avg generation throughput: 108.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 14:16:48 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 99.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:16:49 [logger.py:43] Received request cmpl-2e2c327c406349ada1f0cef6f5f7830c-0: prompt: 'Explain the role of BOS and EOS tokens in transformers.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 12027, 7420, 278, 6297, 310, 350, 3267, 322, 382, 3267, 18897, 297, 4327, 414, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:50386 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:16:49 [engine.py:331] Added request cmpl-2e2c327c406349ada1f0cef6f5f7830c-0.
INFO 08-09 14:16:53 [metrics.py:417] Avg prompt throughput: 3.2 tokens/s, Avg generation throughput: 107.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 14:16:58 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 106.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:16:59 [logger.py:43] Received request cmpl-dde901104eed4160b1a80ce7343f4c41-0: prompt: 'What is a floating-point tensor and how is it represented?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 263, 16526, 29899, 3149, 12489, 322, 920, 338, 372, 9875, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:41766 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:16:59 [engine.py:331] Added request cmpl-dde901104eed4160b1a80ce7343f4c41-0.
INFO 08-09 14:17:03 [metrics.py:417] Avg prompt throughput: 2.8 tokens/s, Avg generation throughput: 105.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 14:17:08 [logger.py:43] Received request cmpl-3e3551a34d554fdcb6a7f33bd282dfa3-0: prompt: 'What are hypernetworks in neural architectures?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 526, 11266, 11618, 29879, 297, 19677, 6956, 1973, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:47916 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:17:08 [engine.py:331] Added request cmpl-3e3551a34d554fdcb6a7f33bd282dfa3-0.
INFO 08-09 14:17:08 [metrics.py:417] Avg prompt throughput: 2.2 tokens/s, Avg generation throughput: 106.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 14:17:13 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 101.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 14:17:16 [logger.py:43] Received request cmpl-ab0692dff43c4d12b0023f84596e5f86-0: prompt: 'Explain MoE (Mixture of Experts) architecture.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 12027, 7420, 4546, 29923, 313, 29924, 29875, 15546, 310, 28224, 1372, 29897, 11258, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:57088 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:17:16 [engine.py:331] Added request cmpl-ab0692dff43c4d12b0023f84596e5f86-0.
INFO 08-09 14:17:18 [metrics.py:417] Avg prompt throughput: 3.0 tokens/s, Avg generation throughput: 107.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 14:17:23 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 106.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:17:25 [logger.py:43] Received request cmpl-42464d49279d441caf19c2c7d6864d72-0: prompt: 'What is distillation in model compression?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 1320, 453, 362, 297, 1904, 24221, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:35856 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:17:25 [engine.py:331] Added request cmpl-42464d49279d441caf19c2c7d6864d72-0.
INFO 08-09 14:17:28 [metrics.py:417] Avg prompt throughput: 2.0 tokens/s, Avg generation throughput: 109.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 14:17:33 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 111.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:17:34 [logger.py:43] Received request cmpl-450fa5ccfd774086b6c459198a01c1f0-0: prompt: 'How do you implement top-k and top-p sampling?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 437, 366, 2334, 2246, 29899, 29895, 322, 2246, 29899, 29886, 23460, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:35872 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:17:34 [engine.py:331] Added request cmpl-450fa5ccfd774086b6c459198a01c1f0-0.
INFO 08-09 14:17:38 [metrics.py:417] Avg prompt throughput: 2.8 tokens/s, Avg generation throughput: 112.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 14:17:43 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 106.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:17:43 [logger.py:43] Received request cmpl-4e0b2444a99c4211adf6c4d265af478d-0: prompt: 'What is a vector database and how is it used with LLMs?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 263, 4608, 2566, 322, 920, 338, 372, 1304, 411, 365, 26369, 29879, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:56160 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:17:43 [engine.py:331] Added request cmpl-4e0b2444a99c4211adf6c4d265af478d-0.
INFO 08-09 14:17:48 [metrics.py:417] Avg prompt throughput: 3.2 tokens/s, Avg generation throughput: 104.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 14:17:53 [logger.py:43] Received request cmpl-b376bb6a63614f6e8b45bb392f1a3e41-0: prompt: 'Describe how you would use RAG (Retrieval Augmented Generation).', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 20355, 915, 920, 366, 723, 671, 390, 10051, 313, 8015, 2546, 791, 22333, 358, 287, 28203, 467], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:42484 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:17:53 [engine.py:331] Added request cmpl-b376bb6a63614f6e8b45bb392f1a3e41-0.
INFO 08-09 14:17:53 [metrics.py:417] Avg prompt throughput: 3.6 tokens/s, Avg generation throughput: 107.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 14:17:58 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 109.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 14:18:02 [logger.py:43] Received request cmpl-f6eb962c730145718d1f2e80d677fbc9-0: prompt: 'What is streaming inference and how is it used?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 24820, 27262, 322, 920, 338, 372, 1304, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:59698 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:18:02 [engine.py:331] Added request cmpl-f6eb962c730145718d1f2e80d677fbc9-0.
INFO 08-09 14:18:03 [metrics.py:417] Avg prompt throughput: 2.2 tokens/s, Avg generation throughput: 101.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 14:18:08 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 106.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:18:12 [logger.py:43] Received request cmpl-f93fb685ef6c4f7ab8a9bbfa57f04471-0: prompt: 'How does TTFT relate to user-perceived latency?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 947, 323, 29911, 7818, 29279, 304, 1404, 29899, 546, 346, 2347, 23316, 1270, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:45916 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:18:12 [engine.py:331] Added request cmpl-f93fb685ef6c4f7ab8a9bbfa57f04471-0.
INFO 08-09 14:18:13 [metrics.py:417] Avg prompt throughput: 3.2 tokens/s, Avg generation throughput: 105.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 14:18:18 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 110.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:18:21 [logger.py:43] Received request cmpl-d6569b06c5414aef834a30beba357eed-0: prompt: 'Explain transformer decoder blocks layer-by-layer.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 12027, 7420, 4327, 261, 1602, 6119, 10930, 7546, 29899, 1609, 29899, 13148, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:36348 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:18:21 [engine.py:331] Added request cmpl-d6569b06c5414aef834a30beba357eed-0.
INFO 08-09 14:18:23 [metrics.py:417] Avg prompt throughput: 2.8 tokens/s, Avg generation throughput: 105.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 14:18:28 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 108.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:18:30 [logger.py:43] Received request cmpl-0ccb0d20967f426a91f1831e12fd37f3-0: prompt: 'What is a KV cache in transformer inference?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 263, 476, 29963, 7090, 297, 4327, 261, 27262, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:34056 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:18:30 [engine.py:331] Added request cmpl-0ccb0d20967f426a91f1831e12fd37f3-0.
INFO 08-09 14:18:33 [metrics.py:417] Avg prompt throughput: 2.4 tokens/s, Avg generation throughput: 105.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 14:18:38 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 109.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:18:40 [logger.py:43] Received request cmpl-b11f435d27514f8dbd4024218914a9b5-0: prompt: 'Describe token-by-token generation and its challenges.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 20355, 915, 5993, 29899, 1609, 29899, 6979, 12623, 322, 967, 18066, 267, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:42548 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:18:40 [engine.py:331] Added request cmpl-b11f435d27514f8dbd4024218914a9b5-0.
INFO 08-09 14:18:43 [metrics.py:417] Avg prompt throughput: 2.8 tokens/s, Avg generation throughput: 106.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 14:18:48 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 106.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:18:49 [logger.py:43] Received request cmpl-b83b2ff737244fb0a0e5ad121e822a23-0: prompt: 'How does LLaMA 2 compare to GPT-3.5?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 947, 365, 5661, 1529, 29871, 29906, 7252, 304, 402, 7982, 29899, 29941, 29889, 29945, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:37506 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:18:49 [engine.py:331] Added request cmpl-b83b2ff737244fb0a0e5ad121e822a23-0.
INFO 08-09 14:18:53 [metrics.py:417] Avg prompt throughput: 3.4 tokens/s, Avg generation throughput: 110.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 14:18:58 [logger.py:43] Received request cmpl-1dd1b3a62b4b4c0799f96a541bf3d34e-0: prompt: 'Explain the training data pipeline for a large model.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 12027, 7420, 278, 6694, 848, 16439, 363, 263, 2919, 1904, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:55414 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:18:58 [engine.py:331] Added request cmpl-1dd1b3a62b4b4c0799f96a541bf3d34e-0.
INFO 08-09 14:18:58 [metrics.py:417] Avg prompt throughput: 2.4 tokens/s, Avg generation throughput: 109.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 14:19:03 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 110.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 14:19:07 [logger.py:43] Received request cmpl-a1332cc6e6f142eb9ed8039ab479699d-0: prompt: 'What are the ethical concerns with generative models?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 526, 278, 11314, 936, 21838, 411, 1176, 1230, 4733, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:59568 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:19:07 [engine.py:331] Added request cmpl-a1332cc6e6f142eb9ed8039ab479699d-0.
INFO 08-09 14:19:08 [metrics.py:417] Avg prompt throughput: 2.4 tokens/s, Avg generation throughput: 107.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 14:19:13 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 108.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:19:16 [logger.py:43] Received request cmpl-94958e8874f44ccdbdfcf19789b82c42-0: prompt: 'How do transformers process multilingual text?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 437, 4327, 414, 1889, 1773, 6504, 950, 1426, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:47264 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:19:16 [engine.py:331] Added request cmpl-94958e8874f44ccdbdfcf19789b82c42-0.
INFO 08-09 14:19:18 [metrics.py:417] Avg prompt throughput: 2.2 tokens/s, Avg generation throughput: 110.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 14:19:23 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 102.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:19:26 [logger.py:43] Received request cmpl-72357173570b4dd3b9fae3394828578b-0: prompt: 'What are flash attention mechanisms?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 526, 11013, 8570, 7208, 12903, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:56796 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:19:26 [engine.py:331] Added request cmpl-72357173570b4dd3b9fae3394828578b-0.
INFO 08-09 14:19:28 [metrics.py:417] Avg prompt throughput: 1.6 tokens/s, Avg generation throughput: 107.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 14:19:33 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 104.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:19:35 [logger.py:43] Received request cmpl-670adea6ae464f2386d98cef47b9c292-0: prompt: 'Describe how memory-efficient attention works.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 20355, 915, 920, 3370, 29899, 8462, 8570, 1736, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:44586 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:19:35 [engine.py:331] Added request cmpl-670adea6ae464f2386d98cef47b9c292-0.
INFO 08-09 14:19:38 [metrics.py:417] Avg prompt throughput: 2.0 tokens/s, Avg generation throughput: 107.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 14:19:43 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 109.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:19:44 [logger.py:43] Received request cmpl-3536219a45b24e3ba04029ee41cbf553-0: prompt: 'What is inference throughput and how is it calculated?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 27262, 1549, 649, 322, 920, 338, 372, 12833, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:53346 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:19:44 [engine.py:331] Added request cmpl-3536219a45b24e3ba04029ee41cbf553-0.
INFO 08-09 14:19:48 [metrics.py:417] Avg prompt throughput: 2.4 tokens/s, Avg generation throughput: 108.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 14:19:53 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 102.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:19:54 [logger.py:43] Received request cmpl-64d23d887924413c9975a45b150970a1-0: prompt: 'Explain how masked self-attention works in BERT.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 12027, 7420, 920, 11105, 287, 1583, 29899, 1131, 2509, 1736, 297, 350, 20161, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:53362 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:19:54 [engine.py:331] Added request cmpl-64d23d887924413c9975a45b150970a1-0.
INFO 08-09 14:19:58 [metrics.py:417] Avg prompt throughput: 3.0 tokens/s, Avg generation throughput: 105.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 14:20:03 [logger.py:43] Received request cmpl-49dc5f0493c64600a33525835bbe4c3a-0: prompt: 'How do instruction-tuned models differ from base models?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 437, 15278, 29899, 29873, 348, 287, 4733, 1163, 515, 2967, 4733, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:49518 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:20:03 [engine.py:331] Added request cmpl-49dc5f0493c64600a33525835bbe4c3a-0.
INFO 08-09 14:20:03 [metrics.py:417] Avg prompt throughput: 2.8 tokens/s, Avg generation throughput: 105.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 14:20:08 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 109.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 14:20:13 [logger.py:43] Received request cmpl-79daea08a4eb48f5813520f97bf6e1e2-0: prompt: 'Describe an RLHF (Reinforcement Learning from Human Feedback) process.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 20355, 915, 385, 390, 29931, 29950, 29943, 313, 1123, 262, 1454, 13561, 29257, 515, 12968, 5169, 287, 1627, 29897, 1889, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:50116 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:20:13 [engine.py:331] Added request cmpl-79daea08a4eb48f5813520f97bf6e1e2-0.
INFO 08-09 14:20:13 [metrics.py:417] Avg prompt throughput: 4.4 tokens/s, Avg generation throughput: 107.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 14:20:18 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 90.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 14:20:23 [logger.py:43] Received request cmpl-5eff3a2b74fb4b72a19d9e69c61bd473-0: prompt: 'What is SFT (Supervised Fine-Tuning) in LLM training?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 317, 7818, 313, 19111, 11292, 28896, 29899, 29911, 27964, 29897, 297, 365, 26369, 6694, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:56086 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:20:23 [engine.py:331] Added request cmpl-5eff3a2b74fb4b72a19d9e69c61bd473-0.
INFO 08-09 14:20:23 [metrics.py:417] Avg prompt throughput: 3.6 tokens/s, Avg generation throughput: 102.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 14:20:28 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 90.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 14:20:33 [logger.py:43] Received request cmpl-3dffe564a9b044b9917f639acbe7a232-0: prompt: 'Explain the role of HPU in Gaudi chips.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 12027, 7420, 278, 6297, 310, 379, 7056, 297, 10415, 4749, 521, 4512, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:38992 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:20:33 [engine.py:331] Added request cmpl-3dffe564a9b044b9917f639acbe7a232-0.
INFO 08-09 14:20:33 [metrics.py:417] Avg prompt throughput: 2.8 tokens/s, Avg generation throughput: 102.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 14:20:37 [logger.py:43] Received request cmpl-5f83571f63ae43569fbf3ca9f4e127bb-0: prompt: 'What is a causal attention mask?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 263, 3269, 284, 8570, 11105, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:34772 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:20:37 [engine.py:331] Added request cmpl-5f83571f63ae43569fbf3ca9f4e127bb-0.
INFO 08-09 14:20:38 [metrics.py:417] Avg prompt throughput: 1.8 tokens/s, Avg generation throughput: 105.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 14:20:43 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 108.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:20:46 [logger.py:43] Received request cmpl-cd73385cb3c94380b9e0df8dc06e9aab-0: prompt: 'Describe the HF `generate()` function in Transformers.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 20355, 915, 278, 379, 29943, 421, 17158, 2555, 740, 297, 4103, 689, 414, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:34040 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:20:46 [engine.py:331] Added request cmpl-cd73385cb3c94380b9e0df8dc06e9aab-0.
INFO 08-09 14:20:48 [metrics.py:417] Avg prompt throughput: 3.0 tokens/s, Avg generation throughput: 106.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 14:20:53 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 104.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:20:56 [logger.py:43] Received request cmpl-10f4f6edb1b74155936d1709929ef962-0: prompt: 'Explain differences between T5, BART, and GPT models.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 12027, 7420, 12651, 1546, 323, 29945, 29892, 350, 8322, 29892, 322, 402, 7982, 4733, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:41970 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:20:56 [engine.py:331] Added request cmpl-10f4f6edb1b74155936d1709929ef962-0.
INFO 08-09 14:20:58 [metrics.py:417] Avg prompt throughput: 3.2 tokens/s, Avg generation throughput: 100.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 14:21:03 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 106.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:21:05 [logger.py:43] Received request cmpl-e0c831d096b6406a90ee6be4229f4518-0: prompt: 'What is model checkpointing and resumption?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 1904, 1423, 3149, 292, 322, 620, 28069, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:33128 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:21:05 [engine.py:331] Added request cmpl-e0c831d096b6406a90ee6be4229f4518-0.
INFO 08-09 14:21:08 [metrics.py:417] Avg prompt throughput: 2.2 tokens/s, Avg generation throughput: 106.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 14:21:14 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 105.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:21:15 [logger.py:43] Received request cmpl-dfbc444b885b45b4ba0e6c6cbf4584bb-0: prompt: 'Describe how checkpoint sharding works in distributed training.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 20355, 915, 920, 1423, 3149, 528, 20272, 1736, 297, 13235, 6694, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:53832 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:21:15 [engine.py:331] Added request cmpl-dfbc444b885b45b4ba0e6c6cbf4584bb-0.
INFO 08-09 14:21:19 [metrics.py:417] Avg prompt throughput: 2.6 tokens/s, Avg generation throughput: 108.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 14:21:24 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 107.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:21:24 [logger.py:43] Received request cmpl-5aa0dc480ff047099566e27e88557c1e-0: prompt: 'How are model weights stored in HuggingFace format?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 526, 1904, 18177, 6087, 297, 379, 688, 3460, 23360, 3402, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:53834 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:21:24 [engine.py:331] Added request cmpl-5aa0dc480ff047099566e27e88557c1e-0.
INFO 08-09 14:21:29 [metrics.py:417] Avg prompt throughput: 2.6 tokens/s, Avg generation throughput: 107.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 14:21:33 [logger.py:43] Received request cmpl-13cf10c0194d4c71bf08854d15abf128-0: prompt: 'Explain the impact of batch size on training and inference.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 12027, 7420, 278, 10879, 310, 9853, 2159, 373, 6694, 322, 27262, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:47570 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:21:33 [engine.py:331] Added request cmpl-13cf10c0194d4c71bf08854d15abf128-0.
INFO 08-09 14:21:34 [metrics.py:417] Avg prompt throughput: 2.6 tokens/s, Avg generation throughput: 107.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 14:21:39 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 110.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 14:21:42 [logger.py:43] Received request cmpl-d5e7a59e62424173a1d07c488469176b-0: prompt: 'What are fused kernels and why are they fast?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 526, 285, 3880, 413, 824, 1379, 322, 2020, 526, 896, 5172, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:40180 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:21:42 [engine.py:331] Added request cmpl-d5e7a59e62424173a1d07c488469176b-0.
INFO 08-09 14:21:44 [metrics.py:417] Avg prompt throughput: 2.8 tokens/s, Avg generation throughput: 105.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 14:21:49 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 110.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:21:52 [logger.py:43] Received request cmpl-3c0e2797ee4f4ce0ab2d1574f9351593-0: prompt: 'How does the `AutoModelForCausalLM` class work?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 947, 278, 421, 12300, 3195, 2831, 29907, 1485, 284, 26369, 29952, 770, 664, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:34320 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:21:52 [engine.py:331] Added request cmpl-3c0e2797ee4f4ce0ab2d1574f9351593-0.
INFO 08-09 14:21:54 [metrics.py:417] Avg prompt throughput: 3.2 tokens/s, Avg generation throughput: 109.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 14:21:55 [logger.py:43] Received request cmpl-0658c182ce854f25876d5e11a9ff65f9-0: prompt: 'What are tensor parallel and tensor slicing?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 526, 12489, 8943, 322, 12489, 269, 506, 292, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:42594 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:21:55 [engine.py:331] Added request cmpl-0658c182ce854f25876d5e11a9ff65f9-0.
INFO 08-09 14:21:59 [metrics.py:417] Avg prompt throughput: 2.2 tokens/s, Avg generation throughput: 106.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 14:22:04 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 108.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:22:04 [logger.py:43] Received request cmpl-949ff954bdf94bf2983c9fed24d97fc6-0: prompt: 'Describe the key steps of model serving using vLLM.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 20355, 915, 278, 1820, 6576, 310, 1904, 16330, 773, 325, 2208, 29924, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:42606 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:22:04 [engine.py:331] Added request cmpl-949ff954bdf94bf2983c9fed24d97fc6-0.
INFO 08-09 14:22:09 [metrics.py:417] Avg prompt throughput: 2.8 tokens/s, Avg generation throughput: 106.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 14:22:14 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 105.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:22:14 [logger.py:43] Received request cmpl-83bfe184f7584759a7110488e2f443f7-0: prompt: 'What are inference graphs in deep learning?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 526, 27262, 18445, 297, 6483, 6509, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:39210 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:22:14 [engine.py:331] Added request cmpl-83bfe184f7584759a7110488e2f443f7-0.
INFO 08-09 14:22:19 [metrics.py:417] Avg prompt throughput: 1.8 tokens/s, Avg generation throughput: 105.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 14:22:23 [logger.py:43] Received request cmpl-0d9d0b55908f4703aac850f6e66cb9ba-0: prompt: 'What is a streaming response in OpenAI-style APIs?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 263, 24820, 2933, 297, 4673, 23869, 29899, 3293, 23649, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:37146 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:22:23 [engine.py:331] Added request cmpl-0d9d0b55908f4703aac850f6e66cb9ba-0.
INFO 08-09 14:22:24 [metrics.py:417] Avg prompt throughput: 2.6 tokens/s, Avg generation throughput: 104.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 14:22:29 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 106.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 14:22:33 [logger.py:43] Received request cmpl-f9989a1ee72c41f3b9fdbfe827807bce-0: prompt: 'How does speculative decoding reduce latency?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 947, 1580, 28524, 1602, 3689, 10032, 23316, 1270, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:37916 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:22:33 [engine.py:331] Added request cmpl-f9989a1ee72c41f3b9fdbfe827807bce-0.
INFO 08-09 14:22:34 [metrics.py:417] Avg prompt throughput: 2.2 tokens/s, Avg generation throughput: 101.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 14:22:39 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 104.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 14:22:42 [logger.py:43] Received request cmpl-1b7618380abb4c50ac029614fda50dfc-0: prompt: 'What is JAX and how is it different from PyTorch?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 435, 6604, 322, 920, 338, 372, 1422, 515, 10772, 29911, 25350, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:34402 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:22:42 [engine.py:331] Added request cmpl-1b7618380abb4c50ac029614fda50dfc-0.
INFO 08-09 14:22:44 [metrics.py:417] Avg prompt throughput: 3.0 tokens/s, Avg generation throughput: 105.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 14:22:49 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 104.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:22:52 [logger.py:43] Received request cmpl-ac2a244cf005497e9b01b0cf4ea3362b-0: prompt: 'Describe lazy loading of weights in vLLM.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 20355, 915, 17366, 8363, 310, 18177, 297, 325, 2208, 29924, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:60320 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:22:52 [engine.py:331] Added request cmpl-ac2a244cf005497e9b01b0cf4ea3362b-0.
INFO 08-09 14:22:54 [metrics.py:417] Avg prompt throughput: 2.4 tokens/s, Avg generation throughput: 103.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 14:22:59 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 104.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:23:02 [logger.py:43] Received request cmpl-8500c271d65c4ca4b1e94cdee3779285-0: prompt: 'Explain the role of SynapseAI in Gaudi software stack.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 12027, 7420, 278, 6297, 310, 10829, 481, 344, 23869, 297, 10415, 4749, 7047, 5096, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:52896 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:23:02 [engine.py:331] Added request cmpl-8500c271d65c4ca4b1e94cdee3779285-0.
INFO 08-09 14:23:04 [metrics.py:417] Avg prompt throughput: 3.2 tokens/s, Avg generation throughput: 104.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 14:23:09 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 96.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:23:12 [logger.py:43] Received request cmpl-ecf2c206c6cf477ea4338ba0fdbc1601-0: prompt: 'What are the differences between HPU and GPU tensors?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 526, 278, 12651, 1546, 379, 7056, 322, 22796, 25187, 943, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:38388 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:23:12 [engine.py:331] Added request cmpl-ecf2c206c6cf477ea4338ba0fdbc1601-0.
INFO 08-09 14:23:14 [metrics.py:417] Avg prompt throughput: 2.6 tokens/s, Avg generation throughput: 98.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 14:23:19 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 96.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:23:22 [logger.py:43] Received request cmpl-ef9e0b87a30d4e409dd453bf359fdd93-0: prompt: 'What are the limitations of LLaMA 2?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 526, 278, 27028, 310, 365, 5661, 1529, 29871, 29906, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:52488 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:23:22 [engine.py:331] Added request cmpl-ef9e0b87a30d4e409dd453bf359fdd93-0.
INFO 08-09 14:23:24 [metrics.py:417] Avg prompt throughput: 2.4 tokens/s, Avg generation throughput: 101.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 14:23:29 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 108.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:23:31 [logger.py:43] Received request cmpl-4a7e36d9bdf444aa81acc8c4b9fc4b3f-0: prompt: 'How is multilingual performance evaluated in LLMs?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 338, 1773, 6504, 950, 4180, 19030, 297, 365, 26369, 29879, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:55870 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:23:31 [engine.py:331] Added request cmpl-4a7e36d9bdf444aa81acc8c4b9fc4b3f-0.
INFO 08-09 14:23:34 [metrics.py:417] Avg prompt throughput: 2.6 tokens/s, Avg generation throughput: 104.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 14:23:39 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 105.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:23:41 [logger.py:43] Received request cmpl-33c0b8bb96b84414851e49de6f862224-0: prompt: 'Describe an example use of prompt tuning.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 20355, 915, 385, 1342, 671, 310, 9508, 18515, 292, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:38614 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:23:41 [engine.py:331] Added request cmpl-33c0b8bb96b84414851e49de6f862224-0.
INFO 08-09 14:23:44 [metrics.py:417] Avg prompt throughput: 2.2 tokens/s, Avg generation throughput: 105.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 14:23:49 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 107.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:23:50 [logger.py:43] Received request cmpl-c757381d888d4cbe8809fce376b2ae79-0: prompt: 'What is the attention bottleneck and how to address it?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 278, 8570, 18046, 29880, 1600, 384, 322, 920, 304, 3211, 372, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:36866 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:23:50 [engine.py:331] Added request cmpl-c757381d888d4cbe8809fce376b2ae79-0.
INFO 08-09 14:23:54 [metrics.py:417] Avg prompt throughput: 3.0 tokens/s, Avg generation throughput: 109.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 14:23:59 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 110.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:23:59 [logger.py:43] Received request cmpl-07a2a821b2b941b089038c29e89d51b2-0: prompt: 'How does token alignment affect downstream tasks?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 947, 5993, 22239, 6602, 1623, 5461, 9595, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:47188 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:23:59 [engine.py:331] Added request cmpl-07a2a821b2b941b089038c29e89d51b2-0.
INFO 08-09 14:24:04 [metrics.py:417] Avg prompt throughput: 2.0 tokens/s, Avg generation throughput: 106.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 14:24:08 [logger.py:43] Received request cmpl-bb251e0138a644a9b69d1505b6901cca-0: prompt: 'What are the challenges in inference on edge devices?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 526, 278, 18066, 267, 297, 27262, 373, 7636, 9224, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:36002 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:24:08 [engine.py:331] Added request cmpl-bb251e0138a644a9b69d1505b6901cca-0.
INFO 08-09 14:24:09 [metrics.py:417] Avg prompt throughput: 2.4 tokens/s, Avg generation throughput: 110.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 14:24:14 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 106.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 14:24:14 [logger.py:43] Received request cmpl-52a333b5472f466a8977074681e748aa-0: prompt: 'Explain the principle behind rotary embeddings.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 12027, 7420, 278, 12502, 5742, 5731, 653, 8297, 29881, 886, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:36008 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:24:14 [engine.py:331] Added request cmpl-52a333b5472f466a8977074681e748aa-0.
INFO 08-09 14:24:18 [logger.py:43] Received request cmpl-c3043687e35348749551cef513036c62-0: prompt: 'How does temperature affect language model output?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 947, 10430, 6602, 4086, 1904, 1962, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:36214 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:24:18 [engine.py:331] Added request cmpl-c3043687e35348749551cef513036c62-0.
INFO 08-09 14:24:19 [metrics.py:417] Avg prompt throughput: 4.2 tokens/s, Avg generation throughput: 103.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 14:24:24 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 109.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 14:24:27 [logger.py:43] Received request cmpl-cd7abd801129481d8e2e0eaad2848930-0: prompt: 'What are stop sequences in generation APIs?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 526, 5040, 15602, 297, 12623, 23649, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:53680 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:24:27 [engine.py:331] Added request cmpl-cd7abd801129481d8e2e0eaad2848930-0.
INFO 08-09 14:24:29 [metrics.py:417] Avg prompt throughput: 1.8 tokens/s, Avg generation throughput: 107.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 14:24:34 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 109.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:24:37 [logger.py:43] Received request cmpl-50f7e69198964235b6b3203ace147038-0: prompt: 'Describe token streaming latency optimization.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 20355, 915, 5993, 24820, 23316, 1270, 13883, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:57900 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:24:37 [engine.py:331] Added request cmpl-50f7e69198964235b6b3203ace147038-0.
INFO 08-09 14:24:39 [metrics.py:417] Avg prompt throughput: 1.8 tokens/s, Avg generation throughput: 102.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 14:24:44 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 106.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:24:46 [logger.py:43] Received request cmpl-f57bdfa766e040828a2665f0f1ac8730-0: prompt: 'How to evaluate hallucinations in language models?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 304, 14707, 12713, 1682, 262, 800, 297, 4086, 4733, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:34518 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:24:46 [engine.py:331] Added request cmpl-f57bdfa766e040828a2665f0f1ac8730-0.
INFO 08-09 14:24:49 [metrics.py:417] Avg prompt throughput: 2.4 tokens/s, Avg generation throughput: 104.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 14:24:54 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 107.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:24:56 [logger.py:43] Received request cmpl-f7d9f441e95a4be38a2dcb2d778b0880-0: prompt: 'What is the purpose of `bos_token_id`?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 278, 6437, 310, 421, 27737, 29918, 6979, 29918, 333, 6522], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:49088 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:24:56 [engine.py:331] Added request cmpl-f7d9f441e95a4be38a2dcb2d778b0880-0.
INFO 08-09 14:24:59 [metrics.py:417] Avg prompt throughput: 2.6 tokens/s, Avg generation throughput: 101.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 14:25:04 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 105.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:25:05 [logger.py:43] Received request cmpl-b182eca6654f4e5985f21f767416b1b4-0: prompt: 'How are fused attention layers implemented?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 526, 285, 3880, 8570, 15359, 8762, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:58708 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:25:05 [engine.py:331] Added request cmpl-b182eca6654f4e5985f21f767416b1b4-0.
INFO 08-09 14:25:09 [metrics.py:417] Avg prompt throughput: 1.8 tokens/s, Avg generation throughput: 109.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 14:25:14 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 107.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:25:14 [logger.py:43] Received request cmpl-064abecd263342209175cfc4bb525873-0: prompt: 'Describe a pipeline for deploying LLaMA 2 in production.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 20355, 915, 263, 16439, 363, 7246, 292, 365, 5661, 1529, 29871, 29906, 297, 5802, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:36054 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:25:14 [engine.py:331] Added request cmpl-064abecd263342209175cfc4bb525873-0.
INFO 08-09 14:25:19 [metrics.py:417] Avg prompt throughput: 3.2 tokens/s, Avg generation throughput: 102.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 14:25:24 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 108.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:25:24 [logger.py:43] Received request cmpl-65919d7699d043aba9f699be04105f43-0: prompt: 'How does instruction following improve model usability?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 947, 15278, 1494, 11157, 1904, 502, 3097, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:36062 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:25:24 [engine.py:331] Added request cmpl-65919d7699d043aba9f699be04105f43-0.
INFO 08-09 14:25:29 [metrics.py:417] Avg prompt throughput: 2.0 tokens/s, Avg generation throughput: 108.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 14:25:33 [logger.py:43] Received request cmpl-75334d12d70f43f1a3dc55405ab5bbf0-0: prompt: 'What is a sequence-to-sequence model?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 263, 5665, 29899, 517, 29899, 16506, 1904, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:46210 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:25:33 [engine.py:331] Added request cmpl-75334d12d70f43f1a3dc55405ab5bbf0-0.
INFO 08-09 14:25:34 [metrics.py:417] Avg prompt throughput: 2.2 tokens/s, Avg generation throughput: 109.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 14:25:39 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 110.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 14:25:42 [logger.py:43] Received request cmpl-a03b537bcd514fd6a3ae41541d6a6bd1-0: prompt: 'What is the purpose of `pad_token_id` in generation?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 278, 6437, 310, 421, 8305, 29918, 6979, 29918, 333, 29952, 297, 12623, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:50648 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:25:42 [engine.py:331] Added request cmpl-a03b537bcd514fd6a3ae41541d6a6bd1-0.
INFO 08-09 14:25:44 [metrics.py:417] Avg prompt throughput: 3.2 tokens/s, Avg generation throughput: 104.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 14:25:48 [logger.py:43] Received request cmpl-99fec1ddc77e4921aa6952ac01ce0a49-0: prompt: 'Explain the concept of auto-regressive generation.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 12027, 7420, 278, 6964, 310, 4469, 29899, 276, 3663, 573, 12623, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:37756 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:25:48 [engine.py:331] Added request cmpl-99fec1ddc77e4921aa6952ac01ce0a49-0.
INFO 08-09 14:25:49 [metrics.py:417] Avg prompt throughput: 2.6 tokens/s, Avg generation throughput: 109.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 14:25:54 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 106.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 14:25:58 [logger.py:43] Received request cmpl-3b9facafb65341c3a98cde9da4c523f7-0: prompt: 'How is top-p sampling different from nucleus sampling?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 338, 2246, 29899, 29886, 23460, 1422, 515, 22699, 375, 23460, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:37360 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:25:58 [engine.py:331] Added request cmpl-3b9facafb65341c3a98cde9da4c523f7-0.
INFO 08-09 14:25:59 [metrics.py:417] Avg prompt throughput: 2.6 tokens/s, Avg generation throughput: 104.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 14:26:03 [logger.py:43] Received request cmpl-bccc425126f049fcbfa1edb9d146b99d-0: prompt: 'What are decoding strategies for safe responses?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 526, 1602, 3689, 16650, 583, 363, 9109, 20890, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:37376 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:26:03 [engine.py:331] Added request cmpl-bccc425126f049fcbfa1edb9d146b99d-0.
INFO 08-09 14:26:04 [metrics.py:417] Avg prompt throughput: 2.2 tokens/s, Avg generation throughput: 105.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 14:26:09 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 107.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 14:26:13 [logger.py:43] Received request cmpl-fe6e815119bd4268aebae2662c9efa87-0: prompt: 'How are model prompts structured in ChatML?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 526, 1904, 9508, 29879, 2281, 2955, 297, 678, 271, 1988, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:55310 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:26:13 [engine.py:331] Added request cmpl-fe6e815119bd4268aebae2662c9efa87-0.
INFO 08-09 14:26:14 [metrics.py:417] Avg prompt throughput: 2.6 tokens/s, Avg generation throughput: 107.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 14:26:19 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 108.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:26:22 [logger.py:43] Received request cmpl-45d7625f175b4d43be2f099110b40b43-0: prompt: 'What is a system prompt in chat inference?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 263, 1788, 9508, 297, 13563, 27262, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:59938 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:26:22 [engine.py:331] Added request cmpl-45d7625f175b4d43be2f099110b40b43-0.
INFO 08-09 14:26:24 [metrics.py:417] Avg prompt throughput: 2.0 tokens/s, Avg generation throughput: 105.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 14:26:29 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 103.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:26:32 [logger.py:43] Received request cmpl-ed355c4a074943c5a89e46d7f614ac70-0: prompt: 'How do you batch requests for high throughput?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 437, 366, 9853, 7274, 363, 1880, 1549, 649, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:43198 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:26:32 [engine.py:331] Added request cmpl-ed355c4a074943c5a89e46d7f614ac70-0.
INFO 08-09 14:26:34 [metrics.py:417] Avg prompt throughput: 2.2 tokens/s, Avg generation throughput: 104.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 14:26:39 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 94.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:26:42 [logger.py:43] Received request cmpl-2f49be7ac27744458d41c0caa1bb73c4-0: prompt: 'Explain model serving with `transformers + vLLM`.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 12027, 7420, 1904, 16330, 411, 421, 9067, 414, 718, 325, 2208, 29924, 1412], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:46718 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:26:42 [engine.py:331] Added request cmpl-2f49be7ac27744458d41c0caa1bb73c4-0.
INFO 08-09 14:26:44 [metrics.py:417] Avg prompt throughput: 2.8 tokens/s, Avg generation throughput: 102.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 14:26:49 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 103.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:26:51 [logger.py:43] Received request cmpl-9227f1f31af6434880fd48d315eb4837-0: prompt: 'What is Triton Inference Server?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 323, 768, 265, 512, 1659, 5656, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:49800 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:26:51 [engine.py:331] Added request cmpl-9227f1f31af6434880fd48d315eb4837-0.
INFO 08-09 14:26:54 [metrics.py:417] Avg prompt throughput: 2.0 tokens/s, Avg generation throughput: 106.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 14:26:59 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 106.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:27:01 [logger.py:43] Received request cmpl-87710f08aac14c999454966ca6b52d60-0: prompt: 'Describe the significance of large context windows.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 20355, 915, 278, 26002, 310, 2919, 3030, 5417, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:57514 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:27:01 [engine.py:331] Added request cmpl-87710f08aac14c999454966ca6b52d60-0.
INFO 08-09 14:27:04 [metrics.py:417] Avg prompt throughput: 2.0 tokens/s, Avg generation throughput: 106.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 14:27:09 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 105.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:27:10 [logger.py:43] Received request cmpl-411c3159b62f40fd88fbfc826e91188f-0: prompt: 'What is dynamic quantization in transformers?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 7343, 4323, 2133, 297, 4327, 414, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:47850 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:27:10 [engine.py:331] Added request cmpl-411c3159b62f40fd88fbfc826e91188f-0.
INFO 08-09 14:27:14 [metrics.py:417] Avg prompt throughput: 2.0 tokens/s, Avg generation throughput: 108.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 14:27:19 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 108.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:27:19 [logger.py:43] Received request cmpl-4f7cbbb55b914f80a269f9b0aef407f2-0: prompt: 'How to prevent prompt injection in LLMs?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 304, 5557, 9508, 20859, 297, 365, 26369, 29879, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:35614 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:27:19 [engine.py:331] Added request cmpl-4f7cbbb55b914f80a269f9b0aef407f2-0.
INFO 08-09 14:27:24 [metrics.py:417] Avg prompt throughput: 2.2 tokens/s, Avg generation throughput: 105.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 14:27:29 [logger.py:43] Received request cmpl-a5cd85bfd27a45f9b2c0b9d8f33bdbc5-0: prompt: 'What are attention scores and how are they computed?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 526, 8570, 19435, 322, 920, 526, 896, 15712, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:46550 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:27:29 [engine.py:331] Added request cmpl-a5cd85bfd27a45f9b2c0b9d8f33bdbc5-0.
INFO 08-09 14:27:29 [metrics.py:417] Avg prompt throughput: 2.2 tokens/s, Avg generation throughput: 109.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 14:27:34 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 107.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 14:27:38 [logger.py:43] Received request cmpl-20c783c1dfee4c439d86499ae5bb3ca7-0: prompt: 'What are residual connections in transformers?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 526, 10995, 950, 12368, 297, 4327, 414, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:60570 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:27:38 [engine.py:331] Added request cmpl-20c783c1dfee4c439d86499ae5bb3ca7-0.
INFO 08-09 14:27:39 [metrics.py:417] Avg prompt throughput: 2.0 tokens/s, Avg generation throughput: 104.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 14:27:44 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 110.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:27:47 [logger.py:43] Received request cmpl-f43a5ed2812a4b6fae51e5b264790116-0: prompt: 'How to handle long sequences in generation?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 304, 4386, 1472, 15602, 297, 12623, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:47664 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:27:47 [engine.py:331] Added request cmpl-f43a5ed2812a4b6fae51e5b264790116-0.
INFO 08-09 14:27:49 [metrics.py:417] Avg prompt throughput: 1.8 tokens/s, Avg generation throughput: 108.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 14:27:54 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 106.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:27:57 [logger.py:43] Received request cmpl-2df029d9665f4951a187b6d78c3178f0-0: prompt: 'Explain the use of flash attention v2.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 12027, 7420, 278, 671, 310, 11013, 8570, 325, 29906, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:38214 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:27:57 [engine.py:331] Added request cmpl-2df029d9665f4951a187b6d78c3178f0-0.
INFO 08-09 14:27:59 [metrics.py:417] Avg prompt throughput: 2.2 tokens/s, Avg generation throughput: 105.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 14:28:04 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 106.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:28:06 [logger.py:43] Received request cmpl-fe9ccc7c640a42afa8e378e40b9496e8-0: prompt: 'What is a greedy search in inference?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 263, 1395, 7584, 2740, 297, 27262, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:38184 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:28:06 [engine.py:331] Added request cmpl-fe9ccc7c640a42afa8e378e40b9496e8-0.
INFO 08-09 14:28:09 [metrics.py:417] Avg prompt throughput: 2.0 tokens/s, Avg generation throughput: 110.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 14:28:14 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 108.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:28:15 [logger.py:43] Received request cmpl-35238b26f69743ca8ff972602099c043-0: prompt: 'How do checkpoints improve fault tolerance?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 437, 1423, 9748, 11157, 12570, 20341, 749, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:54742 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:28:15 [engine.py:331] Added request cmpl-35238b26f69743ca8ff972602099c043-0.
INFO 08-09 14:28:19 [metrics.py:417] Avg prompt throughput: 2.0 tokens/s, Avg generation throughput: 107.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 14:28:24 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 108.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:28:24 [logger.py:43] Received request cmpl-6d6bc20aff8a4a88b46f225adc3b667b-0: prompt: 'Describe how text-to-text generation works.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 20355, 915, 920, 1426, 29899, 517, 29899, 726, 12623, 1736, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:54758 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:28:24 [engine.py:331] Added request cmpl-6d6bc20aff8a4a88b46f225adc3b667b-0.
INFO 08-09 14:28:29 [metrics.py:417] Avg prompt throughput: 2.4 tokens/s, Avg generation throughput: 104.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 14:28:34 [logger.py:43] Received request cmpl-7b8aab3256ea44b1a180231caf05471f-0: prompt: 'Explain cosine similarity in embeddings.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 12027, 7420, 6776, 457, 29501, 297, 8297, 29881, 886, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:43232 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:28:34 [engine.py:331] Added request cmpl-7b8aab3256ea44b1a180231caf05471f-0.
INFO 08-09 14:28:34 [metrics.py:417] Avg prompt throughput: 2.2 tokens/s, Avg generation throughput: 108.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 14:28:39 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 107.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 14:28:39 [logger.py:43] Received request cmpl-473bbf4c7c614091a18f96a417b9d85d-0: prompt: 'What is token-wise parallelism?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 5993, 29899, 3538, 8943, 1608, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:43480 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:28:39 [engine.py:331] Added request cmpl-473bbf4c7c614091a18f96a417b9d85d-0.
INFO 08-09 14:28:44 [metrics.py:417] Avg prompt throughput: 1.8 tokens/s, Avg generation throughput: 108.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 14:28:49 [logger.py:43] Received request cmpl-18e34df6812540fcaf3c3e95a938c5bb-0: prompt: 'How do shared embeddings help in decoder-only models?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 437, 7258, 8297, 29881, 886, 1371, 297, 1602, 6119, 29899, 6194, 4733, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:55022 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:28:49 [engine.py:331] Added request cmpl-18e34df6812540fcaf3c3e95a938c5bb-0.
INFO 08-09 14:28:49 [metrics.py:417] Avg prompt throughput: 3.0 tokens/s, Avg generation throughput: 101.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 14:28:54 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 105.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 14:28:59 [logger.py:43] Received request cmpl-f4e55104766b4f39983e2868bd699b73-0: prompt: 'Explain how to measure latency in model output.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 12027, 7420, 920, 304, 5645, 23316, 1270, 297, 1904, 1962, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:56202 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:28:59 [engine.py:331] Added request cmpl-f4e55104766b4f39983e2868bd699b73-0.
INFO 08-09 14:28:59 [metrics.py:417] Avg prompt throughput: 2.4 tokens/s, Avg generation throughput: 98.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 14:29:04 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 107.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 14:29:08 [logger.py:43] Received request cmpl-bab2f70f55404dfcbad6654925f80c04-0: prompt: 'What is the purpose of dropout during training?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 278, 6437, 310, 5768, 449, 2645, 6694, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:41286 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:29:08 [engine.py:331] Added request cmpl-bab2f70f55404dfcbad6654925f80c04-0.
INFO 08-09 14:29:09 [metrics.py:417] Avg prompt throughput: 2.2 tokens/s, Avg generation throughput: 105.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 14:29:14 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 108.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:29:17 [logger.py:43] Received request cmpl-12664ca92341411685912f065672a753-0: prompt: 'What is the role of GELU activation in transformers?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 278, 6297, 310, 402, 6670, 29965, 26229, 297, 4327, 414, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:54940 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:29:17 [engine.py:331] Added request cmpl-12664ca92341411685912f065672a753-0.
INFO 08-09 14:29:19 [metrics.py:417] Avg prompt throughput: 2.8 tokens/s, Avg generation throughput: 105.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 14:29:24 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 96.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:29:27 [logger.py:43] Received request cmpl-763b5fc7e012453aab02f621ebe78684-0: prompt: 'Explain forward and backward pass in transformers.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 12027, 7420, 6375, 322, 1250, 1328, 1209, 297, 4327, 414, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:53940 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:29:27 [engine.py:331] Added request cmpl-763b5fc7e012453aab02f621ebe78684-0.
INFO 08-09 14:29:29 [metrics.py:417] Avg prompt throughput: 2.4 tokens/s, Avg generation throughput: 101.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 14:29:34 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 103.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:29:37 [logger.py:43] Received request cmpl-a326538cb767468080a16f9efa294fb3-0: prompt: 'What is speculative decoding and how does it help?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 1580, 28524, 1602, 3689, 322, 920, 947, 372, 1371, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:56354 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:29:37 [engine.py:331] Added request cmpl-a326538cb767468080a16f9efa294fb3-0.
INFO 08-09 14:29:39 [metrics.py:417] Avg prompt throughput: 2.6 tokens/s, Avg generation throughput: 104.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 14:29:44 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 109.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:29:46 [logger.py:43] Received request cmpl-b2a80dfa19354dad80bf04be481d6ebe-0: prompt: 'How to optimize HPU memory usage during inference?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 304, 24656, 379, 7056, 3370, 8744, 2645, 27262, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:42796 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:29:46 [engine.py:331] Added request cmpl-b2a80dfa19354dad80bf04be481d6ebe-0.
INFO 08-09 14:29:47 [logger.py:43] Received request cmpl-01b2bb886c02424e95d45050e7cadc37-0: prompt: 'Explain profiling for LLM inference performance.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 12027, 7420, 20077, 292, 363, 365, 26369, 27262, 4180, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:42806 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:29:47 [engine.py:331] Added request cmpl-01b2bb886c02424e95d45050e7cadc37-0.
INFO 08-09 14:29:49 [metrics.py:417] Avg prompt throughput: 4.4 tokens/s, Avg generation throughput: 105.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 14:29:54 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 101.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:29:57 [logger.py:43] Received request cmpl-d094699e2d604eb09486c22bb13dfdf1-0: prompt: 'How to compute inter-token latency per request?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 304, 10272, 1006, 29899, 6979, 23316, 1270, 639, 2009, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:52694 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:29:57 [engine.py:331] Added request cmpl-d094699e2d604eb09486c22bb13dfdf1-0.
INFO 08-09 14:29:59 [metrics.py:417] Avg prompt throughput: 2.4 tokens/s, Avg generation throughput: 108.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 14:30:04 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 107.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:30:06 [logger.py:43] Received request cmpl-9b8b6153eda843af8c64d4486e31119d-0: prompt: 'What is the difference between BFloat16 and Float16?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 278, 4328, 1546, 350, 11031, 29896, 29953, 322, 27842, 29896, 29953, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:33738 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:30:06 [engine.py:331] Added request cmpl-9b8b6153eda843af8c64d4486e31119d-0.
INFO 08-09 14:30:09 [metrics.py:417] Avg prompt throughput: 3.0 tokens/s, Avg generation throughput: 106.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 14:30:14 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 104.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:30:15 [logger.py:43] Received request cmpl-5e38293adccf413f84eb0c7dd43ee83f-0: prompt: 'What are logits and how are they interpreted?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 526, 1480, 1169, 322, 920, 526, 896, 21551, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:59746 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:30:15 [engine.py:331] Added request cmpl-5e38293adccf413f84eb0c7dd43ee83f-0.
INFO 08-09 14:30:19 [metrics.py:417] Avg prompt throughput: 2.2 tokens/s, Avg generation throughput: 105.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 14:30:24 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 102.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:30:25 [logger.py:43] Received request cmpl-272947694b5742b384d98ffd5b8995e1-0: prompt: 'What is log probability in token prediction?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 1480, 6976, 297, 5993, 18988, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:38824 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:30:25 [engine.py:331] Added request cmpl-272947694b5742b384d98ffd5b8995e1-0.
INFO 08-09 14:30:29 [metrics.py:417] Avg prompt throughput: 1.8 tokens/s, Avg generation throughput: 105.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 14:30:34 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 106.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:30:35 [logger.py:43] Received request cmpl-a87243530ec047a78565e492a07deb34-0: prompt: 'How to use `TextStreamer` for streaming output?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 304, 671, 421, 1626, 3835, 261, 29952, 363, 24820, 1962, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:58704 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:30:35 [engine.py:331] Added request cmpl-a87243530ec047a78565e492a07deb34-0.
INFO 08-09 14:30:39 [metrics.py:417] Avg prompt throughput: 2.6 tokens/s, Avg generation throughput: 102.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 14:30:44 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 105.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:30:44 [logger.py:43] Received request cmpl-740ce77255bc4992b916d024ebc57633-0: prompt: 'How to tokenize and detokenize a prompt manually?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 304, 5993, 675, 322, 1439, 4476, 675, 263, 9508, 7522, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:58718 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:30:44 [engine.py:331] Added request cmpl-740ce77255bc4992b916d024ebc57633-0.
INFO 08-09 14:30:49 [metrics.py:417] Avg prompt throughput: 2.6 tokens/s, Avg generation throughput: 102.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 14:30:54 [logger.py:43] Received request cmpl-051c9e55905247529a7c6e41acf7bb4f-0: prompt: 'Describe how a prompt flows through the transformer layers.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 20355, 915, 920, 263, 9508, 24536, 1549, 278, 4327, 261, 15359, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:51314 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:30:54 [engine.py:331] Added request cmpl-051c9e55905247529a7c6e41acf7bb4f-0.
INFO 08-09 14:30:54 [metrics.py:417] Avg prompt throughput: 2.6 tokens/s, Avg generation throughput: 108.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 14:30:59 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 106.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 14:31:03 [logger.py:43] Received request cmpl-7cb0a5e25a0c44d89034aad974de5689-0: prompt: 'How to handle EOS in streaming responses?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 304, 4386, 382, 3267, 297, 24820, 20890, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:46402 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:31:03 [engine.py:331] Added request cmpl-7cb0a5e25a0c44d89034aad974de5689-0.
INFO 08-09 14:31:04 [metrics.py:417] Avg prompt throughput: 2.0 tokens/s, Avg generation throughput: 108.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 14:31:09 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 104.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:31:12 [logger.py:43] Received request cmpl-2b7850ca0bb9492a9d7325d14596cd4d-0: prompt: 'What is a model config JSON in HF models?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 263, 1904, 2295, 4663, 297, 379, 29943, 4733, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:48670 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:31:12 [engine.py:331] Added request cmpl-2b7850ca0bb9492a9d7325d14596cd4d-0.
INFO 08-09 14:31:14 [metrics.py:417] Avg prompt throughput: 2.4 tokens/s, Avg generation throughput: 103.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 14:31:19 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 102.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:31:22 [logger.py:43] Received request cmpl-41bb9e6ec2b1478d8f0b39de5335fb8f-0: prompt: 'What is kv_cache reuse in decoder models?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 10908, 29918, 8173, 24270, 297, 1602, 6119, 4733, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:52076 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:31:22 [engine.py:331] Added request cmpl-41bb9e6ec2b1478d8f0b39de5335fb8f-0.
INFO 08-09 14:31:24 [metrics.py:417] Avg prompt throughput: 2.4 tokens/s, Avg generation throughput: 106.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 14:31:29 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 106.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:31:31 [logger.py:43] Received request cmpl-a950ae53248244058bd4898fcb876ec0-0: prompt: 'Describe a scenario of real-time chatbot streaming.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 20355, 915, 263, 10483, 310, 1855, 29899, 2230, 13563, 7451, 24820, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:53228 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:31:31 [engine.py:331] Added request cmpl-a950ae53248244058bd4898fcb876ec0-0.
INFO 08-09 14:31:34 [metrics.py:417] Avg prompt throughput: 2.6 tokens/s, Avg generation throughput: 95.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 14:31:39 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 108.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:31:41 [logger.py:43] Received request cmpl-a29230e8e56442e2a873abda55b48566-0: prompt: 'How to compute throughput with variable prompts?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 304, 10272, 1549, 649, 411, 2286, 9508, 29879, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:40636 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:31:41 [engine.py:331] Added request cmpl-a29230e8e56442e2a873abda55b48566-0.
INFO 08-09 14:31:44 [metrics.py:417] Avg prompt throughput: 2.2 tokens/s, Avg generation throughput: 106.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 14:31:49 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 107.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:31:51 [logger.py:43] Received request cmpl-e16321bcfc0246798c3fa4c5de83ff84-0: prompt: 'How does `max_new_tokens` differ from `max_length`?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 947, 421, 3317, 29918, 1482, 29918, 517, 12360, 29952, 1163, 515, 421, 3317, 29918, 2848, 6522], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:58408 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:31:51 [engine.py:331] Added request cmpl-e16321bcfc0246798c3fa4c5de83ff84-0.
INFO 08-09 14:31:51 [logger.py:43] Received request cmpl-cf11c4edfdcb49338fab7e82db0dc0da-0: prompt: 'Explain the difference between loss and perplexity.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 12027, 7420, 278, 4328, 1546, 6410, 322, 639, 10709, 537, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:58422 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:31:51 [engine.py:331] Added request cmpl-cf11c4edfdcb49338fab7e82db0dc0da-0.
INFO 08-09 14:31:54 [metrics.py:417] Avg prompt throughput: 6.0 tokens/s, Avg generation throughput: 106.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 14:31:59 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 109.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:32:01 [logger.py:43] Received request cmpl-6bc2e86ca04347519cfab498fd891d4d-0: prompt: 'How to log benchmark results in a structured CSV?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 304, 1480, 23513, 2582, 297, 263, 2281, 2955, 16874, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:45628 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:32:01 [engine.py:331] Added request cmpl-6bc2e86ca04347519cfab498fd891d4d-0.
INFO 08-09 14:32:04 [metrics.py:417] Avg prompt throughput: 2.4 tokens/s, Avg generation throughput: 100.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 14:32:09 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 107.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:32:10 [logger.py:43] Received request cmpl-8a4dbf3db18a40268c836520df320174-0: prompt: 'What are attention heads and how do they specialize?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 526, 8570, 15883, 322, 920, 437, 896, 4266, 675, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:48218 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:32:10 [engine.py:331] Added request cmpl-8a4dbf3db18a40268c836520df320174-0.
INFO 08-09 14:32:14 [metrics.py:417] Avg prompt throughput: 2.4 tokens/s, Avg generation throughput: 107.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 14:32:19 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 108.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:32:20 [logger.py:43] Received request cmpl-71ef1a1b59dc4b9295c5d9baa78cd637-0: prompt: 'How to ensure reproducible benchmarking?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 304, 9801, 9483, 15520, 23513, 292, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:56332 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:32:20 [engine.py:331] Added request cmpl-71ef1a1b59dc4b9295c5d9baa78cd637-0.
INFO 08-09 14:32:24 [metrics.py:417] Avg prompt throughput: 1.8 tokens/s, Avg generation throughput: 111.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 14:32:29 [logger.py:43] Received request cmpl-6a95f14b537842a69fc4a763ab31a0c1-0: prompt: 'What are model shards and when are they used?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 526, 1904, 528, 3163, 322, 746, 526, 896, 1304, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:57218 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:32:29 [engine.py:331] Added request cmpl-6a95f14b537842a69fc4a763ab31a0c1-0.
INFO 08-09 14:32:29 [metrics.py:417] Avg prompt throughput: 2.4 tokens/s, Avg generation throughput: 108.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 14:32:34 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 108.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 14:32:38 [logger.py:43] Received request cmpl-30ac7d1133224ef4ac49e1ea679da0a2-0: prompt: 'Explain the relationship between prompt length and latency.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 12027, 7420, 278, 9443, 1546, 9508, 3309, 322, 23316, 1270, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:53368 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:32:38 [engine.py:331] Added request cmpl-30ac7d1133224ef4ac49e1ea679da0a2-0.
INFO 08-09 14:32:39 [metrics.py:417] Avg prompt throughput: 2.4 tokens/s, Avg generation throughput: 103.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 14:32:44 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 99.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 14:32:48 [logger.py:43] Received request cmpl-c92c2ee61e594b8588cd0573cb28154e-0: prompt: 'What is batch inference and why is it faster?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 9853, 27262, 322, 2020, 338, 372, 8473, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:50820 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:32:48 [engine.py:331] Added request cmpl-c92c2ee61e594b8588cd0573cb28154e-0.
INFO 08-09 14:32:49 [metrics.py:417] Avg prompt throughput: 2.2 tokens/s, Avg generation throughput: 109.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 14:32:54 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 107.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:32:56 [logger.py:43] Received request cmpl-a63157cfeac048c3bc91ae3840eaf733-0: prompt: 'What are rotary sin-cos embeddings?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 526, 5731, 653, 4457, 29899, 3944, 8297, 29881, 886, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:36214 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:32:56 [engine.py:331] Added request cmpl-a63157cfeac048c3bc91ae3840eaf733-0.
INFO 08-09 14:32:59 [metrics.py:417] Avg prompt throughput: 2.4 tokens/s, Avg generation throughput: 96.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 14:33:03 [logger.py:43] Received request cmpl-26d2778843574b12914f4c5d5dc9eb6f-0: prompt: "Describe HuggingFace's `AutoTokenizer` features.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 20355, 915, 379, 688, 3460, 23360, 29915, 29879, 421, 12300, 6066, 3950, 29952, 5680, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:36228 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:33:03 [engine.py:331] Added request cmpl-26d2778843574b12914f4c5d5dc9eb6f-0.
INFO 08-09 14:33:04 [metrics.py:417] Avg prompt throughput: 3.2 tokens/s, Avg generation throughput: 106.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 14:33:09 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 107.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:33:12 [logger.py:43] Received request cmpl-32dfa40947fc469da8e69064ed2a18f0-0: prompt: 'How does positional encoding interact with attention?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 947, 2602, 284, 8025, 16254, 411, 8570, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:38972 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:33:12 [engine.py:331] Added request cmpl-32dfa40947fc469da8e69064ed2a18f0-0.
INFO 08-09 14:33:14 [metrics.py:417] Avg prompt throughput: 2.0 tokens/s, Avg generation throughput: 105.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 14:33:18 [logger.py:43] Received request cmpl-7949161df84c418a9cea4669d690a142-0: prompt: 'What is a streaming callback in Transformers?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 263, 24820, 6939, 297, 4103, 689, 414, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:59862 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:33:18 [engine.py:331] Added request cmpl-7949161df84c418a9cea4669d690a142-0.
INFO 08-09 14:33:19 [metrics.py:417] Avg prompt throughput: 2.2 tokens/s, Avg generation throughput: 103.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 14:33:23 [logger.py:43] Received request cmpl-b9ed50e7c9a14f2d88f77ebeba9a7e96-0: prompt: 'How to benchmark models using curl and Python?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 304, 23513, 4733, 773, 11051, 322, 5132, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:59878 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:33:23 [engine.py:331] Added request cmpl-b9ed50e7c9a14f2d88f77ebeba9a7e96-0.
INFO 08-09 14:33:24 [metrics.py:417] Avg prompt throughput: 2.0 tokens/s, Avg generation throughput: 104.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 14:33:29 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 111.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:33:32 [logger.py:43] Received request cmpl-8c8c0a3600c1458da559e409d523b97e-0: prompt: 'How to parse streaming responses from vLLM?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 304, 6088, 24820, 20890, 515, 325, 2208, 29924, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:46286 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:33:32 [engine.py:331] Added request cmpl-8c8c0a3600c1458da559e409d523b97e-0.
INFO 08-09 14:33:34 [metrics.py:417] Avg prompt throughput: 2.2 tokens/s, Avg generation throughput: 106.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 14:33:39 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 109.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:33:41 [logger.py:43] Received request cmpl-c91e525ff41f4f9c8d3975944b67f73d-0: prompt: 'How does deep caching work in inference?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 947, 6483, 22488, 664, 297, 27262, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:44386 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:33:41 [engine.py:331] Added request cmpl-c91e525ff41f4f9c8d3975944b67f73d-0.
INFO 08-09 14:33:44 [metrics.py:417] Avg prompt throughput: 1.8 tokens/s, Avg generation throughput: 108.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 14:33:45 [logger.py:43] Received request cmpl-8c30aec97fb14fe4a9c32e2c44a45c74-0: prompt: 'What is a prompt-truncation error?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 263, 9508, 29899, 509, 4661, 362, 1059, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:60850 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:33:45 [engine.py:331] Added request cmpl-8c30aec97fb14fe4a9c32e2c44a45c74-0.
INFO 08-09 14:33:47 [logger.py:43] Received request cmpl-7a0c65ec2b9a48f48ebecc0bd225dd1d-0: prompt: 'Explain the role of tokenizer config files.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 12027, 7420, 278, 6297, 310, 5993, 3950, 2295, 2066, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:60866 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:33:47 [engine.py:331] Added request cmpl-7a0c65ec2b9a48f48ebecc0bd225dd1d-0.
INFO 08-09 14:33:49 [metrics.py:417] Avg prompt throughput: 4.4 tokens/s, Avg generation throughput: 105.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 14:33:54 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 101.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:33:57 [logger.py:43] Received request cmpl-7b3b1e0a25084079b71c5b77cb305938-0: prompt: 'How to batch multiple prompts into one request?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 304, 9853, 2999, 9508, 29879, 964, 697, 2009, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:42510 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:33:57 [engine.py:331] Added request cmpl-7b3b1e0a25084079b71c5b77cb305938-0.
INFO 08-09 14:33:59 [logger.py:43] Received request cmpl-b018d770504d45d38558acf6f3887119-0: prompt: 'What is a chat template in HuggingFace?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 263, 13563, 4472, 297, 379, 688, 3460, 23360, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:42520 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:33:59 [engine.py:331] Added request cmpl-b018d770504d45d38558acf6f3887119-0.
INFO 08-09 14:33:59 [metrics.py:417] Avg prompt throughput: 4.6 tokens/s, Avg generation throughput: 107.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 14:34:04 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 111.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 14:34:08 [logger.py:43] Received request cmpl-0fd66319de184a33a27f12a2ad546863-0: prompt: 'What is inference latency and why is it important?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 27262, 23316, 1270, 322, 2020, 338, 372, 4100, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:41974 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:34:08 [engine.py:331] Added request cmpl-0fd66319de184a33a27f12a2ad546863-0.
INFO 08-09 14:34:09 [metrics.py:417] Avg prompt throughput: 2.4 tokens/s, Avg generation throughput: 105.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 14:34:14 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 108.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:34:16 [logger.py:43] Received request cmpl-5a64ab060b0f4f128d31181705832711-0: prompt: 'How to reduce memory footprint of LLaMA models?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 304, 10032, 3370, 3661, 2158, 310, 365, 5661, 1529, 4733, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:52826 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:34:16 [engine.py:331] Added request cmpl-5a64ab060b0f4f128d31181705832711-0.
INFO 08-09 14:34:19 [metrics.py:417] Avg prompt throughput: 2.6 tokens/s, Avg generation throughput: 105.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 14:34:24 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 102.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:34:26 [logger.py:43] Received request cmpl-19500a13937948ad8c163bfed937ec1e-0: prompt: 'What is the difference between `generation_config.json` and `config.json`?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 278, 4328, 1546, 421, 4738, 362, 29918, 2917, 29889, 3126, 29952, 322, 421, 2917, 29889, 3126, 6522], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:56482 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:34:26 [engine.py:331] Added request cmpl-19500a13937948ad8c163bfed937ec1e-0.
INFO 08-09 14:34:29 [metrics.py:417] Avg prompt throughput: 4.0 tokens/s, Avg generation throughput: 104.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 14:34:34 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 110.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:34:35 [logger.py:43] Received request cmpl-f00d4081bf3f4f6dba00a339dd4635f7-0: prompt: 'What is a quantized model and how is it served?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 263, 4323, 1891, 1904, 322, 920, 338, 372, 6766, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:55414 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:34:35 [engine.py:331] Added request cmpl-f00d4081bf3f4f6dba00a339dd4635f7-0.
INFO 08-09 14:34:39 [metrics.py:417] Avg prompt throughput: 2.6 tokens/s, Avg generation throughput: 109.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 14:34:44 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 106.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:34:44 [logger.py:43] Received request cmpl-23e8cf02fbf74281a1df65558a0c8223-0: prompt: 'How to handle prompt overflow in generation?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 304, 4386, 9508, 11969, 297, 12623, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:55428 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:34:44 [engine.py:331] Added request cmpl-23e8cf02fbf74281a1df65558a0c8223-0.
INFO 08-09 14:34:49 [metrics.py:417] Avg prompt throughput: 1.8 tokens/s, Avg generation throughput: 107.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 14:34:54 [logger.py:43] Received request cmpl-defce2e65a884961b32cf274cbaea5b1-0: prompt: 'What is warmup time in inference pipelines?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 14294, 786, 931, 297, 27262, 8450, 24210, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:51330 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:34:54 [engine.py:331] Added request cmpl-defce2e65a884961b32cf274cbaea5b1-0.
INFO 08-09 14:34:54 [metrics.py:417] Avg prompt throughput: 2.2 tokens/s, Avg generation throughput: 104.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 14:34:59 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 110.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 14:35:03 [logger.py:43] Received request cmpl-ad9650fd68ab4fa9969281591d1e1efd-0: prompt: 'How to stream JSON tokens using curl?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 304, 4840, 4663, 18897, 773, 11051, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:43804 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:35:03 [engine.py:331] Added request cmpl-ad9650fd68ab4fa9969281591d1e1efd-0.
INFO 08-09 14:35:04 [metrics.py:417] Avg prompt throughput: 1.8 tokens/s, Avg generation throughput: 105.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 14:35:09 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 112.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:35:12 [logger.py:43] Received request cmpl-a3a4bb3894b5473fa2f7048336d1254b-0: prompt: 'What is a `generate_with_streaming()` wrapper?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 263, 421, 17158, 29918, 2541, 29918, 5461, 292, 2555, 14476, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:52930 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:35:12 [engine.py:331] Added request cmpl-a3a4bb3894b5473fa2f7048336d1254b-0.
INFO 08-09 14:35:14 [metrics.py:417] Avg prompt throughput: 2.8 tokens/s, Avg generation throughput: 105.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 14:35:19 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 102.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:35:22 [logger.py:43] Received request cmpl-c8804f6ef53040ad9d87242a02603619-0: prompt: 'How to evaluate TTFT and ITL for multiple prompts?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 304, 14707, 323, 29911, 7818, 322, 13315, 29931, 363, 2999, 9508, 29879, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:40078 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:35:22 [engine.py:331] Added request cmpl-c8804f6ef53040ad9d87242a02603619-0.
INFO 08-09 14:35:24 [metrics.py:417] Avg prompt throughput: 3.0 tokens/s, Avg generation throughput: 101.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 14:35:29 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 106.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:35:31 [logger.py:43] Received request cmpl-2070990714c546de8ff520e58d4d95bb-0: prompt: 'What are server cold starts in model serving?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 526, 1923, 11220, 8665, 297, 1904, 16330, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:57670 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:35:31 [engine.py:331] Added request cmpl-2070990714c546de8ff520e58d4d95bb-0.
INFO 08-09 14:35:34 [metrics.py:417] Avg prompt throughput: 2.0 tokens/s, Avg generation throughput: 110.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 14:35:39 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 106.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:35:41 [logger.py:43] Received request cmpl-4907d50af1c64761b509a210f51f3296-0: prompt: 'Explain how to avoid padding inefficiencies.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 12027, 7420, 920, 304, 4772, 7164, 297, 29872, 2416, 819, 2478, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:56292 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:35:41 [engine.py:331] Added request cmpl-4907d50af1c64761b509a210f51f3296-0.
INFO 08-09 14:35:44 [metrics.py:417] Avg prompt throughput: 2.6 tokens/s, Avg generation throughput: 106.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 14:35:49 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 109.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:35:50 [logger.py:43] Received request cmpl-f7fdbee540a843958a11ebbb663e5795-0: prompt: 'How does tokenizer pre-processing affect TTFT?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 947, 5993, 3950, 758, 29899, 19170, 6602, 323, 29911, 7818, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:57434 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:35:50 [engine.py:331] Added request cmpl-f7fdbee540a843958a11ebbb663e5795-0.
INFO 08-09 14:35:54 [metrics.py:417] Avg prompt throughput: 2.6 tokens/s, Avg generation throughput: 110.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 14:35:59 [logger.py:43] Received request cmpl-dadcf7bfe66548008add6c39527c568f-0: prompt: 'How to track per-prompt latency in logs?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 304, 5702, 639, 29899, 14032, 415, 23316, 1270, 297, 10748, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:48464 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:35:59 [engine.py:331] Added request cmpl-dadcf7bfe66548008add6c39527c568f-0.
INFO 08-09 14:35:59 [metrics.py:417] Avg prompt throughput: 2.6 tokens/s, Avg generation throughput: 108.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 14:36:03 [logger.py:43] Received request cmpl-8ccccbf4095b49d29e015d83baccacce-0: prompt: 'Explain streaming benchmarks with high concurrency.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 12027, 7420, 24820, 23513, 29879, 411, 1880, 3022, 10880, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:48468 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:36:03 [engine.py:331] Added request cmpl-8ccccbf4095b49d29e015d83baccacce-0.
INFO 08-09 14:36:04 [metrics.py:417] Avg prompt throughput: 2.2 tokens/s, Avg generation throughput: 104.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 14:36:09 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 108.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 14:36:13 [logger.py:43] Received request cmpl-6c1b423912b843279d46d9c405fd8001-0: prompt: 'How to calculate power usage per prompt?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 304, 8147, 3081, 8744, 639, 9508, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:33850 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:36:13 [engine.py:331] Added request cmpl-6c1b423912b843279d46d9c405fd8001-0.
INFO 08-09 14:36:14 [metrics.py:417] Avg prompt throughput: 1.8 tokens/s, Avg generation throughput: 105.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 14:36:19 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 105.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:36:22 [logger.py:43] Received request cmpl-a2986303f8d54a9aa5573e1473b1cab0-0: prompt: 'How does the tokenizer handle emojis and symbols?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 947, 278, 5993, 3950, 4386, 953, 3848, 275, 322, 15072, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:51852 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:36:22 [engine.py:331] Added request cmpl-a2986303f8d54a9aa5573e1473b1cab0-0.
INFO 08-09 14:36:24 [logger.py:43] Received request cmpl-7a8bf96e1ca1486394ac82f289787f8c-0: prompt: 'How does model size affect inference latency?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 947, 1904, 2159, 6602, 27262, 23316, 1270, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:51856 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:36:24 [engine.py:331] Added request cmpl-7a8bf96e1ca1486394ac82f289787f8c-0.
INFO 08-09 14:36:24 [metrics.py:417] Avg prompt throughput: 4.6 tokens/s, Avg generation throughput: 106.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 14:36:29 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 109.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 14:36:33 [logger.py:43] Received request cmpl-92dff935e2d4476abb6e148bad834487-0: prompt: 'How to debug slow streaming responses?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 304, 4744, 5232, 24820, 20890, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:48686 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:36:33 [engine.py:331] Added request cmpl-92dff935e2d4476abb6e148bad834487-0.
INFO 08-09 14:36:34 [metrics.py:417] Avg prompt throughput: 1.6 tokens/s, Avg generation throughput: 109.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 14:36:37 [logger.py:43] Received request cmpl-e209f917df1d41d5a4cde4c64e8763ad-0: prompt: 'What causes generation to stall midway?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 9946, 12623, 304, 380, 497, 7145, 1582, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:32902 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:36:37 [engine.py:331] Added request cmpl-e209f917df1d41d5a4cde4c64e8763ad-0.
INFO 08-09 14:36:39 [metrics.py:417] Avg prompt throughput: 2.0 tokens/s, Avg generation throughput: 107.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 14:36:44 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 105.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:36:47 [logger.py:43] Received request cmpl-b9cc3e2a110c4968b1fb602b6b8e34fc-0: prompt: 'How to load a model on HPU using HF Transformers?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 304, 2254, 263, 1904, 373, 379, 7056, 773, 379, 29943, 4103, 689, 414, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:48774 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:36:47 [engine.py:331] Added request cmpl-b9cc3e2a110c4968b1fb602b6b8e34fc-0.
INFO 08-09 14:36:49 [metrics.py:417] Avg prompt throughput: 3.2 tokens/s, Avg generation throughput: 103.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 14:36:54 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 110.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:36:56 [logger.py:43] Received request cmpl-54394aa710a942e1a0514c67b28bdc4f-0: prompt: 'How to pass prompt + history into a chat model?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 304, 1209, 9508, 718, 4955, 964, 263, 13563, 1904, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:40612 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:36:56 [engine.py:331] Added request cmpl-54394aa710a942e1a0514c67b28bdc4f-0.
INFO 08-09 14:36:59 [metrics.py:417] Avg prompt throughput: 2.4 tokens/s, Avg generation throughput: 109.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 14:37:04 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 103.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:37:05 [logger.py:43] Received request cmpl-74182e64a71e46ec8943a8fb1a6551bd-0: prompt: 'What is an OpenAI-compatible inference API?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 385, 4673, 23869, 29899, 23712, 27262, 3450, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:54638 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:37:05 [engine.py:331] Added request cmpl-74182e64a71e46ec8943a8fb1a6551bd-0.
INFO 08-09 14:37:09 [metrics.py:417] Avg prompt throughput: 2.2 tokens/s, Avg generation throughput: 103.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 14:37:14 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 102.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:37:15 [logger.py:43] Received request cmpl-ed9e0dc289994c8981eeca33740d75dc-0: prompt: 'How to handle decoding errors in stream?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 304, 4386, 1602, 3689, 4436, 297, 4840, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:49392 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:37:15 [engine.py:331] Added request cmpl-ed9e0dc289994c8981eeca33740d75dc-0.
INFO 08-09 14:37:19 [logger.py:43] Received request cmpl-93f8a65ee9ed4bd9b6cd3d0fda1a1750-0: prompt: 'What are temperature vs randomness in sampling?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 526, 10430, 7186, 4036, 2264, 297, 23460, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:49400 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:37:19 [engine.py:331] Added request cmpl-93f8a65ee9ed4bd9b6cd3d0fda1a1750-0.
INFO 08-09 14:37:19 [metrics.py:417] Avg prompt throughput: 4.0 tokens/s, Avg generation throughput: 107.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 14:37:24 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 104.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 14:37:28 [logger.py:43] Received request cmpl-8836ea0b532e442eb93dbfa598efc263-0: prompt: 'Explain token logprobs in output.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 12027, 7420, 5993, 1480, 771, 5824, 297, 1962, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:44644 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:37:28 [engine.py:331] Added request cmpl-8836ea0b532e442eb93dbfa598efc263-0.
INFO 08-09 14:37:29 [metrics.py:417] Avg prompt throughput: 2.0 tokens/s, Avg generation throughput: 104.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 14:37:34 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 99.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 14:37:38 [logger.py:43] Received request cmpl-0aee63cf8ddb4a3d8df3d2bac45a00ed-0: prompt: 'How to measure and reduce TTFT on Gaudi?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 304, 5645, 322, 10032, 323, 29911, 7818, 373, 10415, 4749, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:47436 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:37:38 [engine.py:331] Added request cmpl-0aee63cf8ddb4a3d8df3d2bac45a00ed-0.
INFO 08-09 14:37:39 [metrics.py:417] Avg prompt throughput: 2.6 tokens/s, Avg generation throughput: 102.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 14:37:44 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 106.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:37:48 [logger.py:43] Received request cmpl-79050e462c554f7ab579e912427582e6-0: prompt: 'Explain the internals of HF `generate()` loop.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 12027, 7420, 278, 2836, 1338, 310, 379, 29943, 421, 17158, 2555, 2425, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:35818 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:37:48 [engine.py:331] Added request cmpl-79050e462c554f7ab579e912427582e6-0.
INFO 08-09 14:37:49 [metrics.py:417] Avg prompt throughput: 2.8 tokens/s, Avg generation throughput: 102.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 14:37:54 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 103.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:37:57 [logger.py:43] Received request cmpl-38984595fe6a4e0e9455d2885733c1be-0: prompt: 'What is speculative token generation?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 1580, 28524, 5993, 12623, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:39798 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:37:57 [engine.py:331] Added request cmpl-38984595fe6a4e0e9455d2885733c1be-0.
INFO 08-09 14:37:59 [metrics.py:417] Avg prompt throughput: 1.6 tokens/s, Avg generation throughput: 105.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 14:38:04 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 110.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:38:07 [logger.py:43] Received request cmpl-cccfde8377ec47d0ac3a81d1934d083a-0: prompt: 'What are possible failure points in vLLM serving?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 526, 1950, 10672, 3291, 297, 325, 2208, 29924, 16330, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:51788 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:38:07 [engine.py:331] Added request cmpl-cccfde8377ec47d0ac3a81d1934d083a-0.
INFO 08-09 14:38:09 [metrics.py:417] Avg prompt throughput: 2.4 tokens/s, Avg generation throughput: 108.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 14:38:14 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 109.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:38:16 [logger.py:43] Received request cmpl-d82994fecf604a77977eb5568ddeac16-0: prompt: 'How to catch stream decoding errors gracefully?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 304, 4380, 4840, 1602, 3689, 4436, 17659, 3730, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:47006 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:38:16 [engine.py:331] Added request cmpl-d82994fecf604a77977eb5568ddeac16-0.
INFO 08-09 14:38:19 [metrics.py:417] Avg prompt throughput: 2.2 tokens/s, Avg generation throughput: 108.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 14:38:24 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 109.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:38:25 [logger.py:43] Received request cmpl-fe8aba7df832426bb97338fb939c268a-0: prompt: 'How to enable/disable kv caching in transformers?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 304, 9025, 29914, 20472, 10908, 22488, 297, 4327, 414, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:55106 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:38:25 [engine.py:331] Added request cmpl-fe8aba7df832426bb97338fb939c268a-0.
INFO 08-09 14:38:29 [logger.py:43] Received request cmpl-5405d8634a0149279f2c8b2969fa16ce-0: prompt: 'What are challenges of multilingual streaming inference?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 526, 18066, 267, 310, 1773, 6504, 950, 24820, 27262, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:55118 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:38:29 [engine.py:331] Added request cmpl-5405d8634a0149279f2c8b2969fa16ce-0.
INFO 08-09 14:38:29 [metrics.py:417] Avg prompt throughput: 4.8 tokens/s, Avg generation throughput: 101.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 14:38:34 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 104.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 14:38:38 [logger.py:43] Received request cmpl-e575b6646c07462095394b76910385d3-0: prompt: 'How to benchmark with different model precisions?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 304, 23513, 411, 1422, 1904, 12132, 1080, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:59272 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:38:38 [engine.py:331] Added request cmpl-e575b6646c07462095394b76910385d3-0.
INFO 08-09 14:38:39 [metrics.py:417] Avg prompt throughput: 2.0 tokens/s, Avg generation throughput: 106.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 14:38:44 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 102.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:38:48 [logger.py:43] Received request cmpl-182a54d96d7e4a2e8c84c02aa72a1426-0: prompt: 'What is the tradeoff between token latency and batch size?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 278, 11302, 2696, 1546, 5993, 23316, 1270, 322, 9853, 2159, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:42704 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:38:48 [engine.py:331] Added request cmpl-182a54d96d7e4a2e8c84c02aa72a1426-0.
INFO 08-09 14:38:49 [metrics.py:417] Avg prompt throughput: 2.8 tokens/s, Avg generation throughput: 104.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 14:38:54 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 105.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:38:57 [logger.py:43] Received request cmpl-57cb51a84a734362a393043ed927a194-0: prompt: 'Describe the advantages and limitations of transformer-based neural networks.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 20355, 915, 278, 25486, 322, 27028, 310, 4327, 261, 29899, 6707, 19677, 14379, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:58988 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:38:57 [engine.py:331] Added request cmpl-57cb51a84a734362a393043ed927a194-0.
INFO 08-09 14:38:59 [metrics.py:417] Avg prompt throughput: 3.0 tokens/s, Avg generation throughput: 107.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 14:39:04 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 106.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:39:07 [logger.py:43] Received request cmpl-e31f207900d94863b3d860e550a28a25-0: prompt: 'Explain the concept of overfitting in machine learning and how to prevent it.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 12027, 7420, 278, 6964, 310, 975, 29888, 5367, 297, 4933, 6509, 322, 920, 304, 5557, 372, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:50950 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:39:07 [engine.py:331] Added request cmpl-e31f207900d94863b3d860e550a28a25-0.
INFO 08-09 14:39:09 [metrics.py:417] Avg prompt throughput: 3.6 tokens/s, Avg generation throughput: 105.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 14:39:12 [logger.py:43] Received request cmpl-0db33126eaf74f9a837c4c36146afaf0-0: prompt: 'How does the attention mechanism work in transformer models?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 947, 278, 8570, 13336, 664, 297, 4327, 261, 4733, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:50966 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:39:12 [engine.py:331] Added request cmpl-0db33126eaf74f9a837c4c36146afaf0-0.
INFO 08-09 14:39:14 [metrics.py:417] Avg prompt throughput: 2.4 tokens/s, Avg generation throughput: 104.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 14:39:19 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 108.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:39:21 [logger.py:43] Received request cmpl-63f5d99bbfab42aea62a124635723856-0: prompt: 'Summarize the main differences between supervised and unsupervised learning.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 6991, 3034, 675, 278, 1667, 12651, 1546, 2428, 11292, 322, 443, 9136, 11292, 6509, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:47588 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:39:21 [engine.py:331] Added request cmpl-63f5d99bbfab42aea62a124635723856-0.
INFO 08-09 14:39:24 [metrics.py:417] Avg prompt throughput: 3.2 tokens/s, Avg generation throughput: 103.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 14:39:29 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 107.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:39:31 [logger.py:43] Received request cmpl-00244caaa0f249ab93acdf0b39f911e6-0: prompt: 'What are the ethical concerns associated with deploying large language models?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 526, 278, 11314, 936, 21838, 6942, 411, 7246, 292, 2919, 4086, 4733, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:60724 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:39:31 [engine.py:331] Added request cmpl-00244caaa0f249ab93acdf0b39f911e6-0.
INFO 08-09 14:39:34 [metrics.py:417] Avg prompt throughput: 3.0 tokens/s, Avg generation throughput: 107.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 14:39:39 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 107.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:39:40 [logger.py:43] Received request cmpl-548b81e8ac1d46738233131787ab0ddd-0: prompt: 'Explain how gradient descent works in training deep neural networks.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 12027, 7420, 920, 16030, 26815, 1736, 297, 6694, 6483, 19677, 14379, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:40558 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:39:40 [engine.py:331] Added request cmpl-548b81e8ac1d46738233131787ab0ddd-0.
INFO 08-09 14:39:44 [metrics.py:417] Avg prompt throughput: 2.6 tokens/s, Avg generation throughput: 107.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 14:39:49 [logger.py:43] Received request cmpl-26b08464462b4809b94ac239f240fb1d-0: prompt: 'Describe the role of activation functions in deep learning.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 20355, 915, 278, 6297, 310, 26229, 3168, 297, 6483, 6509, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:43890 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:39:49 [engine.py:331] Added request cmpl-26b08464462b4809b94ac239f240fb1d-0.
INFO 08-09 14:39:49 [metrics.py:417] Avg prompt throughput: 2.4 tokens/s, Avg generation throughput: 106.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 14:39:55 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 102.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 14:39:59 [logger.py:43] Received request cmpl-324e97b901aa461a88aad5579f666f61-0: prompt: 'How can transfer learning improve model performance on small datasets?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 508, 6782, 6509, 11157, 1904, 4180, 373, 2319, 20035, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:54534 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:39:59 [engine.py:331] Added request cmpl-324e97b901aa461a88aad5579f666f61-0.
INFO 08-09 14:40:00 [metrics.py:417] Avg prompt throughput: 2.4 tokens/s, Avg generation throughput: 106.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 14:40:05 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 109.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 14:40:08 [logger.py:43] Received request cmpl-68c3089d7d614b27bd0ed63790b808d0-0: prompt: 'Explain the concept of reinforcement learning with an example.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 12027, 7420, 278, 6964, 310, 15561, 1454, 13561, 6509, 411, 385, 1342, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:38538 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:40:08 [engine.py:331] Added request cmpl-68c3089d7d614b27bd0ed63790b808d0-0.
INFO 08-09 14:40:10 [metrics.py:417] Avg prompt throughput: 2.8 tokens/s, Avg generation throughput: 105.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 14:40:15 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 106.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:40:17 [logger.py:43] Received request cmpl-ee919203e1c845288980ad884fca1871-0: prompt: 'What are the main components of a convolutional neural network (CNN)?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 526, 278, 1667, 7117, 310, 263, 26851, 284, 19677, 3564, 313, 29907, 10262, 6877], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:45328 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:40:17 [engine.py:331] Added request cmpl-ee919203e1c845288980ad884fca1871-0.
INFO 08-09 14:40:20 [metrics.py:417] Avg prompt throughput: 3.2 tokens/s, Avg generation throughput: 105.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 14:40:25 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 110.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:40:27 [logger.py:43] Received request cmpl-09fb50c2c48647e8ad6f9bada12b7849-0: prompt: 'Describe how large language models like GPT or LLaMA are trained and fine-tuned.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 20355, 915, 920, 2919, 4086, 4733, 763, 402, 7982, 470, 365, 5661, 1529, 526, 16370, 322, 2691, 29899, 29873, 348, 287, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:46556 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:40:27 [engine.py:331] Added request cmpl-09fb50c2c48647e8ad6f9bada12b7849-0.
INFO 08-09 14:40:30 [metrics.py:417] Avg prompt throughput: 4.6 tokens/s, Avg generation throughput: 108.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 14:40:35 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 102.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:40:36 [logger.py:43] Received request cmpl-5fe5b4c08def4894affc172a1c94667c-0: prompt: 'Compare and contrast BERT and GPT in terms of architecture and use cases.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 3831, 598, 322, 12814, 350, 20161, 322, 402, 7982, 297, 4958, 310, 11258, 322, 671, 4251, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:45776 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:40:36 [engine.py:331] Added request cmpl-5fe5b4c08def4894affc172a1c94667c-0.
INFO 08-09 14:40:40 [metrics.py:417] Avg prompt throughput: 3.6 tokens/s, Avg generation throughput: 111.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 14:40:45 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 107.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:40:55 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 8.5 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 14:41:06 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
