Calling add_step_closure function does not have any effect. It's lazy mode only functionality. (warning logged once)
Calling mark_step function does not have any effect. It's lazy mode only functionality. (warning logged once)
Calling iter_mark_step function does not have any effect. It's lazy mode only functionality. (warning logged once)
INFO 08-15 10:14:26 [__init__.py:254] Automatically detected platform hpu.
INFO 08-15 10:14:29 [config.py:1975] Disabled the custom all-reduce kernel because it is not supported on current platform.
INFO 08-15 10:14:30 [config.py:1975] Disabled the custom all-reduce kernel because it is not supported on current platform.
INFO 08-15 10:14:30 [config.py:1975] Disabled the custom all-reduce kernel because it is not supported on current platform.
INFO 08-15 10:14:31 [config.py:1975] Disabled the custom all-reduce kernel because it is not supported on current platform.
INFO 08-15 10:14:31 [api_server.py:1289] vLLM API server version 0.6.3.dev5453+ge9c83fca1
INFO 08-15 10:14:32 [config.py:1975] Disabled the custom all-reduce kernel because it is not supported on current platform.
INFO 08-15 10:14:32 [cli_args.py:309] non-default args: {'model': '/software/data/llama_inference/Llama-2-7b-hf/'}
INFO 08-15 10:14:40 [config.py:822] This model supports multiple tasks: {'embed', 'reward', 'score', 'generate', 'classify'}. Defaulting to 'generate'.
WARNING 08-15 10:14:40 [config.py:3236] Your device 'hpu' doesn't support torch.float16. Falling back to torch.bfloat16 for compatibility.
WARNING 08-15 10:14:40 [config.py:3287] Casting torch.float16 to torch.bfloat16.
INFO 08-15 10:14:40 [arg_utils.py:1706] hpu is experimental on VLLM_USE_V1=1. Falling back to V0 Engine.
INFO 08-15 10:14:40 [config.py:1975] Disabled the custom all-reduce kernel because it is not supported on current platform.
INFO 08-15 10:14:40 [api_server.py:267] Started engine process with PID 3864739
Calling add_step_closure function does not have any effect. It's lazy mode only functionality. (warning logged once)
Calling mark_step function does not have any effect. It's lazy mode only functionality. (warning logged once)
Calling iter_mark_step function does not have any effect. It's lazy mode only functionality. (warning logged once)
WARNING 08-15 10:14:43 [env_override.py:17] NCCL_CUMEM_ENABLE is set to 0, skipping override. This may increase memory overhead with cudagraph+allreduce: https://github.com/NVIDIA/nccl/issues/1234
INFO 08-15 10:14:44 [__init__.py:254] Automatically detected platform hpu.
INFO 08-15 10:14:47 [llm_engine.py:239] Initializing a V0 LLM engine (v0.6.3.dev5453+ge9c83fca1) with config: model='/software/data/llama_inference/Llama-2-7b-hf/', speculative_config=None, tokenizer='/software/data/llama_inference/Llama-2-7b-hf/', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=hpu, decoding_config=DecodingConfig(backend='xgrammar', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=None, served_model_name=/software/data/llama_inference/Llama-2-7b-hf/, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, pooler_config=None, compilation_config={"level":0,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":[],"splitting_ops":[],"use_inductor":true,"compile_sizes":[],"inductor_compile_config":{"enable_auto_functionalized_v2":false},"inductor_passes":{},"use_cudagraph":false,"cudagraph_num_of_warmups":0,"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"max_capture_size":256,"local_cache_dir":null}, use_cached_outputs=True, 
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
WARNING 08-15 10:14:47 [rocm.py:28] Failed to import from amdsmi with ModuleNotFoundError("No module named 'amdsmi'")
WARNING 08-15 10:14:47 [rocm.py:33] Failed to import from vllm._C with ModuleNotFoundError("No module named 'vllm._C'")
WARNING 08-15 10:14:47 [rocm.py:39] Failed to import from vllm._rocm_C with ModuleNotFoundError("No module named 'vllm._rocm_C'")
WARNING 08-15 10:14:47 [utils.py:2850] Methods add_prompt_adapter,cache_config,compilation_config,current_platform,list_prompt_adapters,load_config,pin_prompt_adapter,remove_prompt_adapter,scheduler_config not implemented in <vllm.worker.hpu_worker.HPUWorker object at 0x7df3d5815630>
INFO 08-15 10:14:48 [runtime.py:26] Environment:
INFO 08-15 10:14:48 [runtime.py:30]     hw: gaudi2
INFO 08-15 10:14:48 [runtime.py:30]     build: 1.22.0.538
INFO 08-15 10:14:48 [runtime.py:30]     engine_version: v0
INFO 08-15 10:14:48 [runtime.py:30]     bridge_mode: eager
INFO 08-15 10:14:48 [runtime.py:30]     model_type: llama
INFO 08-15 10:14:48 [runtime.py:26] Features:
INFO 08-15 10:14:48 [runtime.py:30]     fp32_alibi_biases: True
INFO 08-15 10:14:48 [runtime.py:30]     fp32_softmax: False
INFO 08-15 10:14:48 [runtime.py:30]     fused_block_softmax_adjustment: False
INFO 08-15 10:14:48 [runtime.py:30]     fused_block_softmax: False
INFO 08-15 10:14:48 [runtime.py:30]     prompt_attn_impl: fsdpa_impl
INFO 08-15 10:14:48 [runtime.py:30]     skip_warmup: False
INFO 08-15 10:14:48 [runtime.py:30]     merged_prefill: False
INFO 08-15 10:14:48 [runtime.py:30]     use_contiguous_pa: True
INFO 08-15 10:14:48 [runtime.py:30]     use_delayed_sampling: True
INFO 08-15 10:14:48 [runtime.py:30]     use_bucketing: True
INFO 08-15 10:14:48 [runtime.py:30]     bucketing_strategy: exponential_bucketing
INFO 08-15 10:14:48 [runtime.py:26] User flags:
INFO 08-15 10:14:48 [runtime.py:30]     VLLM_USE_V1: False
WARNING 08-15 10:14:48 [hpu.py:135] Pin memory is not supported on HPU.
INFO 08-15 10:14:48 [hpu.py:60] Using HPUAttention backend.
============================= HPU SW VERSION ====================================== 
 HB_BUILD_VER = 1.22.0-538
============================= HPU PT BRIDGE CONFIGURATION ON RANK = 0 ============= 
 PT_HPU_LAZY_MODE = 0
 PT_HPU_RECIPE_CACHE_CONFIG = ,false,1024,false
 PT_HPU_MAX_COMPOUND_OP_SIZE = 9223372036854775807
 PT_HPU_LAZY_ACC_PAR_MODE = 1
 PT_HPU_ENABLE_REFINE_DYNAMIC_SHAPES = 0
 PT_HPU_EAGER_PIPELINE_ENABLE = 1
 PT_HPU_EAGER_COLLECTIVE_PIPELINE_ENABLE = 1
 PT_HPU_ENABLE_LAZY_COLLECTIVES = 1
---------------------------: System Configuration :---------------------------
Num CPU Cores : 160
CPU RAM       : 1007 GB
------------------------------------------------------------------------------
INFO 08-15 10:14:51 [parallel_state.py:1065] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.60it/s]
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:02<00:00,  1.33s/it]
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:02<00:00,  1.23s/it]

INFO 08-15 10:14:54 [default_loader.py:272] Loading weights took 2.68 seconds
INFO 08-15 10:14:54 [hpu_model_runner.py:1184] Pre-loading model weights on hpu:0 took 12.55 GiB of device memory (12.56 GiB/94.62 GiB used) and 303.9 MiB of host memory (316.2 GiB/1007 GiB used)
INFO 08-15 10:14:54 [hpu_model_runner.py:1266] Wrapping in HPU Graph took 0 B of device memory (12.56 GiB/94.62 GiB used) and -3.547 MiB of host memory (316.2 GiB/1007 GiB used)
INFO 08-15 10:14:55 [hpu_model_runner.py:1270] Compiling took 0 B of device memory (12.56 GiB/94.62 GiB used) and -52.9 MiB of host memory (316.2 GiB/1007 GiB used)
INFO 08-15 10:14:55 [hpu_model_runner.py:1274] Loading model weights took in total 12.55 GiB of device memory (12.56 GiB/94.62 GiB used) and 303.2 MiB of host memory (316.2 GiB/1007 GiB used)
WARNING 08-15 10:14:55 [common.py:139] Prompt bucket for (1, 4096, 0) was not prepared. Adding new bucket: (1, 4096, 0)
INFO 08-15 10:14:57 [hpu_worker.py:289] Model profiling run took 476 MiB of device memory (13.02 GiB/94.62 GiB used) and 397.6 MiB of host memory (316.6 GiB/1007 GiB used)
INFO 08-15 10:14:57 [hpu_worker.py:313] Free device memory: 81.6 GiB, 73.44 GiB usable (gpu_memory_utilization=0.9), 7.344 GiB reserved for HPUGraphs (VLLM_GRAPH_RESERVED_MEM=0.1), 66.1 GiB reserved for KV cache
INFO 08-15 10:14:57 [executor_base.py:113] # hpu blocks: 1057, # CPU blocks: 64
INFO 08-15 10:14:57 [executor_base.py:118] Maximum concurrency for 4096 tokens per request: 33.03x
INFO 08-15 10:14:57 [exponential.py:49] Prompt bucket config (min, step, max_warmup, limit) bs:[1, 2, 256, 9], seq:[128, 128, 4096, 13]
INFO 08-15 10:14:57 [common.py:117] Generated 38 prompt buckets [bs, query, num_blocks]: [(1, 128, 0), (1, 256, 0), (1, 384, 0), (1, 512, 0), (1, 640, 0), (1, 768, 0), (1, 896, 0), (1, 1024, 0), (1, 1408, 0), (1, 1792, 0), (1, 2304, 0), (1, 3072, 0), (1, 4096, 0), (2, 128, 0), (2, 256, 0), (2, 384, 0), (2, 512, 0), (2, 640, 0), (2, 768, 0), (2, 896, 0), (2, 1024, 0), (2, 1408, 0), (2, 1792, 0), (4, 128, 0), (4, 256, 0), (4, 384, 0), (4, 512, 0), (4, 640, 0), (4, 768, 0), (4, 896, 0), (4, 1024, 0), (8, 128, 0), (8, 256, 0), (8, 384, 0), (8, 512, 0), (16, 128, 0), (16, 256, 0), (32, 128, 0)]
INFO 08-15 10:14:57 [exponential.py:79] Decode bucket config (min, step, max_warmup, limit) bs:[1, 2, 256, 9], block:[128, 128, 1057, 12]
INFO 08-15 10:14:57 [common.py:117] Generated 72 decode buckets [bs, query, num_blocks]: [(1, 1, 128), (1, 1, 256), (1, 1, 384), (1, 1, 512), (1, 1, 640), (1, 1, 768), (1, 1, 896), (1, 1, 1024), (2, 1, 128), (2, 1, 256), (2, 1, 384), (2, 1, 512), (2, 1, 640), (2, 1, 768), (2, 1, 896), (2, 1, 1024), (4, 1, 128), (4, 1, 256), (4, 1, 384), (4, 1, 512), (4, 1, 640), (4, 1, 768), (4, 1, 896), (4, 1, 1024), (8, 1, 128), (8, 1, 256), (8, 1, 384), (8, 1, 512), (8, 1, 640), (8, 1, 768), (8, 1, 896), (8, 1, 1024), (16, 1, 128), (16, 1, 256), (16, 1, 384), (16, 1, 512), (16, 1, 640), (16, 1, 768), (16, 1, 896), (16, 1, 1024), (32, 1, 128), (32, 1, 256), (32, 1, 384), (32, 1, 512), (32, 1, 640), (32, 1, 768), (32, 1, 896), (32, 1, 1024), (64, 1, 128), (64, 1, 256), (64, 1, 384), (64, 1, 512), (64, 1, 640), (64, 1, 768), (64, 1, 896), (64, 1, 1024), (128, 1, 128), (128, 1, 256), (128, 1, 384), (128, 1, 512), (128, 1, 640), (128, 1, 768), (128, 1, 896), (128, 1, 1024), (256, 1, 128), (256, 1, 256), (256, 1, 384), (256, 1, 512), (256, 1, 640), (256, 1, 768), (256, 1, 896), (256, 1, 1024)]
INFO 08-15 10:14:58 [hpu_worker.py:350] Initializing cache engine took 66.06 GiB of device memory (79.08 GiB/94.62 GiB used) and 4.521 GiB of host memory (321.1 GiB/1007 GiB used)
INFO 08-15 10:14:58 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][1/38] batch_size:32 query_len:128 num_blocks:0 free_mem:15.54 GiB
INFO 08-15 10:15:00 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][2/38] batch_size:16 query_len:256 num_blocks:0 free_mem:15.54 GiB
INFO 08-15 10:15:01 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][3/38] batch_size:16 query_len:128 num_blocks:0 free_mem:15.54 GiB
INFO 08-15 10:15:03 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][4/38] batch_size:8 query_len:512 num_blocks:0 free_mem:15.54 GiB
INFO 08-15 10:15:05 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][5/38] batch_size:8 query_len:384 num_blocks:0 free_mem:15.54 GiB
INFO 08-15 10:15:06 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][6/38] batch_size:8 query_len:256 num_blocks:0 free_mem:15.54 GiB
INFO 08-15 10:15:08 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][7/38] batch_size:8 query_len:128 num_blocks:0 free_mem:15.54 GiB
INFO 08-15 10:15:10 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][8/38] batch_size:4 query_len:1024 num_blocks:0 free_mem:15.54 GiB
INFO 08-15 10:15:12 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][9/38] batch_size:4 query_len:896 num_blocks:0 free_mem:15.54 GiB
INFO 08-15 10:15:13 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][10/38] batch_size:4 query_len:768 num_blocks:0 free_mem:15.54 GiB
INFO 08-15 10:15:15 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][11/38] batch_size:4 query_len:640 num_blocks:0 free_mem:15.54 GiB
INFO 08-15 10:15:17 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][12/38] batch_size:4 query_len:512 num_blocks:0 free_mem:15.54 GiB
INFO 08-15 10:15:19 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][13/38] batch_size:4 query_len:384 num_blocks:0 free_mem:15.54 GiB
INFO 08-15 10:15:20 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][14/38] batch_size:4 query_len:256 num_blocks:0 free_mem:15.54 GiB
INFO 08-15 10:15:22 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][15/38] batch_size:4 query_len:128 num_blocks:0 free_mem:15.54 GiB
INFO 08-15 10:15:24 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][16/38] batch_size:2 query_len:1792 num_blocks:0 free_mem:15.54 GiB
INFO 08-15 10:15:26 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][17/38] batch_size:2 query_len:1408 num_blocks:0 free_mem:15.54 GiB
INFO 08-15 10:15:28 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][18/38] batch_size:2 query_len:1024 num_blocks:0 free_mem:15.54 GiB
INFO 08-15 10:15:30 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][19/38] batch_size:2 query_len:896 num_blocks:0 free_mem:15.54 GiB
INFO 08-15 10:15:32 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][20/38] batch_size:2 query_len:768 num_blocks:0 free_mem:15.54 GiB
INFO 08-15 10:15:34 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][21/38] batch_size:2 query_len:640 num_blocks:0 free_mem:15.54 GiB
INFO 08-15 10:15:36 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][22/38] batch_size:2 query_len:512 num_blocks:0 free_mem:15.54 GiB
INFO 08-15 10:15:38 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][23/38] batch_size:2 query_len:384 num_blocks:0 free_mem:15.54 GiB
INFO 08-15 10:15:40 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][24/38] batch_size:2 query_len:256 num_blocks:0 free_mem:15.54 GiB
INFO 08-15 10:15:42 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][25/38] batch_size:2 query_len:128 num_blocks:0 free_mem:15.54 GiB
INFO 08-15 10:15:44 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][26/38] batch_size:1 query_len:4096 num_blocks:0 free_mem:15.54 GiB
INFO 08-15 10:15:46 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][27/38] batch_size:1 query_len:3072 num_blocks:0 free_mem:15.54 GiB
INFO 08-15 10:15:48 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][28/38] batch_size:1 query_len:2304 num_blocks:0 free_mem:15.54 GiB
INFO 08-15 10:15:50 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][29/38] batch_size:1 query_len:1792 num_blocks:0 free_mem:15.54 GiB
INFO 08-15 10:15:52 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][30/38] batch_size:1 query_len:1408 num_blocks:0 free_mem:15.54 GiB
INFO 08-15 10:15:54 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][31/38] batch_size:1 query_len:1024 num_blocks:0 free_mem:15.54 GiB
INFO 08-15 10:15:57 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][32/38] batch_size:1 query_len:896 num_blocks:0 free_mem:15.54 GiB
INFO 08-15 10:15:59 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][33/38] batch_size:1 query_len:768 num_blocks:0 free_mem:15.54 GiB
INFO 08-15 10:16:01 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][34/38] batch_size:1 query_len:640 num_blocks:0 free_mem:15.54 GiB
INFO 08-15 10:16:03 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][35/38] batch_size:1 query_len:512 num_blocks:0 free_mem:15.54 GiB
INFO 08-15 10:16:06 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][36/38] batch_size:1 query_len:384 num_blocks:0 free_mem:15.54 GiB
INFO 08-15 10:16:08 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][37/38] batch_size:1 query_len:256 num_blocks:0 free_mem:15.54 GiB
INFO 08-15 10:16:10 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][38/38] batch_size:1 query_len:128 num_blocks:0 free_mem:15.54 GiB
INFO 08-15 10:16:12 [hpu_model_runner.py:3029] [Warmup][Graph/decode][1/72] batch_size:256 query_len:1 num_blocks:1024 free_mem:15.54 GiB
INFO 08-15 10:16:15 [hpu_model_runner.py:3029] [Warmup][Graph/decode][2/72] batch_size:256 query_len:1 num_blocks:896 free_mem:15.54 GiB
INFO 08-15 10:16:18 [hpu_model_runner.py:3029] [Warmup][Graph/decode][3/72] batch_size:256 query_len:1 num_blocks:768 free_mem:15.54 GiB
INFO 08-15 10:16:20 [hpu_model_runner.py:3029] [Warmup][Graph/decode][4/72] batch_size:256 query_len:1 num_blocks:640 free_mem:15.54 GiB
INFO 08-15 10:16:23 [hpu_model_runner.py:3029] [Warmup][Graph/decode][5/72] batch_size:256 query_len:1 num_blocks:512 free_mem:15.54 GiB
INFO 08-15 10:16:26 [hpu_model_runner.py:3029] [Warmup][Graph/decode][6/72] batch_size:256 query_len:1 num_blocks:384 free_mem:15.54 GiB
INFO 08-15 10:16:28 [hpu_model_runner.py:3029] [Warmup][Graph/decode][7/72] batch_size:256 query_len:1 num_blocks:256 free_mem:15.54 GiB
INFO 08-15 10:16:31 [hpu_model_runner.py:3029] [Warmup][Graph/decode][8/72] batch_size:256 query_len:1 num_blocks:128 free_mem:15.54 GiB
INFO 08-15 10:16:34 [hpu_model_runner.py:3029] [Warmup][Graph/decode][9/72] batch_size:128 query_len:1 num_blocks:1024 free_mem:15.54 GiB
INFO 08-15 10:16:37 [hpu_model_runner.py:3029] [Warmup][Graph/decode][10/72] batch_size:128 query_len:1 num_blocks:896 free_mem:15.54 GiB
INFO 08-15 10:16:39 [hpu_model_runner.py:3029] [Warmup][Graph/decode][11/72] batch_size:128 query_len:1 num_blocks:768 free_mem:15.54 GiB
INFO 08-15 10:16:42 [hpu_model_runner.py:3029] [Warmup][Graph/decode][12/72] batch_size:128 query_len:1 num_blocks:640 free_mem:15.54 GiB
INFO 08-15 10:16:45 [hpu_model_runner.py:3029] [Warmup][Graph/decode][13/72] batch_size:128 query_len:1 num_blocks:512 free_mem:15.54 GiB
INFO 08-15 10:16:48 [hpu_model_runner.py:3029] [Warmup][Graph/decode][14/72] batch_size:128 query_len:1 num_blocks:384 free_mem:15.54 GiB
INFO 08-15 10:16:50 [hpu_model_runner.py:3029] [Warmup][Graph/decode][15/72] batch_size:128 query_len:1 num_blocks:256 free_mem:15.54 GiB
INFO 08-15 10:16:53 [hpu_model_runner.py:3029] [Warmup][Graph/decode][16/72] batch_size:128 query_len:1 num_blocks:128 free_mem:15.54 GiB
INFO 08-15 10:16:56 [hpu_model_runner.py:3029] [Warmup][Graph/decode][17/72] batch_size:64 query_len:1 num_blocks:1024 free_mem:15.54 GiB
INFO 08-15 10:16:59 [hpu_model_runner.py:3029] [Warmup][Graph/decode][18/72] batch_size:64 query_len:1 num_blocks:896 free_mem:15.54 GiB
INFO 08-15 10:17:02 [hpu_model_runner.py:3029] [Warmup][Graph/decode][19/72] batch_size:64 query_len:1 num_blocks:768 free_mem:15.54 GiB
INFO 08-15 10:17:05 [hpu_model_runner.py:3029] [Warmup][Graph/decode][20/72] batch_size:64 query_len:1 num_blocks:640 free_mem:15.54 GiB
INFO 08-15 10:17:08 [hpu_model_runner.py:3029] [Warmup][Graph/decode][21/72] batch_size:64 query_len:1 num_blocks:512 free_mem:15.54 GiB
INFO 08-15 10:17:11 [hpu_model_runner.py:3029] [Warmup][Graph/decode][22/72] batch_size:64 query_len:1 num_blocks:384 free_mem:15.54 GiB
INFO 08-15 10:17:13 [hpu_model_runner.py:3029] [Warmup][Graph/decode][23/72] batch_size:64 query_len:1 num_blocks:256 free_mem:15.54 GiB
INFO 08-15 10:17:16 [hpu_model_runner.py:3029] [Warmup][Graph/decode][24/72] batch_size:64 query_len:1 num_blocks:128 free_mem:15.54 GiB
INFO 08-15 10:17:19 [hpu_model_runner.py:3029] [Warmup][Graph/decode][25/72] batch_size:32 query_len:1 num_blocks:1024 free_mem:15.54 GiB
INFO 08-15 10:17:22 [hpu_model_runner.py:3029] [Warmup][Graph/decode][26/72] batch_size:32 query_len:1 num_blocks:896 free_mem:15.54 GiB
INFO 08-15 10:17:25 [hpu_model_runner.py:3029] [Warmup][Graph/decode][27/72] batch_size:32 query_len:1 num_blocks:768 free_mem:15.54 GiB
INFO 08-15 10:17:29 [hpu_model_runner.py:3029] [Warmup][Graph/decode][28/72] batch_size:32 query_len:1 num_blocks:640 free_mem:15.54 GiB
INFO 08-15 10:17:31 [hpu_model_runner.py:3029] [Warmup][Graph/decode][29/72] batch_size:32 query_len:1 num_blocks:512 free_mem:15.54 GiB
INFO 08-15 10:17:34 [hpu_model_runner.py:3029] [Warmup][Graph/decode][30/72] batch_size:32 query_len:1 num_blocks:384 free_mem:15.54 GiB
INFO 08-15 10:17:38 [hpu_model_runner.py:3029] [Warmup][Graph/decode][31/72] batch_size:32 query_len:1 num_blocks:256 free_mem:15.54 GiB
INFO 08-15 10:17:41 [hpu_model_runner.py:3029] [Warmup][Graph/decode][32/72] batch_size:32 query_len:1 num_blocks:128 free_mem:15.54 GiB
INFO 08-15 10:17:44 [hpu_model_runner.py:3029] [Warmup][Graph/decode][33/72] batch_size:16 query_len:1 num_blocks:1024 free_mem:15.54 GiB
INFO 08-15 10:17:47 [hpu_model_runner.py:3029] [Warmup][Graph/decode][34/72] batch_size:16 query_len:1 num_blocks:896 free_mem:15.54 GiB
INFO 08-15 10:17:50 [hpu_model_runner.py:3029] [Warmup][Graph/decode][35/72] batch_size:16 query_len:1 num_blocks:768 free_mem:15.54 GiB
INFO 08-15 10:17:53 [hpu_model_runner.py:3029] [Warmup][Graph/decode][36/72] batch_size:16 query_len:1 num_blocks:640 free_mem:15.54 GiB
INFO 08-15 10:17:56 [hpu_model_runner.py:3029] [Warmup][Graph/decode][37/72] batch_size:16 query_len:1 num_blocks:512 free_mem:15.54 GiB
INFO 08-15 10:18:00 [hpu_model_runner.py:3029] [Warmup][Graph/decode][38/72] batch_size:16 query_len:1 num_blocks:384 free_mem:15.54 GiB
INFO 08-15 10:18:03 [hpu_model_runner.py:3029] [Warmup][Graph/decode][39/72] batch_size:16 query_len:1 num_blocks:256 free_mem:15.54 GiB
INFO 08-15 10:18:06 [hpu_model_runner.py:3029] [Warmup][Graph/decode][40/72] batch_size:16 query_len:1 num_blocks:128 free_mem:15.54 GiB
INFO 08-15 10:18:09 [hpu_model_runner.py:3029] [Warmup][Graph/decode][41/72] batch_size:8 query_len:1 num_blocks:1024 free_mem:15.54 GiB
INFO 08-15 10:18:12 [hpu_model_runner.py:3029] [Warmup][Graph/decode][42/72] batch_size:8 query_len:1 num_blocks:896 free_mem:15.54 GiB
INFO 08-15 10:18:16 [hpu_model_runner.py:3029] [Warmup][Graph/decode][43/72] batch_size:8 query_len:1 num_blocks:768 free_mem:15.54 GiB
INFO 08-15 10:18:19 [hpu_model_runner.py:3029] [Warmup][Graph/decode][44/72] batch_size:8 query_len:1 num_blocks:640 free_mem:15.54 GiB
INFO 08-15 10:18:22 [hpu_model_runner.py:3029] [Warmup][Graph/decode][45/72] batch_size:8 query_len:1 num_blocks:512 free_mem:15.54 GiB
INFO 08-15 10:18:26 [hpu_model_runner.py:3029] [Warmup][Graph/decode][46/72] batch_size:8 query_len:1 num_blocks:384 free_mem:15.54 GiB
INFO 08-15 10:18:29 [hpu_model_runner.py:3029] [Warmup][Graph/decode][47/72] batch_size:8 query_len:1 num_blocks:256 free_mem:15.54 GiB
INFO 08-15 10:18:33 [hpu_model_runner.py:3029] [Warmup][Graph/decode][48/72] batch_size:8 query_len:1 num_blocks:128 free_mem:15.54 GiB
INFO 08-15 10:18:36 [hpu_model_runner.py:3029] [Warmup][Graph/decode][49/72] batch_size:4 query_len:1 num_blocks:1024 free_mem:15.54 GiB
INFO 08-15 10:18:40 [hpu_model_runner.py:3029] [Warmup][Graph/decode][50/72] batch_size:4 query_len:1 num_blocks:896 free_mem:15.54 GiB
INFO 08-15 10:18:43 [hpu_model_runner.py:3029] [Warmup][Graph/decode][51/72] batch_size:4 query_len:1 num_blocks:768 free_mem:15.54 GiB
INFO 08-15 10:18:46 [hpu_model_runner.py:3029] [Warmup][Graph/decode][52/72] batch_size:4 query_len:1 num_blocks:640 free_mem:15.54 GiB
INFO 08-15 10:18:50 [hpu_model_runner.py:3029] [Warmup][Graph/decode][53/72] batch_size:4 query_len:1 num_blocks:512 free_mem:15.54 GiB
INFO 08-15 10:18:53 [hpu_model_runner.py:3029] [Warmup][Graph/decode][54/72] batch_size:4 query_len:1 num_blocks:384 free_mem:15.54 GiB
INFO 08-15 10:18:57 [hpu_model_runner.py:3029] [Warmup][Graph/decode][55/72] batch_size:4 query_len:1 num_blocks:256 free_mem:15.54 GiB
INFO 08-15 10:19:00 [hpu_model_runner.py:3029] [Warmup][Graph/decode][56/72] batch_size:4 query_len:1 num_blocks:128 free_mem:15.54 GiB
INFO 08-15 10:19:04 [hpu_model_runner.py:3029] [Warmup][Graph/decode][57/72] batch_size:2 query_len:1 num_blocks:1024 free_mem:15.54 GiB
INFO 08-15 10:19:08 [hpu_model_runner.py:3029] [Warmup][Graph/decode][58/72] batch_size:2 query_len:1 num_blocks:896 free_mem:15.54 GiB
INFO 08-15 10:19:11 [hpu_model_runner.py:3029] [Warmup][Graph/decode][59/72] batch_size:2 query_len:1 num_blocks:768 free_mem:15.54 GiB
INFO 08-15 10:19:15 [hpu_model_runner.py:3029] [Warmup][Graph/decode][60/72] batch_size:2 query_len:1 num_blocks:640 free_mem:15.54 GiB
INFO 08-15 10:19:19 [hpu_model_runner.py:3029] [Warmup][Graph/decode][61/72] batch_size:2 query_len:1 num_blocks:512 free_mem:15.54 GiB
INFO 08-15 10:19:22 [hpu_model_runner.py:3029] [Warmup][Graph/decode][62/72] batch_size:2 query_len:1 num_blocks:384 free_mem:15.54 GiB
INFO 08-15 10:19:26 [hpu_model_runner.py:3029] [Warmup][Graph/decode][63/72] batch_size:2 query_len:1 num_blocks:256 free_mem:15.54 GiB
INFO 08-15 10:19:30 [hpu_model_runner.py:3029] [Warmup][Graph/decode][64/72] batch_size:2 query_len:1 num_blocks:128 free_mem:15.54 GiB
INFO 08-15 10:19:33 [hpu_model_runner.py:3029] [Warmup][Graph/decode][65/72] batch_size:1 query_len:1 num_blocks:1024 free_mem:15.54 GiB
INFO 08-15 10:19:37 [hpu_model_runner.py:3029] [Warmup][Graph/decode][66/72] batch_size:1 query_len:1 num_blocks:896 free_mem:15.54 GiB
INFO 08-15 10:19:41 [hpu_model_runner.py:3029] [Warmup][Graph/decode][67/72] batch_size:1 query_len:1 num_blocks:768 free_mem:15.54 GiB
INFO 08-15 10:19:45 [hpu_model_runner.py:3029] [Warmup][Graph/decode][68/72] batch_size:1 query_len:1 num_blocks:640 free_mem:15.54 GiB
INFO 08-15 10:19:49 [hpu_model_runner.py:3029] [Warmup][Graph/decode][69/72] batch_size:1 query_len:1 num_blocks:512 free_mem:15.54 GiB
INFO 08-15 10:19:53 [hpu_model_runner.py:3029] [Warmup][Graph/decode][70/72] batch_size:1 query_len:1 num_blocks:384 free_mem:15.54 GiB
INFO 08-15 10:19:56 [hpu_model_runner.py:3029] [Warmup][Graph/decode][71/72] batch_size:1 query_len:1 num_blocks:256 free_mem:15.54 GiB
INFO 08-15 10:20:00 [hpu_model_runner.py:3029] [Warmup][Graph/decode][72/72] batch_size:1 query_len:1 num_blocks:128 free_mem:15.54 GiB
INFO 08-15 10:20:04 [hpu_model_runner.py:3152] Prompt captured:38 (100.0%) used_mem:31.75 KiB
INFO 08-15 10:20:04 [hpu_model_runner.py:3152] Decode captured:72 (100.0%) used_mem:0 B
INFO 08-15 10:20:04 [hpu_model_runner.py:3280] Warmup finished in 306 secs, allocated 31.75 KiB of device memory
INFO 08-15 10:20:04 [llm_engine.py:431] init engine (profile, create kv cache, warmup model) took 308.79 seconds
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
WARNING 08-15 10:20:04 [config.py:1364] Default sampling parameters have been overridden by the model's Hugging Face generation config recommended from the model creator. If this is not intended, please relaunch vLLM instance with `--generation-config vllm`.
INFO 08-15 10:20:04 [serving_chat.py:118] Using default chat sampling params from model: {'temperature': 0.9, 'top_p': 0.6}
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
INFO 08-15 10:20:04 [serving_completion.py:66] Using default completion sampling params from model: {'temperature': 0.9, 'top_p': 0.6}
INFO 08-15 10:20:04 [api_server.py:1351] Starting vLLM API server 0 on http://0.0.0.0:8000
INFO 08-15 10:20:04 [launcher.py:29] Available routes are:
INFO 08-15 10:20:04 [launcher.py:37] Route: /openapi.json, Methods: HEAD, GET
INFO 08-15 10:20:04 [launcher.py:37] Route: /docs, Methods: HEAD, GET
INFO 08-15 10:20:04 [launcher.py:37] Route: /docs/oauth2-redirect, Methods: HEAD, GET
INFO 08-15 10:20:04 [launcher.py:37] Route: /redoc, Methods: HEAD, GET
INFO 08-15 10:20:04 [launcher.py:37] Route: /health, Methods: GET
INFO 08-15 10:20:04 [launcher.py:37] Route: /load, Methods: GET
INFO 08-15 10:20:04 [launcher.py:37] Route: /ping, Methods: POST
INFO 08-15 10:20:04 [launcher.py:37] Route: /ping, Methods: GET
INFO 08-15 10:20:04 [launcher.py:37] Route: /tokenize, Methods: POST
INFO 08-15 10:20:04 [launcher.py:37] Route: /detokenize, Methods: POST
INFO 08-15 10:20:04 [launcher.py:37] Route: /v1/models, Methods: GET
INFO 08-15 10:20:04 [launcher.py:37] Route: /version, Methods: GET
INFO 08-15 10:20:04 [launcher.py:37] Route: /v1/chat/completions, Methods: POST
INFO 08-15 10:20:04 [launcher.py:37] Route: /v1/completions, Methods: POST
INFO 08-15 10:20:04 [launcher.py:37] Route: /v1/embeddings, Methods: POST
INFO 08-15 10:20:04 [launcher.py:37] Route: /pooling, Methods: POST
INFO 08-15 10:20:04 [launcher.py:37] Route: /classify, Methods: POST
INFO 08-15 10:20:04 [launcher.py:37] Route: /score, Methods: POST
INFO 08-15 10:20:04 [launcher.py:37] Route: /v1/score, Methods: POST
INFO 08-15 10:20:04 [launcher.py:37] Route: /v1/audio/transcriptions, Methods: POST
INFO 08-15 10:20:04 [launcher.py:37] Route: /rerank, Methods: POST
INFO 08-15 10:20:04 [launcher.py:37] Route: /v1/rerank, Methods: POST
INFO 08-15 10:20:04 [launcher.py:37] Route: /v2/rerank, Methods: POST
INFO 08-15 10:20:04 [launcher.py:37] Route: /invocations, Methods: POST
INFO 08-15 10:20:04 [launcher.py:37] Route: /metrics, Methods: GET
INFO:     Started server process [3862924]
INFO:     Waiting for application startup.
INFO:     Application startup complete.
INFO 08-15 11:31:45 [logger.py:43] Received request cmpl-6073fa0bc83c48c7a6ce585a020f5e2e-0: prompt: 'What is LLaMA 2 and how does it work?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 365, 5661, 1529, 29871, 29906, 322, 920, 947, 372, 664, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:52590 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-15 11:31:45 [engine.py:331] Added request cmpl-6073fa0bc83c48c7a6ce585a020f5e2e-0.
INFO 08-15 11:31:45 [metrics.py:417] Avg prompt throughput: 1.8 tokens/s, Avg generation throughput: 0.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-15 11:31:50 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 78.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.5%, CPU KV cache usage: 0.0%.
INFO 08-15 11:31:55 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 78.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.8%, CPU KV cache usage: 0.0%.
INFO 08-15 11:31:58 [logger.py:43] Received request cmpl-da5ca3d315884f519438f00a431ef63c-0: prompt: 'Explain the architecture of a transformer model.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 12027, 7420, 278, 11258, 310, 263, 4327, 261, 1904, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:56676 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-15 11:31:58 [engine.py:331] Added request cmpl-da5ca3d315884f519438f00a431ef63c-0.
INFO 08-15 11:32:00 [metrics.py:417] Avg prompt throughput: 2.2 tokens/s, Avg generation throughput: 78.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 08-15 11:32:05 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 78.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.6%, CPU KV cache usage: 0.0%.
INFO 08-15 11:32:10 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 78.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.9%, CPU KV cache usage: 0.0%.
INFO 08-15 11:32:11 [logger.py:43] Received request cmpl-03d776585068483d96ecb60c26b630b3-0: prompt: 'What are the core differences between CNN and RNN?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 526, 278, 7136, 12651, 1546, 29696, 322, 390, 10262, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:51656 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-15 11:32:11 [engine.py:331] Added request cmpl-03d776585068483d96ecb60c26b630b3-0.
INFO 08-15 11:32:15 [metrics.py:417] Avg prompt throughput: 2.4 tokens/s, Avg generation throughput: 78.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.4%, CPU KV cache usage: 0.0%.
INFO 08-15 11:32:20 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 78.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.8%, CPU KV cache usage: 0.0%.
INFO 08-15 11:32:23 [logger.py:43] Received request cmpl-9d71c9c1538c482ebe3d84571f10e5f6-0: prompt: 'Describe how attention mechanism improves translation models.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 20355, 915, 920, 8570, 13336, 4857, 1960, 13962, 4733, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:34622 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-15 11:32:23 [engine.py:331] Added request cmpl-9d71c9c1538c482ebe3d84571f10e5f6-0.
INFO 08-15 11:32:25 [metrics.py:417] Avg prompt throughput: 2.2 tokens/s, Avg generation throughput: 78.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 08-15 11:32:30 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 78.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.6%, CPU KV cache usage: 0.0%.
INFO 08-15 11:32:35 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 78.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.9%, CPU KV cache usage: 0.0%.
INFO 08-15 11:32:36 [logger.py:43] Received request cmpl-e7e8e2dca54c4f2c812207127c18f1ad-0: prompt: 'What is the role of positional encoding in transformers?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 278, 6297, 310, 2602, 284, 8025, 297, 4327, 414, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:34596 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-15 11:32:36 [engine.py:331] Added request cmpl-e7e8e2dca54c4f2c812207127c18f1ad-0.
INFO 08-15 11:32:40 [metrics.py:417] Avg prompt throughput: 2.6 tokens/s, Avg generation throughput: 78.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.4%, CPU KV cache usage: 0.0%.
INFO 08-15 11:32:45 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 78.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.7%, CPU KV cache usage: 0.0%.
INFO 08-15 11:32:49 [logger.py:43] Received request cmpl-10c8ae87c47e42fb810baca52cc1f894-0: prompt: 'How does self-attention differ from cross-attention?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 947, 1583, 29899, 1131, 2509, 1163, 515, 4891, 29899, 1131, 2509, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:39978 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-15 11:32:49 [engine.py:331] Added request cmpl-10c8ae87c47e42fb810baca52cc1f894-0.
INFO 08-15 11:32:50 [metrics.py:417] Avg prompt throughput: 2.8 tokens/s, Avg generation throughput: 78.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 08-15 11:32:55 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 78.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.6%, CPU KV cache usage: 0.0%.
INFO 08-15 11:33:00 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 78.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.9%, CPU KV cache usage: 0.0%.
INFO 08-15 11:33:01 [logger.py:43] Received request cmpl-a18098e3590a485a9d9c7b7aa8165048-0: prompt: 'Explain the concept of tokenization in NLP.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 12027, 7420, 278, 6964, 310, 5993, 2133, 297, 405, 13208, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:59960 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-15 11:33:01 [engine.py:331] Added request cmpl-a18098e3590a485a9d9c7b7aa8165048-0.
INFO 08-15 11:33:05 [metrics.py:417] Avg prompt throughput: 2.4 tokens/s, Avg generation throughput: 78.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.4%, CPU KV cache usage: 0.0%.
INFO 08-15 11:33:10 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 78.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.7%, CPU KV cache usage: 0.0%.
INFO 08-15 11:33:14 [logger.py:43] Received request cmpl-b554b947b0604978ac895e02530b312b-0: prompt: 'What is the vanishing gradient problem and how to mitigate it?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 278, 1109, 14424, 16030, 1108, 322, 920, 304, 1380, 335, 403, 372, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:38742 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-15 11:33:14 [engine.py:331] Added request cmpl-b554b947b0604978ac895e02530b312b-0.
INFO 08-15 11:33:15 [metrics.py:417] Avg prompt throughput: 3.2 tokens/s, Avg generation throughput: 78.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-15 11:33:20 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 78.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.6%, CPU KV cache usage: 0.0%.
INFO 08-15 11:33:25 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 78.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.9%, CPU KV cache usage: 0.0%.
INFO 08-15 11:33:27 [logger.py:43] Received request cmpl-b4b555c28c7244ec9413473b7d60c007-0: prompt: 'Describe how Gaudi architecture differs from NVIDIA GPUs.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 20355, 915, 920, 10415, 4749, 11258, 2923, 414, 515, 405, 13044, 10764, 22796, 29879, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:49668 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-15 11:33:27 [engine.py:331] Added request cmpl-b4b555c28c7244ec9413473b7d60c007-0.
INFO 08-15 11:33:30 [metrics.py:417] Avg prompt throughput: 3.2 tokens/s, Avg generation throughput: 78.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.4%, CPU KV cache usage: 0.0%.
INFO 08-15 11:33:36 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 78.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.7%, CPU KV cache usage: 0.0%.
INFO 08-15 11:33:40 [logger.py:43] Received request cmpl-ee3d8f7fa61840ffb7940452e7c96a3f-0: prompt: 'What is model parallelism and how does it help scale large models?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 1904, 8943, 1608, 322, 920, 947, 372, 1371, 6287, 2919, 4733, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:54736 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-15 11:33:40 [engine.py:331] Added request cmpl-ee3d8f7fa61840ffb7940452e7c96a3f-0.
INFO 08-15 11:33:41 [metrics.py:417] Avg prompt throughput: 3.0 tokens/s, Avg generation throughput: 78.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-15 11:33:46 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 78.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.5%, CPU KV cache usage: 0.0%.
INFO 08-15 11:33:51 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 78.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.8%, CPU KV cache usage: 0.0%.
INFO 08-15 11:33:52 [logger.py:43] Received request cmpl-e6de2fce61d24c698fb8fb6dca9ea0c4-0: prompt: 'Explain the difference between GPT and BERT architectures.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 12027, 7420, 278, 4328, 1546, 402, 7982, 322, 350, 20161, 6956, 1973, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:59554 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-15 11:33:52 [engine.py:331] Added request cmpl-e6de2fce61d24c698fb8fb6dca9ea0c4-0.
INFO 08-15 11:33:56 [metrics.py:417] Avg prompt throughput: 2.8 tokens/s, Avg generation throughput: 78.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.4%, CPU KV cache usage: 0.0%.
INFO 08-15 11:34:01 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 78.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.7%, CPU KV cache usage: 0.0%.
INFO 08-15 11:34:05 [logger.py:43] Received request cmpl-58f996f894094131855a6f2eaf2f95aa-0: prompt: 'How does prompt engineering affect language model outputs?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 947, 9508, 21639, 6602, 4086, 1904, 14391, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:45404 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-15 11:34:05 [engine.py:331] Added request cmpl-58f996f894094131855a6f2eaf2f95aa-0.
INFO 08-15 11:34:06 [metrics.py:417] Avg prompt throughput: 2.0 tokens/s, Avg generation throughput: 78.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-15 11:34:11 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 78.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.5%, CPU KV cache usage: 0.0%.
INFO 08-15 11:34:12 [logger.py:43] Received request cmpl-8852f0d120c44c309381f033b1599033-0: prompt: 'What is the function of layer normalization in deep networks?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 278, 740, 310, 7546, 4226, 2133, 297, 6483, 14379, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:37462 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-15 11:34:12 [engine.py:331] Added request cmpl-8852f0d120c44c309381f033b1599033-0.
INFO 08-15 11:34:16 [metrics.py:417] Avg prompt throughput: 2.6 tokens/s, Avg generation throughput: 78.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.4%, CPU KV cache usage: 0.0%.
INFO 08-15 11:34:20 [logger.py:43] Received request cmpl-0a1c5f9b333a4b08a562091dc4905577-0: prompt: 'Explain causal language modeling with examples.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 12027, 7420, 3269, 284, 4086, 1904, 292, 411, 6455, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:37474 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-15 11:34:20 [engine.py:331] Added request cmpl-0a1c5f9b333a4b08a562091dc4905577-0.
INFO 08-15 11:34:21 [metrics.py:417] Avg prompt throughput: 2.2 tokens/s, Avg generation throughput: 78.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-15 11:34:26 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 78.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.5%, CPU KV cache usage: 0.0%.
INFO 08-15 11:34:31 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 78.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.8%, CPU KV cache usage: 0.0%.
INFO 08-15 11:34:33 [logger.py:43] Received request cmpl-c3af1ebf02064aa094440bdcdb05ea83-0: prompt: 'What is quantization and how does it improve inference performance?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 4323, 2133, 322, 920, 947, 372, 11157, 27262, 4180, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:46130 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-15 11:34:33 [engine.py:331] Added request cmpl-c3af1ebf02064aa094440bdcdb05ea83-0.
INFO 08-15 11:34:36 [metrics.py:417] Avg prompt throughput: 2.6 tokens/s, Avg generation throughput: 78.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 08-15 11:34:41 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 78.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.6%, CPU KV cache usage: 0.0%.
INFO 08-15 11:34:46 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 78.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.9%, CPU KV cache usage: 0.0%.
INFO 08-15 11:34:46 [logger.py:43] Received request cmpl-69b5a925281d4b9f9992b9715160c33e-0: prompt: 'Describe how fine-tuning works for large language models.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 20355, 915, 920, 2691, 29899, 29873, 27964, 1736, 363, 2919, 4086, 4733, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:60802 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-15 11:34:46 [engine.py:331] Added request cmpl-69b5a925281d4b9f9992b9715160c33e-0.
INFO 08-15 11:34:51 [metrics.py:417] Avg prompt throughput: 2.8 tokens/s, Avg generation throughput: 78.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.5%, CPU KV cache usage: 0.0%.
INFO 08-15 11:34:56 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 78.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.8%, CPU KV cache usage: 0.0%.
INFO 08-15 11:34:58 [logger.py:43] Received request cmpl-238da0908ebe4bc4a30e5ea88b8e5493-0: prompt: 'What are LoRA adapters and how do they help in parameter-efficient training?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 526, 4309, 4717, 594, 481, 2153, 322, 920, 437, 896, 1371, 297, 3443, 29899, 8462, 6694, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:47934 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-15 11:34:58 [engine.py:331] Added request cmpl-238da0908ebe4bc4a30e5ea88b8e5493-0.
INFO 08-15 11:35:01 [metrics.py:417] Avg prompt throughput: 3.8 tokens/s, Avg generation throughput: 78.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 08-15 11:35:06 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 78.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.6%, CPU KV cache usage: 0.0%.
INFO 08-15 11:35:11 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 78.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.9%, CPU KV cache usage: 0.0%.
INFO 08-15 11:35:11 [logger.py:43] Received request cmpl-bf245cd7503a4b42974fde1bf2426600-0: prompt: 'Explain the purpose of rotary positional embeddings.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 12027, 7420, 278, 6437, 310, 5731, 653, 2602, 284, 8297, 29881, 886, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:56478 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-15 11:35:11 [engine.py:331] Added request cmpl-bf245cd7503a4b42974fde1bf2426600-0.
INFO 08-15 11:35:16 [metrics.py:417] Avg prompt throughput: 2.8 tokens/s, Avg generation throughput: 78.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.4%, CPU KV cache usage: 0.0%.
INFO 08-15 11:35:21 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 78.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.8%, CPU KV cache usage: 0.0%.
INFO 08-15 11:35:24 [logger.py:43] Received request cmpl-e0a00f238dc649529d97e811f63b23b0-0: prompt: 'How does DeepSpeed improve model training efficiency?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 947, 21784, 26539, 11157, 1904, 6694, 19201, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:34378 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-15 11:35:24 [engine.py:331] Added request cmpl-e0a00f238dc649529d97e811f63b23b0-0.
INFO 08-15 11:35:26 [metrics.py:417] Avg prompt throughput: 2.0 tokens/s, Avg generation throughput: 78.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 08-15 11:35:26 [logger.py:43] Received request cmpl-a273ee91f0c3489abab76ae3aa649860-0: prompt: 'Compare data parallelism and pipeline parallelism.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 3831, 598, 848, 8943, 1608, 322, 16439, 8943, 1608, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:34390 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-15 11:35:26 [engine.py:331] Added request cmpl-a273ee91f0c3489abab76ae3aa649860-0.
INFO 08-15 11:35:31 [metrics.py:417] Avg prompt throughput: 2.2 tokens/s, Avg generation throughput: 78.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.4%, CPU KV cache usage: 0.0%.
INFO 08-15 11:35:36 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 78.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.7%, CPU KV cache usage: 0.0%.
INFO 08-15 11:35:39 [logger.py:43] Received request cmpl-9212a80824e342c19fc8f3aebd0d2532-0: prompt: 'What is ALiBi positional encoding?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 14445, 29875, 20517, 2602, 284, 8025, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:39790 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-15 11:35:39 [engine.py:331] Added request cmpl-9212a80824e342c19fc8f3aebd0d2532-0.
INFO 08-15 11:35:41 [metrics.py:417] Avg prompt throughput: 2.0 tokens/s, Avg generation throughput: 78.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 08-15 11:35:46 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 78.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.6%, CPU KV cache usage: 0.0%.
INFO 08-15 11:35:51 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 78.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.9%, CPU KV cache usage: 0.0%.
INFO 08-15 11:35:52 [logger.py:43] Received request cmpl-e7a3b5e58c1d44949034af049b816161-0: prompt: 'Describe the purpose of HF Transformers library.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 20355, 915, 278, 6437, 310, 379, 29943, 4103, 689, 414, 3489, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:57454 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-15 11:35:52 [engine.py:331] Added request cmpl-e7a3b5e58c1d44949034af049b816161-0.
INFO 08-15 11:35:56 [metrics.py:417] Avg prompt throughput: 2.6 tokens/s, Avg generation throughput: 78.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.4%, CPU KV cache usage: 0.0%.
INFO 08-15 11:36:01 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 78.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.7%, CPU KV cache usage: 0.0%.
INFO 08-15 11:36:04 [logger.py:43] Received request cmpl-fd3b8080bd874825a7e744e1950b1128-0: prompt: 'How do attention heads capture contextual meaning?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 437, 8570, 15883, 10446, 3030, 950, 6593, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:47214 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-15 11:36:04 [engine.py:331] Added request cmpl-fd3b8080bd874825a7e744e1950b1128-0.
INFO 08-15 11:36:06 [metrics.py:417] Avg prompt throughput: 2.0 tokens/s, Avg generation throughput: 78.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-15 11:36:11 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 78.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.6%, CPU KV cache usage: 0.0%.
INFO 08-15 11:36:16 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 78.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.9%, CPU KV cache usage: 0.0%.
INFO 08-15 11:36:17 [logger.py:43] Received request cmpl-5063f3f72b4243c8b5576a5df788057e-0: prompt: 'What is dynamic batching in inference servers?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 7343, 9853, 292, 297, 27262, 12424, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:44608 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-15 11:36:17 [engine.py:331] Added request cmpl-5063f3f72b4243c8b5576a5df788057e-0.
INFO 08-15 11:36:21 [metrics.py:417] Avg prompt throughput: 2.0 tokens/s, Avg generation throughput: 78.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.4%, CPU KV cache usage: 0.0%.
INFO 08-15 11:36:26 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 78.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.7%, CPU KV cache usage: 0.0%.
INFO 08-15 11:36:30 [logger.py:43] Received request cmpl-4ae206a8866949bcaf883ef02a7231b4-0: prompt: 'Explain greedy decoding vs beam search vs sampling.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 12027, 7420, 1395, 7584, 1602, 3689, 7186, 22913, 2740, 7186, 23460, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:59738 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-15 11:36:30 [engine.py:331] Added request cmpl-4ae206a8866949bcaf883ef02a7231b4-0.
INFO 08-15 11:36:31 [metrics.py:417] Avg prompt throughput: 2.6 tokens/s, Avg generation throughput: 78.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-15 11:36:36 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 78.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.5%, CPU KV cache usage: 0.0%.
INFO 08-15 11:36:41 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 78.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.8%, CPU KV cache usage: 0.0%.
INFO 08-15 11:36:42 [logger.py:43] Received request cmpl-813c2f6fb4e84da4bc34e92a37ac996a-0: prompt: 'What is perplexity in language models and why is it important?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 639, 10709, 537, 297, 4086, 4733, 322, 2020, 338, 372, 4100, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:37596 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-15 11:36:42 [engine.py:331] Added request cmpl-813c2f6fb4e84da4bc34e92a37ac996a-0.
INFO 08-15 11:36:46 [metrics.py:417] Avg prompt throughput: 3.0 tokens/s, Avg generation throughput: 78.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.4%, CPU KV cache usage: 0.0%.
INFO 08-15 11:36:51 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 78.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.7%, CPU KV cache usage: 0.0%.
INFO 08-15 11:36:55 [logger.py:43] Received request cmpl-fcce14bd134d4ad996f7b2dba19d47a9-0: prompt: 'How does mixed precision training benefit model training?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 947, 12849, 16716, 6694, 14169, 1904, 6694, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:32922 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-15 11:36:55 [engine.py:331] Added request cmpl-fcce14bd134d4ad996f7b2dba19d47a9-0.
INFO 08-15 11:36:56 [metrics.py:417] Avg prompt throughput: 2.0 tokens/s, Avg generation throughput: 78.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-15 11:37:01 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 78.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.5%, CPU KV cache usage: 0.0%.
INFO 08-15 11:37:06 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 78.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.8%, CPU KV cache usage: 0.0%.
INFO 08-15 11:37:08 [logger.py:43] Received request cmpl-7272ffe109bd420589dd04b303fcda6f-0: prompt: 'What is the function of the softmax layer in transformers?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 278, 740, 310, 278, 4964, 3317, 7546, 297, 4327, 414, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:52918 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-15 11:37:08 [engine.py:331] Added request cmpl-7272ffe109bd420589dd04b303fcda6f-0.
INFO 08-15 11:37:11 [metrics.py:417] Avg prompt throughput: 2.8 tokens/s, Avg generation throughput: 78.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 08-15 11:37:16 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 78.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.7%, CPU KV cache usage: 0.0%.
INFO 08-15 11:37:20 [logger.py:43] Received request cmpl-24f62ab1383d45bbb6d7b934ea18e6c5-0: prompt: "Explain embeddings and how they're used in NLP models.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 12027, 7420, 8297, 29881, 886, 322, 920, 896, 29915, 276, 1304, 297, 405, 13208, 4733, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:34874 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-15 11:37:20 [engine.py:331] Added request cmpl-24f62ab1383d45bbb6d7b934ea18e6c5-0.
INFO 08-15 11:37:21 [metrics.py:417] Avg prompt throughput: 3.4 tokens/s, Avg generation throughput: 78.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-15 11:37:26 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 78.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.5%, CPU KV cache usage: 0.0%.
INFO 08-15 11:37:31 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 78.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.8%, CPU KV cache usage: 0.0%.
INFO 08-15 11:37:33 [logger.py:43] Received request cmpl-1a44f62fed374836892cb93f0dffbc0c-0: prompt: "Describe the function of the tokenizer's vocabulary.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 20355, 915, 278, 740, 310, 278, 5993, 3950, 29915, 29879, 7931, 370, 352, 653, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:52058 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-15 11:37:33 [engine.py:331] Added request cmpl-1a44f62fed374836892cb93f0dffbc0c-0.
INFO 08-15 11:37:36 [metrics.py:417] Avg prompt throughput: 3.2 tokens/s, Avg generation throughput: 78.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 08-15 11:37:41 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 78.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.6%, CPU KV cache usage: 0.0%.
INFO 08-15 11:37:46 [logger.py:43] Received request cmpl-e57b2ddf57304fc7a8eb888f7c4a30dc-0: prompt: 'What is causal masking in transformer decoders?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 3269, 284, 11105, 292, 297, 4327, 261, 1602, 397, 414, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:42194 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-15 11:37:46 [engine.py:331] Added request cmpl-e57b2ddf57304fc7a8eb888f7c4a30dc-0.
INFO 08-15 11:37:46 [metrics.py:417] Avg prompt throughput: 2.8 tokens/s, Avg generation throughput: 78.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-15 11:37:51 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 78.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.5%, CPU KV cache usage: 0.0%.
INFO 08-15 11:37:56 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 78.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.8%, CPU KV cache usage: 0.0%.
INFO 08-15 11:37:59 [logger.py:43] Received request cmpl-74398194d51e4336b74b1a186c8b3213-0: prompt: 'How does gradient checkpointing reduce memory usage?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 947, 16030, 1423, 3149, 292, 10032, 3370, 8744, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:32914 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-15 11:37:59 [engine.py:331] Added request cmpl-74398194d51e4336b74b1a186c8b3213-0.
INFO 08-15 11:38:01 [metrics.py:417] Avg prompt throughput: 2.2 tokens/s, Avg generation throughput: 78.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 08-15 11:38:06 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 78.9 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-15 11:38:06 [logger.py:43] Received request cmpl-4988e23f033e42ec9dc9e7305a0bee0d-0: prompt: 'Explain the difference between encoder-decoder and decoder-only models.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 12027, 7420, 278, 4328, 1546, 2094, 6119, 29899, 7099, 6119, 322, 1602, 6119, 29899, 6194, 4733, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:39382 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-15 11:38:06 [engine.py:331] Added request cmpl-4988e23f033e42ec9dc9e7305a0bee0d-0.
INFO 08-15 11:38:11 [metrics.py:417] Avg prompt throughput: 3.6 tokens/s, Avg generation throughput: 78.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.5%, CPU KV cache usage: 0.0%.
INFO 08-15 11:38:16 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 78.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.8%, CPU KV cache usage: 0.0%.
INFO 08-15 11:38:19 [logger.py:43] Received request cmpl-b31980bbdf24411ea7e09bb7aba0d8cf-0: prompt: 'What is zero-shot vs few-shot vs fine-tuned inference?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 5225, 29899, 8962, 7186, 2846, 29899, 8962, 7186, 2691, 29899, 29873, 348, 287, 27262, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:52658 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-15 11:38:19 [engine.py:331] Added request cmpl-b31980bbdf24411ea7e09bb7aba0d8cf-0.
INFO 08-15 11:38:21 [metrics.py:417] Avg prompt throughput: 3.6 tokens/s, Avg generation throughput: 78.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 08-15 11:38:26 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 78.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.6%, CPU KV cache usage: 0.0%.
INFO 08-15 11:38:31 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 78.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.9%, CPU KV cache usage: 0.0%.
INFO 08-15 11:38:31 [logger.py:43] Received request cmpl-cde7bbe0c5c5435caccb9033e6b514b6-0: prompt: 'What are prompt templates and why do they matter?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 526, 9508, 17475, 322, 2020, 437, 896, 4383, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:47966 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-15 11:38:31 [engine.py:331] Added request cmpl-cde7bbe0c5c5435caccb9033e6b514b6-0.
INFO 08-15 11:38:36 [metrics.py:417] Avg prompt throughput: 2.2 tokens/s, Avg generation throughput: 78.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.4%, CPU KV cache usage: 0.0%.
INFO 08-15 11:38:41 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 78.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.8%, CPU KV cache usage: 0.0%.
INFO 08-15 11:38:44 [logger.py:43] Received request cmpl-bdf77ad5d2fe424a87ad2f9bcd53c69e-0: prompt: 'Explain the role of BOS and EOS tokens in transformers.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 12027, 7420, 278, 6297, 310, 350, 3267, 322, 382, 3267, 18897, 297, 4327, 414, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:50012 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-15 11:38:44 [engine.py:331] Added request cmpl-bdf77ad5d2fe424a87ad2f9bcd53c69e-0.
INFO 08-15 11:38:46 [metrics.py:417] Avg prompt throughput: 3.2 tokens/s, Avg generation throughput: 78.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 08-15 11:38:51 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 78.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.6%, CPU KV cache usage: 0.0%.
INFO 08-15 11:38:56 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 78.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.9%, CPU KV cache usage: 0.0%.
INFO 08-15 11:38:57 [logger.py:43] Received request cmpl-e644577b2e29445c8711d55252ca0135-0: prompt: 'What is a floating-point tensor and how is it represented?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 263, 16526, 29899, 3149, 12489, 322, 920, 338, 372, 9875, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:57258 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-15 11:38:57 [engine.py:331] Added request cmpl-e644577b2e29445c8711d55252ca0135-0.
INFO 08-15 11:39:01 [metrics.py:417] Avg prompt throughput: 2.8 tokens/s, Avg generation throughput: 78.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.4%, CPU KV cache usage: 0.0%.
INFO 08-15 11:39:06 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 78.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.7%, CPU KV cache usage: 0.0%.
INFO 08-15 11:39:09 [logger.py:43] Received request cmpl-9a434579822149c9bfe7039633684755-0: prompt: 'What are hypernetworks in neural architectures?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 526, 11266, 11618, 29879, 297, 19677, 6956, 1973, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:45892 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-15 11:39:09 [engine.py:331] Added request cmpl-9a434579822149c9bfe7039633684755-0.
INFO 08-15 11:39:11 [metrics.py:417] Avg prompt throughput: 2.2 tokens/s, Avg generation throughput: 78.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 08-15 11:39:16 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 78.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.6%, CPU KV cache usage: 0.0%.
INFO 08-15 11:39:21 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 78.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.9%, CPU KV cache usage: 0.0%.
INFO 08-15 11:39:22 [logger.py:43] Received request cmpl-f655a3ee5351483fa45caf85d5269d22-0: prompt: 'Explain MoE (Mixture of Experts) architecture.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 12027, 7420, 4546, 29923, 313, 29924, 29875, 15546, 310, 28224, 1372, 29897, 11258, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:59624 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-15 11:39:22 [engine.py:331] Added request cmpl-f655a3ee5351483fa45caf85d5269d22-0.
INFO 08-15 11:39:26 [metrics.py:417] Avg prompt throughput: 3.0 tokens/s, Avg generation throughput: 78.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.4%, CPU KV cache usage: 0.0%.
INFO 08-15 11:39:31 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 78.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.7%, CPU KV cache usage: 0.0%.
INFO 08-15 11:39:35 [logger.py:43] Received request cmpl-7266193063ec4bfd9e42e7d31be24780-0: prompt: 'What is distillation in model compression?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 1320, 453, 362, 297, 1904, 24221, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:40688 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-15 11:39:35 [engine.py:331] Added request cmpl-7266193063ec4bfd9e42e7d31be24780-0.
INFO 08-15 11:39:36 [metrics.py:417] Avg prompt throughput: 2.0 tokens/s, Avg generation throughput: 78.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-15 11:39:41 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 78.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.5%, CPU KV cache usage: 0.0%.
INFO 08-15 11:39:46 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 78.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.9%, CPU KV cache usage: 0.0%.
INFO 08-15 11:39:48 [logger.py:43] Received request cmpl-247afa14db9249ecb1789712bdf53658-0: prompt: 'How do you implement top-k and top-p sampling?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 437, 366, 2334, 2246, 29899, 29895, 322, 2246, 29899, 29886, 23460, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:51528 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-15 11:39:48 [engine.py:331] Added request cmpl-247afa14db9249ecb1789712bdf53658-0.
INFO 08-15 11:39:51 [metrics.py:417] Avg prompt throughput: 2.8 tokens/s, Avg generation throughput: 78.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.4%, CPU KV cache usage: 0.0%.
INFO 08-15 11:39:56 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 78.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.7%, CPU KV cache usage: 0.0%.
INFO 08-15 11:40:00 [logger.py:43] Received request cmpl-b8a4931445884843892a2e381a3c5701-0: prompt: 'What is a vector database and how is it used with LLMs?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 263, 4608, 2566, 322, 920, 338, 372, 1304, 411, 365, 26369, 29879, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:60884 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-15 11:40:00 [engine.py:331] Added request cmpl-b8a4931445884843892a2e381a3c5701-0.
INFO 08-15 11:40:01 [metrics.py:417] Avg prompt throughput: 3.2 tokens/s, Avg generation throughput: 78.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-15 11:40:06 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 78.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.5%, CPU KV cache usage: 0.0%.
INFO 08-15 11:40:11 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 78.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.8%, CPU KV cache usage: 0.0%.
INFO 08-15 11:40:11 [logger.py:43] Received request cmpl-06c6139a5fa4491da002b85130585c8b-0: prompt: 'Describe how you would use RAG (Retrieval Augmented Generation).', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 20355, 915, 920, 366, 723, 671, 390, 10051, 313, 8015, 2546, 791, 22333, 358, 287, 28203, 467], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:50798 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-15 11:40:11 [engine.py:331] Added request cmpl-06c6139a5fa4491da002b85130585c8b-0.
INFO 08-15 11:40:16 [metrics.py:417] Avg prompt throughput: 3.6 tokens/s, Avg generation throughput: 78.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.5%, CPU KV cache usage: 0.0%.
INFO 08-15 11:40:21 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 78.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.8%, CPU KV cache usage: 0.0%.
INFO 08-15 11:40:24 [logger.py:43] Received request cmpl-54d71a87ffd2465799d3aee4f9ce4c74-0: prompt: 'What is streaming inference and how is it used?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 24820, 27262, 322, 920, 338, 372, 1304, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:51482 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-15 11:40:24 [engine.py:331] Added request cmpl-54d71a87ffd2465799d3aee4f9ce4c74-0.
INFO 08-15 11:40:26 [metrics.py:417] Avg prompt throughput: 2.2 tokens/s, Avg generation throughput: 78.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 08-15 11:40:31 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 78.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.6%, CPU KV cache usage: 0.0%.
INFO 08-15 11:40:36 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 78.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.9%, CPU KV cache usage: 0.0%.
INFO 08-15 11:40:37 [logger.py:43] Received request cmpl-3e8d45a2117b4f5ab15d607a9e223d35-0: prompt: 'How does TTFT relate to user-perceived latency?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 947, 323, 29911, 7818, 29279, 304, 1404, 29899, 546, 346, 2347, 23316, 1270, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:43662 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-15 11:40:37 [engine.py:331] Added request cmpl-3e8d45a2117b4f5ab15d607a9e223d35-0.
INFO 08-15 11:40:41 [metrics.py:417] Avg prompt throughput: 3.2 tokens/s, Avg generation throughput: 78.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.4%, CPU KV cache usage: 0.0%.
INFO 08-15 11:40:46 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 78.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.8%, CPU KV cache usage: 0.0%.
INFO 08-15 11:40:49 [logger.py:43] Received request cmpl-a0e02b1e860249689f2b62edab68b041-0: prompt: 'Explain transformer decoder blocks layer-by-layer.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 12027, 7420, 4327, 261, 1602, 6119, 10930, 7546, 29899, 1609, 29899, 13148, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:49098 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-15 11:40:49 [engine.py:331] Added request cmpl-a0e02b1e860249689f2b62edab68b041-0.
INFO 08-15 11:40:51 [metrics.py:417] Avg prompt throughput: 2.8 tokens/s, Avg generation throughput: 78.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 08-15 11:40:56 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 78.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.6%, CPU KV cache usage: 0.0%.
INFO 08-15 11:41:01 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 78.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.9%, CPU KV cache usage: 0.0%.
INFO 08-15 11:41:02 [logger.py:43] Received request cmpl-eae20b28e6ff440899566e1c067cf0b3-0: prompt: 'What is a KV cache in transformer inference?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 263, 476, 29963, 7090, 297, 4327, 261, 27262, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:43348 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-15 11:41:02 [engine.py:331] Added request cmpl-eae20b28e6ff440899566e1c067cf0b3-0.
INFO 08-15 11:41:06 [metrics.py:417] Avg prompt throughput: 2.4 tokens/s, Avg generation throughput: 78.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.4%, CPU KV cache usage: 0.0%.
INFO 08-15 11:41:11 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 78.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.7%, CPU KV cache usage: 0.0%.
INFO 08-15 11:41:15 [logger.py:43] Received request cmpl-97cec31595ca42a798d634a0cb11ae1f-0: prompt: 'Describe token-by-token generation and its challenges.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 20355, 915, 5993, 29899, 1609, 29899, 6979, 12623, 322, 967, 18066, 267, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:59824 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-15 11:41:15 [engine.py:331] Added request cmpl-97cec31595ca42a798d634a0cb11ae1f-0.
INFO 08-15 11:41:16 [metrics.py:417] Avg prompt throughput: 2.8 tokens/s, Avg generation throughput: 78.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 08-15 11:41:21 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 78.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.6%, CPU KV cache usage: 0.0%.
INFO 08-15 11:41:26 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 78.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.9%, CPU KV cache usage: 0.0%.
INFO 08-15 11:41:27 [logger.py:43] Received request cmpl-d62f7d56e80e4b3b8177f44c20f3145f-0: prompt: 'How does LLaMA 2 compare to GPT-3.5?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 947, 365, 5661, 1529, 29871, 29906, 7252, 304, 402, 7982, 29899, 29941, 29889, 29945, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:49470 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-15 11:41:27 [engine.py:331] Added request cmpl-d62f7d56e80e4b3b8177f44c20f3145f-0.
INFO 08-15 11:41:31 [metrics.py:417] Avg prompt throughput: 3.4 tokens/s, Avg generation throughput: 78.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.4%, CPU KV cache usage: 0.0%.
INFO 08-15 11:41:36 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 78.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.7%, CPU KV cache usage: 0.0%.
INFO 08-15 11:41:40 [logger.py:43] Received request cmpl-4337c980cf704c9aab5616b5dcd8faa9-0: prompt: 'Explain the training data pipeline for a large model.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 12027, 7420, 278, 6694, 848, 16439, 363, 263, 2919, 1904, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:53898 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-15 11:41:40 [engine.py:331] Added request cmpl-4337c980cf704c9aab5616b5dcd8faa9-0.
INFO 08-15 11:41:41 [metrics.py:417] Avg prompt throughput: 2.4 tokens/s, Avg generation throughput: 78.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-15 11:41:46 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 78.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.5%, CPU KV cache usage: 0.0%.
INFO 08-15 11:41:51 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 78.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.9%, CPU KV cache usage: 0.0%.
INFO 08-15 11:41:53 [logger.py:43] Received request cmpl-6b575a2b952644e880d866df684b2051-0: prompt: 'What are the ethical concerns with generative models?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 526, 278, 11314, 936, 21838, 411, 1176, 1230, 4733, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:35702 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-15 11:41:53 [engine.py:331] Added request cmpl-6b575a2b952644e880d866df684b2051-0.
INFO 08-15 11:41:56 [metrics.py:417] Avg prompt throughput: 2.4 tokens/s, Avg generation throughput: 78.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.4%, CPU KV cache usage: 0.0%.
INFO 08-15 11:42:01 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 78.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.7%, CPU KV cache usage: 0.0%.
INFO 08-15 11:42:05 [logger.py:43] Received request cmpl-147b6f828a3d41bd887b5085e9668f12-0: prompt: 'How do transformers process multilingual text?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 437, 4327, 414, 1889, 1773, 6504, 950, 1426, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:36698 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-15 11:42:05 [engine.py:331] Added request cmpl-147b6f828a3d41bd887b5085e9668f12-0.
INFO 08-15 11:42:06 [metrics.py:417] Avg prompt throughput: 2.2 tokens/s, Avg generation throughput: 78.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-15 11:42:11 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 78.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.5%, CPU KV cache usage: 0.0%.
INFO 08-15 11:42:16 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 78.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.8%, CPU KV cache usage: 0.0%.
INFO 08-15 11:42:18 [logger.py:43] Received request cmpl-f26e75728f9b4c9a8b0a8b6dcbefdf95-0: prompt: 'What are flash attention mechanisms?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 526, 11013, 8570, 7208, 12903, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:41560 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-15 11:42:18 [engine.py:331] Added request cmpl-f26e75728f9b4c9a8b0a8b6dcbefdf95-0.
INFO 08-15 11:42:21 [metrics.py:417] Avg prompt throughput: 1.6 tokens/s, Avg generation throughput: 78.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.4%, CPU KV cache usage: 0.0%.
INFO 08-15 11:42:26 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 78.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.7%, CPU KV cache usage: 0.0%.
INFO 08-15 11:42:31 [logger.py:43] Received request cmpl-59a14d773bcd424dac32dc6b004c7095-0: prompt: 'Describe how memory-efficient attention works.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 20355, 915, 920, 3370, 29899, 8462, 8570, 1736, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:57322 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-15 11:42:31 [engine.py:331] Added request cmpl-59a14d773bcd424dac32dc6b004c7095-0.
INFO 08-15 11:42:31 [metrics.py:417] Avg prompt throughput: 2.0 tokens/s, Avg generation throughput: 78.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-15 11:42:36 [logger.py:43] Received request cmpl-2c83b4c719364419b9664c3b8decfce4-0: prompt: 'What is inference throughput and how is it calculated?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 27262, 1549, 649, 322, 920, 338, 372, 12833, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:57330 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-15 11:42:36 [engine.py:331] Added request cmpl-2c83b4c719364419b9664c3b8decfce4-0.
INFO 08-15 11:42:36 [metrics.py:417] Avg prompt throughput: 2.4 tokens/s, Avg generation throughput: 78.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-15 11:42:41 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 78.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.5%, CPU KV cache usage: 0.0%.
INFO 08-15 11:42:46 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 78.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.8%, CPU KV cache usage: 0.0%.
INFO 08-15 11:42:48 [logger.py:43] Received request cmpl-ba1f1d51d4314a40be89e949e9b3d0c1-0: prompt: 'Explain how masked self-attention works in BERT.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 12027, 7420, 920, 11105, 287, 1583, 29899, 1131, 2509, 1736, 297, 350, 20161, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:52368 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-15 11:42:48 [engine.py:331] Added request cmpl-ba1f1d51d4314a40be89e949e9b3d0c1-0.
INFO 08-15 11:42:51 [metrics.py:417] Avg prompt throughput: 3.0 tokens/s, Avg generation throughput: 78.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.4%, CPU KV cache usage: 0.0%.
INFO 08-15 11:42:56 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 78.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.7%, CPU KV cache usage: 0.0%.
INFO 08-15 11:43:01 [logger.py:43] Received request cmpl-0a1744863156483b9878017e0088d8cc-0: prompt: 'How do instruction-tuned models differ from base models?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 437, 15278, 29899, 29873, 348, 287, 4733, 1163, 515, 2967, 4733, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:38186 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-15 11:43:01 [engine.py:331] Added request cmpl-0a1744863156483b9878017e0088d8cc-0.
INFO 08-15 11:43:01 [metrics.py:417] Avg prompt throughput: 2.8 tokens/s, Avg generation throughput: 78.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-15 11:43:06 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 78.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.5%, CPU KV cache usage: 0.0%.
INFO 08-15 11:43:11 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 78.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.8%, CPU KV cache usage: 0.0%.
INFO 08-15 11:43:14 [logger.py:43] Received request cmpl-b4178785e4834f7994090160a271d727-0: prompt: 'Describe an RLHF (Reinforcement Learning from Human Feedback) process.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 20355, 915, 385, 390, 29931, 29950, 29943, 313, 1123, 262, 1454, 13561, 29257, 515, 12968, 5169, 287, 1627, 29897, 1889, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:48302 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-15 11:43:14 [engine.py:331] Added request cmpl-b4178785e4834f7994090160a271d727-0.
INFO 08-15 11:43:17 [metrics.py:417] Avg prompt throughput: 4.4 tokens/s, Avg generation throughput: 78.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 08-15 11:43:22 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 78.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.7%, CPU KV cache usage: 0.0%.
INFO 08-15 11:43:26 [logger.py:43] Received request cmpl-ca35fecb447244068651f9c9fb06360c-0: prompt: 'What is SFT (Supervised Fine-Tuning) in LLM training?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 317, 7818, 313, 19111, 11292, 28896, 29899, 29911, 27964, 29897, 297, 365, 26369, 6694, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:54050 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-15 11:43:26 [engine.py:331] Added request cmpl-ca35fecb447244068651f9c9fb06360c-0.
INFO 08-15 11:43:27 [metrics.py:417] Avg prompt throughput: 3.6 tokens/s, Avg generation throughput: 78.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-15 11:43:32 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 78.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.5%, CPU KV cache usage: 0.0%.
INFO 08-15 11:43:37 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 78.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.8%, CPU KV cache usage: 0.0%.
INFO 08-15 11:43:39 [logger.py:43] Received request cmpl-1eabcf6519d345a497151d72440c01f5-0: prompt: 'Explain the role of HPU in Gaudi chips.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 12027, 7420, 278, 6297, 310, 379, 7056, 297, 10415, 4749, 521, 4512, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:35810 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-15 11:43:39 [engine.py:331] Added request cmpl-1eabcf6519d345a497151d72440c01f5-0.
INFO 08-15 11:43:42 [metrics.py:417] Avg prompt throughput: 2.8 tokens/s, Avg generation throughput: 78.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 08-15 11:43:47 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 78.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.6%, CPU KV cache usage: 0.0%.
INFO 08-15 11:43:52 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 78.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.9%, CPU KV cache usage: 0.0%.
INFO 08-15 11:43:52 [logger.py:43] Received request cmpl-097660cb156445a8bf3ab75b9f5dd74c-0: prompt: 'What is a causal attention mask?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 263, 3269, 284, 8570, 11105, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:57398 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-15 11:43:52 [engine.py:331] Added request cmpl-097660cb156445a8bf3ab75b9f5dd74c-0.
INFO 08-15 11:43:57 [metrics.py:417] Avg prompt throughput: 1.8 tokens/s, Avg generation throughput: 78.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.5%, CPU KV cache usage: 0.0%.
INFO 08-15 11:44:02 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 78.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.8%, CPU KV cache usage: 0.0%.
INFO 08-15 11:44:04 [logger.py:43] Received request cmpl-11c8e7b2a7064272989b8038181af203-0: prompt: 'Describe the HF `generate()` function in Transformers.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 20355, 915, 278, 379, 29943, 421, 17158, 2555, 740, 297, 4103, 689, 414, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:49464 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-15 11:44:04 [engine.py:331] Added request cmpl-11c8e7b2a7064272989b8038181af203-0.
INFO 08-15 11:44:07 [metrics.py:417] Avg prompt throughput: 3.0 tokens/s, Avg generation throughput: 78.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 08-15 11:44:12 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 78.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.6%, CPU KV cache usage: 0.0%.
INFO 08-15 11:44:17 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 78.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.9%, CPU KV cache usage: 0.0%.
INFO 08-15 11:44:17 [logger.py:43] Received request cmpl-2435f6c828e644b4914455c951454892-0: prompt: 'Explain differences between T5, BART, and GPT models.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 12027, 7420, 12651, 1546, 323, 29945, 29892, 350, 8322, 29892, 322, 402, 7982, 4733, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:39646 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-15 11:44:17 [engine.py:331] Added request cmpl-2435f6c828e644b4914455c951454892-0.
INFO 08-15 11:44:22 [metrics.py:417] Avg prompt throughput: 3.2 tokens/s, Avg generation throughput: 78.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.4%, CPU KV cache usage: 0.0%.
INFO 08-15 11:44:27 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 78.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.8%, CPU KV cache usage: 0.0%.
INFO 08-15 11:44:30 [logger.py:43] Received request cmpl-7f38a2f078324d97ae51ef717b5620b6-0: prompt: 'What is model checkpointing and resumption?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 1904, 1423, 3149, 292, 322, 620, 28069, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:53958 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-15 11:44:30 [engine.py:331] Added request cmpl-7f38a2f078324d97ae51ef717b5620b6-0.
INFO 08-15 11:44:32 [metrics.py:417] Avg prompt throughput: 2.2 tokens/s, Avg generation throughput: 78.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 08-15 11:44:37 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 78.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.6%, CPU KV cache usage: 0.0%.
INFO 08-15 11:44:42 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 78.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.9%, CPU KV cache usage: 0.0%.
INFO 08-15 11:44:42 [logger.py:43] Received request cmpl-bddd5d78576f491e87088ed96f30d372-0: prompt: 'Describe how checkpoint sharding works in distributed training.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 20355, 915, 920, 1423, 3149, 528, 20272, 1736, 297, 13235, 6694, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:33312 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-15 11:44:42 [engine.py:331] Added request cmpl-bddd5d78576f491e87088ed96f30d372-0.
INFO 08-15 11:44:47 [metrics.py:417] Avg prompt throughput: 2.6 tokens/s, Avg generation throughput: 78.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.4%, CPU KV cache usage: 0.0%.
INFO 08-15 11:44:52 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 78.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.7%, CPU KV cache usage: 0.0%.
INFO 08-15 11:44:55 [logger.py:43] Received request cmpl-755e83bbea3c40ebb90b5e3d45c82f82-0: prompt: 'How are model weights stored in HuggingFace format?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 526, 1904, 18177, 6087, 297, 379, 688, 3460, 23360, 3402, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:58300 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-15 11:44:55 [engine.py:331] Added request cmpl-755e83bbea3c40ebb90b5e3d45c82f82-0.
INFO 08-15 11:44:57 [metrics.py:417] Avg prompt throughput: 2.6 tokens/s, Avg generation throughput: 78.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 08-15 11:45:02 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 78.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.6%, CPU KV cache usage: 0.0%.
INFO 08-15 11:45:07 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 78.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.9%, CPU KV cache usage: 0.0%.
INFO 08-15 11:45:08 [logger.py:43] Received request cmpl-23dc37b9af5f48a7bf4c4cd498c8e69c-0: prompt: 'Explain the impact of batch size on training and inference.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 12027, 7420, 278, 10879, 310, 9853, 2159, 373, 6694, 322, 27262, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:54476 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-15 11:45:08 [engine.py:331] Added request cmpl-23dc37b9af5f48a7bf4c4cd498c8e69c-0.
INFO 08-15 11:45:12 [metrics.py:417] Avg prompt throughput: 2.6 tokens/s, Avg generation throughput: 78.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.4%, CPU KV cache usage: 0.0%.
INFO 08-15 11:45:17 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 78.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.7%, CPU KV cache usage: 0.0%.
INFO 08-15 11:45:21 [logger.py:43] Received request cmpl-2a83e1468b0f4e96afaca1321ac063f8-0: prompt: 'What are fused kernels and why are they fast?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 526, 285, 3880, 413, 824, 1379, 322, 2020, 526, 896, 5172, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:39160 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-15 11:45:21 [engine.py:331] Added request cmpl-2a83e1468b0f4e96afaca1321ac063f8-0.
INFO 08-15 11:45:22 [metrics.py:417] Avg prompt throughput: 2.8 tokens/s, Avg generation throughput: 78.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-15 11:45:27 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 78.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.5%, CPU KV cache usage: 0.0%.
INFO 08-15 11:45:32 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 78.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.9%, CPU KV cache usage: 0.0%.
INFO 08-15 11:45:33 [logger.py:43] Received request cmpl-75be29c5cde34ce4bad6895d2dc6aec9-0: prompt: 'How does the `AutoModelForCausalLM` class work?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 947, 278, 421, 12300, 3195, 2831, 29907, 1485, 284, 26369, 29952, 770, 664, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:49084 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-15 11:45:33 [engine.py:331] Added request cmpl-75be29c5cde34ce4bad6895d2dc6aec9-0.
INFO 08-15 11:45:37 [metrics.py:417] Avg prompt throughput: 3.2 tokens/s, Avg generation throughput: 78.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.4%, CPU KV cache usage: 0.0%.
INFO 08-15 11:45:42 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 78.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.7%, CPU KV cache usage: 0.0%.
INFO 08-15 11:45:46 [logger.py:43] Received request cmpl-d9531e9664d94c04b0be7ec951f19f69-0: prompt: 'What are tensor parallel and tensor slicing?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 526, 12489, 8943, 322, 12489, 269, 506, 292, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:53980 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-15 11:45:46 [engine.py:331] Added request cmpl-d9531e9664d94c04b0be7ec951f19f69-0.
INFO 08-15 11:45:47 [metrics.py:417] Avg prompt throughput: 2.2 tokens/s, Avg generation throughput: 78.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-15 11:45:52 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 78.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.5%, CPU KV cache usage: 0.0%.
INFO 08-15 11:45:57 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 78.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.8%, CPU KV cache usage: 0.0%.
INFO 08-15 11:45:59 [logger.py:43] Received request cmpl-578846949d8b4010b5978167e1f10943-0: prompt: 'Describe the key steps of model serving using vLLM.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 20355, 915, 278, 1820, 6576, 310, 1904, 16330, 773, 325, 2208, 29924, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:38410 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-15 11:45:59 [engine.py:331] Added request cmpl-578846949d8b4010b5978167e1f10943-0.
INFO 08-15 11:46:02 [metrics.py:417] Avg prompt throughput: 2.8 tokens/s, Avg generation throughput: 78.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.4%, CPU KV cache usage: 0.0%.
INFO 08-15 11:46:07 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 78.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.7%, CPU KV cache usage: 0.0%.
INFO 08-15 11:46:11 [logger.py:43] Received request cmpl-1930edb7985a4f1ca527e2e03523ac94-0: prompt: 'What are inference graphs in deep learning?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 526, 27262, 18445, 297, 6483, 6509, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:51594 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-15 11:46:11 [engine.py:331] Added request cmpl-1930edb7985a4f1ca527e2e03523ac94-0.
INFO 08-15 11:46:12 [metrics.py:417] Avg prompt throughput: 1.8 tokens/s, Avg generation throughput: 78.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-15 11:46:17 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 78.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.5%, CPU KV cache usage: 0.0%.
INFO 08-15 11:46:22 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 78.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.8%, CPU KV cache usage: 0.0%.
INFO 08-15 11:46:24 [logger.py:43] Received request cmpl-e9d70db523384915bd2cae2f7b037807-0: prompt: 'What is a streaming response in OpenAI-style APIs?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 263, 24820, 2933, 297, 4673, 23869, 29899, 3293, 23649, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:43652 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-15 11:46:24 [engine.py:331] Added request cmpl-e9d70db523384915bd2cae2f7b037807-0.
INFO 08-15 11:46:27 [metrics.py:417] Avg prompt throughput: 2.6 tokens/s, Avg generation throughput: 78.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 08-15 11:46:32 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 78.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.6%, CPU KV cache usage: 0.0%.
INFO 08-15 11:46:37 [logger.py:43] Received request cmpl-2173bac3bd764f2f8135653160e4bfe5-0: prompt: 'How does speculative decoding reduce latency?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 947, 1580, 28524, 1602, 3689, 10032, 23316, 1270, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:34560 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-15 11:46:37 [engine.py:331] Added request cmpl-2173bac3bd764f2f8135653160e4bfe5-0.
INFO 08-15 11:46:37 [metrics.py:417] Avg prompt throughput: 2.2 tokens/s, Avg generation throughput: 78.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-15 11:46:42 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 78.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.5%, CPU KV cache usage: 0.0%.
INFO 08-15 11:46:45 [logger.py:43] Received request cmpl-79372e05fa72490c862e9c4d0a91de65-0: prompt: 'What is JAX and how is it different from PyTorch?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 435, 6604, 322, 920, 338, 372, 1422, 515, 10772, 29911, 25350, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:49798 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-15 11:46:45 [engine.py:331] Added request cmpl-79372e05fa72490c862e9c4d0a91de65-0.
INFO 08-15 11:46:47 [metrics.py:417] Avg prompt throughput: 3.0 tokens/s, Avg generation throughput: 78.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 08-15 11:46:52 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 78.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.6%, CPU KV cache usage: 0.0%.
INFO 08-15 11:46:57 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 78.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.9%, CPU KV cache usage: 0.0%.
INFO 08-15 11:46:58 [logger.py:43] Received request cmpl-21a44723d56346a3955b365643f250b4-0: prompt: 'Describe lazy loading of weights in vLLM.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 20355, 915, 17366, 8363, 310, 18177, 297, 325, 2208, 29924, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:50164 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-15 11:46:58 [engine.py:331] Added request cmpl-21a44723d56346a3955b365643f250b4-0.
INFO 08-15 11:47:02 [metrics.py:417] Avg prompt throughput: 2.4 tokens/s, Avg generation throughput: 78.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.4%, CPU KV cache usage: 0.0%.
INFO 08-15 11:47:07 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 78.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.7%, CPU KV cache usage: 0.0%.
INFO 08-15 11:47:11 [logger.py:43] Received request cmpl-311c92b766184373be31d9d880665c62-0: prompt: 'Explain the role of SynapseAI in Gaudi software stack.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 12027, 7420, 278, 6297, 310, 10829, 481, 344, 23869, 297, 10415, 4749, 7047, 5096, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:38340 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-15 11:47:11 [engine.py:331] Added request cmpl-311c92b766184373be31d9d880665c62-0.
INFO 08-15 11:47:12 [metrics.py:417] Avg prompt throughput: 3.2 tokens/s, Avg generation throughput: 78.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-15 11:47:17 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 78.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.6%, CPU KV cache usage: 0.0%.
INFO 08-15 11:47:22 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 78.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.9%, CPU KV cache usage: 0.0%.
INFO 08-15 11:47:23 [logger.py:43] Received request cmpl-d97293eb7e3a4bba8b8cf4305b7fce99-0: prompt: 'What are the differences between HPU and GPU tensors?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 526, 278, 12651, 1546, 379, 7056, 322, 22796, 25187, 943, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:59610 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-15 11:47:23 [engine.py:331] Added request cmpl-d97293eb7e3a4bba8b8cf4305b7fce99-0.
INFO 08-15 11:47:27 [metrics.py:417] Avg prompt throughput: 2.6 tokens/s, Avg generation throughput: 78.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.4%, CPU KV cache usage: 0.0%.
INFO 08-15 11:47:32 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 78.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.7%, CPU KV cache usage: 0.0%.
INFO 08-15 11:47:36 [logger.py:43] Received request cmpl-401ee1c83aba459f83c393dd4a1a5dd9-0: prompt: 'What are the limitations of LLaMA 2?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 526, 278, 27028, 310, 365, 5661, 1529, 29871, 29906, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:55320 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-15 11:47:36 [engine.py:331] Added request cmpl-401ee1c83aba459f83c393dd4a1a5dd9-0.
INFO 08-15 11:47:37 [metrics.py:417] Avg prompt throughput: 2.4 tokens/s, Avg generation throughput: 78.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-15 11:47:42 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 78.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.5%, CPU KV cache usage: 0.0%.
INFO 08-15 11:47:47 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 78.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.8%, CPU KV cache usage: 0.0%.
INFO 08-15 11:47:49 [logger.py:43] Received request cmpl-91e839cb7b8b46b8b5e222a5e5be4f3e-0: prompt: 'How is multilingual performance evaluated in LLMs?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 338, 1773, 6504, 950, 4180, 19030, 297, 365, 26369, 29879, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:47806 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-15 11:47:49 [engine.py:331] Added request cmpl-91e839cb7b8b46b8b5e222a5e5be4f3e-0.
INFO 08-15 11:47:52 [metrics.py:417] Avg prompt throughput: 2.6 tokens/s, Avg generation throughput: 78.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.4%, CPU KV cache usage: 0.0%.
INFO 08-15 11:47:55 [logger.py:43] Received request cmpl-fe4fe93a2918459f9024c5d538487664-0: prompt: 'Describe an example use of prompt tuning.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 20355, 915, 385, 1342, 671, 310, 9508, 18515, 292, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:56082 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-15 11:47:55 [engine.py:331] Added request cmpl-fe4fe93a2918459f9024c5d538487664-0.
INFO 08-15 11:47:57 [metrics.py:417] Avg prompt throughput: 2.2 tokens/s, Avg generation throughput: 78.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 08-15 11:48:02 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 78.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.6%, CPU KV cache usage: 0.0%.
INFO 08-15 11:48:07 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 78.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.9%, CPU KV cache usage: 0.0%.
INFO 08-15 11:48:08 [logger.py:43] Received request cmpl-bce1543268564d83ad2daa5e671908d3-0: prompt: 'What is the attention bottleneck and how to address it?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 278, 8570, 18046, 29880, 1600, 384, 322, 920, 304, 3211, 372, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:36024 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-15 11:48:08 [engine.py:331] Added request cmpl-bce1543268564d83ad2daa5e671908d3-0.
INFO 08-15 11:48:12 [metrics.py:417] Avg prompt throughput: 3.0 tokens/s, Avg generation throughput: 78.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.4%, CPU KV cache usage: 0.0%.
INFO 08-15 11:48:17 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 78.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.7%, CPU KV cache usage: 0.0%.
INFO 08-15 11:48:21 [logger.py:43] Received request cmpl-f4e642e4c8fc44c490500f248d352bde-0: prompt: 'How does token alignment affect downstream tasks?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 947, 5993, 22239, 6602, 1623, 5461, 9595, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:40342 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-15 11:48:21 [engine.py:331] Added request cmpl-f4e642e4c8fc44c490500f248d352bde-0.
INFO 08-15 11:48:22 [metrics.py:417] Avg prompt throughput: 2.0 tokens/s, Avg generation throughput: 78.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 08-15 11:48:27 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 78.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.6%, CPU KV cache usage: 0.0%.
INFO 08-15 11:48:32 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 78.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.9%, CPU KV cache usage: 0.0%.
INFO 08-15 11:48:33 [logger.py:43] Received request cmpl-22dda5c55ecb4d8cbbd47f527de9972d-0: prompt: 'What are the challenges in inference on edge devices?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 526, 278, 18066, 267, 297, 27262, 373, 7636, 9224, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:55468 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-15 11:48:33 [engine.py:331] Added request cmpl-22dda5c55ecb4d8cbbd47f527de9972d-0.
INFO 08-15 11:48:37 [metrics.py:417] Avg prompt throughput: 2.4 tokens/s, Avg generation throughput: 78.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.4%, CPU KV cache usage: 0.0%.
INFO 08-15 11:48:42 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 78.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.7%, CPU KV cache usage: 0.0%.
INFO 08-15 11:48:46 [logger.py:43] Received request cmpl-d7bcbbd40d6944efb5ab1d49590456e2-0: prompt: 'Explain the principle behind rotary embeddings.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 12027, 7420, 278, 12502, 5742, 5731, 653, 8297, 29881, 886, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:56542 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-15 11:48:46 [engine.py:331] Added request cmpl-d7bcbbd40d6944efb5ab1d49590456e2-0.
INFO 08-15 11:48:47 [metrics.py:417] Avg prompt throughput: 2.4 tokens/s, Avg generation throughput: 78.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-15 11:48:52 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 78.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.5%, CPU KV cache usage: 0.0%.
INFO 08-15 11:48:57 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 78.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.8%, CPU KV cache usage: 0.0%.
INFO 08-15 11:48:59 [logger.py:43] Received request cmpl-ae79c78756a242f490568f2702ff4a7a-0: prompt: 'How does temperature affect language model output?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 947, 10430, 6602, 4086, 1904, 1962, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:50996 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-15 11:48:59 [engine.py:331] Added request cmpl-ae79c78756a242f490568f2702ff4a7a-0.
INFO 08-15 11:49:02 [metrics.py:417] Avg prompt throughput: 1.8 tokens/s, Avg generation throughput: 78.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.4%, CPU KV cache usage: 0.0%.
INFO 08-15 11:49:07 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 78.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.7%, CPU KV cache usage: 0.0%.
INFO 08-15 11:49:11 [logger.py:43] Received request cmpl-02ac53bd96af480d8f857f341fe65be3-0: prompt: 'What are stop sequences in generation APIs?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 526, 5040, 15602, 297, 12623, 23649, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:53124 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-15 11:49:11 [engine.py:331] Added request cmpl-02ac53bd96af480d8f857f341fe65be3-0.
INFO 08-15 11:49:12 [metrics.py:417] Avg prompt throughput: 1.8 tokens/s, Avg generation throughput: 78.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-15 11:49:17 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 78.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.5%, CPU KV cache usage: 0.0%.
INFO 08-15 11:49:22 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 78.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.8%, CPU KV cache usage: 0.0%.
INFO 08-15 11:49:24 [logger.py:43] Received request cmpl-15587f7368314bdba9196e42bc1f005e-0: prompt: 'Describe token streaming latency optimization.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 20355, 915, 5993, 24820, 23316, 1270, 13883, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:50898 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-15 11:49:24 [engine.py:331] Added request cmpl-15587f7368314bdba9196e42bc1f005e-0.
INFO 08-15 11:49:27 [metrics.py:417] Avg prompt throughput: 1.8 tokens/s, Avg generation throughput: 78.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.4%, CPU KV cache usage: 0.0%.
INFO 08-15 11:49:32 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 78.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.7%, CPU KV cache usage: 0.0%.
INFO 08-15 11:49:37 [logger.py:43] Received request cmpl-04e53adb47504f3c8e627a54af84bd82-0: prompt: 'How to evaluate hallucinations in language models?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 304, 14707, 12713, 1682, 262, 800, 297, 4086, 4733, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:33624 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-15 11:49:37 [engine.py:331] Added request cmpl-04e53adb47504f3c8e627a54af84bd82-0.
INFO 08-15 11:49:37 [metrics.py:417] Avg prompt throughput: 2.4 tokens/s, Avg generation throughput: 78.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-15 11:49:42 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 78.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.5%, CPU KV cache usage: 0.0%.
INFO 08-15 11:49:47 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 78.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.8%, CPU KV cache usage: 0.0%.
INFO 08-15 11:49:49 [logger.py:43] Received request cmpl-dbfaea3651e143c38aeb3cb6845fe0ca-0: prompt: 'What is the purpose of `bos_token_id`?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 278, 6437, 310, 421, 27737, 29918, 6979, 29918, 333, 6522], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:57022 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-15 11:49:49 [engine.py:331] Added request cmpl-dbfaea3651e143c38aeb3cb6845fe0ca-0.
INFO 08-15 11:49:52 [metrics.py:417] Avg prompt throughput: 2.6 tokens/s, Avg generation throughput: 78.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 08-15 11:49:57 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 78.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.6%, CPU KV cache usage: 0.0%.
INFO 08-15 11:50:02 [logger.py:43] Received request cmpl-f6ad0185f4e84515ba6f0e4ff6631140-0: prompt: 'How are fused attention layers implemented?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 526, 285, 3880, 8570, 15359, 8762, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:41754 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-15 11:50:02 [engine.py:331] Added request cmpl-f6ad0185f4e84515ba6f0e4ff6631140-0.
INFO 08-15 11:50:02 [metrics.py:417] Avg prompt throughput: 1.8 tokens/s, Avg generation throughput: 78.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-15 11:50:07 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 78.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.5%, CPU KV cache usage: 0.0%.
INFO 08-15 11:50:12 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 78.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.8%, CPU KV cache usage: 0.0%.
INFO 08-15 11:50:15 [logger.py:43] Received request cmpl-1ae5fea82b2f42388f59239f43d8cd55-0: prompt: 'Describe a pipeline for deploying LLaMA 2 in production.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 20355, 915, 263, 16439, 363, 7246, 292, 365, 5661, 1529, 29871, 29906, 297, 5802, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:33054 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-15 11:50:15 [engine.py:331] Added request cmpl-1ae5fea82b2f42388f59239f43d8cd55-0.
INFO 08-15 11:50:17 [metrics.py:417] Avg prompt throughput: 3.2 tokens/s, Avg generation throughput: 78.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 08-15 11:50:22 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 78.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.6%, CPU KV cache usage: 0.0%.
INFO 08-15 11:50:27 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 78.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.9%, CPU KV cache usage: 0.0%.
INFO 08-15 11:50:27 [logger.py:43] Received request cmpl-508d5ba9fa1e49f2a461278e9da2ec78-0: prompt: 'How does instruction following improve model usability?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 947, 15278, 1494, 11157, 1904, 502, 3097, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:52038 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-15 11:50:27 [engine.py:331] Added request cmpl-508d5ba9fa1e49f2a461278e9da2ec78-0.
INFO 08-15 11:50:32 [metrics.py:417] Avg prompt throughput: 2.0 tokens/s, Avg generation throughput: 78.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.5%, CPU KV cache usage: 0.0%.
INFO 08-15 11:50:37 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 78.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.8%, CPU KV cache usage: 0.0%.
INFO 08-15 11:50:40 [logger.py:43] Received request cmpl-b9d9ea47cb21476d97e6bc6355478ade-0: prompt: 'What is a sequence-to-sequence model?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 263, 5665, 29899, 517, 29899, 16506, 1904, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:32774 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-15 11:50:40 [engine.py:331] Added request cmpl-b9d9ea47cb21476d97e6bc6355478ade-0.
INFO 08-15 11:50:42 [metrics.py:417] Avg prompt throughput: 2.2 tokens/s, Avg generation throughput: 78.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 08-15 11:50:47 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 78.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.6%, CPU KV cache usage: 0.0%.
INFO 08-15 11:50:52 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 78.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.9%, CPU KV cache usage: 0.0%.
INFO 08-15 11:50:53 [logger.py:43] Received request cmpl-86dac5d2844c4287a62b810723c94e80-0: prompt: 'What is the purpose of `pad_token_id` in generation?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 278, 6437, 310, 421, 8305, 29918, 6979, 29918, 333, 29952, 297, 12623, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:56886 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-15 11:50:53 [engine.py:331] Added request cmpl-86dac5d2844c4287a62b810723c94e80-0.
INFO 08-15 11:50:57 [logger.py:43] Received request cmpl-9f219bdee4004d95a5711cec138d89f4-0: prompt: 'Explain the concept of auto-regressive generation.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 12027, 7420, 278, 6964, 310, 4469, 29899, 276, 3663, 573, 12623, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:56890 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-15 11:50:57 [engine.py:331] Added request cmpl-9f219bdee4004d95a5711cec138d89f4-0.
INFO 08-15 11:50:57 [metrics.py:417] Avg prompt throughput: 5.8 tokens/s, Avg generation throughput: 78.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-15 11:51:02 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 78.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.5%, CPU KV cache usage: 0.0%.
INFO 08-15 11:51:07 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 78.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.8%, CPU KV cache usage: 0.0%.
INFO 08-15 11:51:09 [logger.py:43] Received request cmpl-e78194cfe266495184c7dfc3733695ad-0: prompt: 'How is top-p sampling different from nucleus sampling?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 338, 2246, 29899, 29886, 23460, 1422, 515, 22699, 375, 23460, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:34134 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-15 11:51:09 [engine.py:331] Added request cmpl-e78194cfe266495184c7dfc3733695ad-0.
INFO 08-15 11:51:12 [metrics.py:417] Avg prompt throughput: 2.6 tokens/s, Avg generation throughput: 78.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.4%, CPU KV cache usage: 0.0%.
INFO 08-15 11:51:13 [logger.py:43] Received request cmpl-1fca105e8c6c4f8e87375e7509e0267e-0: prompt: 'What are decoding strategies for safe responses?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 526, 1602, 3689, 16650, 583, 363, 9109, 20890, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:45430 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-15 11:51:13 [engine.py:331] Added request cmpl-1fca105e8c6c4f8e87375e7509e0267e-0.
INFO 08-15 11:51:17 [metrics.py:417] Avg prompt throughput: 2.2 tokens/s, Avg generation throughput: 78.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.4%, CPU KV cache usage: 0.0%.
INFO 08-15 11:51:22 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 78.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.7%, CPU KV cache usage: 0.0%.
INFO 08-15 11:51:26 [logger.py:43] Received request cmpl-927a85bca19f40d1a634f10337734430-0: prompt: 'How are model prompts structured in ChatML?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 526, 1904, 9508, 29879, 2281, 2955, 297, 678, 271, 1988, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:48848 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-15 11:51:26 [engine.py:331] Added request cmpl-927a85bca19f40d1a634f10337734430-0.
INFO 08-15 11:51:27 [metrics.py:417] Avg prompt throughput: 2.6 tokens/s, Avg generation throughput: 78.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-15 11:51:32 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 78.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.6%, CPU KV cache usage: 0.0%.
INFO 08-15 11:51:37 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 78.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.9%, CPU KV cache usage: 0.0%.
INFO 08-15 11:51:39 [logger.py:43] Received request cmpl-5290558caf5447bfbef5d9e6f3144223-0: prompt: 'What is a system prompt in chat inference?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 263, 1788, 9508, 297, 13563, 27262, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:35068 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-15 11:51:39 [engine.py:331] Added request cmpl-5290558caf5447bfbef5d9e6f3144223-0.
INFO 08-15 11:51:42 [metrics.py:417] Avg prompt throughput: 2.0 tokens/s, Avg generation throughput: 78.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.4%, CPU KV cache usage: 0.0%.
INFO 08-15 11:51:47 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 78.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.7%, CPU KV cache usage: 0.0%.
INFO 08-15 11:51:51 [logger.py:43] Received request cmpl-dd9706b04fb14ee6a7726a7d49b4e33b-0: prompt: 'How do you batch requests for high throughput?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 437, 366, 9853, 7274, 363, 1880, 1549, 649, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:54472 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-15 11:51:51 [engine.py:331] Added request cmpl-dd9706b04fb14ee6a7726a7d49b4e33b-0.
INFO 08-15 11:51:52 [metrics.py:417] Avg prompt throughput: 2.2 tokens/s, Avg generation throughput: 78.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-15 11:51:57 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 78.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.5%, CPU KV cache usage: 0.0%.
INFO 08-15 11:52:02 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 78.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.8%, CPU KV cache usage: 0.0%.
INFO 08-15 11:52:04 [logger.py:43] Received request cmpl-bf4333ba2ef54431ad7f938a6ab7fe10-0: prompt: 'Explain model serving with `transformers + vLLM`.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 12027, 7420, 1904, 16330, 411, 421, 9067, 414, 718, 325, 2208, 29924, 1412], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:45634 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-15 11:52:04 [engine.py:331] Added request cmpl-bf4333ba2ef54431ad7f938a6ab7fe10-0.
INFO 08-15 11:52:07 [metrics.py:417] Avg prompt throughput: 2.8 tokens/s, Avg generation throughput: 78.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.4%, CPU KV cache usage: 0.0%.
INFO 08-15 11:52:12 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 78.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.7%, CPU KV cache usage: 0.0%.
INFO 08-15 11:52:17 [logger.py:43] Received request cmpl-497d90dd1c4d4cdc95da7c124a777c82-0: prompt: 'What is Triton Inference Server?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 323, 768, 265, 512, 1659, 5656, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:36990 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-15 11:52:17 [engine.py:331] Added request cmpl-497d90dd1c4d4cdc95da7c124a777c82-0.
INFO 08-15 11:52:17 [metrics.py:417] Avg prompt throughput: 2.0 tokens/s, Avg generation throughput: 78.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-15 11:52:22 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 78.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.5%, CPU KV cache usage: 0.0%.
INFO 08-15 11:52:27 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 78.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.8%, CPU KV cache usage: 0.0%.
INFO 08-15 11:52:29 [logger.py:43] Received request cmpl-a5b222dbb515465488404c78bc1413e7-0: prompt: 'Describe the significance of large context windows.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 20355, 915, 278, 26002, 310, 2919, 3030, 5417, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:42478 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-15 11:52:29 [engine.py:331] Added request cmpl-a5b222dbb515465488404c78bc1413e7-0.
INFO 08-15 11:52:32 [metrics.py:417] Avg prompt throughput: 2.0 tokens/s, Avg generation throughput: 78.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 08-15 11:52:37 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 78.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.7%, CPU KV cache usage: 0.0%.
INFO 08-15 11:52:42 [logger.py:43] Received request cmpl-dbfcefc94a594877ac60ad83d7aac196-0: prompt: 'What is dynamic quantization in transformers?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 7343, 4323, 2133, 297, 4327, 414, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:37360 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-15 11:52:42 [engine.py:331] Added request cmpl-dbfcefc94a594877ac60ad83d7aac196-0.
INFO 08-15 11:52:42 [metrics.py:417] Avg prompt throughput: 2.0 tokens/s, Avg generation throughput: 78.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-15 11:52:48 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 78.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.5%, CPU KV cache usage: 0.0%.
INFO 08-15 11:52:53 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 78.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.8%, CPU KV cache usage: 0.0%.
INFO 08-15 11:52:55 [logger.py:43] Received request cmpl-86692c4cb57140e7aac02c198f2a84e2-0: prompt: 'How to prevent prompt injection in LLMs?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 304, 5557, 9508, 20859, 297, 365, 26369, 29879, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:56062 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-15 11:52:55 [engine.py:331] Added request cmpl-86692c4cb57140e7aac02c198f2a84e2-0.
INFO 08-15 11:52:58 [metrics.py:417] Avg prompt throughput: 2.2 tokens/s, Avg generation throughput: 78.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 08-15 11:53:03 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 78.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.6%, CPU KV cache usage: 0.0%.
INFO 08-15 11:53:07 [logger.py:43] Received request cmpl-7b350dbd9849497c8c632079d3c45f56-0: prompt: 'What are attention scores and how are they computed?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 526, 8570, 19435, 322, 920, 526, 896, 15712, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:36850 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-15 11:53:07 [engine.py:331] Added request cmpl-7b350dbd9849497c8c632079d3c45f56-0.
INFO 08-15 11:53:08 [metrics.py:417] Avg prompt throughput: 2.2 tokens/s, Avg generation throughput: 78.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-15 11:53:13 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 78.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.5%, CPU KV cache usage: 0.0%.
INFO 08-15 11:53:18 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 78.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.8%, CPU KV cache usage: 0.0%.
INFO 08-15 11:53:20 [logger.py:43] Received request cmpl-5d6eb13bc8204ca7a00ed4a8be0c214b-0: prompt: 'What are residual connections in transformers?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 526, 10995, 950, 12368, 297, 4327, 414, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:42246 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-15 11:53:20 [engine.py:331] Added request cmpl-5d6eb13bc8204ca7a00ed4a8be0c214b-0.
INFO 08-15 11:53:23 [metrics.py:417] Avg prompt throughput: 2.0 tokens/s, Avg generation throughput: 78.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 08-15 11:53:28 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 78.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.6%, CPU KV cache usage: 0.0%.
INFO 08-15 11:53:33 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 78.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.9%, CPU KV cache usage: 0.0%.
INFO 08-15 11:53:33 [logger.py:43] Received request cmpl-6022458a6b1c41ae96d6f436d0395177-0: prompt: 'How to handle long sequences in generation?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 304, 4386, 1472, 15602, 297, 12623, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:49484 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-15 11:53:33 [engine.py:331] Added request cmpl-6022458a6b1c41ae96d6f436d0395177-0.
INFO 08-15 11:53:38 [metrics.py:417] Avg prompt throughput: 1.8 tokens/s, Avg generation throughput: 78.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.4%, CPU KV cache usage: 0.0%.
INFO 08-15 11:53:43 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 78.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.8%, CPU KV cache usage: 0.0%.
INFO 08-15 11:53:46 [logger.py:43] Received request cmpl-e8167fc72ff44bfa8d1ade6e5d6abbb7-0: prompt: 'Explain the use of flash attention v2.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 12027, 7420, 278, 671, 310, 11013, 8570, 325, 29906, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:56538 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-15 11:53:46 [engine.py:331] Added request cmpl-e8167fc72ff44bfa8d1ade6e5d6abbb7-0.
INFO 08-15 11:53:48 [metrics.py:417] Avg prompt throughput: 2.2 tokens/s, Avg generation throughput: 78.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 08-15 11:53:53 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 78.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.6%, CPU KV cache usage: 0.0%.
INFO 08-15 11:53:58 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 78.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.9%, CPU KV cache usage: 0.0%.
INFO 08-15 11:53:58 [logger.py:43] Received request cmpl-baf0ee31426944eaa873aeeb6273427a-0: prompt: 'What is a greedy search in inference?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 263, 1395, 7584, 2740, 297, 27262, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:48930 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-15 11:53:58 [engine.py:331] Added request cmpl-baf0ee31426944eaa873aeeb6273427a-0.
INFO 08-15 11:54:03 [metrics.py:417] Avg prompt throughput: 2.0 tokens/s, Avg generation throughput: 78.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.4%, CPU KV cache usage: 0.0%.
INFO 08-15 11:54:08 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 78.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.7%, CPU KV cache usage: 0.0%.
INFO 08-15 11:54:11 [logger.py:43] Received request cmpl-f7de2acae5ab4485982341d6537c7189-0: prompt: 'How do checkpoints improve fault tolerance?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 437, 1423, 9748, 11157, 12570, 20341, 749, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:54650 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-15 11:54:11 [engine.py:331] Added request cmpl-f7de2acae5ab4485982341d6537c7189-0.
INFO 08-15 11:54:13 [metrics.py:417] Avg prompt throughput: 2.0 tokens/s, Avg generation throughput: 78.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 08-15 11:54:18 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 78.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.6%, CPU KV cache usage: 0.0%.
INFO 08-15 11:54:23 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 78.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.9%, CPU KV cache usage: 0.0%.
INFO 08-15 11:54:24 [logger.py:43] Received request cmpl-f7f9de5372094ec4a0ab971a45931f70-0: prompt: 'Describe how text-to-text generation works.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 20355, 915, 920, 1426, 29899, 517, 29899, 726, 12623, 1736, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:48666 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-15 11:54:24 [engine.py:331] Added request cmpl-f7f9de5372094ec4a0ab971a45931f70-0.
INFO 08-15 11:54:28 [metrics.py:417] Avg prompt throughput: 2.4 tokens/s, Avg generation throughput: 78.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.4%, CPU KV cache usage: 0.0%.
INFO 08-15 11:54:33 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 78.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.7%, CPU KV cache usage: 0.0%.
INFO 08-15 11:54:36 [logger.py:43] Received request cmpl-0bbcc6d2588641acb184684883e1dbb5-0: prompt: 'Explain cosine similarity in embeddings.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 12027, 7420, 6776, 457, 29501, 297, 8297, 29881, 886, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:56666 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-15 11:54:36 [engine.py:331] Added request cmpl-0bbcc6d2588641acb184684883e1dbb5-0.
INFO 08-15 11:54:38 [metrics.py:417] Avg prompt throughput: 2.2 tokens/s, Avg generation throughput: 78.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-15 11:54:43 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 78.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.6%, CPU KV cache usage: 0.0%.
INFO 08-15 11:54:48 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 78.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.9%, CPU KV cache usage: 0.0%.
INFO 08-15 11:54:49 [logger.py:43] Received request cmpl-de1a008c41784642982d7e5b4cd7db7d-0: prompt: 'What is token-wise parallelism?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 5993, 29899, 3538, 8943, 1608, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:46482 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-15 11:54:49 [engine.py:331] Added request cmpl-de1a008c41784642982d7e5b4cd7db7d-0.
INFO 08-15 11:54:53 [metrics.py:417] Avg prompt throughput: 1.8 tokens/s, Avg generation throughput: 78.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.4%, CPU KV cache usage: 0.0%.
INFO 08-15 11:54:58 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 78.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.7%, CPU KV cache usage: 0.0%.
INFO 08-15 11:55:02 [logger.py:43] Received request cmpl-356d0c8de17544b39f1d31c0d0732d76-0: prompt: 'How do shared embeddings help in decoder-only models?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 437, 7258, 8297, 29881, 886, 1371, 297, 1602, 6119, 29899, 6194, 4733, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:54210 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-15 11:55:02 [engine.py:331] Added request cmpl-356d0c8de17544b39f1d31c0d0732d76-0.
INFO 08-15 11:55:03 [metrics.py:417] Avg prompt throughput: 3.0 tokens/s, Avg generation throughput: 78.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-15 11:55:08 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 78.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.5%, CPU KV cache usage: 0.0%.
INFO 08-15 11:55:13 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 78.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.8%, CPU KV cache usage: 0.0%.
INFO 08-15 11:55:14 [logger.py:43] Received request cmpl-5a49a321bef648e2a5ffdebdb56baef3-0: prompt: 'Explain how to measure latency in model output.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 12027, 7420, 920, 304, 5645, 23316, 1270, 297, 1904, 1962, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:56556 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-15 11:55:14 [engine.py:331] Added request cmpl-5a49a321bef648e2a5ffdebdb56baef3-0.
INFO 08-15 11:55:18 [metrics.py:417] Avg prompt throughput: 2.4 tokens/s, Avg generation throughput: 78.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.4%, CPU KV cache usage: 0.0%.
INFO 08-15 11:55:23 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 78.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.7%, CPU KV cache usage: 0.0%.
INFO 08-15 11:55:27 [logger.py:43] Received request cmpl-92fb699205254cb1978cfec3a0198fb6-0: prompt: 'What is the purpose of dropout during training?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 278, 6437, 310, 5768, 449, 2645, 6694, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:40796 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-15 11:55:27 [engine.py:331] Added request cmpl-92fb699205254cb1978cfec3a0198fb6-0.
INFO 08-15 11:55:28 [metrics.py:417] Avg prompt throughput: 2.2 tokens/s, Avg generation throughput: 78.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-15 11:55:33 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 78.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.5%, CPU KV cache usage: 0.0%.
INFO 08-15 11:55:38 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 78.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.8%, CPU KV cache usage: 0.0%.
INFO 08-15 11:55:40 [logger.py:43] Received request cmpl-d4dce7cfd127412b85701765dd854cc0-0: prompt: 'What is the role of GELU activation in transformers?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 278, 6297, 310, 402, 6670, 29965, 26229, 297, 4327, 414, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:43504 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-15 11:55:40 [engine.py:331] Added request cmpl-d4dce7cfd127412b85701765dd854cc0-0.
INFO 08-15 11:55:43 [metrics.py:417] Avg prompt throughput: 2.8 tokens/s, Avg generation throughput: 78.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 08-15 11:55:48 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 78.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.7%, CPU KV cache usage: 0.0%.
INFO 08-15 11:55:52 [logger.py:43] Received request cmpl-4c1af35379ff45e19aa634f29593554c-0: prompt: 'Explain forward and backward pass in transformers.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 12027, 7420, 6375, 322, 1250, 1328, 1209, 297, 4327, 414, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:52886 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-15 11:55:52 [engine.py:331] Added request cmpl-4c1af35379ff45e19aa634f29593554c-0.
INFO 08-15 11:55:53 [metrics.py:417] Avg prompt throughput: 2.4 tokens/s, Avg generation throughput: 78.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-15 11:55:58 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 78.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.5%, CPU KV cache usage: 0.0%.
INFO 08-15 11:56:03 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 78.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.8%, CPU KV cache usage: 0.0%.
INFO 08-15 11:56:05 [logger.py:43] Received request cmpl-104390d72072487eaf78c141c3ef83cc-0: prompt: 'What is speculative decoding and how does it help?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 1580, 28524, 1602, 3689, 322, 920, 947, 372, 1371, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:34066 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-15 11:56:05 [engine.py:331] Added request cmpl-104390d72072487eaf78c141c3ef83cc-0.
INFO 08-15 11:56:08 [metrics.py:417] Avg prompt throughput: 2.6 tokens/s, Avg generation throughput: 78.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 08-15 11:56:13 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 78.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.6%, CPU KV cache usage: 0.0%.
INFO 08-15 11:56:18 [logger.py:43] Received request cmpl-b7a003c842ed43f1a4d9a0246d3f3efe-0: prompt: 'How to optimize HPU memory usage during inference?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 304, 24656, 379, 7056, 3370, 8744, 2645, 27262, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:37822 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-15 11:56:18 [engine.py:331] Added request cmpl-b7a003c842ed43f1a4d9a0246d3f3efe-0.
INFO 08-15 11:56:18 [metrics.py:417] Avg prompt throughput: 2.2 tokens/s, Avg generation throughput: 78.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-15 11:56:23 [logger.py:43] Received request cmpl-77dc8d82ce2b4230be63ce7ca0b94394-0: prompt: 'Explain profiling for LLM inference performance.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 12027, 7420, 20077, 292, 363, 365, 26369, 27262, 4180, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:47242 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-15 11:56:23 [engine.py:331] Added request cmpl-77dc8d82ce2b4230be63ce7ca0b94394-0.
INFO 08-15 11:56:23 [metrics.py:417] Avg prompt throughput: 2.2 tokens/s, Avg generation throughput: 78.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-15 11:56:28 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 78.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.5%, CPU KV cache usage: 0.0%.
INFO 08-15 11:56:33 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 78.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.8%, CPU KV cache usage: 0.0%.
INFO 08-15 11:56:36 [logger.py:43] Received request cmpl-45b4871004954fff9b9681056e505c5a-0: prompt: 'How to compute inter-token latency per request?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 304, 10272, 1006, 29899, 6979, 23316, 1270, 639, 2009, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:43942 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-15 11:56:36 [engine.py:331] Added request cmpl-45b4871004954fff9b9681056e505c5a-0.
INFO 08-15 11:56:38 [metrics.py:417] Avg prompt throughput: 2.4 tokens/s, Avg generation throughput: 78.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 08-15 11:56:43 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 78.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.6%, CPU KV cache usage: 0.0%.
INFO 08-15 11:56:48 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 78.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.9%, CPU KV cache usage: 0.0%.
INFO 08-15 11:56:48 [logger.py:43] Received request cmpl-734fa376222a4110b9fccf3720a71d20-0: prompt: 'What is the difference between BFloat16 and Float16?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 278, 4328, 1546, 350, 11031, 29896, 29953, 322, 27842, 29896, 29953, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:41934 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-15 11:56:48 [engine.py:331] Added request cmpl-734fa376222a4110b9fccf3720a71d20-0.
INFO 08-15 11:56:53 [metrics.py:417] Avg prompt throughput: 3.0 tokens/s, Avg generation throughput: 78.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.5%, CPU KV cache usage: 0.0%.
INFO 08-15 11:56:58 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 78.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.8%, CPU KV cache usage: 0.0%.
INFO 08-15 11:57:01 [logger.py:43] Received request cmpl-51852cd55d5642f88c3295f36d97a075-0: prompt: 'What are logits and how are they interpreted?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 526, 1480, 1169, 322, 920, 526, 896, 21551, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:33826 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-15 11:57:01 [engine.py:331] Added request cmpl-51852cd55d5642f88c3295f36d97a075-0.
INFO 08-15 11:57:03 [metrics.py:417] Avg prompt throughput: 2.2 tokens/s, Avg generation throughput: 78.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 08-15 11:57:08 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 78.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.6%, CPU KV cache usage: 0.0%.
INFO 08-15 11:57:13 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 78.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.9%, CPU KV cache usage: 0.0%.
INFO 08-15 11:57:14 [logger.py:43] Received request cmpl-ad803d20c5ba4d379278e99708c1132c-0: prompt: 'What is log probability in token prediction?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 1480, 6976, 297, 5993, 18988, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:47904 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-15 11:57:14 [engine.py:331] Added request cmpl-ad803d20c5ba4d379278e99708c1132c-0.
INFO 08-15 11:57:18 [metrics.py:417] Avg prompt throughput: 1.8 tokens/s, Avg generation throughput: 78.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.4%, CPU KV cache usage: 0.0%.
INFO 08-15 11:57:23 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 78.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.7%, CPU KV cache usage: 0.0%.
INFO 08-15 11:57:26 [logger.py:43] Received request cmpl-cbbc32a203dc489f8482861129a98d87-0: prompt: 'How to use `TextStreamer` for streaming output?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 304, 671, 421, 1626, 3835, 261, 29952, 363, 24820, 1962, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:44822 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-15 11:57:26 [engine.py:331] Added request cmpl-cbbc32a203dc489f8482861129a98d87-0.
INFO 08-15 11:57:28 [metrics.py:417] Avg prompt throughput: 2.6 tokens/s, Avg generation throughput: 78.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 08-15 11:57:30 [logger.py:43] Received request cmpl-8fe0c3e6263d4b9eae0392c802b9f885-0: prompt: 'How to tokenize and detokenize a prompt manually?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 304, 5993, 675, 322, 1439, 4476, 675, 263, 9508, 7522, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:44836 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-15 11:57:30 [engine.py:331] Added request cmpl-8fe0c3e6263d4b9eae0392c802b9f885-0.
INFO 08-15 11:57:33 [metrics.py:417] Avg prompt throughput: 2.6 tokens/s, Avg generation throughput: 78.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.4%, CPU KV cache usage: 0.0%.
INFO 08-15 11:57:38 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 78.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.7%, CPU KV cache usage: 0.0%.
INFO 08-15 11:57:42 [logger.py:43] Received request cmpl-79a0e1feca57466caf1c7e642b9c3a46-0: prompt: 'Describe how a prompt flows through the transformer layers.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 20355, 915, 920, 263, 9508, 24536, 1549, 278, 4327, 261, 15359, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:54632 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-15 11:57:42 [engine.py:331] Added request cmpl-79a0e1feca57466caf1c7e642b9c3a46-0.
INFO 08-15 11:57:43 [metrics.py:417] Avg prompt throughput: 2.6 tokens/s, Avg generation throughput: 78.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-15 11:57:48 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 78.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.5%, CPU KV cache usage: 0.0%.
INFO 08-15 11:57:53 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 78.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.8%, CPU KV cache usage: 0.0%.
INFO 08-15 11:57:55 [logger.py:43] Received request cmpl-5424f359fc354c1ea0b5883f3fe5fd91-0: prompt: 'How to handle EOS in streaming responses?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 304, 4386, 382, 3267, 297, 24820, 20890, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:37300 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-15 11:57:55 [engine.py:331] Added request cmpl-5424f359fc354c1ea0b5883f3fe5fd91-0.
INFO 08-15 11:57:58 [metrics.py:417] Avg prompt throughput: 2.0 tokens/s, Avg generation throughput: 78.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 08-15 11:57:59 [logger.py:43] Received request cmpl-fb179b027a6344c19ddccef86e419e41-0: prompt: 'What is a model config JSON in HF models?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 263, 1904, 2295, 4663, 297, 379, 29943, 4733, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:37306 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-15 11:57:59 [engine.py:331] Added request cmpl-fb179b027a6344c19ddccef86e419e41-0.
INFO 08-15 11:58:03 [metrics.py:417] Avg prompt throughput: 2.4 tokens/s, Avg generation throughput: 78.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.4%, CPU KV cache usage: 0.0%.
INFO 08-15 11:58:08 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 78.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.7%, CPU KV cache usage: 0.0%.
INFO 08-15 11:58:12 [logger.py:43] Received request cmpl-95c734af02e44ba18f7fcd447e1a82a8-0: prompt: 'What is kv_cache reuse in decoder models?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 10908, 29918, 8173, 24270, 297, 1602, 6119, 4733, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:43498 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-15 11:58:12 [engine.py:331] Added request cmpl-95c734af02e44ba18f7fcd447e1a82a8-0.
INFO 08-15 11:58:13 [metrics.py:417] Avg prompt throughput: 2.4 tokens/s, Avg generation throughput: 78.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-15 11:58:18 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 78.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.5%, CPU KV cache usage: 0.0%.
INFO 08-15 11:58:23 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 78.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.8%, CPU KV cache usage: 0.0%.
INFO 08-15 11:58:25 [logger.py:43] Received request cmpl-ae0580d332b14079b1aec5a808df71b7-0: prompt: 'Describe a scenario of real-time chatbot streaming.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 20355, 915, 263, 10483, 310, 1855, 29899, 2230, 13563, 7451, 24820, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:48790 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-15 11:58:25 [engine.py:331] Added request cmpl-ae0580d332b14079b1aec5a808df71b7-0.
INFO 08-15 11:58:28 [metrics.py:417] Avg prompt throughput: 2.6 tokens/s, Avg generation throughput: 78.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.4%, CPU KV cache usage: 0.0%.
INFO 08-15 11:58:33 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 78.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.7%, CPU KV cache usage: 0.0%.
INFO 08-15 11:58:37 [logger.py:43] Received request cmpl-22fa61ff3e8a48ba8e622f4918bf3a1c-0: prompt: 'How to compute throughput with variable prompts?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 304, 10272, 1549, 649, 411, 2286, 9508, 29879, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:52732 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-15 11:58:37 [engine.py:331] Added request cmpl-22fa61ff3e8a48ba8e622f4918bf3a1c-0.
INFO 08-15 11:58:38 [metrics.py:417] Avg prompt throughput: 2.2 tokens/s, Avg generation throughput: 78.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-15 11:58:43 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 78.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.5%, CPU KV cache usage: 0.0%.
INFO 08-15 11:58:48 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 78.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.8%, CPU KV cache usage: 0.0%.
INFO 08-15 11:58:50 [logger.py:43] Received request cmpl-46109c0d07514ba28cd4144af66d41c9-0: prompt: 'How does `max_new_tokens` differ from `max_length`?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 947, 421, 3317, 29918, 1482, 29918, 517, 12360, 29952, 1163, 515, 421, 3317, 29918, 2848, 6522], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:60494 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-15 11:58:50 [engine.py:331] Added request cmpl-46109c0d07514ba28cd4144af66d41c9-0.
INFO 08-15 11:58:53 [metrics.py:417] Avg prompt throughput: 3.6 tokens/s, Avg generation throughput: 78.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.4%, CPU KV cache usage: 0.0%.
INFO 08-15 11:58:56 [logger.py:43] Received request cmpl-ca66fcad86e54d90b3d7c7ea38210443-0: prompt: 'Explain the difference between loss and perplexity.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 12027, 7420, 278, 4328, 1546, 6410, 322, 639, 10709, 537, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:44356 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-15 11:58:56 [engine.py:331] Added request cmpl-ca66fcad86e54d90b3d7c7ea38210443-0.
INFO 08-15 11:58:58 [metrics.py:417] Avg prompt throughput: 2.4 tokens/s, Avg generation throughput: 78.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 08-15 11:59:03 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 78.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.6%, CPU KV cache usage: 0.0%.
INFO 08-15 11:59:08 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 78.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.9%, CPU KV cache usage: 0.0%.
INFO 08-15 11:59:08 [logger.py:43] Received request cmpl-9cbff9fc4c14478b98b3d01bc2df096b-0: prompt: 'How to log benchmark results in a structured CSV?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 304, 1480, 23513, 2582, 297, 263, 2281, 2955, 16874, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:44250 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-15 11:59:08 [engine.py:331] Added request cmpl-9cbff9fc4c14478b98b3d01bc2df096b-0.
INFO 08-15 11:59:13 [metrics.py:417] Avg prompt throughput: 2.4 tokens/s, Avg generation throughput: 78.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.4%, CPU KV cache usage: 0.0%.
INFO 08-15 11:59:18 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 78.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.8%, CPU KV cache usage: 0.0%.
INFO 08-15 11:59:21 [logger.py:43] Received request cmpl-ca8bb97141a841a0b64ca4dae2e678aa-0: prompt: 'What are attention heads and how do they specialize?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 526, 8570, 15883, 322, 920, 437, 896, 4266, 675, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:55226 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-15 11:59:21 [engine.py:331] Added request cmpl-ca8bb97141a841a0b64ca4dae2e678aa-0.
INFO 08-15 11:59:23 [metrics.py:417] Avg prompt throughput: 2.4 tokens/s, Avg generation throughput: 78.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 08-15 11:59:28 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 78.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.6%, CPU KV cache usage: 0.0%.
INFO 08-15 11:59:33 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 78.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.9%, CPU KV cache usage: 0.0%.
INFO 08-15 11:59:34 [logger.py:43] Received request cmpl-36b61074b13345249ce99f84837d1a78-0: prompt: 'How to ensure reproducible benchmarking?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 304, 9801, 9483, 15520, 23513, 292, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:42400 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-15 11:59:34 [engine.py:331] Added request cmpl-36b61074b13345249ce99f84837d1a78-0.
INFO 08-15 11:59:38 [metrics.py:417] Avg prompt throughput: 1.8 tokens/s, Avg generation throughput: 78.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.4%, CPU KV cache usage: 0.0%.
INFO 08-15 11:59:43 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 78.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.7%, CPU KV cache usage: 0.0%.
INFO 08-15 11:59:47 [logger.py:43] Received request cmpl-8a5bca59f5424418aa3d40f7eaaa6092-0: prompt: 'What are model shards and when are they used?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 526, 1904, 528, 3163, 322, 746, 526, 896, 1304, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:50232 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-15 11:59:47 [engine.py:331] Added request cmpl-8a5bca59f5424418aa3d40f7eaaa6092-0.
INFO 08-15 11:59:48 [metrics.py:417] Avg prompt throughput: 2.4 tokens/s, Avg generation throughput: 78.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 08-15 11:59:53 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 78.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.6%, CPU KV cache usage: 0.0%.
INFO 08-15 11:59:58 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 78.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.9%, CPU KV cache usage: 0.0%.
INFO 08-15 11:59:59 [logger.py:43] Received request cmpl-88c3f4bbbb4e4ab894e51066a2b327f0-0: prompt: 'Explain the relationship between prompt length and latency.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 12027, 7420, 278, 9443, 1546, 9508, 3309, 322, 23316, 1270, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:33634 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-15 11:59:59 [engine.py:331] Added request cmpl-88c3f4bbbb4e4ab894e51066a2b327f0-0.
INFO 08-15 12:00:03 [metrics.py:417] Avg prompt throughput: 2.4 tokens/s, Avg generation throughput: 78.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.4%, CPU KV cache usage: 0.0%.
INFO 08-15 12:00:08 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 78.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.7%, CPU KV cache usage: 0.0%.
INFO 08-15 12:00:12 [logger.py:43] Received request cmpl-083f83797e654ee198ecb37746ee2d41-0: prompt: 'What is batch inference and why is it faster?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 9853, 27262, 322, 2020, 338, 372, 8473, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:36864 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-15 12:00:12 [engine.py:331] Added request cmpl-083f83797e654ee198ecb37746ee2d41-0.
INFO 08-15 12:00:13 [metrics.py:417] Avg prompt throughput: 2.2 tokens/s, Avg generation throughput: 78.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-15 12:00:17 [logger.py:43] Received request cmpl-c2501f888aae4346aae5b1448e79d05e-0: prompt: 'What are rotary sin-cos embeddings?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 526, 5731, 653, 4457, 29899, 3944, 8297, 29881, 886, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:36874 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-15 12:00:17 [engine.py:331] Added request cmpl-c2501f888aae4346aae5b1448e79d05e-0.
INFO 08-15 12:00:18 [metrics.py:417] Avg prompt throughput: 2.4 tokens/s, Avg generation throughput: 78.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 08-15 12:00:23 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 78.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.6%, CPU KV cache usage: 0.0%.
INFO 08-15 12:00:28 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 78.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.9%, CPU KV cache usage: 0.0%.
INFO 08-15 12:00:29 [logger.py:43] Received request cmpl-478eba74ce5c4ef59aa41fcf4a5e67da-0: prompt: "Describe HuggingFace's `AutoTokenizer` features.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 20355, 915, 379, 688, 3460, 23360, 29915, 29879, 421, 12300, 6066, 3950, 29952, 5680, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:56822 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-15 12:00:29 [engine.py:331] Added request cmpl-478eba74ce5c4ef59aa41fcf4a5e67da-0.
INFO 08-15 12:00:33 [metrics.py:417] Avg prompt throughput: 3.2 tokens/s, Avg generation throughput: 78.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.4%, CPU KV cache usage: 0.0%.
INFO 08-15 12:00:38 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 78.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.7%, CPU KV cache usage: 0.0%.
INFO 08-15 12:00:42 [logger.py:43] Received request cmpl-2375be0ecbb44aff83e787953cb1ea24-0: prompt: 'How does positional encoding interact with attention?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 947, 2602, 284, 8025, 16254, 411, 8570, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:51882 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-15 12:00:42 [engine.py:331] Added request cmpl-2375be0ecbb44aff83e787953cb1ea24-0.
INFO 08-15 12:00:43 [metrics.py:417] Avg prompt throughput: 2.0 tokens/s, Avg generation throughput: 78.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-15 12:00:48 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 78.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.5%, CPU KV cache usage: 0.0%.
INFO 08-15 12:00:53 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 78.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.9%, CPU KV cache usage: 0.0%.
INFO 08-15 12:00:55 [logger.py:43] Received request cmpl-3429026d0c3b4ccab095b782c1c66f37-0: prompt: 'What is a streaming callback in Transformers?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 263, 24820, 6939, 297, 4103, 689, 414, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:50866 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-15 12:00:55 [engine.py:331] Added request cmpl-3429026d0c3b4ccab095b782c1c66f37-0.
INFO 08-15 12:00:58 [metrics.py:417] Avg prompt throughput: 2.2 tokens/s, Avg generation throughput: 78.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.4%, CPU KV cache usage: 0.0%.
INFO 08-15 12:00:58 [logger.py:43] Received request cmpl-ee212344babb4ace979f34a20b0049dc-0: prompt: 'How to benchmark models using curl and Python?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 304, 23513, 4733, 773, 11051, 322, 5132, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:50872 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-15 12:00:58 [engine.py:331] Added request cmpl-ee212344babb4ace979f34a20b0049dc-0.
INFO 08-15 12:01:03 [metrics.py:417] Avg prompt throughput: 2.0 tokens/s, Avg generation throughput: 78.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.5%, CPU KV cache usage: 0.0%.
INFO 08-15 12:01:08 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 78.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.8%, CPU KV cache usage: 0.0%.
INFO 08-15 12:01:11 [logger.py:43] Received request cmpl-2ce652d54a36497793a5606771670e8d-0: prompt: 'How to parse streaming responses from vLLM?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 304, 6088, 24820, 20890, 515, 325, 2208, 29924, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:37586 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-15 12:01:11 [engine.py:331] Added request cmpl-2ce652d54a36497793a5606771670e8d-0.
INFO 08-15 12:01:13 [metrics.py:417] Avg prompt throughput: 2.2 tokens/s, Avg generation throughput: 78.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 08-15 12:01:18 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 78.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.6%, CPU KV cache usage: 0.0%.
INFO 08-15 12:01:23 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 78.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.9%, CPU KV cache usage: 0.0%.
INFO 08-15 12:01:24 [logger.py:43] Received request cmpl-215cd50a9c11464fb11fafbeff50b0c0-0: prompt: 'How does deep caching work in inference?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 947, 6483, 22488, 664, 297, 27262, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:38046 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-15 12:01:24 [engine.py:331] Added request cmpl-215cd50a9c11464fb11fafbeff50b0c0-0.
INFO 08-15 12:01:28 [metrics.py:417] Avg prompt throughput: 1.8 tokens/s, Avg generation throughput: 78.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.4%, CPU KV cache usage: 0.0%.
INFO 08-15 12:01:33 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 78.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.8%, CPU KV cache usage: 0.0%.
INFO 08-15 12:01:36 [logger.py:43] Received request cmpl-4540cb38bba946ce82a25a17298abf03-0: prompt: 'What is a prompt-truncation error?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 263, 9508, 29899, 509, 4661, 362, 1059, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:46678 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-15 12:01:36 [engine.py:331] Added request cmpl-4540cb38bba946ce82a25a17298abf03-0.
INFO 08-15 12:01:38 [metrics.py:417] Avg prompt throughput: 2.2 tokens/s, Avg generation throughput: 78.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 08-15 12:01:43 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 78.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.6%, CPU KV cache usage: 0.0%.
INFO 08-15 12:01:48 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 78.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.9%, CPU KV cache usage: 0.0%.
INFO 08-15 12:01:49 [logger.py:43] Received request cmpl-2716b2d68f8942cea4fadff7ad3c8586-0: prompt: 'Explain the role of tokenizer config files.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 12027, 7420, 278, 6297, 310, 5993, 3950, 2295, 2066, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:33322 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-15 12:01:49 [engine.py:331] Added request cmpl-2716b2d68f8942cea4fadff7ad3c8586-0.
INFO 08-15 12:01:53 [metrics.py:417] Avg prompt throughput: 2.2 tokens/s, Avg generation throughput: 78.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.4%, CPU KV cache usage: 0.0%.
INFO 08-15 12:01:58 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 78.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.7%, CPU KV cache usage: 0.0%.
INFO 08-15 12:02:02 [logger.py:43] Received request cmpl-21c6818d40204b7c9f0f616043982679-0: prompt: 'How to batch multiple prompts into one request?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 304, 9853, 2999, 9508, 29879, 964, 697, 2009, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:56950 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-15 12:02:02 [engine.py:331] Added request cmpl-21c6818d40204b7c9f0f616043982679-0.
INFO 08-15 12:02:03 [metrics.py:417] Avg prompt throughput: 2.2 tokens/s, Avg generation throughput: 78.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 08-15 12:02:08 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 78.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.6%, CPU KV cache usage: 0.0%.
INFO 08-15 12:02:13 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 78.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.9%, CPU KV cache usage: 0.0%.
INFO 08-15 12:02:15 [logger.py:43] Received request cmpl-fb2acd3c041342d0b71df93e079ce173-0: prompt: 'What is a chat template in HuggingFace?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 263, 13563, 4472, 297, 379, 688, 3460, 23360, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:37674 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-15 12:02:15 [engine.py:331] Added request cmpl-fb2acd3c041342d0b71df93e079ce173-0.
INFO 08-15 12:02:18 [metrics.py:417] Avg prompt throughput: 2.4 tokens/s, Avg generation throughput: 78.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.4%, CPU KV cache usage: 0.0%.
INFO 08-15 12:02:23 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 78.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.7%, CPU KV cache usage: 0.0%.
INFO 08-15 12:02:27 [logger.py:43] Received request cmpl-0ca3b19e19c24092ad564583a1db6fa5-0: prompt: 'What is inference latency and why is it important?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 27262, 23316, 1270, 322, 2020, 338, 372, 4100, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:54046 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-15 12:02:27 [engine.py:331] Added request cmpl-0ca3b19e19c24092ad564583a1db6fa5-0.
INFO 08-15 12:02:28 [metrics.py:417] Avg prompt throughput: 2.4 tokens/s, Avg generation throughput: 78.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-15 12:02:33 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 78.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.5%, CPU KV cache usage: 0.0%.
INFO 08-15 12:02:38 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 78.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.9%, CPU KV cache usage: 0.0%.
INFO 08-15 12:02:40 [logger.py:43] Received request cmpl-75b1c0b0ad224ad4a8e6917d09cd2fda-0: prompt: 'How to reduce memory footprint of LLaMA models?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 304, 10032, 3370, 3661, 2158, 310, 365, 5661, 1529, 4733, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:58432 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-15 12:02:40 [engine.py:331] Added request cmpl-75b1c0b0ad224ad4a8e6917d09cd2fda-0.
INFO 08-15 12:02:44 [metrics.py:417] Avg prompt throughput: 2.6 tokens/s, Avg generation throughput: 78.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.4%, CPU KV cache usage: 0.0%.
INFO 08-15 12:02:49 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 78.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.7%, CPU KV cache usage: 0.0%.
INFO 08-15 12:02:53 [logger.py:43] Received request cmpl-9a190952f3b8404a9883c73bad32817f-0: prompt: 'What is the difference between `generation_config.json` and `config.json`?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 278, 4328, 1546, 421, 4738, 362, 29918, 2917, 29889, 3126, 29952, 322, 421, 2917, 29889, 3126, 6522], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:56154 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-15 12:02:53 [engine.py:331] Added request cmpl-9a190952f3b8404a9883c73bad32817f-0.
INFO 08-15 12:02:54 [metrics.py:417] Avg prompt throughput: 4.0 tokens/s, Avg generation throughput: 78.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-15 12:02:59 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 78.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.5%, CPU KV cache usage: 0.0%.
INFO 08-15 12:03:03 [logger.py:43] Received request cmpl-214ceda21b034ff088e8427f28d185d7-0: prompt: 'What is a quantized model and how is it served?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 263, 4323, 1891, 1904, 322, 920, 338, 372, 6766, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:52520 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-15 12:03:03 [engine.py:331] Added request cmpl-214ceda21b034ff088e8427f28d185d7-0.
INFO 08-15 12:03:04 [metrics.py:417] Avg prompt throughput: 2.6 tokens/s, Avg generation throughput: 78.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-15 12:03:09 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 78.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.5%, CPU KV cache usage: 0.0%.
INFO 08-15 12:03:14 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 78.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.8%, CPU KV cache usage: 0.0%.
INFO 08-15 12:03:15 [logger.py:43] Received request cmpl-fd63396f19b344bc8ae318fa80216c14-0: prompt: 'How to handle prompt overflow in generation?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 304, 4386, 9508, 11969, 297, 12623, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:39150 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-15 12:03:15 [engine.py:331] Added request cmpl-fd63396f19b344bc8ae318fa80216c14-0.
INFO 08-15 12:03:19 [metrics.py:417] Avg prompt throughput: 1.8 tokens/s, Avg generation throughput: 78.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 08-15 12:03:21 [logger.py:43] Received request cmpl-ae95d8cfe2134c07a84f3f59d634eded-0: prompt: 'What is warmup time in inference pipelines?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 14294, 786, 931, 297, 27262, 8450, 24210, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:55592 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-15 12:03:21 [engine.py:331] Added request cmpl-ae95d8cfe2134c07a84f3f59d634eded-0.
INFO 08-15 12:03:24 [metrics.py:417] Avg prompt throughput: 2.2 tokens/s, Avg generation throughput: 78.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 08-15 12:03:29 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 78.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.6%, CPU KV cache usage: 0.0%.
INFO 08-15 12:03:34 [logger.py:43] Received request cmpl-07bea75237ef45ec802caacfa7657fd2-0: prompt: 'How to stream JSON tokens using curl?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 304, 4840, 4663, 18897, 773, 11051, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:46276 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-15 12:03:34 [engine.py:331] Added request cmpl-07bea75237ef45ec802caacfa7657fd2-0.
INFO 08-15 12:03:34 [metrics.py:417] Avg prompt throughput: 1.8 tokens/s, Avg generation throughput: 78.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-15 12:03:39 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 78.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.5%, CPU KV cache usage: 0.0%.
INFO 08-15 12:03:44 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 78.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.8%, CPU KV cache usage: 0.0%.
INFO 08-15 12:03:46 [logger.py:43] Received request cmpl-ff346692207e4c929221d909d05e7a9b-0: prompt: 'What is a `generate_with_streaming()` wrapper?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 263, 421, 17158, 29918, 2541, 29918, 5461, 292, 2555, 14476, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:56804 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-15 12:03:46 [engine.py:331] Added request cmpl-ff346692207e4c929221d909d05e7a9b-0.
INFO 08-15 12:03:49 [metrics.py:417] Avg prompt throughput: 2.8 tokens/s, Avg generation throughput: 78.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 08-15 12:03:54 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 78.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.6%, CPU KV cache usage: 0.0%.
INFO 08-15 12:03:59 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 78.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.9%, CPU KV cache usage: 0.0%.
INFO 08-15 12:03:59 [logger.py:43] Received request cmpl-707ff35799ff4d1d83a0a0368ed315d9-0: prompt: 'How to evaluate TTFT and ITL for multiple prompts?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 304, 14707, 323, 29911, 7818, 322, 13315, 29931, 363, 2999, 9508, 29879, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:59750 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-15 12:03:59 [engine.py:331] Added request cmpl-707ff35799ff4d1d83a0a0368ed315d9-0.
INFO 08-15 12:04:04 [metrics.py:417] Avg prompt throughput: 3.0 tokens/s, Avg generation throughput: 78.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.4%, CPU KV cache usage: 0.0%.
INFO 08-15 12:04:09 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 78.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.8%, CPU KV cache usage: 0.0%.
INFO 08-15 12:04:12 [logger.py:43] Received request cmpl-e981f19de0f546ea8d64686a631dcd3b-0: prompt: 'What are server cold starts in model serving?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 526, 1923, 11220, 8665, 297, 1904, 16330, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:57504 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-15 12:04:12 [engine.py:331] Added request cmpl-e981f19de0f546ea8d64686a631dcd3b-0.
INFO 08-15 12:04:14 [metrics.py:417] Avg prompt throughput: 2.0 tokens/s, Avg generation throughput: 78.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 08-15 12:04:19 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 78.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.6%, CPU KV cache usage: 0.0%.
INFO 08-15 12:04:24 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 78.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.9%, CPU KV cache usage: 0.0%.
INFO 08-15 12:04:24 [logger.py:43] Received request cmpl-41422e50cdd348ba86f01613856e4bae-0: prompt: 'Explain how to avoid padding inefficiencies.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 12027, 7420, 920, 304, 4772, 7164, 297, 29872, 2416, 819, 2478, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:35418 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-15 12:04:24 [engine.py:331] Added request cmpl-41422e50cdd348ba86f01613856e4bae-0.
INFO 08-15 12:04:29 [metrics.py:417] Avg prompt throughput: 2.6 tokens/s, Avg generation throughput: 78.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.4%, CPU KV cache usage: 0.0%.
INFO 08-15 12:04:34 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 78.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.7%, CPU KV cache usage: 0.0%.
INFO 08-15 12:04:37 [logger.py:43] Received request cmpl-4cdea4dd45ae43268b6a91c93d94dccc-0: prompt: 'How does tokenizer pre-processing affect TTFT?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 947, 5993, 3950, 758, 29899, 19170, 6602, 323, 29911, 7818, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:47588 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-15 12:04:37 [engine.py:331] Added request cmpl-4cdea4dd45ae43268b6a91c93d94dccc-0.
INFO 08-15 12:04:39 [metrics.py:417] Avg prompt throughput: 2.6 tokens/s, Avg generation throughput: 78.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 08-15 12:04:44 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 78.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.6%, CPU KV cache usage: 0.0%.
INFO 08-15 12:04:49 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 78.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.9%, CPU KV cache usage: 0.0%.
INFO 08-15 12:04:50 [logger.py:43] Received request cmpl-6de09a37f9674da0a1a9e3917ec4e38e-0: prompt: 'How to track per-prompt latency in logs?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 304, 5702, 639, 29899, 14032, 415, 23316, 1270, 297, 10748, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:39806 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-15 12:04:50 [engine.py:331] Added request cmpl-6de09a37f9674da0a1a9e3917ec4e38e-0.
INFO 08-15 12:04:54 [metrics.py:417] Avg prompt throughput: 2.6 tokens/s, Avg generation throughput: 78.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.4%, CPU KV cache usage: 0.0%.
INFO 08-15 12:04:59 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 78.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.7%, CPU KV cache usage: 0.0%.
INFO 08-15 12:05:02 [logger.py:43] Received request cmpl-d3990e99bc68463e8ce552eaa4e01460-0: prompt: 'Explain streaming benchmarks with high concurrency.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 12027, 7420, 24820, 23513, 29879, 411, 1880, 3022, 10880, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:41732 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-15 12:05:02 [engine.py:331] Added request cmpl-d3990e99bc68463e8ce552eaa4e01460-0.
INFO 08-15 12:05:04 [metrics.py:417] Avg prompt throughput: 2.2 tokens/s, Avg generation throughput: 78.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-15 12:05:09 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 78.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.5%, CPU KV cache usage: 0.0%.
INFO 08-15 12:05:14 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 78.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.9%, CPU KV cache usage: 0.0%.
INFO 08-15 12:05:15 [logger.py:43] Received request cmpl-37b68e3ccf444418a47d4dff9c29affd-0: prompt: 'How to calculate power usage per prompt?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 304, 8147, 3081, 8744, 639, 9508, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:52308 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-15 12:05:15 [engine.py:331] Added request cmpl-37b68e3ccf444418a47d4dff9c29affd-0.
INFO 08-15 12:05:19 [metrics.py:417] Avg prompt throughput: 1.8 tokens/s, Avg generation throughput: 78.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.4%, CPU KV cache usage: 0.0%.
INFO 08-15 12:05:24 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 78.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.7%, CPU KV cache usage: 0.0%.
INFO 08-15 12:05:28 [logger.py:43] Received request cmpl-c33d84e156cb4bf8b3a7b4ace27e93fc-0: prompt: 'How does the tokenizer handle emojis and symbols?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 947, 278, 5993, 3950, 4386, 953, 3848, 275, 322, 15072, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:49742 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-15 12:05:28 [engine.py:331] Added request cmpl-c33d84e156cb4bf8b3a7b4ace27e93fc-0.
INFO 08-15 12:05:29 [metrics.py:417] Avg prompt throughput: 2.6 tokens/s, Avg generation throughput: 78.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-15 12:05:34 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 78.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.5%, CPU KV cache usage: 0.0%.
INFO 08-15 12:05:39 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 78.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.8%, CPU KV cache usage: 0.0%.
INFO 08-15 12:05:41 [logger.py:43] Received request cmpl-39344ff28b70463480a0a93530994cec-0: prompt: 'How does model size affect inference latency?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 947, 1904, 2159, 6602, 27262, 23316, 1270, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:42860 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-15 12:05:41 [engine.py:331] Added request cmpl-39344ff28b70463480a0a93530994cec-0.
INFO 08-15 12:05:44 [metrics.py:417] Avg prompt throughput: 2.0 tokens/s, Avg generation throughput: 78.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.4%, CPU KV cache usage: 0.0%.
INFO 08-15 12:05:49 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 78.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.7%, CPU KV cache usage: 0.0%.
INFO 08-15 12:05:53 [logger.py:43] Received request cmpl-14d0e737a0ee4609aedc97ae88faad13-0: prompt: 'How to debug slow streaming responses?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 304, 4744, 5232, 24820, 20890, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:37288 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-15 12:05:53 [engine.py:331] Added request cmpl-14d0e737a0ee4609aedc97ae88faad13-0.
INFO 08-15 12:05:54 [metrics.py:417] Avg prompt throughput: 1.6 tokens/s, Avg generation throughput: 78.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-15 12:05:59 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 78.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.5%, CPU KV cache usage: 0.0%.
INFO 08-15 12:06:04 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 78.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.8%, CPU KV cache usage: 0.0%.
INFO 08-15 12:06:06 [logger.py:43] Received request cmpl-424f1558c1e842daae0760fc93d8c2ab-0: prompt: 'What causes generation to stall midway?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 9946, 12623, 304, 380, 497, 7145, 1582, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:40292 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-15 12:06:06 [engine.py:331] Added request cmpl-424f1558c1e842daae0760fc93d8c2ab-0.
INFO 08-15 12:06:09 [metrics.py:417] Avg prompt throughput: 2.0 tokens/s, Avg generation throughput: 78.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 08-15 12:06:14 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 78.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.6%, CPU KV cache usage: 0.0%.
INFO 08-15 12:06:19 [logger.py:43] Received request cmpl-5d9a13e8db7741f5b5271a452b28428d-0: prompt: 'How to load a model on HPU using HF Transformers?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 304, 2254, 263, 1904, 373, 379, 7056, 773, 379, 29943, 4103, 689, 414, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:51528 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-15 12:06:19 [engine.py:331] Added request cmpl-5d9a13e8db7741f5b5271a452b28428d-0.
INFO 08-15 12:06:19 [metrics.py:417] Avg prompt throughput: 3.2 tokens/s, Avg generation throughput: 78.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-15 12:06:24 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 78.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.5%, CPU KV cache usage: 0.0%.
INFO 08-15 12:06:29 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 78.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.8%, CPU KV cache usage: 0.0%.
INFO 08-15 12:06:30 [logger.py:43] Received request cmpl-ca4450580e84476fbbd711c6949c0560-0: prompt: 'How to pass prompt + history into a chat model?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 304, 1209, 9508, 718, 4955, 964, 263, 13563, 1904, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:39442 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-15 12:06:30 [engine.py:331] Added request cmpl-ca4450580e84476fbbd711c6949c0560-0.
INFO 08-15 12:06:34 [metrics.py:417] Avg prompt throughput: 2.4 tokens/s, Avg generation throughput: 78.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.4%, CPU KV cache usage: 0.0%.
INFO 08-15 12:06:39 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 78.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.7%, CPU KV cache usage: 0.0%.
INFO 08-15 12:06:43 [logger.py:43] Received request cmpl-9a2f735a284243f897f1af01d76b6d69-0: prompt: 'What is an OpenAI-compatible inference API?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 385, 4673, 23869, 29899, 23712, 27262, 3450, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:36966 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-15 12:06:43 [engine.py:331] Added request cmpl-9a2f735a284243f897f1af01d76b6d69-0.
INFO 08-15 12:06:44 [metrics.py:417] Avg prompt throughput: 2.2 tokens/s, Avg generation throughput: 78.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-15 12:06:49 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 78.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.5%, CPU KV cache usage: 0.0%.
INFO 08-15 12:06:54 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 78.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.9%, CPU KV cache usage: 0.0%.
INFO 08-15 12:06:55 [logger.py:43] Received request cmpl-6754da85f6b34f87a33175073a19e012-0: prompt: 'How to handle decoding errors in stream?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 304, 4386, 1602, 3689, 4436, 297, 4840, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:60442 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-15 12:06:55 [engine.py:331] Added request cmpl-6754da85f6b34f87a33175073a19e012-0.
INFO 08-15 12:06:59 [metrics.py:417] Avg prompt throughput: 2.0 tokens/s, Avg generation throughput: 78.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.4%, CPU KV cache usage: 0.0%.
INFO 08-15 12:07:04 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 78.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.7%, CPU KV cache usage: 0.0%.
INFO 08-15 12:07:08 [logger.py:43] Received request cmpl-c16a576b63b24e69a4f652c0a3e340a9-0: prompt: 'What are temperature vs randomness in sampling?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 526, 10430, 7186, 4036, 2264, 297, 23460, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:56166 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-15 12:07:08 [engine.py:331] Added request cmpl-c16a576b63b24e69a4f652c0a3e340a9-0.
INFO 08-15 12:07:09 [metrics.py:417] Avg prompt throughput: 2.0 tokens/s, Avg generation throughput: 78.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-15 12:07:14 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 78.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.5%, CPU KV cache usage: 0.0%.
INFO 08-15 12:07:19 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 78.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.8%, CPU KV cache usage: 0.0%.
INFO 08-15 12:07:21 [logger.py:43] Received request cmpl-6cd1f4e2210d40799d3e5e65d0102cbb-0: prompt: 'Explain token logprobs in output.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 12027, 7420, 5993, 1480, 771, 5824, 297, 1962, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:43522 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-15 12:07:21 [engine.py:331] Added request cmpl-6cd1f4e2210d40799d3e5e65d0102cbb-0.
INFO 08-15 12:07:24 [metrics.py:417] Avg prompt throughput: 2.0 tokens/s, Avg generation throughput: 78.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.4%, CPU KV cache usage: 0.0%.
INFO 08-15 12:07:29 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 78.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.7%, CPU KV cache usage: 0.0%.
INFO 08-15 12:07:33 [logger.py:43] Received request cmpl-57f9be1123944a83ad0d2f669a2eadb1-0: prompt: 'How to measure and reduce TTFT on Gaudi?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 304, 5645, 322, 10032, 323, 29911, 7818, 373, 10415, 4749, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:38394 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-15 12:07:33 [engine.py:331] Added request cmpl-57f9be1123944a83ad0d2f669a2eadb1-0.
INFO 08-15 12:07:34 [metrics.py:417] Avg prompt throughput: 2.6 tokens/s, Avg generation throughput: 78.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-15 12:07:39 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 78.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.5%, CPU KV cache usage: 0.0%.
INFO 08-15 12:07:44 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 78.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.8%, CPU KV cache usage: 0.0%.
INFO 08-15 12:07:46 [logger.py:43] Received request cmpl-c812c9b8c0204f478c04ae9c34911267-0: prompt: 'Explain the internals of HF `generate()` loop.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 12027, 7420, 278, 2836, 1338, 310, 379, 29943, 421, 17158, 2555, 2425, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:39154 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-15 12:07:46 [engine.py:331] Added request cmpl-c812c9b8c0204f478c04ae9c34911267-0.
INFO 08-15 12:07:49 [metrics.py:417] Avg prompt throughput: 2.8 tokens/s, Avg generation throughput: 78.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 08-15 12:07:54 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 78.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.6%, CPU KV cache usage: 0.0%.
INFO 08-15 12:07:59 [logger.py:43] Received request cmpl-6deed4ef0e9847319da7c82d9847e4d1-0: prompt: 'What is speculative token generation?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 1580, 28524, 5993, 12623, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:54470 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-15 12:07:59 [engine.py:331] Added request cmpl-6deed4ef0e9847319da7c82d9847e4d1-0.
INFO 08-15 12:07:59 [metrics.py:417] Avg prompt throughput: 1.6 tokens/s, Avg generation throughput: 78.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-15 12:08:04 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 78.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.5%, CPU KV cache usage: 0.0%.
INFO 08-15 12:08:09 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 78.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.8%, CPU KV cache usage: 0.0%.
INFO 08-15 12:08:12 [logger.py:43] Received request cmpl-922a9843119649cf95950c48df5928a1-0: prompt: 'What are possible failure points in vLLM serving?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 526, 1950, 10672, 3291, 297, 325, 2208, 29924, 16330, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:42984 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-15 12:08:12 [engine.py:331] Added request cmpl-922a9843119649cf95950c48df5928a1-0.
INFO 08-15 12:08:14 [metrics.py:417] Avg prompt throughput: 2.4 tokens/s, Avg generation throughput: 78.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 08-15 12:08:19 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 78.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.6%, CPU KV cache usage: 0.0%.
INFO 08-15 12:08:24 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 78.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.9%, CPU KV cache usage: 0.0%.
INFO 08-15 12:08:24 [logger.py:43] Received request cmpl-7655b927b3c540b3bb9a816499ba3f5f-0: prompt: 'How to catch stream decoding errors gracefully?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 304, 4380, 4840, 1602, 3689, 4436, 17659, 3730, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:54286 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-15 12:08:24 [engine.py:331] Added request cmpl-7655b927b3c540b3bb9a816499ba3f5f-0.
INFO 08-15 12:08:29 [metrics.py:417] Avg prompt throughput: 2.2 tokens/s, Avg generation throughput: 78.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.5%, CPU KV cache usage: 0.0%.
INFO 08-15 12:08:34 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 78.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.8%, CPU KV cache usage: 0.0%.
INFO 08-15 12:08:37 [logger.py:43] Received request cmpl-329d7ea6a2c54d68a547076f1bf23df4-0: prompt: 'How to enable/disable kv caching in transformers?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 304, 9025, 29914, 20472, 10908, 22488, 297, 4327, 414, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:56130 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-15 12:08:37 [engine.py:331] Added request cmpl-329d7ea6a2c54d68a547076f1bf23df4-0.
INFO 08-15 12:08:39 [metrics.py:417] Avg prompt throughput: 2.4 tokens/s, Avg generation throughput: 78.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 08-15 12:08:44 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 78.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.6%, CPU KV cache usage: 0.0%.
INFO 08-15 12:08:49 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 78.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.9%, CPU KV cache usage: 0.0%.
INFO 08-15 12:08:50 [logger.py:43] Received request cmpl-fc45908b61b640ff85ca4caae0f5c07b-0: prompt: 'What are challenges of multilingual streaming inference?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 526, 18066, 267, 310, 1773, 6504, 950, 24820, 27262, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:53548 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-15 12:08:50 [engine.py:331] Added request cmpl-fc45908b61b640ff85ca4caae0f5c07b-0.
INFO 08-15 12:08:54 [metrics.py:417] Avg prompt throughput: 2.4 tokens/s, Avg generation throughput: 78.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.4%, CPU KV cache usage: 0.0%.
INFO 08-15 12:08:59 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 78.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.7%, CPU KV cache usage: 0.0%.
INFO 08-15 12:09:02 [logger.py:43] Received request cmpl-9f669fc132a148349f6a6a42ff8b6fc7-0: prompt: 'How to benchmark with different model precisions?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 304, 23513, 411, 1422, 1904, 12132, 1080, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:33236 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-15 12:09:02 [engine.py:331] Added request cmpl-9f669fc132a148349f6a6a42ff8b6fc7-0.
INFO 08-15 12:09:04 [metrics.py:417] Avg prompt throughput: 2.0 tokens/s, Avg generation throughput: 78.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 08-15 12:09:09 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 78.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.6%, CPU KV cache usage: 0.0%.
INFO 08-15 12:09:14 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 78.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.9%, CPU KV cache usage: 0.0%.
INFO 08-15 12:09:15 [logger.py:43] Received request cmpl-b64eb45ea9ea47c1a0483980a80ddbc8-0: prompt: 'What is the tradeoff between token latency and batch size?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 278, 11302, 2696, 1546, 5993, 23316, 1270, 322, 9853, 2159, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:51876 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-15 12:09:15 [engine.py:331] Added request cmpl-b64eb45ea9ea47c1a0483980a80ddbc8-0.
INFO 08-15 12:09:19 [metrics.py:417] Avg prompt throughput: 2.8 tokens/s, Avg generation throughput: 78.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.4%, CPU KV cache usage: 0.0%.
INFO 08-15 12:09:24 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 78.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.7%, CPU KV cache usage: 0.0%.
INFO 08-15 12:09:28 [logger.py:43] Received request cmpl-2ce342985c124d788bc72f2d66356104-0: prompt: 'Describe the advantages and limitations of transformer-based neural networks.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 20355, 915, 278, 25486, 322, 27028, 310, 4327, 261, 29899, 6707, 19677, 14379, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:40428 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-15 12:09:28 [engine.py:331] Added request cmpl-2ce342985c124d788bc72f2d66356104-0.
INFO 08-15 12:09:29 [metrics.py:417] Avg prompt throughput: 3.0 tokens/s, Avg generation throughput: 78.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 08-15 12:09:34 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 78.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.6%, CPU KV cache usage: 0.0%.
INFO 08-15 12:09:39 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 78.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.9%, CPU KV cache usage: 0.0%.
INFO 08-15 12:09:40 [logger.py:43] Received request cmpl-8601713ad3d74a0ba1e538f4330ff8bf-0: prompt: 'Explain the concept of overfitting in machine learning and how to prevent it.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 12027, 7420, 278, 6964, 310, 975, 29888, 5367, 297, 4933, 6509, 322, 920, 304, 5557, 372, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:58620 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-15 12:09:40 [engine.py:331] Added request cmpl-8601713ad3d74a0ba1e538f4330ff8bf-0.
INFO 08-15 12:09:44 [metrics.py:417] Avg prompt throughput: 3.6 tokens/s, Avg generation throughput: 78.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.4%, CPU KV cache usage: 0.0%.
INFO 08-15 12:09:49 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 78.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.7%, CPU KV cache usage: 0.0%.
INFO 08-15 12:09:53 [logger.py:43] Received request cmpl-296872f12eb0436d8c4ae19aea3402b6-0: prompt: 'How does the attention mechanism work in transformer models?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 947, 278, 8570, 13336, 664, 297, 4327, 261, 4733, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:60976 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-15 12:09:53 [engine.py:331] Added request cmpl-296872f12eb0436d8c4ae19aea3402b6-0.
INFO 08-15 12:09:54 [metrics.py:417] Avg prompt throughput: 2.4 tokens/s, Avg generation throughput: 78.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-15 12:09:59 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 78.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.5%, CPU KV cache usage: 0.0%.
INFO 08-15 12:10:04 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 78.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.8%, CPU KV cache usage: 0.0%.
INFO 08-15 12:10:06 [logger.py:43] Received request cmpl-d0ab321632984360b0b5b2fbcd5a08ff-0: prompt: 'Summarize the main differences between supervised and unsupervised learning.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 6991, 3034, 675, 278, 1667, 12651, 1546, 2428, 11292, 322, 443, 9136, 11292, 6509, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:47836 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-15 12:10:06 [engine.py:331] Added request cmpl-d0ab321632984360b0b5b2fbcd5a08ff-0.
INFO 08-15 12:10:09 [metrics.py:417] Avg prompt throughput: 3.2 tokens/s, Avg generation throughput: 78.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.4%, CPU KV cache usage: 0.0%.
INFO 08-15 12:10:14 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 78.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.7%, CPU KV cache usage: 0.0%.
INFO 08-15 12:10:18 [logger.py:43] Received request cmpl-991e4175e33e472db035700bd242206b-0: prompt: 'What are the ethical concerns associated with deploying large language models?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 526, 278, 11314, 936, 21838, 6942, 411, 7246, 292, 2919, 4086, 4733, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:38434 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-15 12:10:18 [engine.py:331] Added request cmpl-991e4175e33e472db035700bd242206b-0.
INFO 08-15 12:10:19 [metrics.py:417] Avg prompt throughput: 3.0 tokens/s, Avg generation throughput: 78.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-15 12:10:24 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 78.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.5%, CPU KV cache usage: 0.0%.
INFO 08-15 12:10:29 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 78.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.8%, CPU KV cache usage: 0.0%.
INFO 08-15 12:10:31 [logger.py:43] Received request cmpl-4aa641e854e14422b6e0f92ba69f2ff6-0: prompt: 'Explain how gradient descent works in training deep neural networks.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 12027, 7420, 920, 16030, 26815, 1736, 297, 6694, 6483, 19677, 14379, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:39022 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-15 12:10:31 [engine.py:331] Added request cmpl-4aa641e854e14422b6e0f92ba69f2ff6-0.
INFO 08-15 12:10:34 [metrics.py:417] Avg prompt throughput: 2.6 tokens/s, Avg generation throughput: 78.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.4%, CPU KV cache usage: 0.0%.
INFO 08-15 12:10:39 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 78.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.7%, CPU KV cache usage: 0.0%.
INFO 08-15 12:10:44 [logger.py:43] Received request cmpl-edad3ed355104de3902776b1a53f9544-0: prompt: 'Describe the role of activation functions in deep learning.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 20355, 915, 278, 6297, 310, 26229, 3168, 297, 6483, 6509, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:41694 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-15 12:10:44 [engine.py:331] Added request cmpl-edad3ed355104de3902776b1a53f9544-0.
INFO 08-15 12:10:44 [metrics.py:417] Avg prompt throughput: 2.4 tokens/s, Avg generation throughput: 78.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-15 12:10:49 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 78.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.5%, CPU KV cache usage: 0.0%.
INFO 08-15 12:10:54 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 78.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.8%, CPU KV cache usage: 0.0%.
INFO 08-15 12:10:57 [logger.py:43] Received request cmpl-0b4f18fff63b40dba0f2612d8b0006f1-0: prompt: 'How can transfer learning improve model performance on small datasets?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 508, 6782, 6509, 11157, 1904, 4180, 373, 2319, 20035, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:36524 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-15 12:10:57 [engine.py:331] Added request cmpl-0b4f18fff63b40dba0f2612d8b0006f1-0.
INFO 08-15 12:10:59 [metrics.py:417] Avg prompt throughput: 2.4 tokens/s, Avg generation throughput: 78.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 08-15 12:11:04 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 78.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.6%, CPU KV cache usage: 0.0%.
INFO 08-15 12:11:09 [logger.py:43] Received request cmpl-3c30562129a44e179624ec3118bd6854-0: prompt: 'Explain the concept of reinforcement learning with an example.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 12027, 7420, 278, 6964, 310, 15561, 1454, 13561, 6509, 411, 385, 1342, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:54978 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-15 12:11:09 [engine.py:331] Added request cmpl-3c30562129a44e179624ec3118bd6854-0.
INFO 08-15 12:11:09 [metrics.py:417] Avg prompt throughput: 2.8 tokens/s, Avg generation throughput: 78.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-15 12:11:14 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 78.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.5%, CPU KV cache usage: 0.0%.
INFO 08-15 12:11:19 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 78.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.8%, CPU KV cache usage: 0.0%.
INFO 08-15 12:11:22 [logger.py:43] Received request cmpl-81bb68288af541c5a35adfd53557e470-0: prompt: 'What are the main components of a convolutional neural network (CNN)?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 526, 278, 1667, 7117, 310, 263, 26851, 284, 19677, 3564, 313, 29907, 10262, 6877], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:53872 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-15 12:11:22 [engine.py:331] Added request cmpl-81bb68288af541c5a35adfd53557e470-0.
INFO 08-15 12:11:24 [metrics.py:417] Avg prompt throughput: 3.2 tokens/s, Avg generation throughput: 78.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 08-15 12:11:29 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 78.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.6%, CPU KV cache usage: 0.0%.
INFO 08-15 12:11:34 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 78.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.9%, CPU KV cache usage: 0.0%.
INFO 08-15 12:11:35 [logger.py:43] Received request cmpl-b495282ebc10403bba0543a60a2de666-0: prompt: 'Describe how large language models like GPT or LLaMA are trained and fine-tuned.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 20355, 915, 920, 2919, 4086, 4733, 763, 402, 7982, 470, 365, 5661, 1529, 526, 16370, 322, 2691, 29899, 29873, 348, 287, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:37272 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-15 12:11:35 [engine.py:331] Added request cmpl-b495282ebc10403bba0543a60a2de666-0.
INFO 08-15 12:11:39 [metrics.py:417] Avg prompt throughput: 4.6 tokens/s, Avg generation throughput: 78.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.5%, CPU KV cache usage: 0.0%.
INFO 08-15 12:11:44 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 78.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.8%, CPU KV cache usage: 0.0%.
INFO 08-15 12:11:47 [logger.py:43] Received request cmpl-5eb18186acb9489f970086f3e8b62014-0: prompt: 'Compare and contrast BERT and GPT in terms of architecture and use cases.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 3831, 598, 322, 12814, 350, 20161, 322, 402, 7982, 297, 4958, 310, 11258, 322, 671, 4251, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:39218 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-15 12:11:47 [engine.py:331] Added request cmpl-5eb18186acb9489f970086f3e8b62014-0.
INFO 08-15 12:11:49 [metrics.py:417] Avg prompt throughput: 3.6 tokens/s, Avg generation throughput: 78.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 08-15 12:11:54 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 78.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.6%, CPU KV cache usage: 0.0%.
INFO 08-15 12:11:59 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 78.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.9%, CPU KV cache usage: 0.0%.
INFO 08-15 12:12:10 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 3.9 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-15 12:12:20 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
Received Interrupt
INFO 08-15 12:14:24 [launcher.py:80] Shutting down FastAPI HTTP server.
INFO:     Shutting down
INFO:     Waiting for application shutdown.
INFO:     Application shutdown complete.
/usr/lib/python3.10/multiprocessing/resource_tracker.py:224: UserWarning: resource_tracker: There appear to be 1 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
