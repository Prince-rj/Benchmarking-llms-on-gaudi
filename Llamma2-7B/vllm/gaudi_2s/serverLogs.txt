Calling add_step_closure function does not have any effect. It's lazy mode only functionality. (warning logged once)
Calling mark_step function does not have any effect. It's lazy mode only functionality. (warning logged once)
Calling iter_mark_step function does not have any effect. It's lazy mode only functionality. (warning logged once)
INFO 08-09 12:32:33 [__init__.py:254] Automatically detected platform hpu.
INFO 08-09 12:32:36 [config.py:1975] Disabled the custom all-reduce kernel because it is not supported on current platform.
INFO 08-09 12:32:37 [config.py:1975] Disabled the custom all-reduce kernel because it is not supported on current platform.
INFO 08-09 12:32:37 [config.py:1975] Disabled the custom all-reduce kernel because it is not supported on current platform.
INFO 08-09 12:32:38 [config.py:1975] Disabled the custom all-reduce kernel because it is not supported on current platform.
INFO 08-09 12:32:38 [api_server.py:1289] vLLM API server version 0.6.3.dev5453+ge9c83fca1
INFO 08-09 12:32:39 [config.py:1975] Disabled the custom all-reduce kernel because it is not supported on current platform.
INFO 08-09 12:32:39 [cli_args.py:309] non-default args: {'model': '/software/data/llama_inference/Llama-2-7b-hf/', 'tensor_parallel_size': 2}
INFO 08-09 12:32:46 [config.py:822] This model supports multiple tasks: {'score', 'reward', 'embed', 'classify', 'generate'}. Defaulting to 'generate'.
WARNING 08-09 12:32:46 [config.py:3236] Your device 'hpu' doesn't support torch.float16. Falling back to torch.bfloat16 for compatibility.
WARNING 08-09 12:32:46 [config.py:3287] Casting torch.float16 to torch.bfloat16.
INFO 08-09 12:32:46 [arg_utils.py:1706] hpu is experimental on VLLM_USE_V1=1. Falling back to V0 Engine.
INFO 08-09 12:32:46 [config.py:1941] Defaulting to use mp for distributed inference
INFO 08-09 12:32:46 [config.py:1975] Disabled the custom all-reduce kernel because it is not supported on current platform.
INFO 08-09 12:32:46 [api_server.py:267] Started engine process with PID 13573
Calling add_step_closure function does not have any effect. It's lazy mode only functionality. (warning logged once)
Calling mark_step function does not have any effect. It's lazy mode only functionality. (warning logged once)
Calling iter_mark_step function does not have any effect. It's lazy mode only functionality. (warning logged once)
WARNING 08-09 12:32:49 [env_override.py:17] NCCL_CUMEM_ENABLE is set to 0, skipping override. This may increase memory overhead with cudagraph+allreduce: https://github.com/NVIDIA/nccl/issues/1234
INFO 08-09 12:32:50 [__init__.py:254] Automatically detected platform hpu.
INFO 08-09 12:32:53 [llm_engine.py:239] Initializing a V0 LLM engine (v0.6.3.dev5453+ge9c83fca1) with config: model='/software/data/llama_inference/Llama-2-7b-hf/', speculative_config=None, tokenizer='/software/data/llama_inference/Llama-2-7b-hf/', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, pipeline_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=hpu, decoding_config=DecodingConfig(backend='xgrammar', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=None, served_model_name=/software/data/llama_inference/Llama-2-7b-hf/, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, pooler_config=None, compilation_config={"level":0,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":[],"splitting_ops":[],"use_inductor":true,"compile_sizes":[],"inductor_compile_config":{"enable_auto_functionalized_v2":false},"inductor_passes":{},"use_cudagraph":false,"cudagraph_num_of_warmups":0,"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"max_capture_size":256,"local_cache_dir":null}, use_cached_outputs=True, 
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
WARNING 08-09 12:32:53 [multiproc_worker_utils.py:325] Reducing Torch parallelism from 36 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
WARNING 08-09 12:32:53 [rocm.py:28] Failed to import from amdsmi with ModuleNotFoundError("No module named 'amdsmi'")
WARNING 08-09 12:32:53 [rocm.py:33] Failed to import from vllm._C with ModuleNotFoundError("No module named 'vllm._C'")
WARNING 08-09 12:32:53 [rocm.py:39] Failed to import from vllm._rocm_C with ModuleNotFoundError("No module named 'vllm._rocm_C'")
WARNING 08-09 12:32:53 [utils.py:2850] Methods add_prompt_adapter,cache_config,compilation_config,current_platform,list_prompt_adapters,load_config,pin_prompt_adapter,remove_prompt_adapter,scheduler_config not implemented in <vllm.worker.hpu_worker.HPUWorker object at 0x76087d9c56f0>
INFO 08-09 12:32:54 [runtime.py:26] Environment:
INFO 08-09 12:32:54 [runtime.py:30]     hw: gaudi2
INFO 08-09 12:32:54 [runtime.py:30]     build: 1.22.0.425
INFO 08-09 12:32:54 [runtime.py:30]     engine_version: v0
INFO 08-09 12:32:54 [runtime.py:30]     bridge_mode: eager
INFO 08-09 12:32:54 [runtime.py:30]     model_type: llama
INFO 08-09 12:32:54 [runtime.py:26] Features:
INFO 08-09 12:32:54 [runtime.py:30]     fp32_alibi_biases: True
INFO 08-09 12:32:54 [runtime.py:30]     fp32_softmax: False
INFO 08-09 12:32:54 [runtime.py:30]     fused_block_softmax_adjustment: False
INFO 08-09 12:32:54 [runtime.py:30]     fused_block_softmax: False
INFO 08-09 12:32:54 [runtime.py:30]     prompt_attn_impl: fsdpa_impl
INFO 08-09 12:32:54 [runtime.py:30]     skip_warmup: False
INFO 08-09 12:32:54 [runtime.py:30]     merged_prefill: False
INFO 08-09 12:32:54 [runtime.py:30]     use_contiguous_pa: True
INFO 08-09 12:32:54 [runtime.py:30]     use_delayed_sampling: True
INFO 08-09 12:32:54 [runtime.py:30]     use_bucketing: True
INFO 08-09 12:32:54 [runtime.py:30]     bucketing_strategy: exponential_bucketing
INFO 08-09 12:32:54 [runtime.py:26] User flags:
INFO 08-09 12:32:54 [runtime.py:30]     VLLM_USE_V1: False
WARNING 08-09 12:32:54 [hpu.py:135] Pin memory is not supported on HPU.
INFO 08-09 12:32:54 [hpu.py:60] Using HPUAttention backend.
Calling add_step_closure function does not have any effect. It's lazy mode only functionality. (warning logged once)
Calling mark_step function does not have any effect. It's lazy mode only functionality. (warning logged once)
Calling iter_mark_step function does not have any effect. It's lazy mode only functionality. (warning logged once)
WARNING 08-09 12:32:56 [env_override.py:17] NCCL_CUMEM_ENABLE is set to 0, skipping override. This may increase memory overhead with cudagraph+allreduce: https://github.com/NVIDIA/nccl/issues/1234
INFO 08-09 12:32:57 [__init__.py:254] Automatically detected platform hpu.
[1;36m(VllmWorkerProcess pid=13653)[0;0m INFO 08-09 12:32:59 [multiproc_worker_utils.py:228] Worker ready; awaiting tasks
[1;36m(VllmWorkerProcess pid=13653)[0;0m WARNING 08-09 12:33:00 [rocm.py:28] Failed to import from amdsmi with ModuleNotFoundError("No module named 'amdsmi'")
[1;36m(VllmWorkerProcess pid=13653)[0;0m WARNING 08-09 12:33:00 [rocm.py:33] Failed to import from vllm._C with ModuleNotFoundError("No module named 'vllm._C'")
[1;36m(VllmWorkerProcess pid=13653)[0;0m WARNING 08-09 12:33:00 [rocm.py:39] Failed to import from vllm._rocm_C with ModuleNotFoundError("No module named 'vllm._rocm_C'")
[1;36m(VllmWorkerProcess pid=13653)[0;0m WARNING 08-09 12:33:00 [utils.py:2850] Methods add_prompt_adapter,cache_config,compilation_config,current_platform,list_prompt_adapters,load_config,pin_prompt_adapter,remove_prompt_adapter,scheduler_config not implemented in <vllm.worker.hpu_worker.HPUWorker object at 0x7167b673e0b0>
[1;36m(VllmWorkerProcess pid=13653)[0;0m INFO 08-09 12:33:01 [runtime.py:26] Environment:
[1;36m(VllmWorkerProcess pid=13653)[0;0m INFO 08-09 12:33:01 [runtime.py:30]     hw: gaudi2
[1;36m(VllmWorkerProcess pid=13653)[0;0m INFO 08-09 12:33:01 [runtime.py:30]     build: 1.22.0.425
[1;36m(VllmWorkerProcess pid=13653)[0;0m INFO 08-09 12:33:01 [runtime.py:30]     engine_version: v0
[1;36m(VllmWorkerProcess pid=13653)[0;0m INFO 08-09 12:33:01 [runtime.py:30]     bridge_mode: eager
[1;36m(VllmWorkerProcess pid=13653)[0;0m INFO 08-09 12:33:01 [runtime.py:30]     model_type: llama
[1;36m(VllmWorkerProcess pid=13653)[0;0m INFO 08-09 12:33:01 [runtime.py:26] Features:
[1;36m(VllmWorkerProcess pid=13653)[0;0m INFO 08-09 12:33:01 [runtime.py:30]     fp32_alibi_biases: True
[1;36m(VllmWorkerProcess pid=13653)[0;0m INFO 08-09 12:33:01 [runtime.py:30]     fp32_softmax: False
[1;36m(VllmWorkerProcess pid=13653)[0;0m INFO 08-09 12:33:01 [runtime.py:30]     fused_block_softmax_adjustment: False
[1;36m(VllmWorkerProcess pid=13653)[0;0m INFO 08-09 12:33:01 [runtime.py:30]     fused_block_softmax: False
[1;36m(VllmWorkerProcess pid=13653)[0;0m INFO 08-09 12:33:01 [runtime.py:30]     prompt_attn_impl: fsdpa_impl
[1;36m(VllmWorkerProcess pid=13653)[0;0m INFO 08-09 12:33:01 [runtime.py:30]     skip_warmup: False
[1;36m(VllmWorkerProcess pid=13653)[0;0m INFO 08-09 12:33:01 [runtime.py:30]     merged_prefill: False
[1;36m(VllmWorkerProcess pid=13653)[0;0m INFO 08-09 12:33:01 [runtime.py:30]     use_contiguous_pa: True
[1;36m(VllmWorkerProcess pid=13653)[0;0m INFO 08-09 12:33:01 [runtime.py:30]     use_delayed_sampling: True
[1;36m(VllmWorkerProcess pid=13653)[0;0m INFO 08-09 12:33:01 [runtime.py:30]     use_bucketing: True
[1;36m(VllmWorkerProcess pid=13653)[0;0m INFO 08-09 12:33:01 [runtime.py:30]     bucketing_strategy: exponential_bucketing
[1;36m(VllmWorkerProcess pid=13653)[0;0m INFO 08-09 12:33:01 [runtime.py:26] User flags:
[1;36m(VllmWorkerProcess pid=13653)[0;0m INFO 08-09 12:33:01 [runtime.py:30]     VLLM_USE_V1: False
[1;36m(VllmWorkerProcess pid=13653)[0;0m WARNING 08-09 12:33:01 [hpu.py:135] Pin memory is not supported on HPU.
[1;36m(VllmWorkerProcess pid=13653)[0;0m INFO 08-09 12:33:01 [hpu.py:60] Using HPUAttention backend.
============================= HPU SW VERSION ====================================== 
 HB_BUILD_VER = 1.22.0-425
============================= HPU PT BRIDGE CONFIGURATION ON RANK = 0 ============= 
 PT_HPU_LAZY_MODE = 0
 PT_HPU_RECIPE_CACHE_CONFIG = ,false,1024,false
 PT_HPU_MAX_COMPOUND_OP_SIZE = 9223372036854775807
 PT_HPU_LAZY_ACC_PAR_MODE = 1
 PT_HPU_ENABLE_REFINE_DYNAMIC_SHAPES = 0
 PT_HPU_EAGER_PIPELINE_ENABLE = 1
 PT_HPU_EAGER_COLLECTIVE_PIPELINE_ENABLE = 1
 PT_HPU_ENABLE_LAZY_COLLECTIVES = 1
---------------------------: System Configuration :---------------------------
Num CPU Cores : 160
CPU RAM       : 1007 GB
------------------------------------------------------------------------------
============================= HPU SW VERSION ====================================== 
 HB_BUILD_VER = 1.22.0-425
============================= HPU PT BRIDGE CONFIGURATION ON RANK = 0 ============= 
 PT_HPU_LAZY_MODE = 0
 PT_HPU_RECIPE_CACHE_CONFIG = ,false,1024,false
 PT_HPU_MAX_COMPOUND_OP_SIZE = 9223372036854775807
 PT_HPU_LAZY_ACC_PAR_MODE = 1
 PT_HPU_ENABLE_REFINE_DYNAMIC_SHAPES = 0
 PT_HPU_EAGER_PIPELINE_ENABLE = 1
 PT_HPU_EAGER_COLLECTIVE_PIPELINE_ENABLE = 1
 PT_HPU_ENABLE_LAZY_COLLECTIVES = 1
---------------------------: System Configuration :---------------------------
Num CPU Cores : 160
CPU RAM       : 1007 GB
------------------------------------------------------------------------------
INFO 08-09 12:33:05 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[1], buffer_handle=(1, 4194304, 6, 'psm_5f66510d'), local_subscribe_addr='ipc:///tmp/529ce57b-6f0a-4e65-bb22-99fc7a902eed', remote_subscribe_addr=None, remote_addr_ipv6=False)
INFO 08-09 12:33:05 [parallel_state.py:1065] rank 0 in world size 2 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(VllmWorkerProcess pid=13653)[0;0m INFO 08-09 12:33:05 [parallel_state.py:1065] rank 1 in world size 2 is assigned as DP rank 0, PP rank 0, TP rank 1, EP rank 1
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  2.28it/s]
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.12it/s]
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.21it/s]

INFO 08-09 12:33:07 [default_loader.py:272] Loading weights took 1.77 seconds
[1;36m(VllmWorkerProcess pid=13653)[0;0m INFO 08-09 12:33:08 [default_loader.py:272] Loading weights took 1.77 seconds
INFO 08-09 12:33:08 [hpu_model_runner.py:1184] Pre-loading model weights on hpu:0 took 6.277 GiB of device memory (6.282 GiB/94.62 GiB used) and 131.2 MiB of host memory (238.4 GiB/1007 GiB used)
[1;36m(VllmWorkerProcess pid=13653)[0;0m INFO 08-09 12:33:08 [hpu_model_runner.py:1184] Pre-loading model weights on hpu:0 took 6.277 GiB of device memory (6.282 GiB/94.62 GiB used) and 68.44 MiB of host memory (238.4 GiB/1007 GiB used)
INFO 08-09 12:33:08 [hpu_model_runner.py:1266] Wrapping in HPU Graph took 0 B of device memory (6.282 GiB/94.62 GiB used) and -26.21 MiB of host memory (238.4 GiB/1007 GiB used)
[1;36m(VllmWorkerProcess pid=13653)[0;0m INFO 08-09 12:33:08 [hpu_model_runner.py:1266] Wrapping in HPU Graph took 0 B of device memory (6.282 GiB/94.62 GiB used) and -1.984 MiB of host memory (238.4 GiB/1007 GiB used)
INFO 08-09 12:33:08 [hpu_model_runner.py:1270] Compiling took 0 B of device memory (6.282 GiB/94.62 GiB used) and -11.99 MiB of host memory (238.4 GiB/1007 GiB used)
[1;36m(VllmWorkerProcess pid=13653)[0;0m INFO 08-09 12:33:09 [hpu_model_runner.py:1270] Compiling took 0 B of device memory (6.282 GiB/94.62 GiB used) and 2.035 MiB of host memory (238.4 GiB/1007 GiB used)
INFO 08-09 12:33:09 [hpu_model_runner.py:1274] Loading model weights took in total 6.277 GiB of device memory (6.282 GiB/94.62 GiB used) and 97.79 MiB of host memory (238.4 GiB/1007 GiB used)
[1;36m(VllmWorkerProcess pid=13653)[0;0m INFO 08-09 12:33:09 [hpu_model_runner.py:1274] Loading model weights took in total 6.277 GiB of device memory (6.282 GiB/94.62 GiB used) and 101.9 MiB of host memory (238.4 GiB/1007 GiB used)
WARNING 08-09 12:33:09 [common.py:139] Prompt bucket for (1, 4096, 0) was not prepared. Adding new bucket: (1, 4096, 0)
[1;36m(VllmWorkerProcess pid=13653)[0;0m WARNING 08-09 12:33:09 [common.py:139] Prompt bucket for (1, 4096, 0) was not prepared. Adding new bucket: (1, 4096, 0)
INFO 08-09 12:33:11 [hpu_worker.py:289] Model profiling run took 380 MiB of device memory (6.653 GiB/94.62 GiB used) and 162 MiB of host memory (238.5 GiB/1007 GiB used)
INFO 08-09 12:33:11 [hpu_worker.py:313] Free device memory: 87.97 GiB, 79.17 GiB usable (gpu_memory_utilization=0.9), 7.917 GiB reserved for HPUGraphs (VLLM_GRAPH_RESERVED_MEM=0.1), 71.26 GiB reserved for KV cache
[1;36m(VllmWorkerProcess pid=13653)[0;0m INFO 08-09 12:33:11 [hpu_worker.py:289] Model profiling run took 380 MiB of device memory (6.653 GiB/94.62 GiB used) and 159.3 MiB of host memory (238.5 GiB/1007 GiB used)
[1;36m(VllmWorkerProcess pid=13653)[0;0m INFO 08-09 12:33:11 [hpu_worker.py:313] Free device memory: 87.97 GiB, 79.17 GiB usable (gpu_memory_utilization=0.9), 7.917 GiB reserved for HPUGraphs (VLLM_GRAPH_RESERVED_MEM=0.1), 71.26 GiB reserved for KV cache
INFO 08-09 12:33:11 [executor_base.py:113] # hpu blocks: 2280, # CPU blocks: 128
INFO 08-09 12:33:11 [executor_base.py:118] Maximum concurrency for 4096 tokens per request: 71.25x
INFO 08-09 12:33:11 [exponential.py:49] Prompt bucket config (min, step, max_warmup, limit) bs:[1, 2, 256, 9], seq:[128, 128, 4096, 13]
[1;36m(VllmWorkerProcess pid=13653)[0;0m INFO 08-09 12:33:11 [exponential.py:49] Prompt bucket config (min, step, max_warmup, limit) bs:[1, 2, 256, 9], seq:[128, 128, 4096, 13]
INFO 08-09 12:33:11 [common.py:117] Generated 38 prompt buckets [bs, query, num_blocks]: [(1, 128, 0), (1, 256, 0), (1, 384, 0), (1, 512, 0), (1, 640, 0), (1, 768, 0), (1, 896, 0), (1, 1024, 0), (1, 1408, 0), (1, 1792, 0), (1, 2304, 0), (1, 3072, 0), (1, 4096, 0), (2, 128, 0), (2, 256, 0), (2, 384, 0), (2, 512, 0), (2, 640, 0), (2, 768, 0), (2, 896, 0), (2, 1024, 0), (2, 1408, 0), (2, 1792, 0), (4, 128, 0), (4, 256, 0), (4, 384, 0), (4, 512, 0), (4, 640, 0), (4, 768, 0), (4, 896, 0), (4, 1024, 0), (8, 128, 0), (8, 256, 0), (8, 384, 0), (8, 512, 0), (16, 128, 0), (16, 256, 0), (32, 128, 0)]
INFO 08-09 12:33:11 [exponential.py:79] Decode bucket config (min, step, max_warmup, limit) bs:[1, 2, 256, 9], block:[128, 128, 2280, 13]
[1;36m(VllmWorkerProcess pid=13653)[0;0m INFO 08-09 12:33:11 [common.py:117] Generated 38 prompt buckets [bs, query, num_blocks]: [(1, 128, 0), (1, 256, 0), (1, 384, 0), (1, 512, 0), (1, 640, 0), (1, 768, 0), (1, 896, 0), (1, 1024, 0), (1, 1408, 0), (1, 1792, 0), (1, 2304, 0), (1, 3072, 0), (1, 4096, 0), (2, 128, 0), (2, 256, 0), (2, 384, 0), (2, 512, 0), (2, 640, 0), (2, 768, 0), (2, 896, 0), (2, 1024, 0), (2, 1408, 0), (2, 1792, 0), (4, 128, 0), (4, 256, 0), (4, 384, 0), (4, 512, 0), (4, 640, 0), (4, 768, 0), (4, 896, 0), (4, 1024, 0), (8, 128, 0), (8, 256, 0), (8, 384, 0), (8, 512, 0), (16, 128, 0), (16, 256, 0), (32, 128, 0)]
[1;36m(VllmWorkerProcess pid=13653)[0;0m INFO 08-09 12:33:11 [exponential.py:79] Decode bucket config (min, step, max_warmup, limit) bs:[1, 2, 256, 9], block:[128, 128, 2280, 13]
INFO 08-09 12:33:11 [common.py:117] Generated 117 decode buckets [bs, query, num_blocks]: [(1, 1, 128), (1, 1, 256), (1, 1, 384), (1, 1, 512), (1, 1, 640), (1, 1, 768), (1, 1, 896), (1, 1, 1024), (1, 1, 1152), (1, 1, 1280), (1, 1, 1536), (1, 1, 1920), (1, 1, 2280), (2, 1, 128), (2, 1, 256), (2, 1, 384), (2, 1, 512), (2, 1, 640), (2, 1, 768), (2, 1, 896), (2, 1, 1024), (2, 1, 1152), (2, 1, 1280), (2, 1, 1536), (2, 1, 1920), (2, 1, 2280), (4, 1, 128), (4, 1, 256), (4, 1, 384), (4, 1, 512), (4, 1, 640), (4, 1, 768), (4, 1, 896), (4, 1, 1024), (4, 1, 1152), (4, 1, 1280), (4, 1, 1536), (4, 1, 1920), (4, 1, 2280), (8, 1, 128), (8, 1, 256), (8, 1, 384), (8, 1, 512), (8, 1, 640), (8, 1, 768), (8, 1, 896), (8, 1, 1024), (8, 1, 1152), (8, 1, 1280), (8, 1, 1536), (8, 1, 1920), (8, 1, 2280), (16, 1, 128), (16, 1, 256), (16, 1, 384), (16, 1, 512), (16, 1, 640), (16, 1, 768), (16, 1, 896), (16, 1, 1024), (16, 1, 1152), (16, 1, 1280), (16, 1, 1536), (16, 1, 1920), (16, 1, 2280), (32, 1, 128), (32, 1, 256), (32, 1, 384), (32, 1, 512), (32, 1, 640), (32, 1, 768), (32, 1, 896), (32, 1, 1024), (32, 1, 1152), (32, 1, 1280), (32, 1, 1536), (32, 1, 1920), (32, 1, 2280), (64, 1, 128), (64, 1, 256), (64, 1, 384), (64, 1, 512), (64, 1, 640), (64, 1, 768), (64, 1, 896), (64, 1, 1024), (64, 1, 1152), (64, 1, 1280), (64, 1, 1536), (64, 1, 1920), (64, 1, 2280), (128, 1, 128), (128, 1, 256), (128, 1, 384), (128, 1, 512), (128, 1, 640), (128, 1, 768), (128, 1, 896), (128, 1, 1024), (128, 1, 1152), (128, 1, 1280), (128, 1, 1536), (128, 1, 1920), (128, 1, 2280), (256, 1, 128), (256, 1, 256), (256, 1, 384), (256, 1, 512), (256, 1, 640), (256, 1, 768), (256, 1, 896), (256, 1, 1024), (256, 1, 1152), (256, 1, 1280), (256, 1, 1536), (256, 1, 1920), (256, 1, 2280)]
[1;36m(VllmWorkerProcess pid=13653)[0;0m INFO 08-09 12:33:11 [common.py:117] Generated 117 decode buckets [bs, query, num_blocks]: [(1, 1, 128), (1, 1, 256), (1, 1, 384), (1, 1, 512), (1, 1, 640), (1, 1, 768), (1, 1, 896), (1, 1, 1024), (1, 1, 1152), (1, 1, 1280), (1, 1, 1536), (1, 1, 1920), (1, 1, 2280), (2, 1, 128), (2, 1, 256), (2, 1, 384), (2, 1, 512), (2, 1, 640), (2, 1, 768), (2, 1, 896), (2, 1, 1024), (2, 1, 1152), (2, 1, 1280), (2, 1, 1536), (2, 1, 1920), (2, 1, 2280), (4, 1, 128), (4, 1, 256), (4, 1, 384), (4, 1, 512), (4, 1, 640), (4, 1, 768), (4, 1, 896), (4, 1, 1024), (4, 1, 1152), (4, 1, 1280), (4, 1, 1536), (4, 1, 1920), (4, 1, 2280), (8, 1, 128), (8, 1, 256), (8, 1, 384), (8, 1, 512), (8, 1, 640), (8, 1, 768), (8, 1, 896), (8, 1, 1024), (8, 1, 1152), (8, 1, 1280), (8, 1, 1536), (8, 1, 1920), (8, 1, 2280), (16, 1, 128), (16, 1, 256), (16, 1, 384), (16, 1, 512), (16, 1, 640), (16, 1, 768), (16, 1, 896), (16, 1, 1024), (16, 1, 1152), (16, 1, 1280), (16, 1, 1536), (16, 1, 1920), (16, 1, 2280), (32, 1, 128), (32, 1, 256), (32, 1, 384), (32, 1, 512), (32, 1, 640), (32, 1, 768), (32, 1, 896), (32, 1, 1024), (32, 1, 1152), (32, 1, 1280), (32, 1, 1536), (32, 1, 1920), (32, 1, 2280), (64, 1, 128), (64, 1, 256), (64, 1, 384), (64, 1, 512), (64, 1, 640), (64, 1, 768), (64, 1, 896), (64, 1, 1024), (64, 1, 1152), (64, 1, 1280), (64, 1, 1536), (64, 1, 1920), (64, 1, 2280), (128, 1, 128), (128, 1, 256), (128, 1, 384), (128, 1, 512), (128, 1, 640), (128, 1, 768), (128, 1, 896), (128, 1, 1024), (128, 1, 1152), (128, 1, 1280), (128, 1, 1536), (128, 1, 1920), (128, 1, 2280), (256, 1, 128), (256, 1, 256), (256, 1, 384), (256, 1, 512), (256, 1, 640), (256, 1, 768), (256, 1, 896), (256, 1, 1024), (256, 1, 1152), (256, 1, 1280), (256, 1, 1536), (256, 1, 1920), (256, 1, 2280)]
[1;36m(VllmWorkerProcess pid=13653)[0;0m INFO 08-09 12:33:13 [hpu_worker.py:350] Initializing cache engine took 71.25 GiB of device memory (77.9 GiB/94.62 GiB used) and 7.719 GiB of host memory (246.3 GiB/1007 GiB used)
INFO 08-09 12:33:13 [hpu_worker.py:350] Initializing cache engine took 71.25 GiB of device memory (77.9 GiB/94.62 GiB used) and 7.719 GiB of host memory (246.3 GiB/1007 GiB used)
[1;36m(VllmWorkerProcess pid=13653)[0;0m INFO 08-09 12:33:13 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][1/38] batch_size:32 query_len:128 num_blocks:0 free_mem:16.72 GiB
INFO 08-09 12:33:13 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][1/38] batch_size:32 query_len:128 num_blocks:0 free_mem:16.72 GiB
INFO 08-09 12:33:15 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][2/38] batch_size:16 query_len:256 num_blocks:0 free_mem:16.72 GiB
[1;36m(VllmWorkerProcess pid=13653)[0;0m INFO 08-09 12:33:15 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][2/38] batch_size:16 query_len:256 num_blocks:0 free_mem:16.72 GiB
INFO 08-09 12:33:17 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][3/38] batch_size:16 query_len:128 num_blocks:0 free_mem:16.72 GiB
[1;36m(VllmWorkerProcess pid=13653)[0;0m INFO 08-09 12:33:17 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][3/38] batch_size:16 query_len:128 num_blocks:0 free_mem:16.72 GiB
[1;36m(VllmWorkerProcess pid=13653)[0;0m INFO 08-09 12:33:19 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][4/38] batch_size:8 query_len:512 num_blocks:0 free_mem:16.72 GiB
INFO 08-09 12:33:19 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][4/38] batch_size:8 query_len:512 num_blocks:0 free_mem:16.72 GiB
[1;36m(VllmWorkerProcess pid=13653)[0;0m INFO 08-09 12:33:21 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][5/38] batch_size:8 query_len:384 num_blocks:0 free_mem:16.72 GiB
INFO 08-09 12:33:21 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][5/38] batch_size:8 query_len:384 num_blocks:0 free_mem:16.72 GiB
[1;36m(VllmWorkerProcess pid=13653)[0;0m INFO 08-09 12:33:24 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][6/38] batch_size:8 query_len:256 num_blocks:0 free_mem:16.72 GiB
INFO 08-09 12:33:24 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][6/38] batch_size:8 query_len:256 num_blocks:0 free_mem:16.72 GiB
INFO 08-09 12:33:26 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][7/38] batch_size:8 query_len:128 num_blocks:0 free_mem:16.72 GiB
[1;36m(VllmWorkerProcess pid=13653)[0;0m INFO 08-09 12:33:26 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][7/38] batch_size:8 query_len:128 num_blocks:0 free_mem:16.72 GiB
INFO 08-09 12:33:28 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][8/38] batch_size:4 query_len:1024 num_blocks:0 free_mem:16.72 GiB
[1;36m(VllmWorkerProcess pid=13653)[0;0m INFO 08-09 12:33:28 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][8/38] batch_size:4 query_len:1024 num_blocks:0 free_mem:16.72 GiB
INFO 08-09 12:33:30 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][9/38] batch_size:4 query_len:896 num_blocks:0 free_mem:16.72 GiB
[1;36m(VllmWorkerProcess pid=13653)[0;0m INFO 08-09 12:33:30 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][9/38] batch_size:4 query_len:896 num_blocks:0 free_mem:16.72 GiB
INFO 08-09 12:33:32 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][10/38] batch_size:4 query_len:768 num_blocks:0 free_mem:16.72 GiB
[1;36m(VllmWorkerProcess pid=13653)[0;0m INFO 08-09 12:33:32 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][10/38] batch_size:4 query_len:768 num_blocks:0 free_mem:16.72 GiB
INFO 08-09 12:33:34 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][11/38] batch_size:4 query_len:640 num_blocks:0 free_mem:16.72 GiB
[1;36m(VllmWorkerProcess pid=13653)[0;0m INFO 08-09 12:33:34 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][11/38] batch_size:4 query_len:640 num_blocks:0 free_mem:16.72 GiB
INFO 08-09 12:33:37 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][12/38] batch_size:4 query_len:512 num_blocks:0 free_mem:16.72 GiB
[1;36m(VllmWorkerProcess pid=13653)[0;0m INFO 08-09 12:33:37 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][12/38] batch_size:4 query_len:512 num_blocks:0 free_mem:16.72 GiB
INFO 08-09 12:33:39 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][13/38] batch_size:4 query_len:384 num_blocks:0 free_mem:16.72 GiB
[1;36m(VllmWorkerProcess pid=13653)[0;0m INFO 08-09 12:33:39 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][13/38] batch_size:4 query_len:384 num_blocks:0 free_mem:16.72 GiB
[1;36m(VllmWorkerProcess pid=13653)[0;0m INFO 08-09 12:33:41 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][14/38] batch_size:4 query_len:256 num_blocks:0 free_mem:16.72 GiB
INFO 08-09 12:33:41 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][14/38] batch_size:4 query_len:256 num_blocks:0 free_mem:16.72 GiB
INFO 08-09 12:33:43 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][15/38] batch_size:4 query_len:128 num_blocks:0 free_mem:16.72 GiB
[1;36m(VllmWorkerProcess pid=13653)[0;0m INFO 08-09 12:33:43 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][15/38] batch_size:4 query_len:128 num_blocks:0 free_mem:16.72 GiB
INFO 08-09 12:33:45 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][16/38] batch_size:2 query_len:1792 num_blocks:0 free_mem:16.72 GiB
[1;36m(VllmWorkerProcess pid=13653)[0;0m INFO 08-09 12:33:45 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][16/38] batch_size:2 query_len:1792 num_blocks:0 free_mem:16.72 GiB
[1;36m(VllmWorkerProcess pid=13653)[0;0m INFO 08-09 12:33:48 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][17/38] batch_size:2 query_len:1408 num_blocks:0 free_mem:16.72 GiB
INFO 08-09 12:33:48 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][17/38] batch_size:2 query_len:1408 num_blocks:0 free_mem:16.72 GiB
INFO 08-09 12:33:50 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][18/38] batch_size:2 query_len:1024 num_blocks:0 free_mem:16.72 GiB
[1;36m(VllmWorkerProcess pid=13653)[0;0m INFO 08-09 12:33:50 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][18/38] batch_size:2 query_len:1024 num_blocks:0 free_mem:16.72 GiB
INFO 08-09 12:33:52 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][19/38] batch_size:2 query_len:896 num_blocks:0 free_mem:16.72 GiB
[1;36m(VllmWorkerProcess pid=13653)[0;0m INFO 08-09 12:33:52 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][19/38] batch_size:2 query_len:896 num_blocks:0 free_mem:16.72 GiB
INFO 08-09 12:33:55 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][20/38] batch_size:2 query_len:768 num_blocks:0 free_mem:16.72 GiB
[1;36m(VllmWorkerProcess pid=13653)[0;0m INFO 08-09 12:33:55 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][20/38] batch_size:2 query_len:768 num_blocks:0 free_mem:16.72 GiB
INFO 08-09 12:33:57 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][21/38] batch_size:2 query_len:640 num_blocks:0 free_mem:16.72 GiB
[1;36m(VllmWorkerProcess pid=13653)[0;0m INFO 08-09 12:33:57 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][21/38] batch_size:2 query_len:640 num_blocks:0 free_mem:16.72 GiB
INFO 08-09 12:34:00 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][22/38] batch_size:2 query_len:512 num_blocks:0 free_mem:16.72 GiB
[1;36m(VllmWorkerProcess pid=13653)[0;0m INFO 08-09 12:34:00 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][22/38] batch_size:2 query_len:512 num_blocks:0 free_mem:16.72 GiB
[1;36m(VllmWorkerProcess pid=13653)[0;0m INFO 08-09 12:34:02 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][23/38] batch_size:2 query_len:384 num_blocks:0 free_mem:16.72 GiB
INFO 08-09 12:34:02 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][23/38] batch_size:2 query_len:384 num_blocks:0 free_mem:16.72 GiB
INFO 08-09 12:34:04 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][24/38] batch_size:2 query_len:256 num_blocks:0 free_mem:16.72 GiB
[1;36m(VllmWorkerProcess pid=13653)[0;0m INFO 08-09 12:34:04 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][24/38] batch_size:2 query_len:256 num_blocks:0 free_mem:16.72 GiB
INFO 08-09 12:34:07 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][25/38] batch_size:2 query_len:128 num_blocks:0 free_mem:16.72 GiB
[1;36m(VllmWorkerProcess pid=13653)[0;0m INFO 08-09 12:34:07 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][25/38] batch_size:2 query_len:128 num_blocks:0 free_mem:16.72 GiB
INFO 08-09 12:34:09 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][26/38] batch_size:1 query_len:4096 num_blocks:0 free_mem:16.72 GiB
[1;36m(VllmWorkerProcess pid=13653)[0;0m INFO 08-09 12:34:09 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][26/38] batch_size:1 query_len:4096 num_blocks:0 free_mem:16.72 GiB
INFO 08-09 12:34:11 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][27/38] batch_size:1 query_len:3072 num_blocks:0 free_mem:16.72 GiB
[1;36m(VllmWorkerProcess pid=13653)[0;0m INFO 08-09 12:34:11 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][27/38] batch_size:1 query_len:3072 num_blocks:0 free_mem:16.72 GiB
[1;36m(VllmWorkerProcess pid=13653)[0;0m INFO 08-09 12:34:14 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][28/38] batch_size:1 query_len:2304 num_blocks:0 free_mem:16.72 GiB
INFO 08-09 12:34:14 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][28/38] batch_size:1 query_len:2304 num_blocks:0 free_mem:16.72 GiB
INFO 08-09 12:34:16 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][29/38] batch_size:1 query_len:1792 num_blocks:0 free_mem:16.72 GiB
[1;36m(VllmWorkerProcess pid=13653)[0;0m INFO 08-09 12:34:16 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][29/38] batch_size:1 query_len:1792 num_blocks:0 free_mem:16.72 GiB
INFO 08-09 12:34:19 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][30/38] batch_size:1 query_len:1408 num_blocks:0 free_mem:16.72 GiB
[1;36m(VllmWorkerProcess pid=13653)[0;0m INFO 08-09 12:34:19 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][30/38] batch_size:1 query_len:1408 num_blocks:0 free_mem:16.72 GiB
INFO 08-09 12:34:22 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][31/38] batch_size:1 query_len:1024 num_blocks:0 free_mem:16.72 GiB
[1;36m(VllmWorkerProcess pid=13653)[0;0m INFO 08-09 12:34:22 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][31/38] batch_size:1 query_len:1024 num_blocks:0 free_mem:16.72 GiB
[1;36m(VllmWorkerProcess pid=13653)[0;0m INFO 08-09 12:34:24 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][32/38] batch_size:1 query_len:896 num_blocks:0 free_mem:16.72 GiB
INFO 08-09 12:34:24 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][32/38] batch_size:1 query_len:896 num_blocks:0 free_mem:16.72 GiB
[1;36m(VllmWorkerProcess pid=13653)[0;0m INFO 08-09 12:34:27 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][33/38] batch_size:1 query_len:768 num_blocks:0 free_mem:16.72 GiB
INFO 08-09 12:34:27 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][33/38] batch_size:1 query_len:768 num_blocks:0 free_mem:16.72 GiB
INFO 08-09 12:34:29 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][34/38] batch_size:1 query_len:640 num_blocks:0 free_mem:16.72 GiB
[1;36m(VllmWorkerProcess pid=13653)[0;0m INFO 08-09 12:34:29 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][34/38] batch_size:1 query_len:640 num_blocks:0 free_mem:16.72 GiB
INFO 08-09 12:34:32 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][35/38] batch_size:1 query_len:512 num_blocks:0 free_mem:16.72 GiB
[1;36m(VllmWorkerProcess pid=13653)[0;0m INFO 08-09 12:34:32 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][35/38] batch_size:1 query_len:512 num_blocks:0 free_mem:16.72 GiB
INFO 08-09 12:34:34 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][36/38] batch_size:1 query_len:384 num_blocks:0 free_mem:16.72 GiB
[1;36m(VllmWorkerProcess pid=13653)[0;0m INFO 08-09 12:34:34 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][36/38] batch_size:1 query_len:384 num_blocks:0 free_mem:16.72 GiB
INFO 08-09 12:34:37 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][37/38] batch_size:1 query_len:256 num_blocks:0 free_mem:16.72 GiB
[1;36m(VllmWorkerProcess pid=13653)[0;0m INFO 08-09 12:34:37 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][37/38] batch_size:1 query_len:256 num_blocks:0 free_mem:16.72 GiB
INFO 08-09 12:34:40 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][38/38] batch_size:1 query_len:128 num_blocks:0 free_mem:16.72 GiB
[1;36m(VllmWorkerProcess pid=13653)[0;0m INFO 08-09 12:34:40 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][38/38] batch_size:1 query_len:128 num_blocks:0 free_mem:16.72 GiB
INFO 08-09 12:34:42 [hpu_model_runner.py:3029] [Warmup][Graph/decode][1/117] batch_size:256 query_len:1 num_blocks:2280 free_mem:16.72 GiB
[1;36m(VllmWorkerProcess pid=13653)[0;0m INFO 08-09 12:34:42 [hpu_model_runner.py:3029] [Warmup][Graph/decode][1/117] batch_size:256 query_len:1 num_blocks:2280 free_mem:16.72 GiB
INFO 08-09 12:34:45 [hpu_model_runner.py:3029] [Warmup][Graph/decode][2/117] batch_size:256 query_len:1 num_blocks:1920 free_mem:16.72 GiB
[1;36m(VllmWorkerProcess pid=13653)[0;0m INFO 08-09 12:34:45 [hpu_model_runner.py:3029] [Warmup][Graph/decode][2/117] batch_size:256 query_len:1 num_blocks:1920 free_mem:16.72 GiB
INFO 08-09 12:34:48 [hpu_model_runner.py:3029] [Warmup][Graph/decode][3/117] batch_size:256 query_len:1 num_blocks:1536 free_mem:16.72 GiB
[1;36m(VllmWorkerProcess pid=13653)[0;0m INFO 08-09 12:34:48 [hpu_model_runner.py:3029] [Warmup][Graph/decode][3/117] batch_size:256 query_len:1 num_blocks:1536 free_mem:16.72 GiB
INFO 08-09 12:34:51 [hpu_model_runner.py:3029] [Warmup][Graph/decode][4/117] batch_size:256 query_len:1 num_blocks:1280 free_mem:16.72 GiB
[1;36m(VllmWorkerProcess pid=13653)[0;0m INFO 08-09 12:34:51 [hpu_model_runner.py:3029] [Warmup][Graph/decode][4/117] batch_size:256 query_len:1 num_blocks:1280 free_mem:16.72 GiB
INFO 08-09 12:34:54 [hpu_model_runner.py:3029] [Warmup][Graph/decode][5/117] batch_size:256 query_len:1 num_blocks:1152 free_mem:16.72 GiB
[1;36m(VllmWorkerProcess pid=13653)[0;0m INFO 08-09 12:34:54 [hpu_model_runner.py:3029] [Warmup][Graph/decode][5/117] batch_size:256 query_len:1 num_blocks:1152 free_mem:16.72 GiB
INFO 08-09 12:34:56 [hpu_model_runner.py:3029] [Warmup][Graph/decode][6/117] batch_size:256 query_len:1 num_blocks:1024 free_mem:16.72 GiB
[1;36m(VllmWorkerProcess pid=13653)[0;0m INFO 08-09 12:34:56 [hpu_model_runner.py:3029] [Warmup][Graph/decode][6/117] batch_size:256 query_len:1 num_blocks:1024 free_mem:16.72 GiB
INFO 08-09 12:34:59 [hpu_model_runner.py:3029] [Warmup][Graph/decode][7/117] batch_size:256 query_len:1 num_blocks:896 free_mem:16.72 GiB
[1;36m(VllmWorkerProcess pid=13653)[0;0m INFO 08-09 12:34:59 [hpu_model_runner.py:3029] [Warmup][Graph/decode][7/117] batch_size:256 query_len:1 num_blocks:896 free_mem:16.72 GiB
INFO 08-09 12:35:02 [hpu_model_runner.py:3029] [Warmup][Graph/decode][8/117] batch_size:256 query_len:1 num_blocks:768 free_mem:16.72 GiB
[1;36m(VllmWorkerProcess pid=13653)[0;0m INFO 08-09 12:35:02 [hpu_model_runner.py:3029] [Warmup][Graph/decode][8/117] batch_size:256 query_len:1 num_blocks:768 free_mem:16.72 GiB
INFO 08-09 12:35:05 [hpu_model_runner.py:3029] [Warmup][Graph/decode][9/117] batch_size:256 query_len:1 num_blocks:640 free_mem:16.72 GiB
[1;36m(VllmWorkerProcess pid=13653)[0;0m INFO 08-09 12:35:05 [hpu_model_runner.py:3029] [Warmup][Graph/decode][9/117] batch_size:256 query_len:1 num_blocks:640 free_mem:16.72 GiB
INFO 08-09 12:35:08 [hpu_model_runner.py:3029] [Warmup][Graph/decode][10/117] batch_size:256 query_len:1 num_blocks:512 free_mem:16.72 GiB
[1;36m(VllmWorkerProcess pid=13653)[0;0m INFO 08-09 12:35:08 [hpu_model_runner.py:3029] [Warmup][Graph/decode][10/117] batch_size:256 query_len:1 num_blocks:512 free_mem:16.72 GiB
INFO 08-09 12:35:11 [hpu_model_runner.py:3029] [Warmup][Graph/decode][11/117] batch_size:256 query_len:1 num_blocks:384 free_mem:16.72 GiB
[1;36m(VllmWorkerProcess pid=13653)[0;0m INFO 08-09 12:35:11 [hpu_model_runner.py:3029] [Warmup][Graph/decode][11/117] batch_size:256 query_len:1 num_blocks:384 free_mem:16.72 GiB
INFO 08-09 12:35:14 [hpu_model_runner.py:3029] [Warmup][Graph/decode][12/117] batch_size:256 query_len:1 num_blocks:256 free_mem:16.72 GiB
[1;36m(VllmWorkerProcess pid=13653)[0;0m INFO 08-09 12:35:14 [hpu_model_runner.py:3029] [Warmup][Graph/decode][12/117] batch_size:256 query_len:1 num_blocks:256 free_mem:16.72 GiB
INFO 08-09 12:35:17 [hpu_model_runner.py:3029] [Warmup][Graph/decode][13/117] batch_size:256 query_len:1 num_blocks:128 free_mem:16.72 GiB
[1;36m(VllmWorkerProcess pid=13653)[0;0m INFO 08-09 12:35:17 [hpu_model_runner.py:3029] [Warmup][Graph/decode][13/117] batch_size:256 query_len:1 num_blocks:128 free_mem:16.72 GiB
INFO 08-09 12:35:20 [hpu_model_runner.py:3029] [Warmup][Graph/decode][14/117] batch_size:128 query_len:1 num_blocks:2280 free_mem:16.72 GiB
[1;36m(VllmWorkerProcess pid=13653)[0;0m INFO 08-09 12:35:20 [hpu_model_runner.py:3029] [Warmup][Graph/decode][14/117] batch_size:128 query_len:1 num_blocks:2280 free_mem:16.72 GiB
INFO 08-09 12:35:23 [hpu_model_runner.py:3029] [Warmup][Graph/decode][15/117] batch_size:128 query_len:1 num_blocks:1920 free_mem:16.72 GiB
[1;36m(VllmWorkerProcess pid=13653)[0;0m INFO 08-09 12:35:23 [hpu_model_runner.py:3029] [Warmup][Graph/decode][15/117] batch_size:128 query_len:1 num_blocks:1920 free_mem:16.72 GiB
INFO 08-09 12:35:26 [hpu_model_runner.py:3029] [Warmup][Graph/decode][16/117] batch_size:128 query_len:1 num_blocks:1536 free_mem:16.72 GiB
[1;36m(VllmWorkerProcess pid=13653)[0;0m INFO 08-09 12:35:26 [hpu_model_runner.py:3029] [Warmup][Graph/decode][16/117] batch_size:128 query_len:1 num_blocks:1536 free_mem:16.72 GiB
[1;36m(VllmWorkerProcess pid=13653)[0;0m INFO 08-09 12:35:29 [hpu_model_runner.py:3029] [Warmup][Graph/decode][17/117] batch_size:128 query_len:1 num_blocks:1280 free_mem:16.72 GiB
INFO 08-09 12:35:29 [hpu_model_runner.py:3029] [Warmup][Graph/decode][17/117] batch_size:128 query_len:1 num_blocks:1280 free_mem:16.72 GiB
INFO 08-09 12:35:32 [hpu_model_runner.py:3029] [Warmup][Graph/decode][18/117] batch_size:128 query_len:1 num_blocks:1152 free_mem:16.72 GiB
[1;36m(VllmWorkerProcess pid=13653)[0;0m INFO 08-09 12:35:32 [hpu_model_runner.py:3029] [Warmup][Graph/decode][18/117] batch_size:128 query_len:1 num_blocks:1152 free_mem:16.72 GiB
INFO 08-09 12:35:35 [hpu_model_runner.py:3029] [Warmup][Graph/decode][19/117] batch_size:128 query_len:1 num_blocks:1024 free_mem:16.72 GiB
[1;36m(VllmWorkerProcess pid=13653)[0;0m INFO 08-09 12:35:35 [hpu_model_runner.py:3029] [Warmup][Graph/decode][19/117] batch_size:128 query_len:1 num_blocks:1024 free_mem:16.72 GiB
INFO 08-09 12:35:38 [hpu_model_runner.py:3029] [Warmup][Graph/decode][20/117] batch_size:128 query_len:1 num_blocks:896 free_mem:16.72 GiB
[1;36m(VllmWorkerProcess pid=13653)[0;0m INFO 08-09 12:35:38 [hpu_model_runner.py:3029] [Warmup][Graph/decode][20/117] batch_size:128 query_len:1 num_blocks:896 free_mem:16.72 GiB
[1;36m(VllmWorkerProcess pid=13653)[0;0m INFO 08-09 12:35:41 [hpu_model_runner.py:3029] [Warmup][Graph/decode][21/117] batch_size:128 query_len:1 num_blocks:768 free_mem:16.72 GiB
INFO 08-09 12:35:41 [hpu_model_runner.py:3029] [Warmup][Graph/decode][21/117] batch_size:128 query_len:1 num_blocks:768 free_mem:16.72 GiB
INFO 08-09 12:35:44 [hpu_model_runner.py:3029] [Warmup][Graph/decode][22/117] batch_size:128 query_len:1 num_blocks:640 free_mem:16.72 GiB
[1;36m(VllmWorkerProcess pid=13653)[0;0m INFO 08-09 12:35:44 [hpu_model_runner.py:3029] [Warmup][Graph/decode][22/117] batch_size:128 query_len:1 num_blocks:640 free_mem:16.72 GiB
INFO 08-09 12:35:47 [hpu_model_runner.py:3029] [Warmup][Graph/decode][23/117] batch_size:128 query_len:1 num_blocks:512 free_mem:16.72 GiB
[1;36m(VllmWorkerProcess pid=13653)[0;0m INFO 08-09 12:35:47 [hpu_model_runner.py:3029] [Warmup][Graph/decode][23/117] batch_size:128 query_len:1 num_blocks:512 free_mem:16.72 GiB
INFO 08-09 12:35:51 [hpu_model_runner.py:3029] [Warmup][Graph/decode][24/117] batch_size:128 query_len:1 num_blocks:384 free_mem:16.72 GiB
[1;36m(VllmWorkerProcess pid=13653)[0;0m INFO 08-09 12:35:51 [hpu_model_runner.py:3029] [Warmup][Graph/decode][24/117] batch_size:128 query_len:1 num_blocks:384 free_mem:16.72 GiB
INFO 08-09 12:35:54 [hpu_model_runner.py:3029] [Warmup][Graph/decode][25/117] batch_size:128 query_len:1 num_blocks:256 free_mem:16.72 GiB
[1;36m(VllmWorkerProcess pid=13653)[0;0m INFO 08-09 12:35:54 [hpu_model_runner.py:3029] [Warmup][Graph/decode][25/117] batch_size:128 query_len:1 num_blocks:256 free_mem:16.72 GiB
INFO 08-09 12:35:57 [hpu_model_runner.py:3029] [Warmup][Graph/decode][26/117] batch_size:128 query_len:1 num_blocks:128 free_mem:16.72 GiB
[1;36m(VllmWorkerProcess pid=13653)[0;0m INFO 08-09 12:35:57 [hpu_model_runner.py:3029] [Warmup][Graph/decode][26/117] batch_size:128 query_len:1 num_blocks:128 free_mem:16.72 GiB
[1;36m(VllmWorkerProcess pid=13653)[0;0m INFO 08-09 12:36:00 [hpu_model_runner.py:3029] [Warmup][Graph/decode][27/117] batch_size:64 query_len:1 num_blocks:2280 free_mem:16.72 GiB
INFO 08-09 12:36:00 [hpu_model_runner.py:3029] [Warmup][Graph/decode][27/117] batch_size:64 query_len:1 num_blocks:2280 free_mem:16.72 GiB
INFO 08-09 12:36:04 [hpu_model_runner.py:3029] [Warmup][Graph/decode][28/117] batch_size:64 query_len:1 num_blocks:1920 free_mem:16.72 GiB
[1;36m(VllmWorkerProcess pid=13653)[0;0m INFO 08-09 12:36:04 [hpu_model_runner.py:3029] [Warmup][Graph/decode][28/117] batch_size:64 query_len:1 num_blocks:1920 free_mem:16.72 GiB
INFO 08-09 12:36:07 [hpu_model_runner.py:3029] [Warmup][Graph/decode][29/117] batch_size:64 query_len:1 num_blocks:1536 free_mem:16.72 GiB
[1;36m(VllmWorkerProcess pid=13653)[0;0m INFO 08-09 12:36:07 [hpu_model_runner.py:3029] [Warmup][Graph/decode][29/117] batch_size:64 query_len:1 num_blocks:1536 free_mem:16.72 GiB
INFO 08-09 12:36:10 [hpu_model_runner.py:3029] [Warmup][Graph/decode][30/117] batch_size:64 query_len:1 num_blocks:1280 free_mem:16.72 GiB
[1;36m(VllmWorkerProcess pid=13653)[0;0m INFO 08-09 12:36:10 [hpu_model_runner.py:3029] [Warmup][Graph/decode][30/117] batch_size:64 query_len:1 num_blocks:1280 free_mem:16.72 GiB
INFO 08-09 12:36:14 [hpu_model_runner.py:3029] [Warmup][Graph/decode][31/117] batch_size:64 query_len:1 num_blocks:1152 free_mem:16.72 GiB
[1;36m(VllmWorkerProcess pid=13653)[0;0m INFO 08-09 12:36:14 [hpu_model_runner.py:3029] [Warmup][Graph/decode][31/117] batch_size:64 query_len:1 num_blocks:1152 free_mem:16.72 GiB
INFO 08-09 12:36:17 [hpu_model_runner.py:3029] [Warmup][Graph/decode][32/117] batch_size:64 query_len:1 num_blocks:1024 free_mem:16.72 GiB
[1;36m(VllmWorkerProcess pid=13653)[0;0m INFO 08-09 12:36:17 [hpu_model_runner.py:3029] [Warmup][Graph/decode][32/117] batch_size:64 query_len:1 num_blocks:1024 free_mem:16.72 GiB
[1;36m(VllmWorkerProcess pid=13653)[0;0m INFO 08-09 12:36:20 [hpu_model_runner.py:3029] [Warmup][Graph/decode][33/117] batch_size:64 query_len:1 num_blocks:896 free_mem:16.72 GiB
INFO 08-09 12:36:20 [hpu_model_runner.py:3029] [Warmup][Graph/decode][33/117] batch_size:64 query_len:1 num_blocks:896 free_mem:16.72 GiB
INFO 08-09 12:36:24 [hpu_model_runner.py:3029] [Warmup][Graph/decode][34/117] batch_size:64 query_len:1 num_blocks:768 free_mem:16.72 GiB
[1;36m(VllmWorkerProcess pid=13653)[0;0m INFO 08-09 12:36:24 [hpu_model_runner.py:3029] [Warmup][Graph/decode][34/117] batch_size:64 query_len:1 num_blocks:768 free_mem:16.72 GiB
INFO 08-09 12:36:27 [hpu_model_runner.py:3029] [Warmup][Graph/decode][35/117] batch_size:64 query_len:1 num_blocks:640 free_mem:16.72 GiB
[1;36m(VllmWorkerProcess pid=13653)[0;0m INFO 08-09 12:36:27 [hpu_model_runner.py:3029] [Warmup][Graph/decode][35/117] batch_size:64 query_len:1 num_blocks:640 free_mem:16.72 GiB
INFO 08-09 12:36:30 [hpu_model_runner.py:3029] [Warmup][Graph/decode][36/117] batch_size:64 query_len:1 num_blocks:512 free_mem:16.72 GiB
[1;36m(VllmWorkerProcess pid=13653)[0;0m INFO 08-09 12:36:30 [hpu_model_runner.py:3029] [Warmup][Graph/decode][36/117] batch_size:64 query_len:1 num_blocks:512 free_mem:16.72 GiB
INFO 08-09 12:36:34 [hpu_model_runner.py:3029] [Warmup][Graph/decode][37/117] batch_size:64 query_len:1 num_blocks:384 free_mem:16.72 GiB
[1;36m(VllmWorkerProcess pid=13653)[0;0m INFO 08-09 12:36:34 [hpu_model_runner.py:3029] [Warmup][Graph/decode][37/117] batch_size:64 query_len:1 num_blocks:384 free_mem:16.72 GiB
INFO 08-09 12:36:37 [hpu_model_runner.py:3029] [Warmup][Graph/decode][38/117] batch_size:64 query_len:1 num_blocks:256 free_mem:16.72 GiB
[1;36m(VllmWorkerProcess pid=13653)[0;0m INFO 08-09 12:36:37 [hpu_model_runner.py:3029] [Warmup][Graph/decode][38/117] batch_size:64 query_len:1 num_blocks:256 free_mem:16.72 GiB
INFO 08-09 12:36:41 [hpu_model_runner.py:3029] [Warmup][Graph/decode][39/117] batch_size:64 query_len:1 num_blocks:128 free_mem:16.72 GiB
[1;36m(VllmWorkerProcess pid=13653)[0;0m INFO 08-09 12:36:41 [hpu_model_runner.py:3029] [Warmup][Graph/decode][39/117] batch_size:64 query_len:1 num_blocks:128 free_mem:16.72 GiB
INFO 08-09 12:36:44 [hpu_model_runner.py:3029] [Warmup][Graph/decode][40/117] batch_size:32 query_len:1 num_blocks:2280 free_mem:16.72 GiB
[1;36m(VllmWorkerProcess pid=13653)[0;0m INFO 08-09 12:36:44 [hpu_model_runner.py:3029] [Warmup][Graph/decode][40/117] batch_size:32 query_len:1 num_blocks:2280 free_mem:16.72 GiB
INFO 08-09 12:36:48 [hpu_model_runner.py:3029] [Warmup][Graph/decode][41/117] batch_size:32 query_len:1 num_blocks:1920 free_mem:16.72 GiB
[1;36m(VllmWorkerProcess pid=13653)[0;0m INFO 08-09 12:36:48 [hpu_model_runner.py:3029] [Warmup][Graph/decode][41/117] batch_size:32 query_len:1 num_blocks:1920 free_mem:16.72 GiB
[1;36m(VllmWorkerProcess pid=13653)[0;0m INFO 08-09 12:36:52 [hpu_model_runner.py:3029] [Warmup][Graph/decode][42/117] batch_size:32 query_len:1 num_blocks:1536 free_mem:16.72 GiB
INFO 08-09 12:36:52 [hpu_model_runner.py:3029] [Warmup][Graph/decode][42/117] batch_size:32 query_len:1 num_blocks:1536 free_mem:16.72 GiB
INFO 08-09 12:36:55 [hpu_model_runner.py:3029] [Warmup][Graph/decode][43/117] batch_size:32 query_len:1 num_blocks:1280 free_mem:16.72 GiB
[1;36m(VllmWorkerProcess pid=13653)[0;0m INFO 08-09 12:36:55 [hpu_model_runner.py:3029] [Warmup][Graph/decode][43/117] batch_size:32 query_len:1 num_blocks:1280 free_mem:16.72 GiB
[1;36m(VllmWorkerProcess pid=13653)[0;0m INFO 08-09 12:36:59 [hpu_model_runner.py:3029] [Warmup][Graph/decode][44/117] batch_size:32 query_len:1 num_blocks:1152 free_mem:16.72 GiB
INFO 08-09 12:36:59 [hpu_model_runner.py:3029] [Warmup][Graph/decode][44/117] batch_size:32 query_len:1 num_blocks:1152 free_mem:16.72 GiB
INFO 08-09 12:37:02 [hpu_model_runner.py:3029] [Warmup][Graph/decode][45/117] batch_size:32 query_len:1 num_blocks:1024 free_mem:16.72 GiB
[1;36m(VllmWorkerProcess pid=13653)[0;0m INFO 08-09 12:37:02 [hpu_model_runner.py:3029] [Warmup][Graph/decode][45/117] batch_size:32 query_len:1 num_blocks:1024 free_mem:16.72 GiB
INFO 08-09 12:37:06 [hpu_model_runner.py:3029] [Warmup][Graph/decode][46/117] batch_size:32 query_len:1 num_blocks:896 free_mem:16.72 GiB
[1;36m(VllmWorkerProcess pid=13653)[0;0m INFO 08-09 12:37:06 [hpu_model_runner.py:3029] [Warmup][Graph/decode][46/117] batch_size:32 query_len:1 num_blocks:896 free_mem:16.72 GiB
INFO 08-09 12:37:10 [hpu_model_runner.py:3029] [Warmup][Graph/decode][47/117] batch_size:32 query_len:1 num_blocks:768 free_mem:16.72 GiB
[1;36m(VllmWorkerProcess pid=13653)[0;0m INFO 08-09 12:37:10 [hpu_model_runner.py:3029] [Warmup][Graph/decode][47/117] batch_size:32 query_len:1 num_blocks:768 free_mem:16.72 GiB
INFO 08-09 12:37:13 [hpu_model_runner.py:3029] [Warmup][Graph/decode][48/117] batch_size:32 query_len:1 num_blocks:640 free_mem:16.72 GiB
[1;36m(VllmWorkerProcess pid=13653)[0;0m INFO 08-09 12:37:13 [hpu_model_runner.py:3029] [Warmup][Graph/decode][48/117] batch_size:32 query_len:1 num_blocks:640 free_mem:16.72 GiB
INFO 08-09 12:37:17 [hpu_model_runner.py:3029] [Warmup][Graph/decode][49/117] batch_size:32 query_len:1 num_blocks:512 free_mem:16.72 GiB
[1;36m(VllmWorkerProcess pid=13653)[0;0m INFO 08-09 12:37:17 [hpu_model_runner.py:3029] [Warmup][Graph/decode][49/117] batch_size:32 query_len:1 num_blocks:512 free_mem:16.72 GiB
INFO 08-09 12:37:21 [hpu_model_runner.py:3029] [Warmup][Graph/decode][50/117] batch_size:32 query_len:1 num_blocks:384 free_mem:16.72 GiB
[1;36m(VllmWorkerProcess pid=13653)[0;0m INFO 08-09 12:37:21 [hpu_model_runner.py:3029] [Warmup][Graph/decode][50/117] batch_size:32 query_len:1 num_blocks:384 free_mem:16.72 GiB
INFO 08-09 12:37:25 [hpu_model_runner.py:3029] [Warmup][Graph/decode][51/117] batch_size:32 query_len:1 num_blocks:256 free_mem:16.72 GiB
[1;36m(VllmWorkerProcess pid=13653)[0;0m INFO 08-09 12:37:25 [hpu_model_runner.py:3029] [Warmup][Graph/decode][51/117] batch_size:32 query_len:1 num_blocks:256 free_mem:16.72 GiB
INFO 08-09 12:37:28 [hpu_model_runner.py:3029] [Warmup][Graph/decode][52/117] batch_size:32 query_len:1 num_blocks:128 free_mem:16.72 GiB
[1;36m(VllmWorkerProcess pid=13653)[0;0m INFO 08-09 12:37:28 [hpu_model_runner.py:3029] [Warmup][Graph/decode][52/117] batch_size:32 query_len:1 num_blocks:128 free_mem:16.72 GiB
INFO 08-09 12:37:32 [hpu_model_runner.py:3029] [Warmup][Graph/decode][53/117] batch_size:16 query_len:1 num_blocks:2280 free_mem:16.72 GiB
[1;36m(VllmWorkerProcess pid=13653)[0;0m INFO 08-09 12:37:32 [hpu_model_runner.py:3029] [Warmup][Graph/decode][53/117] batch_size:16 query_len:1 num_blocks:2280 free_mem:16.72 GiB
[1;36m(VllmWorkerProcess pid=13653)[0;0m INFO 08-09 12:37:36 [hpu_model_runner.py:3029] [Warmup][Graph/decode][54/117] batch_size:16 query_len:1 num_blocks:1920 free_mem:16.72 GiB
INFO 08-09 12:37:36 [hpu_model_runner.py:3029] [Warmup][Graph/decode][54/117] batch_size:16 query_len:1 num_blocks:1920 free_mem:16.72 GiB
INFO 08-09 12:37:40 [hpu_model_runner.py:3029] [Warmup][Graph/decode][55/117] batch_size:16 query_len:1 num_blocks:1536 free_mem:16.72 GiB
[1;36m(VllmWorkerProcess pid=13653)[0;0m INFO 08-09 12:37:40 [hpu_model_runner.py:3029] [Warmup][Graph/decode][55/117] batch_size:16 query_len:1 num_blocks:1536 free_mem:16.72 GiB
INFO 08-09 12:37:44 [hpu_model_runner.py:3029] [Warmup][Graph/decode][56/117] batch_size:16 query_len:1 num_blocks:1280 free_mem:16.72 GiB
[1;36m(VllmWorkerProcess pid=13653)[0;0m INFO 08-09 12:37:44 [hpu_model_runner.py:3029] [Warmup][Graph/decode][56/117] batch_size:16 query_len:1 num_blocks:1280 free_mem:16.72 GiB
[1;36m(VllmWorkerProcess pid=13653)[0;0m INFO 08-09 12:37:47 [hpu_model_runner.py:3029] [Warmup][Graph/decode][57/117] batch_size:16 query_len:1 num_blocks:1152 free_mem:16.72 GiB
INFO 08-09 12:37:47 [hpu_model_runner.py:3029] [Warmup][Graph/decode][57/117] batch_size:16 query_len:1 num_blocks:1152 free_mem:16.72 GiB
INFO 08-09 12:37:51 [hpu_model_runner.py:3029] [Warmup][Graph/decode][58/117] batch_size:16 query_len:1 num_blocks:1024 free_mem:16.72 GiB
[1;36m(VllmWorkerProcess pid=13653)[0;0m INFO 08-09 12:37:51 [hpu_model_runner.py:3029] [Warmup][Graph/decode][58/117] batch_size:16 query_len:1 num_blocks:1024 free_mem:16.72 GiB
[1;36m(VllmWorkerProcess pid=13653)[0;0m INFO 08-09 12:37:55 [hpu_model_runner.py:3029] [Warmup][Graph/decode][59/117] batch_size:16 query_len:1 num_blocks:896 free_mem:16.72 GiB
INFO 08-09 12:37:55 [hpu_model_runner.py:3029] [Warmup][Graph/decode][59/117] batch_size:16 query_len:1 num_blocks:896 free_mem:16.72 GiB
INFO 08-09 12:37:59 [hpu_model_runner.py:3029] [Warmup][Graph/decode][60/117] batch_size:16 query_len:1 num_blocks:768 free_mem:16.72 GiB
[1;36m(VllmWorkerProcess pid=13653)[0;0m INFO 08-09 12:37:59 [hpu_model_runner.py:3029] [Warmup][Graph/decode][60/117] batch_size:16 query_len:1 num_blocks:768 free_mem:16.72 GiB
[1;36m(VllmWorkerProcess pid=13653)[0;0m INFO 08-09 12:38:03 [hpu_model_runner.py:3029] [Warmup][Graph/decode][61/117] batch_size:16 query_len:1 num_blocks:640 free_mem:16.72 GiB
INFO 08-09 12:38:03 [hpu_model_runner.py:3029] [Warmup][Graph/decode][61/117] batch_size:16 query_len:1 num_blocks:640 free_mem:16.72 GiB
[1;36m(VllmWorkerProcess pid=13653)[0;0m INFO 08-09 12:38:07 [hpu_model_runner.py:3029] [Warmup][Graph/decode][62/117] batch_size:16 query_len:1 num_blocks:512 free_mem:16.72 GiB
INFO 08-09 12:38:07 [hpu_model_runner.py:3029] [Warmup][Graph/decode][62/117] batch_size:16 query_len:1 num_blocks:512 free_mem:16.72 GiB
INFO 08-09 12:38:11 [hpu_model_runner.py:3029] [Warmup][Graph/decode][63/117] batch_size:16 query_len:1 num_blocks:384 free_mem:16.72 GiB
[1;36m(VllmWorkerProcess pid=13653)[0;0m INFO 08-09 12:38:11 [hpu_model_runner.py:3029] [Warmup][Graph/decode][63/117] batch_size:16 query_len:1 num_blocks:384 free_mem:16.72 GiB
INFO 08-09 12:38:15 [hpu_model_runner.py:3029] [Warmup][Graph/decode][64/117] batch_size:16 query_len:1 num_blocks:256 free_mem:16.72 GiB
[1;36m(VllmWorkerProcess pid=13653)[0;0m INFO 08-09 12:38:15 [hpu_model_runner.py:3029] [Warmup][Graph/decode][64/117] batch_size:16 query_len:1 num_blocks:256 free_mem:16.72 GiB
INFO 08-09 12:38:19 [hpu_model_runner.py:3029] [Warmup][Graph/decode][65/117] batch_size:16 query_len:1 num_blocks:128 free_mem:16.72 GiB
[1;36m(VllmWorkerProcess pid=13653)[0;0m INFO 08-09 12:38:19 [hpu_model_runner.py:3029] [Warmup][Graph/decode][65/117] batch_size:16 query_len:1 num_blocks:128 free_mem:16.72 GiB
INFO 08-09 12:38:23 [hpu_model_runner.py:3029] [Warmup][Graph/decode][66/117] batch_size:8 query_len:1 num_blocks:2280 free_mem:16.72 GiB
[1;36m(VllmWorkerProcess pid=13653)[0;0m INFO 08-09 12:38:23 [hpu_model_runner.py:3029] [Warmup][Graph/decode][66/117] batch_size:8 query_len:1 num_blocks:2280 free_mem:16.72 GiB
[1;36m(VllmWorkerProcess pid=13653)[0;0m INFO 08-09 12:38:27 [hpu_model_runner.py:3029] [Warmup][Graph/decode][67/117] batch_size:8 query_len:1 num_blocks:1920 free_mem:16.72 GiB
INFO 08-09 12:38:27 [hpu_model_runner.py:3029] [Warmup][Graph/decode][67/117] batch_size:8 query_len:1 num_blocks:1920 free_mem:16.72 GiB
[1;36m(VllmWorkerProcess pid=13653)[0;0m INFO 08-09 12:38:31 [hpu_model_runner.py:3029] [Warmup][Graph/decode][68/117] batch_size:8 query_len:1 num_blocks:1536 free_mem:16.72 GiB
INFO 08-09 12:38:31 [hpu_model_runner.py:3029] [Warmup][Graph/decode][68/117] batch_size:8 query_len:1 num_blocks:1536 free_mem:16.72 GiB
INFO 08-09 12:38:35 [hpu_model_runner.py:3029] [Warmup][Graph/decode][69/117] batch_size:8 query_len:1 num_blocks:1280 free_mem:16.72 GiB
[1;36m(VllmWorkerProcess pid=13653)[0;0m INFO 08-09 12:38:35 [hpu_model_runner.py:3029] [Warmup][Graph/decode][69/117] batch_size:8 query_len:1 num_blocks:1280 free_mem:16.72 GiB
INFO 08-09 12:38:39 [hpu_model_runner.py:3029] [Warmup][Graph/decode][70/117] batch_size:8 query_len:1 num_blocks:1152 free_mem:16.72 GiB
[1;36m(VllmWorkerProcess pid=13653)[0;0m INFO 08-09 12:38:39 [hpu_model_runner.py:3029] [Warmup][Graph/decode][70/117] batch_size:8 query_len:1 num_blocks:1152 free_mem:16.72 GiB
[1;36m(VllmWorkerProcess pid=13653)[0;0m INFO 08-09 12:38:43 [hpu_model_runner.py:3029] [Warmup][Graph/decode][71/117] batch_size:8 query_len:1 num_blocks:1024 free_mem:16.72 GiB
INFO 08-09 12:38:43 [hpu_model_runner.py:3029] [Warmup][Graph/decode][71/117] batch_size:8 query_len:1 num_blocks:1024 free_mem:16.72 GiB
[1;36m(VllmWorkerProcess pid=13653)[0;0m INFO 08-09 12:38:48 [hpu_model_runner.py:3029] [Warmup][Graph/decode][72/117] batch_size:8 query_len:1 num_blocks:896 free_mem:16.72 GiB
INFO 08-09 12:38:48 [hpu_model_runner.py:3029] [Warmup][Graph/decode][72/117] batch_size:8 query_len:1 num_blocks:896 free_mem:16.72 GiB
INFO 08-09 12:38:52 [hpu_model_runner.py:3029] [Warmup][Graph/decode][73/117] batch_size:8 query_len:1 num_blocks:768 free_mem:16.72 GiB
[1;36m(VllmWorkerProcess pid=13653)[0;0m INFO 08-09 12:38:52 [hpu_model_runner.py:3029] [Warmup][Graph/decode][73/117] batch_size:8 query_len:1 num_blocks:768 free_mem:16.72 GiB
INFO 08-09 12:38:56 [hpu_model_runner.py:3029] [Warmup][Graph/decode][74/117] batch_size:8 query_len:1 num_blocks:640 free_mem:16.72 GiB
[1;36m(VllmWorkerProcess pid=13653)[0;0m INFO 08-09 12:38:56 [hpu_model_runner.py:3029] [Warmup][Graph/decode][74/117] batch_size:8 query_len:1 num_blocks:640 free_mem:16.72 GiB
INFO 08-09 12:39:00 [hpu_model_runner.py:3029] [Warmup][Graph/decode][75/117] batch_size:8 query_len:1 num_blocks:512 free_mem:16.72 GiB
[1;36m(VllmWorkerProcess pid=13653)[0;0m INFO 08-09 12:39:00 [hpu_model_runner.py:3029] [Warmup][Graph/decode][75/117] batch_size:8 query_len:1 num_blocks:512 free_mem:16.72 GiB
INFO 08-09 12:39:04 [hpu_model_runner.py:3029] [Warmup][Graph/decode][76/117] batch_size:8 query_len:1 num_blocks:384 free_mem:16.72 GiB
[1;36m(VllmWorkerProcess pid=13653)[0;0m INFO 08-09 12:39:04 [hpu_model_runner.py:3029] [Warmup][Graph/decode][76/117] batch_size:8 query_len:1 num_blocks:384 free_mem:16.72 GiB
[1;36m(VllmWorkerProcess pid=13653)[0;0m INFO 08-09 12:39:08 [hpu_model_runner.py:3029] [Warmup][Graph/decode][77/117] batch_size:8 query_len:1 num_blocks:256 free_mem:16.72 GiB
INFO 08-09 12:39:08 [hpu_model_runner.py:3029] [Warmup][Graph/decode][77/117] batch_size:8 query_len:1 num_blocks:256 free_mem:16.72 GiB
INFO 08-09 12:39:13 [hpu_model_runner.py:3029] [Warmup][Graph/decode][78/117] batch_size:8 query_len:1 num_blocks:128 free_mem:16.72 GiB
[1;36m(VllmWorkerProcess pid=13653)[0;0m INFO 08-09 12:39:13 [hpu_model_runner.py:3029] [Warmup][Graph/decode][78/117] batch_size:8 query_len:1 num_blocks:128 free_mem:16.72 GiB
INFO 08-09 12:39:17 [hpu_model_runner.py:3029] [Warmup][Graph/decode][79/117] batch_size:4 query_len:1 num_blocks:2280 free_mem:16.72 GiB
[1;36m(VllmWorkerProcess pid=13653)[0;0m INFO 08-09 12:39:17 [hpu_model_runner.py:3029] [Warmup][Graph/decode][79/117] batch_size:4 query_len:1 num_blocks:2280 free_mem:16.72 GiB
INFO 08-09 12:39:22 [hpu_model_runner.py:3029] [Warmup][Graph/decode][80/117] batch_size:4 query_len:1 num_blocks:1920 free_mem:16.72 GiB
[1;36m(VllmWorkerProcess pid=13653)[0;0m INFO 08-09 12:39:22 [hpu_model_runner.py:3029] [Warmup][Graph/decode][80/117] batch_size:4 query_len:1 num_blocks:1920 free_mem:16.72 GiB
INFO 08-09 12:39:26 [hpu_model_runner.py:3029] [Warmup][Graph/decode][81/117] batch_size:4 query_len:1 num_blocks:1536 free_mem:16.72 GiB
[1;36m(VllmWorkerProcess pid=13653)[0;0m INFO 08-09 12:39:26 [hpu_model_runner.py:3029] [Warmup][Graph/decode][81/117] batch_size:4 query_len:1 num_blocks:1536 free_mem:16.72 GiB
INFO 08-09 12:39:30 [hpu_model_runner.py:3029] [Warmup][Graph/decode][82/117] batch_size:4 query_len:1 num_blocks:1280 free_mem:16.72 GiB
[1;36m(VllmWorkerProcess pid=13653)[0;0m INFO 08-09 12:39:30 [hpu_model_runner.py:3029] [Warmup][Graph/decode][82/117] batch_size:4 query_len:1 num_blocks:1280 free_mem:16.72 GiB
INFO 08-09 12:39:34 [hpu_model_runner.py:3029] [Warmup][Graph/decode][83/117] batch_size:4 query_len:1 num_blocks:1152 free_mem:16.72 GiB
[1;36m(VllmWorkerProcess pid=13653)[0;0m INFO 08-09 12:39:34 [hpu_model_runner.py:3029] [Warmup][Graph/decode][83/117] batch_size:4 query_len:1 num_blocks:1152 free_mem:16.72 GiB
INFO 08-09 12:39:39 [hpu_model_runner.py:3029] [Warmup][Graph/decode][84/117] batch_size:4 query_len:1 num_blocks:1024 free_mem:16.72 GiB
[1;36m(VllmWorkerProcess pid=13653)[0;0m INFO 08-09 12:39:39 [hpu_model_runner.py:3029] [Warmup][Graph/decode][84/117] batch_size:4 query_len:1 num_blocks:1024 free_mem:16.72 GiB
[1;36m(VllmWorkerProcess pid=13653)[0;0m INFO 08-09 12:39:43 [hpu_model_runner.py:3029] [Warmup][Graph/decode][85/117] batch_size:4 query_len:1 num_blocks:896 free_mem:16.72 GiB
INFO 08-09 12:39:43 [hpu_model_runner.py:3029] [Warmup][Graph/decode][85/117] batch_size:4 query_len:1 num_blocks:896 free_mem:16.72 GiB
INFO 08-09 12:39:47 [hpu_model_runner.py:3029] [Warmup][Graph/decode][86/117] batch_size:4 query_len:1 num_blocks:768 free_mem:16.72 GiB
[1;36m(VllmWorkerProcess pid=13653)[0;0m INFO 08-09 12:39:47 [hpu_model_runner.py:3029] [Warmup][Graph/decode][86/117] batch_size:4 query_len:1 num_blocks:768 free_mem:16.72 GiB
INFO 08-09 12:39:52 [hpu_model_runner.py:3029] [Warmup][Graph/decode][87/117] batch_size:4 query_len:1 num_blocks:640 free_mem:16.72 GiB
[1;36m(VllmWorkerProcess pid=13653)[0;0m INFO 08-09 12:39:52 [hpu_model_runner.py:3029] [Warmup][Graph/decode][87/117] batch_size:4 query_len:1 num_blocks:640 free_mem:16.72 GiB
[1;36m(VllmWorkerProcess pid=13653)[0;0m INFO 08-09 12:39:56 [hpu_model_runner.py:3029] [Warmup][Graph/decode][88/117] batch_size:4 query_len:1 num_blocks:512 free_mem:16.72 GiB
INFO 08-09 12:39:56 [hpu_model_runner.py:3029] [Warmup][Graph/decode][88/117] batch_size:4 query_len:1 num_blocks:512 free_mem:16.72 GiB
[1;36m(VllmWorkerProcess pid=13653)[0;0m INFO 08-09 12:40:01 [hpu_model_runner.py:3029] [Warmup][Graph/decode][89/117] batch_size:4 query_len:1 num_blocks:384 free_mem:16.72 GiB
INFO 08-09 12:40:01 [hpu_model_runner.py:3029] [Warmup][Graph/decode][89/117] batch_size:4 query_len:1 num_blocks:384 free_mem:16.72 GiB
INFO 08-09 12:40:05 [hpu_model_runner.py:3029] [Warmup][Graph/decode][90/117] batch_size:4 query_len:1 num_blocks:256 free_mem:16.72 GiB
[1;36m(VllmWorkerProcess pid=13653)[0;0m INFO 08-09 12:40:05 [hpu_model_runner.py:3029] [Warmup][Graph/decode][90/117] batch_size:4 query_len:1 num_blocks:256 free_mem:16.72 GiB
INFO 08-09 12:40:10 [hpu_model_runner.py:3029] [Warmup][Graph/decode][91/117] batch_size:4 query_len:1 num_blocks:128 free_mem:16.72 GiB
[1;36m(VllmWorkerProcess pid=13653)[0;0m INFO 08-09 12:40:10 [hpu_model_runner.py:3029] [Warmup][Graph/decode][91/117] batch_size:4 query_len:1 num_blocks:128 free_mem:16.72 GiB
INFO 08-09 12:40:14 [hpu_model_runner.py:3029] [Warmup][Graph/decode][92/117] batch_size:2 query_len:1 num_blocks:2280 free_mem:16.72 GiB
[1;36m(VllmWorkerProcess pid=13653)[0;0m INFO 08-09 12:40:14 [hpu_model_runner.py:3029] [Warmup][Graph/decode][92/117] batch_size:2 query_len:1 num_blocks:2280 free_mem:16.72 GiB
INFO 08-09 12:40:19 [hpu_model_runner.py:3029] [Warmup][Graph/decode][93/117] batch_size:2 query_len:1 num_blocks:1920 free_mem:16.72 GiB
[1;36m(VllmWorkerProcess pid=13653)[0;0m INFO 08-09 12:40:19 [hpu_model_runner.py:3029] [Warmup][Graph/decode][93/117] batch_size:2 query_len:1 num_blocks:1920 free_mem:16.72 GiB
[1;36m(VllmWorkerProcess pid=13653)[0;0m INFO 08-09 12:40:24 [hpu_model_runner.py:3029] [Warmup][Graph/decode][94/117] batch_size:2 query_len:1 num_blocks:1536 free_mem:16.72 GiB
INFO 08-09 12:40:24 [hpu_model_runner.py:3029] [Warmup][Graph/decode][94/117] batch_size:2 query_len:1 num_blocks:1536 free_mem:16.72 GiB
INFO 08-09 12:40:28 [hpu_model_runner.py:3029] [Warmup][Graph/decode][95/117] batch_size:2 query_len:1 num_blocks:1280 free_mem:16.72 GiB
[1;36m(VllmWorkerProcess pid=13653)[0;0m INFO 08-09 12:40:28 [hpu_model_runner.py:3029] [Warmup][Graph/decode][95/117] batch_size:2 query_len:1 num_blocks:1280 free_mem:16.72 GiB
INFO 08-09 12:40:33 [hpu_model_runner.py:3029] [Warmup][Graph/decode][96/117] batch_size:2 query_len:1 num_blocks:1152 free_mem:16.72 GiB
[1;36m(VllmWorkerProcess pid=13653)[0;0m INFO 08-09 12:40:33 [hpu_model_runner.py:3029] [Warmup][Graph/decode][96/117] batch_size:2 query_len:1 num_blocks:1152 free_mem:16.72 GiB
INFO 08-09 12:40:37 [hpu_model_runner.py:3029] [Warmup][Graph/decode][97/117] batch_size:2 query_len:1 num_blocks:1024 free_mem:16.72 GiB
[1;36m(VllmWorkerProcess pid=13653)[0;0m INFO 08-09 12:40:37 [hpu_model_runner.py:3029] [Warmup][Graph/decode][97/117] batch_size:2 query_len:1 num_blocks:1024 free_mem:16.72 GiB
INFO 08-09 12:40:42 [hpu_model_runner.py:3029] [Warmup][Graph/decode][98/117] batch_size:2 query_len:1 num_blocks:896 free_mem:16.72 GiB
[1;36m(VllmWorkerProcess pid=13653)[0;0m INFO 08-09 12:40:42 [hpu_model_runner.py:3029] [Warmup][Graph/decode][98/117] batch_size:2 query_len:1 num_blocks:896 free_mem:16.72 GiB
INFO 08-09 12:40:47 [hpu_model_runner.py:3029] [Warmup][Graph/decode][99/117] batch_size:2 query_len:1 num_blocks:768 free_mem:16.72 GiB
[1;36m(VllmWorkerProcess pid=13653)[0;0m INFO 08-09 12:40:47 [hpu_model_runner.py:3029] [Warmup][Graph/decode][99/117] batch_size:2 query_len:1 num_blocks:768 free_mem:16.72 GiB
INFO 08-09 12:40:51 [hpu_model_runner.py:3029] [Warmup][Graph/decode][100/117] batch_size:2 query_len:1 num_blocks:640 free_mem:16.72 GiB
[1;36m(VllmWorkerProcess pid=13653)[0;0m INFO 08-09 12:40:51 [hpu_model_runner.py:3029] [Warmup][Graph/decode][100/117] batch_size:2 query_len:1 num_blocks:640 free_mem:16.72 GiB
[1;36m(VllmWorkerProcess pid=13653)[0;0m INFO 08-09 12:40:56 [hpu_model_runner.py:3029] [Warmup][Graph/decode][101/117] batch_size:2 query_len:1 num_blocks:512 free_mem:16.72 GiB
INFO 08-09 12:40:56 [hpu_model_runner.py:3029] [Warmup][Graph/decode][101/117] batch_size:2 query_len:1 num_blocks:512 free_mem:16.72 GiB
INFO 08-09 12:41:01 [hpu_model_runner.py:3029] [Warmup][Graph/decode][102/117] batch_size:2 query_len:1 num_blocks:384 free_mem:16.72 GiB
[1;36m(VllmWorkerProcess pid=13653)[0;0m INFO 08-09 12:41:01 [hpu_model_runner.py:3029] [Warmup][Graph/decode][102/117] batch_size:2 query_len:1 num_blocks:384 free_mem:16.72 GiB
INFO 08-09 12:41:06 [hpu_model_runner.py:3029] [Warmup][Graph/decode][103/117] batch_size:2 query_len:1 num_blocks:256 free_mem:16.72 GiB
[1;36m(VllmWorkerProcess pid=13653)[0;0m INFO 08-09 12:41:06 [hpu_model_runner.py:3029] [Warmup][Graph/decode][103/117] batch_size:2 query_len:1 num_blocks:256 free_mem:16.72 GiB
INFO 08-09 12:41:11 [hpu_model_runner.py:3029] [Warmup][Graph/decode][104/117] batch_size:2 query_len:1 num_blocks:128 free_mem:16.72 GiB
[1;36m(VllmWorkerProcess pid=13653)[0;0m INFO 08-09 12:41:11 [hpu_model_runner.py:3029] [Warmup][Graph/decode][104/117] batch_size:2 query_len:1 num_blocks:128 free_mem:16.72 GiB
INFO 08-09 12:41:15 [hpu_model_runner.py:3029] [Warmup][Graph/decode][105/117] batch_size:1 query_len:1 num_blocks:2280 free_mem:16.72 GiB
[1;36m(VllmWorkerProcess pid=13653)[0;0m INFO 08-09 12:41:15 [hpu_model_runner.py:3029] [Warmup][Graph/decode][105/117] batch_size:1 query_len:1 num_blocks:2280 free_mem:16.72 GiB
INFO 08-09 12:41:21 [hpu_model_runner.py:3029] [Warmup][Graph/decode][106/117] batch_size:1 query_len:1 num_blocks:1920 free_mem:16.72 GiB
[1;36m(VllmWorkerProcess pid=13653)[0;0m INFO 08-09 12:41:21 [hpu_model_runner.py:3029] [Warmup][Graph/decode][106/117] batch_size:1 query_len:1 num_blocks:1920 free_mem:16.72 GiB
INFO 08-09 12:41:25 [hpu_model_runner.py:3029] [Warmup][Graph/decode][107/117] batch_size:1 query_len:1 num_blocks:1536 free_mem:16.72 GiB
[1;36m(VllmWorkerProcess pid=13653)[0;0m INFO 08-09 12:41:25 [hpu_model_runner.py:3029] [Warmup][Graph/decode][107/117] batch_size:1 query_len:1 num_blocks:1536 free_mem:16.72 GiB
INFO 08-09 12:41:30 [hpu_model_runner.py:3029] [Warmup][Graph/decode][108/117] batch_size:1 query_len:1 num_blocks:1280 free_mem:16.72 GiB
[1;36m(VllmWorkerProcess pid=13653)[0;0m INFO 08-09 12:41:30 [hpu_model_runner.py:3029] [Warmup][Graph/decode][108/117] batch_size:1 query_len:1 num_blocks:1280 free_mem:16.72 GiB
[1;36m(VllmWorkerProcess pid=13653)[0;0m INFO 08-09 12:41:35 [hpu_model_runner.py:3029] [Warmup][Graph/decode][109/117] batch_size:1 query_len:1 num_blocks:1152 free_mem:16.72 GiB
INFO 08-09 12:41:35 [hpu_model_runner.py:3029] [Warmup][Graph/decode][109/117] batch_size:1 query_len:1 num_blocks:1152 free_mem:16.72 GiB
INFO 08-09 12:41:40 [hpu_model_runner.py:3029] [Warmup][Graph/decode][110/117] batch_size:1 query_len:1 num_blocks:1024 free_mem:16.72 GiB
[1;36m(VllmWorkerProcess pid=13653)[0;0m INFO 08-09 12:41:40 [hpu_model_runner.py:3029] [Warmup][Graph/decode][110/117] batch_size:1 query_len:1 num_blocks:1024 free_mem:16.72 GiB
[1;36m(VllmWorkerProcess pid=13653)[0;0m INFO 08-09 12:41:45 [hpu_model_runner.py:3029] [Warmup][Graph/decode][111/117] batch_size:1 query_len:1 num_blocks:896 free_mem:16.72 GiB
INFO 08-09 12:41:45 [hpu_model_runner.py:3029] [Warmup][Graph/decode][111/117] batch_size:1 query_len:1 num_blocks:896 free_mem:16.72 GiB
INFO 08-09 12:41:50 [hpu_model_runner.py:3029] [Warmup][Graph/decode][112/117] batch_size:1 query_len:1 num_blocks:768 free_mem:16.72 GiB
[1;36m(VllmWorkerProcess pid=13653)[0;0m INFO 08-09 12:41:50 [hpu_model_runner.py:3029] [Warmup][Graph/decode][112/117] batch_size:1 query_len:1 num_blocks:768 free_mem:16.72 GiB
INFO 08-09 12:41:55 [hpu_model_runner.py:3029] [Warmup][Graph/decode][113/117] batch_size:1 query_len:1 num_blocks:640 free_mem:16.72 GiB
[1;36m(VllmWorkerProcess pid=13653)[0;0m INFO 08-09 12:41:55 [hpu_model_runner.py:3029] [Warmup][Graph/decode][113/117] batch_size:1 query_len:1 num_blocks:640 free_mem:16.72 GiB
[1;36m(VllmWorkerProcess pid=13653)[0;0m INFO 08-09 12:42:00 [hpu_model_runner.py:3029] [Warmup][Graph/decode][114/117] batch_size:1 query_len:1 num_blocks:512 free_mem:16.72 GiB
INFO 08-09 12:42:00 [hpu_model_runner.py:3029] [Warmup][Graph/decode][114/117] batch_size:1 query_len:1 num_blocks:512 free_mem:16.72 GiB
INFO 08-09 12:42:05 [hpu_model_runner.py:3029] [Warmup][Graph/decode][115/117] batch_size:1 query_len:1 num_blocks:384 free_mem:16.72 GiB
[1;36m(VllmWorkerProcess pid=13653)[0;0m INFO 08-09 12:42:05 [hpu_model_runner.py:3029] [Warmup][Graph/decode][115/117] batch_size:1 query_len:1 num_blocks:384 free_mem:16.72 GiB
INFO 08-09 12:42:10 [hpu_model_runner.py:3029] [Warmup][Graph/decode][116/117] batch_size:1 query_len:1 num_blocks:256 free_mem:16.72 GiB
[1;36m(VllmWorkerProcess pid=13653)[0;0m INFO 08-09 12:42:10 [hpu_model_runner.py:3029] [Warmup][Graph/decode][116/117] batch_size:1 query_len:1 num_blocks:256 free_mem:16.72 GiB
INFO 08-09 12:42:15 [hpu_model_runner.py:3029] [Warmup][Graph/decode][117/117] batch_size:1 query_len:1 num_blocks:128 free_mem:16.72 GiB
[1;36m(VllmWorkerProcess pid=13653)[0;0m INFO 08-09 12:42:15 [hpu_model_runner.py:3029] [Warmup][Graph/decode][117/117] batch_size:1 query_len:1 num_blocks:128 free_mem:16.72 GiB
INFO 08-09 12:42:21 [hpu_model_runner.py:3152] Prompt captured:38 (100.0%) used_mem:0 B
[1;36m(VllmWorkerProcess pid=13653)[0;0m INFO 08-09 12:42:21 [hpu_model_runner.py:3152] Prompt captured:38 (100.0%) used_mem:0 B
INFO 08-09 12:42:21 [hpu_model_runner.py:3152] Decode captured:117 (100.0%) used_mem:4 KiB
[1;36m(VllmWorkerProcess pid=13653)[0;0m INFO 08-09 12:42:21 [hpu_model_runner.py:3152] Decode captured:117 (100.0%) used_mem:4 KiB
[1;36m(VllmWorkerProcess pid=13653)[0;0m INFO 08-09 12:42:21 [hpu_model_runner.py:3280] Warmup finished in 547 secs, allocated 0 B of device memory
INFO 08-09 12:42:21 [hpu_model_runner.py:3280] Warmup finished in 547 secs, allocated 0 B of device memory
INFO 08-09 12:42:21 [llm_engine.py:431] init engine (profile, create kv cache, warmup model) took 551.83 seconds
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
WARNING 08-09 12:42:21 [config.py:1364] Default sampling parameters have been overridden by the model's Hugging Face generation config recommended from the model creator. If this is not intended, please relaunch vLLM instance with `--generation-config vllm`.
INFO 08-09 12:42:21 [serving_chat.py:118] Using default chat sampling params from model: {'temperature': 0.9, 'top_p': 0.6}
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
INFO 08-09 12:42:21 [serving_completion.py:66] Using default completion sampling params from model: {'temperature': 0.9, 'top_p': 0.6}
INFO 08-09 12:42:21 [api_server.py:1351] Starting vLLM API server 0 on http://0.0.0.0:8000
INFO 08-09 12:42:21 [launcher.py:29] Available routes are:
INFO 08-09 12:42:21 [launcher.py:37] Route: /openapi.json, Methods: HEAD, GET
INFO 08-09 12:42:21 [launcher.py:37] Route: /docs, Methods: HEAD, GET
INFO 08-09 12:42:21 [launcher.py:37] Route: /docs/oauth2-redirect, Methods: HEAD, GET
INFO 08-09 12:42:21 [launcher.py:37] Route: /redoc, Methods: HEAD, GET
INFO 08-09 12:42:21 [launcher.py:37] Route: /health, Methods: GET
INFO 08-09 12:42:21 [launcher.py:37] Route: /load, Methods: GET
INFO 08-09 12:42:21 [launcher.py:37] Route: /ping, Methods: POST
INFO 08-09 12:42:21 [launcher.py:37] Route: /ping, Methods: GET
INFO 08-09 12:42:21 [launcher.py:37] Route: /tokenize, Methods: POST
INFO 08-09 12:42:21 [launcher.py:37] Route: /detokenize, Methods: POST
INFO 08-09 12:42:21 [launcher.py:37] Route: /v1/models, Methods: GET
INFO 08-09 12:42:21 [launcher.py:37] Route: /version, Methods: GET
INFO 08-09 12:42:21 [launcher.py:37] Route: /v1/chat/completions, Methods: POST
INFO 08-09 12:42:21 [launcher.py:37] Route: /v1/completions, Methods: POST
INFO 08-09 12:42:21 [launcher.py:37] Route: /v1/embeddings, Methods: POST
INFO 08-09 12:42:21 [launcher.py:37] Route: /pooling, Methods: POST
INFO 08-09 12:42:21 [launcher.py:37] Route: /classify, Methods: POST
INFO 08-09 12:42:21 [launcher.py:37] Route: /score, Methods: POST
INFO 08-09 12:42:21 [launcher.py:37] Route: /v1/score, Methods: POST
INFO 08-09 12:42:21 [launcher.py:37] Route: /v1/audio/transcriptions, Methods: POST
INFO 08-09 12:42:21 [launcher.py:37] Route: /rerank, Methods: POST
INFO 08-09 12:42:21 [launcher.py:37] Route: /v1/rerank, Methods: POST
INFO 08-09 12:42:21 [launcher.py:37] Route: /v2/rerank, Methods: POST
INFO 08-09 12:42:21 [launcher.py:37] Route: /invocations, Methods: POST
INFO 08-09 12:42:21 [launcher.py:37] Route: /metrics, Methods: GET
INFO:     Started server process [13419]
INFO:     Waiting for application startup.
INFO:     Application startup complete.
INFO 08-09 12:48:17 [logger.py:43] Received request cmpl-70e1a71abc5d4b6fb9e425962bc56f7d-0: prompt: 'What is LLaMA 2 and how does it work?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 365, 5661, 1529, 29871, 29906, 322, 920, 947, 372, 664, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:57088 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 12:48:17 [engine.py:331] Added request cmpl-70e1a71abc5d4b6fb9e425962bc56f7d-0.
INFO 08-09 12:48:19 [metrics.py:417] Avg prompt throughput: 1.8 tokens/s, Avg generation throughput: 0.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 12:48:24 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 67.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-09 12:48:29 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 97.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.4%, CPU KV cache usage: 0.0%.
INFO 08-09 12:48:31 [logger.py:43] Received request cmpl-007c0f5144e24f229d8edf37929183f7-0: prompt: 'Explain the architecture of a transformer model.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 12027, 7420, 278, 11258, 310, 263, 4327, 261, 1904, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:48870 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 12:48:31 [engine.py:331] Added request cmpl-007c0f5144e24f229d8edf37929183f7-0.
INFO 08-09 12:48:34 [metrics.py:417] Avg prompt throughput: 2.2 tokens/s, Avg generation throughput: 100.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-09 12:48:39 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 101.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.4%, CPU KV cache usage: 0.0%.
INFO 08-09 12:48:40 [logger.py:43] Received request cmpl-b2cd17a0a6774b63894e681691e71c4c-0: prompt: 'What are the core differences between CNN and RNN?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 526, 278, 7136, 12651, 1546, 29696, 322, 390, 10262, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:60786 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 12:48:40 [engine.py:331] Added request cmpl-b2cd17a0a6774b63894e681691e71c4c-0.
INFO 08-09 12:48:44 [metrics.py:417] Avg prompt throughput: 2.4 tokens/s, Avg generation throughput: 101.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-09 12:48:49 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 101.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.4%, CPU KV cache usage: 0.0%.
INFO 08-09 12:48:50 [logger.py:43] Received request cmpl-10bcd21150d64615b9894805ccab76a8-0: prompt: 'Describe how attention mechanism improves translation models.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 20355, 915, 920, 8570, 13336, 4857, 1960, 13962, 4733, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:58620 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 12:48:50 [engine.py:331] Added request cmpl-10bcd21150d64615b9894805ccab76a8-0.
INFO 08-09 12:48:54 [metrics.py:417] Avg prompt throughput: 2.2 tokens/s, Avg generation throughput: 100.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-09 12:48:59 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 99.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.4%, CPU KV cache usage: 0.0%.
INFO 08-09 12:49:00 [logger.py:43] Received request cmpl-43fe5976c84c4e2b8fb397288aca213b-0: prompt: 'What is the role of positional encoding in transformers?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 278, 6297, 310, 2602, 284, 8025, 297, 4327, 414, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:52710 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 12:49:00 [engine.py:331] Added request cmpl-43fe5976c84c4e2b8fb397288aca213b-0.
INFO 08-09 12:49:04 [metrics.py:417] Avg prompt throughput: 2.6 tokens/s, Avg generation throughput: 99.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-09 12:49:09 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 102.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.4%, CPU KV cache usage: 0.0%.
INFO 08-09 12:49:10 [logger.py:43] Received request cmpl-fef3ddd810a54e7fb3b9db7e00541d8d-0: prompt: 'How does self-attention differ from cross-attention?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 947, 1583, 29899, 1131, 2509, 1163, 515, 4891, 29899, 1131, 2509, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:54938 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 12:49:10 [engine.py:331] Added request cmpl-fef3ddd810a54e7fb3b9db7e00541d8d-0.
INFO 08-09 12:49:14 [metrics.py:417] Avg prompt throughput: 2.8 tokens/s, Avg generation throughput: 98.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-09 12:49:17 [logger.py:43] Received request cmpl-5e4b40e4206f4b10a719fb587ff07893-0: prompt: 'Explain the concept of tokenization in NLP.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 12027, 7420, 278, 6964, 310, 5993, 2133, 297, 405, 13208, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:36940 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 12:49:17 [engine.py:331] Added request cmpl-5e4b40e4206f4b10a719fb587ff07893-0.
INFO 08-09 12:49:19 [metrics.py:417] Avg prompt throughput: 2.4 tokens/s, Avg generation throughput: 98.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 12:49:24 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 99.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 08-09 12:49:28 [logger.py:43] Received request cmpl-b5b1100d2f664f37a107ff02d6de6c91-0: prompt: 'What is the vanishing gradient problem and how to mitigate it?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 278, 1109, 14424, 16030, 1108, 322, 920, 304, 1380, 335, 403, 372, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:44454 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 12:49:28 [engine.py:331] Added request cmpl-b5b1100d2f664f37a107ff02d6de6c91-0.
INFO 08-09 12:49:29 [metrics.py:417] Avg prompt throughput: 3.2 tokens/s, Avg generation throughput: 96.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 12:49:31 [logger.py:43] Received request cmpl-7e3eeb6c87f745579ad30f0e38e050cd-0: prompt: 'Describe how Gaudi architecture differs from NVIDIA GPUs.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 20355, 915, 920, 10415, 4749, 11258, 2923, 414, 515, 405, 13044, 10764, 22796, 29879, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:44468 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 12:49:31 [engine.py:331] Added request cmpl-7e3eeb6c87f745579ad30f0e38e050cd-0.
INFO 08-09 12:49:34 [metrics.py:417] Avg prompt throughput: 3.2 tokens/s, Avg generation throughput: 97.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-09 12:49:36 [logger.py:43] Received request cmpl-cf7184f57eaf4b179794aa4055c67436-0: prompt: 'What is model parallelism and how does it help scale large models?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 1904, 8943, 1608, 322, 920, 947, 372, 1371, 6287, 2919, 4733, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:48632 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 12:49:36 [engine.py:331] Added request cmpl-cf7184f57eaf4b179794aa4055c67436-0.
INFO 08-09 12:49:39 [metrics.py:417] Avg prompt throughput: 3.0 tokens/s, Avg generation throughput: 101.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-09 12:49:44 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 100.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.4%, CPU KV cache usage: 0.0%.
INFO 08-09 12:49:46 [logger.py:43] Received request cmpl-ab58ecef3fab46f98a0fdfdbe2068a31-0: prompt: 'Explain the difference between GPT and BERT architectures.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 12027, 7420, 278, 4328, 1546, 402, 7982, 322, 350, 20161, 6956, 1973, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:42514 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 12:49:46 [engine.py:331] Added request cmpl-ab58ecef3fab46f98a0fdfdbe2068a31-0.
INFO 08-09 12:49:49 [metrics.py:417] Avg prompt throughput: 2.8 tokens/s, Avg generation throughput: 92.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-09 12:49:54 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 92.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.4%, CPU KV cache usage: 0.0%.
INFO 08-09 12:49:57 [logger.py:43] Received request cmpl-6ec3e31528054afcae02dc016c3cfd14-0: prompt: 'How does prompt engineering affect language model outputs?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 947, 9508, 21639, 6602, 4086, 1904, 14391, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:39192 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 12:49:57 [engine.py:331] Added request cmpl-6ec3e31528054afcae02dc016c3cfd14-0.
INFO 08-09 12:49:59 [metrics.py:417] Avg prompt throughput: 2.0 tokens/s, Avg generation throughput: 91.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 12:50:04 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 98.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 08-09 12:50:07 [logger.py:43] Received request cmpl-1dc6b42df3374d23bcd7b5cad27e7a99-0: prompt: 'What is the function of layer normalization in deep networks?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 278, 740, 310, 7546, 4226, 2133, 297, 6483, 14379, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:57360 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 12:50:07 [engine.py:331] Added request cmpl-1dc6b42df3374d23bcd7b5cad27e7a99-0.
INFO 08-09 12:50:09 [metrics.py:417] Avg prompt throughput: 2.6 tokens/s, Avg generation throughput: 103.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 12:50:14 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 93.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 08-09 12:50:17 [logger.py:43] Received request cmpl-52132161ba494061921f1f29c82ab16f-0: prompt: 'Explain causal language modeling with examples.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 12027, 7420, 3269, 284, 4086, 1904, 292, 411, 6455, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:50466 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 12:50:17 [engine.py:331] Added request cmpl-52132161ba494061921f1f29c82ab16f-0.
INFO 08-09 12:50:19 [metrics.py:417] Avg prompt throughput: 2.2 tokens/s, Avg generation throughput: 93.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 12:50:24 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 100.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 08-09 12:50:27 [logger.py:43] Received request cmpl-067368a5b3304105944b8d1eca1ce466-0: prompt: 'What is quantization and how does it improve inference performance?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 4323, 2133, 322, 920, 947, 372, 11157, 27262, 4180, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:40306 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 12:50:27 [engine.py:331] Added request cmpl-067368a5b3304105944b8d1eca1ce466-0.
INFO 08-09 12:50:29 [metrics.py:417] Avg prompt throughput: 2.6 tokens/s, Avg generation throughput: 96.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 12:50:34 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 90.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 08-09 12:50:38 [logger.py:43] Received request cmpl-e4878b691c6547e2a2a6740e53538c58-0: prompt: 'Describe how fine-tuning works for large language models.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 20355, 915, 920, 2691, 29899, 29873, 27964, 1736, 363, 2919, 4086, 4733, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:42292 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 12:50:38 [engine.py:331] Added request cmpl-e4878b691c6547e2a2a6740e53538c58-0.
INFO 08-09 12:50:39 [metrics.py:417] Avg prompt throughput: 2.8 tokens/s, Avg generation throughput: 92.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 12:50:44 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 90.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 08-09 12:50:49 [logger.py:43] Received request cmpl-6b83a13a02a94ec9a66d44234265670b-0: prompt: 'What are LoRA adapters and how do they help in parameter-efficient training?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 526, 4309, 4717, 594, 481, 2153, 322, 920, 437, 896, 1371, 297, 3443, 29899, 8462, 6694, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:44578 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 12:50:49 [engine.py:331] Added request cmpl-6b83a13a02a94ec9a66d44234265670b-0.
INFO 08-09 12:50:49 [metrics.py:417] Avg prompt throughput: 3.8 tokens/s, Avg generation throughput: 96.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 12:50:54 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 92.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-09 12:50:59 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 90.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.4%, CPU KV cache usage: 0.0%.
INFO 08-09 12:51:00 [logger.py:43] Received request cmpl-f8acb40329d840dcb80b88ad6933da6b-0: prompt: 'Explain the purpose of rotary positional embeddings.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 12027, 7420, 278, 6437, 310, 5731, 653, 2602, 284, 8297, 29881, 886, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:39262 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 12:51:00 [engine.py:331] Added request cmpl-f8acb40329d840dcb80b88ad6933da6b-0.
INFO 08-09 12:51:04 [metrics.py:417] Avg prompt throughput: 2.8 tokens/s, Avg generation throughput: 100.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-09 12:51:09 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 107.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.4%, CPU KV cache usage: 0.0%.
INFO 08-09 12:51:10 [logger.py:43] Received request cmpl-ef1e4bacc7384df49985ac1f9a9f193d-0: prompt: 'How does DeepSpeed improve model training efficiency?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 947, 21784, 26539, 11157, 1904, 6694, 19201, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:53908 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 12:51:10 [engine.py:331] Added request cmpl-ef1e4bacc7384df49985ac1f9a9f193d-0.
INFO 08-09 12:51:14 [metrics.py:417] Avg prompt throughput: 2.0 tokens/s, Avg generation throughput: 101.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-09 12:51:19 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 101.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.4%, CPU KV cache usage: 0.0%.
INFO 08-09 12:51:19 [logger.py:43] Received request cmpl-88d3acf08ff5487c9a3cc46c1e167d48-0: prompt: 'Compare data parallelism and pipeline parallelism.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 3831, 598, 848, 8943, 1608, 322, 16439, 8943, 1608, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:35460 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 12:51:19 [engine.py:331] Added request cmpl-88d3acf08ff5487c9a3cc46c1e167d48-0.
INFO 08-09 12:51:24 [metrics.py:417] Avg prompt throughput: 2.2 tokens/s, Avg generation throughput: 102.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-09 12:51:29 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 102.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.4%, CPU KV cache usage: 0.0%.
INFO 08-09 12:51:29 [logger.py:43] Received request cmpl-3f57baaca6b7437a868da5d3cc159005-0: prompt: 'What is ALiBi positional encoding?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 14445, 29875, 20517, 2602, 284, 8025, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:54704 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 12:51:29 [engine.py:331] Added request cmpl-3f57baaca6b7437a868da5d3cc159005-0.
INFO 08-09 12:51:34 [metrics.py:417] Avg prompt throughput: 2.0 tokens/s, Avg generation throughput: 98.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-09 12:51:39 [logger.py:43] Received request cmpl-64599c3b15a2425286e5a7c77ea38188-0: prompt: 'Describe the purpose of HF Transformers library.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 20355, 915, 278, 6437, 310, 379, 29943, 4103, 689, 414, 3489, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:56038 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 12:51:39 [engine.py:331] Added request cmpl-64599c3b15a2425286e5a7c77ea38188-0.
INFO 08-09 12:51:39 [metrics.py:417] Avg prompt throughput: 2.6 tokens/s, Avg generation throughput: 101.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 12:51:44 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 98.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-09 12:51:49 [logger.py:43] Received request cmpl-4f6c2412810d45988a432e9912527a99-0: prompt: 'How do attention heads capture contextual meaning?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 437, 8570, 15883, 10446, 3030, 950, 6593, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:35734 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 12:51:49 [engine.py:331] Added request cmpl-4f6c2412810d45988a432e9912527a99-0.
INFO 08-09 12:51:49 [metrics.py:417] Avg prompt throughput: 2.0 tokens/s, Avg generation throughput: 100.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 12:51:54 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 99.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-09 12:51:59 [logger.py:43] Received request cmpl-58a95335ee7d4b499d5e591e2b7bc4ea-0: prompt: 'What is dynamic batching in inference servers?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 7343, 9853, 292, 297, 27262, 12424, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:43240 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 12:51:59 [engine.py:331] Added request cmpl-58a95335ee7d4b499d5e591e2b7bc4ea-0.
INFO 08-09 12:51:59 [metrics.py:417] Avg prompt throughput: 2.0 tokens/s, Avg generation throughput: 99.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 12:52:04 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 97.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-09 12:52:09 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 97.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.4%, CPU KV cache usage: 0.0%.
INFO 08-09 12:52:09 [logger.py:43] Received request cmpl-a2b9eed7aa5749eb87ca1022a78817f1-0: prompt: 'Explain greedy decoding vs beam search vs sampling.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 12027, 7420, 1395, 7584, 1602, 3689, 7186, 22913, 2740, 7186, 23460, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:54888 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 12:52:09 [engine.py:331] Added request cmpl-a2b9eed7aa5749eb87ca1022a78817f1-0.
INFO 08-09 12:52:14 [metrics.py:417] Avg prompt throughput: 2.6 tokens/s, Avg generation throughput: 100.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-09 12:52:19 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 99.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.4%, CPU KV cache usage: 0.0%.
INFO 08-09 12:52:19 [logger.py:43] Received request cmpl-5f2b7c22a38c4c2aae2e7898f3e7d5d1-0: prompt: 'What is perplexity in language models and why is it important?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 639, 10709, 537, 297, 4086, 4733, 322, 2020, 338, 372, 4100, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:35482 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 12:52:19 [engine.py:331] Added request cmpl-5f2b7c22a38c4c2aae2e7898f3e7d5d1-0.
INFO 08-09 12:52:24 [metrics.py:417] Avg prompt throughput: 3.0 tokens/s, Avg generation throughput: 99.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-09 12:52:29 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 103.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.4%, CPU KV cache usage: 0.0%.
INFO 08-09 12:52:29 [logger.py:43] Received request cmpl-e8de1679c52c418da20212ee7c16707f-0: prompt: 'How does mixed precision training benefit model training?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 947, 12849, 16716, 6694, 14169, 1904, 6694, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:49460 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 12:52:29 [engine.py:331] Added request cmpl-e8de1679c52c418da20212ee7c16707f-0.
INFO 08-09 12:52:34 [metrics.py:417] Avg prompt throughput: 2.0 tokens/s, Avg generation throughput: 102.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 08-09 12:52:39 [logger.py:43] Received request cmpl-eb2970ba43244cf39e4fd5b9f433e4af-0: prompt: 'What is the function of the softmax layer in transformers?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 278, 740, 310, 278, 4964, 3317, 7546, 297, 4327, 414, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:41714 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 12:52:39 [engine.py:331] Added request cmpl-eb2970ba43244cf39e4fd5b9f433e4af-0.
INFO 08-09 12:52:39 [metrics.py:417] Avg prompt throughput: 2.8 tokens/s, Avg generation throughput: 103.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 12:52:44 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 103.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 08-09 12:52:49 [logger.py:43] Received request cmpl-ff53c113b12348de91c5a636cb920c8b-0: prompt: "Explain embeddings and how they're used in NLP models.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 12027, 7420, 8297, 29881, 886, 322, 920, 896, 29915, 276, 1304, 297, 405, 13208, 4733, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:60370 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 12:52:49 [engine.py:331] Added request cmpl-ff53c113b12348de91c5a636cb920c8b-0.
INFO 08-09 12:52:49 [metrics.py:417] Avg prompt throughput: 3.4 tokens/s, Avg generation throughput: 104.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 12:52:54 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 98.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 08-09 12:52:59 [logger.py:43] Received request cmpl-00f130e7b2924cde9fb54a2c57e19357-0: prompt: "Describe the function of the tokenizer's vocabulary.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 20355, 915, 278, 740, 310, 278, 5993, 3950, 29915, 29879, 7931, 370, 352, 653, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:43764 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 12:52:59 [engine.py:331] Added request cmpl-00f130e7b2924cde9fb54a2c57e19357-0.
INFO 08-09 12:52:59 [metrics.py:417] Avg prompt throughput: 3.2 tokens/s, Avg generation throughput: 101.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 12:53:04 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 100.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 08-09 12:53:08 [logger.py:43] Received request cmpl-a20152787400400db4765669639d6690-0: prompt: 'What is causal masking in transformer decoders?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 3269, 284, 11105, 292, 297, 4327, 261, 1602, 397, 414, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:44642 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 12:53:08 [engine.py:331] Added request cmpl-a20152787400400db4765669639d6690-0.
INFO 08-09 12:53:09 [metrics.py:417] Avg prompt throughput: 2.8 tokens/s, Avg generation throughput: 104.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 12:53:14 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 98.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 08-09 12:53:18 [logger.py:43] Received request cmpl-69017e40100d48e887d83ad3616f77fe-0: prompt: 'How does gradient checkpointing reduce memory usage?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 947, 16030, 1423, 3149, 292, 10032, 3370, 8744, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:33932 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 12:53:18 [engine.py:331] Added request cmpl-69017e40100d48e887d83ad3616f77fe-0.
INFO 08-09 12:53:19 [metrics.py:417] Avg prompt throughput: 2.2 tokens/s, Avg generation throughput: 98.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 12:53:24 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 100.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 08-09 12:53:28 [logger.py:43] Received request cmpl-e378f248fb7e4720ba4a9b7a80a59417-0: prompt: 'Explain the difference between encoder-decoder and decoder-only models.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 12027, 7420, 278, 4328, 1546, 2094, 6119, 29899, 7099, 6119, 322, 1602, 6119, 29899, 6194, 4733, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:33624 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 12:53:28 [engine.py:331] Added request cmpl-e378f248fb7e4720ba4a9b7a80a59417-0.
INFO 08-09 12:53:29 [metrics.py:417] Avg prompt throughput: 3.6 tokens/s, Avg generation throughput: 99.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 12:53:34 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 98.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 08-09 12:53:38 [logger.py:43] Received request cmpl-ad38709c48cc4a659cd398f6e4ac5d97-0: prompt: 'What is zero-shot vs few-shot vs fine-tuned inference?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 5225, 29899, 8962, 7186, 2846, 29899, 8962, 7186, 2691, 29899, 29873, 348, 287, 27262, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:47432 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 12:53:38 [engine.py:331] Added request cmpl-ad38709c48cc4a659cd398f6e4ac5d97-0.
INFO 08-09 12:53:39 [metrics.py:417] Avg prompt throughput: 3.6 tokens/s, Avg generation throughput: 98.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 12:53:44 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 98.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 08-09 12:53:49 [logger.py:43] Received request cmpl-e17bfc306395445f8db51ceae8317d01-0: prompt: 'What are prompt templates and why do they matter?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 526, 9508, 17475, 322, 2020, 437, 896, 4383, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:37524 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 12:53:49 [engine.py:331] Added request cmpl-e17bfc306395445f8db51ceae8317d01-0.
INFO 08-09 12:53:49 [metrics.py:417] Avg prompt throughput: 2.2 tokens/s, Avg generation throughput: 99.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 12:53:54 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 99.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 08-09 12:53:59 [logger.py:43] Received request cmpl-217be192b9e645a78dd8ac54df440b43-0: prompt: 'Explain the role of BOS and EOS tokens in transformers.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 12027, 7420, 278, 6297, 310, 350, 3267, 322, 382, 3267, 18897, 297, 4327, 414, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:51326 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 12:53:59 [engine.py:331] Added request cmpl-217be192b9e645a78dd8ac54df440b43-0.
INFO 08-09 12:53:59 [metrics.py:417] Avg prompt throughput: 3.2 tokens/s, Avg generation throughput: 100.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 12:54:04 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 100.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 08-09 12:54:09 [logger.py:43] Received request cmpl-7d93b4da93634e3c87e5fad9bdbed66e-0: prompt: 'What is a floating-point tensor and how is it represented?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 263, 16526, 29899, 3149, 12489, 322, 920, 338, 372, 9875, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:38184 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 12:54:09 [engine.py:331] Added request cmpl-7d93b4da93634e3c87e5fad9bdbed66e-0.
INFO 08-09 12:54:09 [metrics.py:417] Avg prompt throughput: 2.8 tokens/s, Avg generation throughput: 92.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 12:54:14 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 95.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 08-09 12:54:19 [logger.py:43] Received request cmpl-54fc40b48e284a4fb6361c8b4ec35b28-0: prompt: 'What are hypernetworks in neural architectures?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 526, 11266, 11618, 29879, 297, 19677, 6956, 1973, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:35816 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 12:54:19 [engine.py:331] Added request cmpl-54fc40b48e284a4fb6361c8b4ec35b28-0.
INFO 08-09 12:54:19 [metrics.py:417] Avg prompt throughput: 2.2 tokens/s, Avg generation throughput: 100.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 12:54:24 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 99.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 08-09 12:54:29 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 96.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.4%, CPU KV cache usage: 0.0%.
INFO 08-09 12:54:29 [logger.py:43] Received request cmpl-750d0e035a614f7dabcdf976ec1ee99c-0: prompt: 'Explain MoE (Mixture of Experts) architecture.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 12027, 7420, 4546, 29923, 313, 29924, 29875, 15546, 310, 28224, 1372, 29897, 11258, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:50496 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 12:54:29 [engine.py:331] Added request cmpl-750d0e035a614f7dabcdf976ec1ee99c-0.
INFO 08-09 12:54:34 [metrics.py:417] Avg prompt throughput: 3.0 tokens/s, Avg generation throughput: 93.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-09 12:54:39 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 98.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.4%, CPU KV cache usage: 0.0%.
INFO 08-09 12:54:40 [logger.py:43] Received request cmpl-6c23ac41f5664f0bb1dda2fe9cd73242-0: prompt: 'What is distillation in model compression?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 1320, 453, 362, 297, 1904, 24221, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:38564 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 12:54:40 [engine.py:331] Added request cmpl-6c23ac41f5664f0bb1dda2fe9cd73242-0.
INFO 08-09 12:54:44 [metrics.py:417] Avg prompt throughput: 2.0 tokens/s, Avg generation throughput: 95.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-09 12:54:49 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 99.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.4%, CPU KV cache usage: 0.0%.
INFO 08-09 12:54:50 [logger.py:43] Received request cmpl-50bea6ad5d0c4d06baa4ca218a211511-0: prompt: 'How do you implement top-k and top-p sampling?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 437, 366, 2334, 2246, 29899, 29895, 322, 2246, 29899, 29886, 23460, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:54098 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 12:54:50 [engine.py:331] Added request cmpl-50bea6ad5d0c4d06baa4ca218a211511-0.
INFO 08-09 12:54:54 [metrics.py:417] Avg prompt throughput: 2.8 tokens/s, Avg generation throughput: 94.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-09 12:54:59 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 98.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.4%, CPU KV cache usage: 0.0%.
INFO 08-09 12:55:00 [logger.py:43] Received request cmpl-10e32c135557482fa1a1d5cef096af20-0: prompt: 'What is a vector database and how is it used with LLMs?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 263, 4608, 2566, 322, 920, 338, 372, 1304, 411, 365, 26369, 29879, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:45842 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 12:55:00 [engine.py:331] Added request cmpl-10e32c135557482fa1a1d5cef096af20-0.
INFO 08-09 12:55:04 [metrics.py:417] Avg prompt throughput: 3.2 tokens/s, Avg generation throughput: 100.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-09 12:55:09 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 101.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.4%, CPU KV cache usage: 0.0%.
INFO 08-09 12:55:10 [logger.py:43] Received request cmpl-d4f7ff1843b743a8a1cf894c6d3721d7-0: prompt: 'Describe how you would use RAG (Retrieval Augmented Generation).', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 20355, 915, 920, 366, 723, 671, 390, 10051, 313, 8015, 2546, 791, 22333, 358, 287, 28203, 467], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:32992 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 12:55:10 [engine.py:331] Added request cmpl-d4f7ff1843b743a8a1cf894c6d3721d7-0.
INFO 08-09 12:55:14 [metrics.py:417] Avg prompt throughput: 3.6 tokens/s, Avg generation throughput: 93.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-09 12:55:19 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 104.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.4%, CPU KV cache usage: 0.0%.
INFO 08-09 12:55:20 [logger.py:43] Received request cmpl-2c0f344378434286bd1e5c88c5aa7ba2-0: prompt: 'What is streaming inference and how is it used?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 24820, 27262, 322, 920, 338, 372, 1304, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:48964 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 12:55:20 [engine.py:331] Added request cmpl-2c0f344378434286bd1e5c88c5aa7ba2-0.
INFO 08-09 12:55:24 [metrics.py:417] Avg prompt throughput: 2.2 tokens/s, Avg generation throughput: 97.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-09 12:55:29 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 94.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.4%, CPU KV cache usage: 0.0%.
INFO 08-09 12:55:31 [logger.py:43] Received request cmpl-360f258837014865a6dba859dee2a3bb-0: prompt: 'How does TTFT relate to user-perceived latency?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 947, 323, 29911, 7818, 29279, 304, 1404, 29899, 546, 346, 2347, 23316, 1270, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:51746 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 12:55:31 [engine.py:331] Added request cmpl-360f258837014865a6dba859dee2a3bb-0.
INFO 08-09 12:55:34 [metrics.py:417] Avg prompt throughput: 3.2 tokens/s, Avg generation throughput: 95.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-09 12:55:39 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 105.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.4%, CPU KV cache usage: 0.0%.
INFO 08-09 12:55:41 [logger.py:43] Received request cmpl-71d29e60470e44c397159f614dd00fb1-0: prompt: 'Explain transformer decoder blocks layer-by-layer.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 12027, 7420, 4327, 261, 1602, 6119, 10930, 7546, 29899, 1609, 29899, 13148, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:48228 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 12:55:41 [engine.py:331] Added request cmpl-71d29e60470e44c397159f614dd00fb1-0.
INFO 08-09 12:55:44 [metrics.py:417] Avg prompt throughput: 2.8 tokens/s, Avg generation throughput: 104.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-09 12:55:49 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 104.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.4%, CPU KV cache usage: 0.0%.
INFO 08-09 12:55:50 [logger.py:43] Received request cmpl-f27661e079004a9eb40312f3be4c39f6-0: prompt: 'What is a KV cache in transformer inference?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 263, 476, 29963, 7090, 297, 4327, 261, 27262, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:57728 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 12:55:50 [engine.py:331] Added request cmpl-f27661e079004a9eb40312f3be4c39f6-0.
INFO 08-09 12:55:54 [metrics.py:417] Avg prompt throughput: 2.4 tokens/s, Avg generation throughput: 101.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-09 12:55:59 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 102.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.4%, CPU KV cache usage: 0.0%.
INFO 08-09 12:56:00 [logger.py:43] Received request cmpl-2caaf3453a1f44e6a23247ac49ae38b0-0: prompt: 'Describe token-by-token generation and its challenges.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 20355, 915, 5993, 29899, 1609, 29899, 6979, 12623, 322, 967, 18066, 267, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:60814 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 12:56:00 [engine.py:331] Added request cmpl-2caaf3453a1f44e6a23247ac49ae38b0-0.
INFO 08-09 12:56:04 [metrics.py:417] Avg prompt throughput: 2.8 tokens/s, Avg generation throughput: 101.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-09 12:56:09 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 104.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.4%, CPU KV cache usage: 0.0%.
INFO 08-09 12:56:10 [logger.py:43] Received request cmpl-d6abebebe61a4df8a91b0f349869b622-0: prompt: 'How does LLaMA 2 compare to GPT-3.5?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 947, 365, 5661, 1529, 29871, 29906, 7252, 304, 402, 7982, 29899, 29941, 29889, 29945, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:48676 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 12:56:10 [engine.py:331] Added request cmpl-d6abebebe61a4df8a91b0f349869b622-0.
INFO 08-09 12:56:14 [metrics.py:417] Avg prompt throughput: 3.4 tokens/s, Avg generation throughput: 99.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-09 12:56:19 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 93.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.4%, CPU KV cache usage: 0.0%.
INFO 08-09 12:56:20 [logger.py:43] Received request cmpl-56e5d0016fc047c48242afc273bb54fa-0: prompt: 'Explain the training data pipeline for a large model.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 12027, 7420, 278, 6694, 848, 16439, 363, 263, 2919, 1904, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:56406 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 12:56:20 [engine.py:331] Added request cmpl-56e5d0016fc047c48242afc273bb54fa-0.
INFO 08-09 12:56:24 [metrics.py:417] Avg prompt throughput: 2.4 tokens/s, Avg generation throughput: 101.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-09 12:56:29 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 100.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.4%, CPU KV cache usage: 0.0%.
INFO 08-09 12:56:30 [logger.py:43] Received request cmpl-d36ad9d83e3e42248454ee141e91e3ce-0: prompt: 'What are the ethical concerns with generative models?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 526, 278, 11314, 936, 21838, 411, 1176, 1230, 4733, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:57322 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 12:56:30 [engine.py:331] Added request cmpl-d36ad9d83e3e42248454ee141e91e3ce-0.
INFO 08-09 12:56:34 [metrics.py:417] Avg prompt throughput: 2.4 tokens/s, Avg generation throughput: 90.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-09 12:56:39 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 92.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.4%, CPU KV cache usage: 0.0%.
INFO 08-09 12:56:41 [logger.py:43] Received request cmpl-03287e645eeb4f8a8d35d1ab9aa55f58-0: prompt: 'How do transformers process multilingual text?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 437, 4327, 414, 1889, 1773, 6504, 950, 1426, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:42916 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 12:56:41 [engine.py:331] Added request cmpl-03287e645eeb4f8a8d35d1ab9aa55f58-0.
INFO 08-09 12:56:44 [metrics.py:417] Avg prompt throughput: 2.2 tokens/s, Avg generation throughput: 91.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-09 12:56:50 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 92.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.4%, CPU KV cache usage: 0.0%.
INFO 08-09 12:56:52 [logger.py:43] Received request cmpl-bcda7f266c2e4cb48e710b9808e6baaa-0: prompt: 'What are flash attention mechanisms?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 526, 11013, 8570, 7208, 12903, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:45062 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 12:56:52 [engine.py:331] Added request cmpl-bcda7f266c2e4cb48e710b9808e6baaa-0.
INFO 08-09 12:56:55 [metrics.py:417] Avg prompt throughput: 1.6 tokens/s, Avg generation throughput: 93.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-09 12:57:00 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 100.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 08-09 12:57:02 [logger.py:43] Received request cmpl-66855fa9fb14490b9414ceeea9b60f83-0: prompt: 'Describe how memory-efficient attention works.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 20355, 915, 920, 3370, 29899, 8462, 8570, 1736, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:35370 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 12:57:02 [engine.py:331] Added request cmpl-66855fa9fb14490b9414ceeea9b60f83-0.
INFO 08-09 12:57:05 [metrics.py:417] Avg prompt throughput: 2.0 tokens/s, Avg generation throughput: 101.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-09 12:57:06 [logger.py:43] Received request cmpl-2124d856bf5d4bed89048c3e50e57ade-0: prompt: 'What is inference throughput and how is it calculated?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 27262, 1549, 649, 322, 920, 338, 372, 12833, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:40810 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 12:57:06 [engine.py:331] Added request cmpl-2124d856bf5d4bed89048c3e50e57ade-0.
INFO 08-09 12:57:10 [metrics.py:417] Avg prompt throughput: 2.4 tokens/s, Avg generation throughput: 95.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-09 12:57:15 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 91.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 08-09 12:57:17 [logger.py:43] Received request cmpl-f492132bc0a74514b46b50b9d5192e74-0: prompt: 'Explain how masked self-attention works in BERT.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 12027, 7420, 920, 11105, 287, 1583, 29899, 1131, 2509, 1736, 297, 350, 20161, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:38926 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 12:57:17 [engine.py:331] Added request cmpl-f492132bc0a74514b46b50b9d5192e74-0.
INFO 08-09 12:57:20 [metrics.py:417] Avg prompt throughput: 3.0 tokens/s, Avg generation throughput: 92.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 12:57:23 [logger.py:43] Received request cmpl-64082c1ed07a4c75817358993ca7dd86-0: prompt: 'How do instruction-tuned models differ from base models?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 437, 15278, 29899, 29873, 348, 287, 4733, 1163, 515, 2967, 4733, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:44558 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 12:57:23 [engine.py:331] Added request cmpl-64082c1ed07a4c75817358993ca7dd86-0.
INFO 08-09 12:57:25 [metrics.py:417] Avg prompt throughput: 2.8 tokens/s, Avg generation throughput: 98.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 12:57:30 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 100.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 08-09 12:57:33 [logger.py:43] Received request cmpl-76d45b24b53c4115b5ac8089628f42df-0: prompt: 'Describe an RLHF (Reinforcement Learning from Human Feedback) process.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 20355, 915, 385, 390, 29931, 29950, 29943, 313, 1123, 262, 1454, 13561, 29257, 515, 12968, 5169, 287, 1627, 29897, 1889, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:34204 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 12:57:33 [engine.py:331] Added request cmpl-76d45b24b53c4115b5ac8089628f42df-0.
INFO 08-09 12:57:35 [metrics.py:417] Avg prompt throughput: 4.4 tokens/s, Avg generation throughput: 100.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 12:57:40 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 102.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 08-09 12:57:43 [logger.py:43] Received request cmpl-dd28f40988ae4615988738485ca4eef4-0: prompt: 'What is SFT (Supervised Fine-Tuning) in LLM training?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 317, 7818, 313, 19111, 11292, 28896, 29899, 29911, 27964, 29897, 297, 365, 26369, 6694, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:57614 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 12:57:43 [engine.py:331] Added request cmpl-dd28f40988ae4615988738485ca4eef4-0.
INFO 08-09 12:57:45 [metrics.py:417] Avg prompt throughput: 3.6 tokens/s, Avg generation throughput: 102.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 12:57:50 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 102.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 08-09 12:57:51 [logger.py:43] Received request cmpl-4738c3c5fa914bee97fc03c2335a1885-0: prompt: 'Explain the role of HPU in Gaudi chips.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 12027, 7420, 278, 6297, 310, 379, 7056, 297, 10415, 4749, 521, 4512, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:57620 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 12:57:51 [engine.py:331] Added request cmpl-4738c3c5fa914bee97fc03c2335a1885-0.
INFO 08-09 12:57:55 [metrics.py:417] Avg prompt throughput: 2.8 tokens/s, Avg generation throughput: 95.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-09 12:58:00 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 93.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.4%, CPU KV cache usage: 0.0%.
INFO 08-09 12:58:02 [logger.py:43] Received request cmpl-cf67be3a8a7f4beba4f02fab0aaeb9ab-0: prompt: 'What is a causal attention mask?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 263, 3269, 284, 8570, 11105, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:52954 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 12:58:02 [engine.py:331] Added request cmpl-cf67be3a8a7f4beba4f02fab0aaeb9ab-0.
INFO 08-09 12:58:05 [metrics.py:417] Avg prompt throughput: 1.8 tokens/s, Avg generation throughput: 101.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-09 12:58:10 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 99.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.4%, CPU KV cache usage: 0.0%.
INFO 08-09 12:58:12 [logger.py:43] Received request cmpl-1667e75d78df46f2a1740545dcdb65d5-0: prompt: 'Describe the HF `generate()` function in Transformers.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 20355, 915, 278, 379, 29943, 421, 17158, 2555, 740, 297, 4103, 689, 414, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:52490 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 12:58:12 [engine.py:331] Added request cmpl-1667e75d78df46f2a1740545dcdb65d5-0.
INFO 08-09 12:58:15 [metrics.py:417] Avg prompt throughput: 3.0 tokens/s, Avg generation throughput: 105.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-09 12:58:20 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 99.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.4%, CPU KV cache usage: 0.0%.
INFO 08-09 12:58:22 [logger.py:43] Received request cmpl-69370048887445719ead2b65967612f1-0: prompt: 'Explain differences between T5, BART, and GPT models.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 12027, 7420, 12651, 1546, 323, 29945, 29892, 350, 8322, 29892, 322, 402, 7982, 4733, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:41148 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 12:58:22 [engine.py:331] Added request cmpl-69370048887445719ead2b65967612f1-0.
INFO 08-09 12:58:25 [metrics.py:417] Avg prompt throughput: 3.2 tokens/s, Avg generation throughput: 96.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-09 12:58:30 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 92.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 08-09 12:58:32 [logger.py:43] Received request cmpl-03da3a640f4e4c5eaf55fce3299054f6-0: prompt: 'What is model checkpointing and resumption?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 1904, 1423, 3149, 292, 322, 620, 28069, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:33964 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 12:58:32 [engine.py:331] Added request cmpl-03da3a640f4e4c5eaf55fce3299054f6-0.
INFO 08-09 12:58:35 [metrics.py:417] Avg prompt throughput: 2.2 tokens/s, Avg generation throughput: 91.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 12:58:40 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 97.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 08-09 12:58:43 [logger.py:43] Received request cmpl-57efc602a1e54bcc856daddb6e2e04f1-0: prompt: 'Describe how checkpoint sharding works in distributed training.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 20355, 915, 920, 1423, 3149, 528, 20272, 1736, 297, 13235, 6694, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:40338 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 12:58:43 [engine.py:331] Added request cmpl-57efc602a1e54bcc856daddb6e2e04f1-0.
INFO 08-09 12:58:45 [metrics.py:417] Avg prompt throughput: 2.6 tokens/s, Avg generation throughput: 100.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 12:58:50 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 101.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 08-09 12:58:53 [logger.py:43] Received request cmpl-5c5de4af2f3b45eb9ba75506486eece0-0: prompt: 'How are model weights stored in HuggingFace format?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 526, 1904, 18177, 6087, 297, 379, 688, 3460, 23360, 3402, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:33372 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 12:58:53 [engine.py:331] Added request cmpl-5c5de4af2f3b45eb9ba75506486eece0-0.
INFO 08-09 12:58:55 [metrics.py:417] Avg prompt throughput: 2.6 tokens/s, Avg generation throughput: 101.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 12:59:00 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 100.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 08-09 12:59:03 [logger.py:43] Received request cmpl-8524f91eccb74f52aca26cabd7fdc1a5-0: prompt: 'Explain the impact of batch size on training and inference.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 12027, 7420, 278, 10879, 310, 9853, 2159, 373, 6694, 322, 27262, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:48756 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 12:59:03 [engine.py:331] Added request cmpl-8524f91eccb74f52aca26cabd7fdc1a5-0.
INFO 08-09 12:59:05 [metrics.py:417] Avg prompt throughput: 2.6 tokens/s, Avg generation throughput: 94.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 12:59:10 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 95.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 08-09 12:59:13 [logger.py:43] Received request cmpl-cd93dbab15164997b10d1cd7d8db38d9-0: prompt: 'What are fused kernels and why are they fast?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 526, 285, 3880, 413, 824, 1379, 322, 2020, 526, 896, 5172, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:51904 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 12:59:13 [engine.py:331] Added request cmpl-cd93dbab15164997b10d1cd7d8db38d9-0.
INFO 08-09 12:59:15 [metrics.py:417] Avg prompt throughput: 2.8 tokens/s, Avg generation throughput: 94.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 12:59:20 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 94.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 08-09 12:59:24 [logger.py:43] Received request cmpl-5411ed48c385419b83f958c9411e6159-0: prompt: 'How does the `AutoModelForCausalLM` class work?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 947, 278, 421, 12300, 3195, 2831, 29907, 1485, 284, 26369, 29952, 770, 664, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:35446 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 12:59:24 [engine.py:331] Added request cmpl-5411ed48c385419b83f958c9411e6159-0.
INFO 08-09 12:59:25 [metrics.py:417] Avg prompt throughput: 3.2 tokens/s, Avg generation throughput: 99.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 12:59:30 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 100.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 08-09 12:59:34 [logger.py:43] Received request cmpl-4f6078023a764117baaab0e2edaf094d-0: prompt: 'What are tensor parallel and tensor slicing?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 526, 12489, 8943, 322, 12489, 269, 506, 292, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:51060 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 12:59:34 [engine.py:331] Added request cmpl-4f6078023a764117baaab0e2edaf094d-0.
INFO 08-09 12:59:35 [metrics.py:417] Avg prompt throughput: 2.2 tokens/s, Avg generation throughput: 99.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 12:59:40 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 101.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 08-09 12:59:44 [logger.py:43] Received request cmpl-59f61f09340846178994f84ce4eb14ef-0: prompt: 'Describe the key steps of model serving using vLLM.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 20355, 915, 278, 1820, 6576, 310, 1904, 16330, 773, 325, 2208, 29924, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:36292 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 12:59:44 [engine.py:331] Added request cmpl-59f61f09340846178994f84ce4eb14ef-0.
INFO 08-09 12:59:45 [metrics.py:417] Avg prompt throughput: 2.8 tokens/s, Avg generation throughput: 101.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 12:59:48 [logger.py:43] Received request cmpl-d2ffa9520cd7464ea7401c81e1acacc7-0: prompt: 'What are inference graphs in deep learning?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 526, 27262, 18445, 297, 6483, 6509, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:36294 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 12:59:48 [engine.py:331] Added request cmpl-d2ffa9520cd7464ea7401c81e1acacc7-0.
INFO 08-09 12:59:50 [metrics.py:417] Avg prompt throughput: 1.8 tokens/s, Avg generation throughput: 103.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 12:59:55 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 95.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 08-09 12:59:59 [logger.py:43] Received request cmpl-aa191568447d458dafd0c6e7ccdb27af-0: prompt: 'What is a streaming response in OpenAI-style APIs?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 263, 24820, 2933, 297, 4673, 23869, 29899, 3293, 23649, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:59834 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 12:59:59 [engine.py:331] Added request cmpl-aa191568447d458dafd0c6e7ccdb27af-0.
INFO 08-09 13:00:00 [metrics.py:417] Avg prompt throughput: 2.6 tokens/s, Avg generation throughput: 91.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:00:05 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 92.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 08-09 13:00:09 [logger.py:43] Received request cmpl-2e5a7b51e9614175bebbc9712f8390dc-0: prompt: 'How does speculative decoding reduce latency?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 947, 1580, 28524, 1602, 3689, 10032, 23316, 1270, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:35270 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:00:09 [engine.py:331] Added request cmpl-2e5a7b51e9614175bebbc9712f8390dc-0.
INFO 08-09 13:00:10 [metrics.py:417] Avg prompt throughput: 2.2 tokens/s, Avg generation throughput: 99.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:00:15 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 102.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 08-09 13:00:19 [logger.py:43] Received request cmpl-d9276a03de6640369c5c07fa9975665e-0: prompt: 'What is JAX and how is it different from PyTorch?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 435, 6604, 322, 920, 338, 372, 1422, 515, 10772, 29911, 25350, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:50058 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:00:19 [engine.py:331] Added request cmpl-d9276a03de6640369c5c07fa9975665e-0.
INFO 08-09 13:00:20 [metrics.py:417] Avg prompt throughput: 3.0 tokens/s, Avg generation throughput: 99.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:00:25 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 100.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 08-09 13:00:29 [logger.py:43] Received request cmpl-e2947ce125fd47a7b11fa138aa081570-0: prompt: 'Describe lazy loading of weights in vLLM.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 20355, 915, 17366, 8363, 310, 18177, 297, 325, 2208, 29924, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:48370 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:00:29 [engine.py:331] Added request cmpl-e2947ce125fd47a7b11fa138aa081570-0.
INFO 08-09 13:00:30 [metrics.py:417] Avg prompt throughput: 2.4 tokens/s, Avg generation throughput: 100.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:00:35 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 101.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 08-09 13:00:39 [logger.py:43] Received request cmpl-7b19e2bad39b42b1bb483c0fbb740a9b-0: prompt: 'Explain the role of SynapseAI in Gaudi software stack.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 12027, 7420, 278, 6297, 310, 10829, 481, 344, 23869, 297, 10415, 4749, 7047, 5096, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:45118 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:00:39 [engine.py:331] Added request cmpl-7b19e2bad39b42b1bb483c0fbb740a9b-0.
INFO 08-09 13:00:40 [metrics.py:417] Avg prompt throughput: 3.2 tokens/s, Avg generation throughput: 96.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:00:45 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 100.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 08-09 13:00:49 [logger.py:43] Received request cmpl-f8eef2b358734ee986553ee3cb06f67b-0: prompt: 'What are the differences between HPU and GPU tensors?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 526, 278, 12651, 1546, 379, 7056, 322, 22796, 25187, 943, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:39160 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:00:49 [engine.py:331] Added request cmpl-f8eef2b358734ee986553ee3cb06f67b-0.
INFO 08-09 13:00:50 [metrics.py:417] Avg prompt throughput: 2.6 tokens/s, Avg generation throughput: 99.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:00:55 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 100.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 08-09 13:00:58 [logger.py:43] Received request cmpl-89e3dc83dc694df0b84854b87b661f71-0: prompt: 'What are the limitations of LLaMA 2?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 526, 278, 27028, 310, 365, 5661, 1529, 29871, 29906, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:49922 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:00:58 [engine.py:331] Added request cmpl-89e3dc83dc694df0b84854b87b661f71-0.
INFO 08-09 13:01:00 [metrics.py:417] Avg prompt throughput: 2.4 tokens/s, Avg generation throughput: 106.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:01:05 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 102.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 08-09 13:01:08 [logger.py:43] Received request cmpl-4f7dd60d3c784b408b3d1e4cd2651f0f-0: prompt: 'How is multilingual performance evaluated in LLMs?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 338, 1773, 6504, 950, 4180, 19030, 297, 365, 26369, 29879, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:39314 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:01:08 [engine.py:331] Added request cmpl-4f7dd60d3c784b408b3d1e4cd2651f0f-0.
INFO 08-09 13:01:10 [metrics.py:417] Avg prompt throughput: 2.6 tokens/s, Avg generation throughput: 98.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:01:15 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 100.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 08-09 13:01:18 [logger.py:43] Received request cmpl-d04009cb06f843da8f99ed26d9a30705-0: prompt: 'Describe an example use of prompt tuning.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 20355, 915, 385, 1342, 671, 310, 9508, 18515, 292, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:49874 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:01:18 [engine.py:331] Added request cmpl-d04009cb06f843da8f99ed26d9a30705-0.
INFO 08-09 13:01:20 [metrics.py:417] Avg prompt throughput: 2.2 tokens/s, Avg generation throughput: 104.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:01:25 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 103.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 08-09 13:01:28 [logger.py:43] Received request cmpl-d556bceecf894e9d9e472fd54746ac70-0: prompt: 'What is the attention bottleneck and how to address it?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 278, 8570, 18046, 29880, 1600, 384, 322, 920, 304, 3211, 372, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:51600 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:01:28 [engine.py:331] Added request cmpl-d556bceecf894e9d9e472fd54746ac70-0.
INFO 08-09 13:01:30 [metrics.py:417] Avg prompt throughput: 3.0 tokens/s, Avg generation throughput: 94.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:01:35 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 95.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 08-09 13:01:38 [logger.py:43] Received request cmpl-bca3a5fd86f744379b629996b758ad65-0: prompt: 'How does token alignment affect downstream tasks?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 947, 5993, 22239, 6602, 1623, 5461, 9595, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:35382 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:01:38 [engine.py:331] Added request cmpl-bca3a5fd86f744379b629996b758ad65-0.
INFO 08-09 13:01:40 [metrics.py:417] Avg prompt throughput: 2.0 tokens/s, Avg generation throughput: 98.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:01:45 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 102.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 08-09 13:01:48 [logger.py:43] Received request cmpl-e5335af1830c477ba9177124863ad3b4-0: prompt: 'What are the challenges in inference on edge devices?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 526, 278, 18066, 267, 297, 27262, 373, 7636, 9224, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:42918 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:01:48 [engine.py:331] Added request cmpl-e5335af1830c477ba9177124863ad3b4-0.
INFO 08-09 13:01:50 [metrics.py:417] Avg prompt throughput: 2.4 tokens/s, Avg generation throughput: 95.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:01:55 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 100.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 08-09 13:01:59 [logger.py:43] Received request cmpl-f4f90360806c4a679f35b6762002a94e-0: prompt: 'Explain the principle behind rotary embeddings.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 12027, 7420, 278, 12502, 5742, 5731, 653, 8297, 29881, 886, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:58822 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:01:59 [engine.py:331] Added request cmpl-f4f90360806c4a679f35b6762002a94e-0.
INFO 08-09 13:02:00 [metrics.py:417] Avg prompt throughput: 2.4 tokens/s, Avg generation throughput: 98.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:02:05 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 98.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 08-09 13:02:09 [logger.py:43] Received request cmpl-088f8c9967a74cb6ae666320a205cade-0: prompt: 'How does temperature affect language model output?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 947, 10430, 6602, 4086, 1904, 1962, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:59016 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:02:09 [engine.py:331] Added request cmpl-088f8c9967a74cb6ae666320a205cade-0.
INFO 08-09 13:02:10 [metrics.py:417] Avg prompt throughput: 1.8 tokens/s, Avg generation throughput: 97.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:02:15 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 96.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 08-09 13:02:19 [logger.py:43] Received request cmpl-3c2b5517e6a742149b30f0f8a0cdbfe8-0: prompt: 'What are stop sequences in generation APIs?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 526, 5040, 15602, 297, 12623, 23649, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:35948 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:02:19 [engine.py:331] Added request cmpl-3c2b5517e6a742149b30f0f8a0cdbfe8-0.
INFO 08-09 13:02:20 [metrics.py:417] Avg prompt throughput: 1.8 tokens/s, Avg generation throughput: 97.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:02:25 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 101.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 08-09 13:02:29 [logger.py:43] Received request cmpl-71eb90e7738746c79feab22c8ad2ad54-0: prompt: 'Describe token streaming latency optimization.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 20355, 915, 5993, 24820, 23316, 1270, 13883, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:50970 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:02:29 [engine.py:331] Added request cmpl-71eb90e7738746c79feab22c8ad2ad54-0.
INFO 08-09 13:02:30 [metrics.py:417] Avg prompt throughput: 1.8 tokens/s, Avg generation throughput: 103.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:02:31 [logger.py:43] Received request cmpl-1bee389f7fd8424cb48aba7542264757-0: prompt: 'How to evaluate hallucinations in language models?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 304, 14707, 12713, 1682, 262, 800, 297, 4086, 4733, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:50984 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:02:31 [engine.py:331] Added request cmpl-1bee389f7fd8424cb48aba7542264757-0.
INFO 08-09 13:02:35 [metrics.py:417] Avg prompt throughput: 2.4 tokens/s, Avg generation throughput: 101.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-09 13:02:36 [logger.py:43] Received request cmpl-9ed9927f92d040a5b9c5830b0267fd15-0: prompt: 'What is the purpose of `bos_token_id`?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 278, 6437, 310, 421, 27737, 29918, 6979, 29918, 333, 6522], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:39720 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:02:36 [engine.py:331] Added request cmpl-9ed9927f92d040a5b9c5830b0267fd15-0.
INFO 08-09 13:02:40 [metrics.py:417] Avg prompt throughput: 2.6 tokens/s, Avg generation throughput: 106.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-09 13:02:45 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 104.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.4%, CPU KV cache usage: 0.0%.
INFO 08-09 13:02:46 [logger.py:43] Received request cmpl-0b3730830baf4348bf7a33d07bd77435-0: prompt: 'How are fused attention layers implemented?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 526, 285, 3880, 8570, 15359, 8762, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:33088 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:02:46 [engine.py:331] Added request cmpl-0b3730830baf4348bf7a33d07bd77435-0.
INFO 08-09 13:02:50 [metrics.py:417] Avg prompt throughput: 1.8 tokens/s, Avg generation throughput: 102.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-09 13:02:55 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 104.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.4%, CPU KV cache usage: 0.0%.
INFO 08-09 13:02:55 [logger.py:43] Received request cmpl-0469e19887ce474cae7beb6c05e2725f-0: prompt: 'Describe a pipeline for deploying LLaMA 2 in production.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 20355, 915, 263, 16439, 363, 7246, 292, 365, 5661, 1529, 29871, 29906, 297, 5802, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:38728 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:02:55 [engine.py:331] Added request cmpl-0469e19887ce474cae7beb6c05e2725f-0.
INFO 08-09 13:03:00 [metrics.py:417] Avg prompt throughput: 3.2 tokens/s, Avg generation throughput: 102.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-09 13:03:05 [logger.py:43] Received request cmpl-e1949364813d45f6aed3d0a768d935c6-0: prompt: 'How does instruction following improve model usability?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 947, 15278, 1494, 11157, 1904, 502, 3097, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:35624 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:03:05 [engine.py:331] Added request cmpl-e1949364813d45f6aed3d0a768d935c6-0.
INFO 08-09 13:03:05 [metrics.py:417] Avg prompt throughput: 2.0 tokens/s, Avg generation throughput: 105.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:03:10 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 103.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 08-09 13:03:10 [logger.py:43] Received request cmpl-c0134014acd24c8897863c49671aac41-0: prompt: 'What is a sequence-to-sequence model?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 263, 5665, 29899, 517, 29899, 16506, 1904, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:35626 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:03:10 [engine.py:331] Added request cmpl-c0134014acd24c8897863c49671aac41-0.
INFO 08-09 13:03:15 [metrics.py:417] Avg prompt throughput: 2.2 tokens/s, Avg generation throughput: 98.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-09 13:03:20 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 102.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.4%, CPU KV cache usage: 0.0%.
INFO 08-09 13:03:20 [logger.py:43] Received request cmpl-b5750193470745de8d7053eec71fbb9f-0: prompt: 'What is the purpose of `pad_token_id` in generation?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 278, 6437, 310, 421, 8305, 29918, 6979, 29918, 333, 29952, 297, 12623, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:60126 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:03:20 [engine.py:331] Added request cmpl-b5750193470745de8d7053eec71fbb9f-0.
INFO 08-09 13:03:25 [metrics.py:417] Avg prompt throughput: 3.2 tokens/s, Avg generation throughput: 99.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-09 13:03:30 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 96.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.4%, CPU KV cache usage: 0.0%.
INFO 08-09 13:03:30 [logger.py:43] Received request cmpl-924817aaa9fb4f7b9f52cc0366289db2-0: prompt: 'Explain the concept of auto-regressive generation.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 12027, 7420, 278, 6964, 310, 4469, 29899, 276, 3663, 573, 12623, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:49294 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:03:30 [engine.py:331] Added request cmpl-924817aaa9fb4f7b9f52cc0366289db2-0.
INFO 08-09 13:03:35 [metrics.py:417] Avg prompt throughput: 2.6 tokens/s, Avg generation throughput: 102.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-09 13:03:40 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 100.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.4%, CPU KV cache usage: 0.0%.
INFO 08-09 13:03:40 [logger.py:43] Received request cmpl-04d8107fa31740d8bbb31d3a3502f1ce-0: prompt: 'How is top-p sampling different from nucleus sampling?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 338, 2246, 29899, 29886, 23460, 1422, 515, 22699, 375, 23460, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:41374 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:03:40 [engine.py:331] Added request cmpl-04d8107fa31740d8bbb31d3a3502f1ce-0.
INFO 08-09 13:03:44 [logger.py:43] Received request cmpl-9b2a26a21c9540b3a8cbf4c8eb77758f-0: prompt: 'What are decoding strategies for safe responses?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 526, 1602, 3689, 16650, 583, 363, 9109, 20890, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:41364 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:03:44 [engine.py:331] Added request cmpl-9b2a26a21c9540b3a8cbf4c8eb77758f-0.
INFO 08-09 13:03:45 [metrics.py:417] Avg prompt throughput: 4.8 tokens/s, Avg generation throughput: 106.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:03:50 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 96.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 08-09 13:03:53 [logger.py:43] Received request cmpl-2ee8b9566a2444c4994f9358307dd9fb-0: prompt: 'How are model prompts structured in ChatML?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 526, 1904, 9508, 29879, 2281, 2955, 297, 678, 271, 1988, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:55692 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:03:53 [engine.py:331] Added request cmpl-2ee8b9566a2444c4994f9358307dd9fb-0.
INFO 08-09 13:03:55 [metrics.py:417] Avg prompt throughput: 2.6 tokens/s, Avg generation throughput: 93.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:04:00 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 98.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 08-09 13:04:03 [logger.py:43] Received request cmpl-cde29fbcaff7460894bfd98fb01badf6-0: prompt: 'What is a system prompt in chat inference?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 263, 1788, 9508, 297, 13563, 27262, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:59272 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:04:03 [engine.py:331] Added request cmpl-cde29fbcaff7460894bfd98fb01badf6-0.
INFO 08-09 13:04:05 [metrics.py:417] Avg prompt throughput: 2.0 tokens/s, Avg generation throughput: 97.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:04:10 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 95.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 08-09 13:04:14 [logger.py:43] Received request cmpl-97454117bcd3486fbc43a8d333819398-0: prompt: 'How do you batch requests for high throughput?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 437, 366, 9853, 7274, 363, 1880, 1549, 649, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:34934 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:04:14 [engine.py:331] Added request cmpl-97454117bcd3486fbc43a8d333819398-0.
INFO 08-09 13:04:15 [metrics.py:417] Avg prompt throughput: 2.2 tokens/s, Avg generation throughput: 90.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:04:20 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 94.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 08-09 13:04:24 [logger.py:43] Received request cmpl-c62308fc32f64dbf97a27030ee2828c6-0: prompt: 'Explain model serving with `transformers + vLLM`.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 12027, 7420, 1904, 16330, 411, 421, 9067, 414, 718, 325, 2208, 29924, 1412], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:43422 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:04:24 [engine.py:331] Added request cmpl-c62308fc32f64dbf97a27030ee2828c6-0.
INFO 08-09 13:04:25 [metrics.py:417] Avg prompt throughput: 2.8 tokens/s, Avg generation throughput: 101.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:04:30 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 99.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 08-09 13:04:34 [logger.py:43] Received request cmpl-b141940d2ea541dcaf406a762148d4bf-0: prompt: 'What is Triton Inference Server?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 323, 768, 265, 512, 1659, 5656, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:47430 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:04:34 [engine.py:331] Added request cmpl-b141940d2ea541dcaf406a762148d4bf-0.
INFO 08-09 13:04:35 [metrics.py:417] Avg prompt throughput: 2.0 tokens/s, Avg generation throughput: 98.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:04:40 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 100.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 08-09 13:04:44 [logger.py:43] Received request cmpl-718989f9229e457cbc51faff853881cc-0: prompt: 'Describe the significance of large context windows.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 20355, 915, 278, 26002, 310, 2919, 3030, 5417, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:42590 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:04:44 [engine.py:331] Added request cmpl-718989f9229e457cbc51faff853881cc-0.
INFO 08-09 13:04:45 [metrics.py:417] Avg prompt throughput: 2.0 tokens/s, Avg generation throughput: 98.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:04:50 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 101.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 08-09 13:04:54 [logger.py:43] Received request cmpl-9280d1a2991e40b38b0b418dea80e821-0: prompt: 'What is dynamic quantization in transformers?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 7343, 4323, 2133, 297, 4327, 414, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:52716 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:04:54 [engine.py:331] Added request cmpl-9280d1a2991e40b38b0b418dea80e821-0.
INFO 08-09 13:04:55 [metrics.py:417] Avg prompt throughput: 2.0 tokens/s, Avg generation throughput: 103.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:05:00 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 102.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 08-09 13:05:04 [logger.py:43] Received request cmpl-e9d7994cf61e431394e23332f6b9da67-0: prompt: 'How to prevent prompt injection in LLMs?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 304, 5557, 9508, 20859, 297, 365, 26369, 29879, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:42290 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:05:04 [engine.py:331] Added request cmpl-e9d7994cf61e431394e23332f6b9da67-0.
INFO 08-09 13:05:05 [metrics.py:417] Avg prompt throughput: 2.2 tokens/s, Avg generation throughput: 92.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:05:10 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 95.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 08-09 13:05:15 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 92.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.4%, CPU KV cache usage: 0.0%.
INFO 08-09 13:05:15 [logger.py:43] Received request cmpl-f986893f3a654d38984893619e52f42d-0: prompt: 'What are attention scores and how are they computed?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 526, 8570, 19435, 322, 920, 526, 896, 15712, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:41744 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:05:15 [engine.py:331] Added request cmpl-f986893f3a654d38984893619e52f42d-0.
INFO 08-09 13:05:20 [metrics.py:417] Avg prompt throughput: 2.2 tokens/s, Avg generation throughput: 92.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-09 13:05:25 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 93.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.4%, CPU KV cache usage: 0.0%.
INFO 08-09 13:05:26 [logger.py:43] Received request cmpl-582c0468259c42e6850f43b1ee368bc9-0: prompt: 'What are residual connections in transformers?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 526, 10995, 950, 12368, 297, 4327, 414, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:37396 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:05:26 [engine.py:331] Added request cmpl-582c0468259c42e6850f43b1ee368bc9-0.
INFO 08-09 13:05:30 [metrics.py:417] Avg prompt throughput: 2.0 tokens/s, Avg generation throughput: 94.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-09 13:05:35 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 99.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.4%, CPU KV cache usage: 0.0%.
INFO 08-09 13:05:36 [logger.py:43] Received request cmpl-99d967b55a104140b6e6d92795d4da4b-0: prompt: 'How to handle long sequences in generation?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 304, 4386, 1472, 15602, 297, 12623, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:42102 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:05:36 [engine.py:331] Added request cmpl-99d967b55a104140b6e6d92795d4da4b-0.
INFO 08-09 13:05:40 [metrics.py:417] Avg prompt throughput: 1.8 tokens/s, Avg generation throughput: 101.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-09 13:05:45 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 102.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.4%, CPU KV cache usage: 0.0%.
INFO 08-09 13:05:46 [logger.py:43] Received request cmpl-2af9199265c44d1b934f1457afb1e17c-0: prompt: 'Explain the use of flash attention v2.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 12027, 7420, 278, 671, 310, 11013, 8570, 325, 29906, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:38754 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:05:46 [engine.py:331] Added request cmpl-2af9199265c44d1b934f1457afb1e17c-0.
INFO 08-09 13:05:50 [metrics.py:417] Avg prompt throughput: 2.2 tokens/s, Avg generation throughput: 97.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-09 13:05:55 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 91.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.4%, CPU KV cache usage: 0.0%.
INFO 08-09 13:05:57 [logger.py:43] Received request cmpl-870c2fc96ab94c1c850032e0084f15da-0: prompt: 'What is a greedy search in inference?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 263, 1395, 7584, 2740, 297, 27262, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:42338 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:05:57 [engine.py:331] Added request cmpl-870c2fc96ab94c1c850032e0084f15da-0.
INFO 08-09 13:06:00 [metrics.py:417] Avg prompt throughput: 2.0 tokens/s, Avg generation throughput: 90.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-09 13:06:05 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 97.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.4%, CPU KV cache usage: 0.0%.
INFO 08-09 13:06:07 [logger.py:43] Received request cmpl-9b1b37e9deec4ef88d6c11a0a4532bca-0: prompt: 'How do checkpoints improve fault tolerance?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 437, 1423, 9748, 11157, 12570, 20341, 749, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:58002 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:06:07 [engine.py:331] Added request cmpl-9b1b37e9deec4ef88d6c11a0a4532bca-0.
INFO 08-09 13:06:10 [metrics.py:417] Avg prompt throughput: 2.0 tokens/s, Avg generation throughput: 96.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-09 13:06:15 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 95.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.4%, CPU KV cache usage: 0.0%.
INFO 08-09 13:06:18 [logger.py:43] Received request cmpl-f04dd5164ba04f6e8532a174e2271893-0: prompt: 'Describe how text-to-text generation works.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 20355, 915, 920, 1426, 29899, 517, 29899, 726, 12623, 1736, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:55722 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:06:18 [engine.py:331] Added request cmpl-f04dd5164ba04f6e8532a174e2271893-0.
INFO 08-09 13:06:20 [metrics.py:417] Avg prompt throughput: 2.4 tokens/s, Avg generation throughput: 89.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:06:25 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 88.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 08-09 13:06:29 [logger.py:43] Received request cmpl-889e5b42ccae42f7baf8a86a203b3f29-0: prompt: 'Explain cosine similarity in embeddings.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 12027, 7420, 6776, 457, 29501, 297, 8297, 29881, 886, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:55446 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:06:29 [engine.py:331] Added request cmpl-889e5b42ccae42f7baf8a86a203b3f29-0.
INFO 08-09 13:06:30 [metrics.py:417] Avg prompt throughput: 2.2 tokens/s, Avg generation throughput: 86.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:06:33 [logger.py:43] Received request cmpl-54b71a16bb034c39935081ac4bdc9ff3-0: prompt: 'What is token-wise parallelism?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 5993, 29899, 3538, 8943, 1608, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:40602 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:06:33 [engine.py:331] Added request cmpl-54b71a16bb034c39935081ac4bdc9ff3-0.
INFO 08-09 13:06:35 [metrics.py:417] Avg prompt throughput: 1.8 tokens/s, Avg generation throughput: 89.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:06:40 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 94.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 08-09 13:06:44 [logger.py:43] Received request cmpl-798657ca4d7143dc97468bb4f35e93b3-0: prompt: 'How do shared embeddings help in decoder-only models?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 437, 7258, 8297, 29881, 886, 1371, 297, 1602, 6119, 29899, 6194, 4733, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:37796 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:06:44 [engine.py:331] Added request cmpl-798657ca4d7143dc97468bb4f35e93b3-0.
INFO 08-09 13:06:45 [metrics.py:417] Avg prompt throughput: 3.0 tokens/s, Avg generation throughput: 100.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:06:49 [logger.py:43] Received request cmpl-7c5227529d66426798ce926131e395c9-0: prompt: 'Explain how to measure latency in model output.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 12027, 7420, 920, 304, 5645, 23316, 1270, 297, 1904, 1962, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:37810 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:06:49 [engine.py:331] Added request cmpl-7c5227529d66426798ce926131e395c9-0.
INFO 08-09 13:06:50 [metrics.py:417] Avg prompt throughput: 2.4 tokens/s, Avg generation throughput: 101.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:06:55 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 101.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 08-09 13:06:59 [logger.py:43] Received request cmpl-7cdd2673648d4f038683342fd4866831-0: prompt: 'What is the purpose of dropout during training?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 278, 6437, 310, 5768, 449, 2645, 6694, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:50094 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:06:59 [engine.py:331] Added request cmpl-7cdd2673648d4f038683342fd4866831-0.
INFO 08-09 13:07:00 [metrics.py:417] Avg prompt throughput: 2.2 tokens/s, Avg generation throughput: 98.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:07:05 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 99.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 08-09 13:07:09 [logger.py:43] Received request cmpl-ffd8276a1ec84adc91acf39e653b5d81-0: prompt: 'What is the role of GELU activation in transformers?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 278, 6297, 310, 402, 6670, 29965, 26229, 297, 4327, 414, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:33092 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:07:09 [engine.py:331] Added request cmpl-ffd8276a1ec84adc91acf39e653b5d81-0.
INFO 08-09 13:07:10 [metrics.py:417] Avg prompt throughput: 2.8 tokens/s, Avg generation throughput: 98.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:07:15 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 91.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 08-09 13:07:18 [logger.py:43] Received request cmpl-3e0df2829e394b45878010794c70a391-0: prompt: 'Explain forward and backward pass in transformers.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 12027, 7420, 6375, 322, 1250, 1328, 1209, 297, 4327, 414, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:55498 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:07:18 [engine.py:331] Added request cmpl-3e0df2829e394b45878010794c70a391-0.
INFO 08-09 13:07:20 [metrics.py:417] Avg prompt throughput: 2.4 tokens/s, Avg generation throughput: 97.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-09 13:07:25 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 96.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 08-09 13:07:28 [logger.py:43] Received request cmpl-c0ae565dbd894f25815f8ab1617b8af1-0: prompt: 'What is speculative decoding and how does it help?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 1580, 28524, 1602, 3689, 322, 920, 947, 372, 1371, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:36372 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:07:28 [engine.py:331] Added request cmpl-c0ae565dbd894f25815f8ab1617b8af1-0.
INFO 08-09 13:07:30 [metrics.py:417] Avg prompt throughput: 2.6 tokens/s, Avg generation throughput: 98.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:07:35 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 102.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 08-09 13:07:38 [logger.py:43] Received request cmpl-1660ad3a5a8f47af8ab11a50b9f89726-0: prompt: 'How to optimize HPU memory usage during inference?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 304, 24656, 379, 7056, 3370, 8744, 2645, 27262, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:53828 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:07:38 [engine.py:331] Added request cmpl-1660ad3a5a8f47af8ab11a50b9f89726-0.
INFO 08-09 13:07:38 [logger.py:43] Received request cmpl-1bc8a291d1974e5981b69740ae68cc00-0: prompt: 'Explain profiling for LLM inference performance.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 12027, 7420, 20077, 292, 363, 365, 26369, 27262, 4180, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:53830 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:07:38 [engine.py:331] Added request cmpl-1bc8a291d1974e5981b69740ae68cc00-0.
INFO 08-09 13:07:40 [metrics.py:417] Avg prompt throughput: 4.4 tokens/s, Avg generation throughput: 101.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:07:45 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 105.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 08-09 13:07:48 [logger.py:43] Received request cmpl-6c6d81ba3a304b1b98749eb8c277c508-0: prompt: 'How to compute inter-token latency per request?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 304, 10272, 1006, 29899, 6979, 23316, 1270, 639, 2009, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:51712 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:07:48 [engine.py:331] Added request cmpl-6c6d81ba3a304b1b98749eb8c277c508-0.
INFO 08-09 13:07:50 [metrics.py:417] Avg prompt throughput: 2.4 tokens/s, Avg generation throughput: 98.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:07:55 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 100.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 08-09 13:07:58 [logger.py:43] Received request cmpl-b2d86500f4af479892bf9efe9af9bd97-0: prompt: 'What is the difference between BFloat16 and Float16?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 278, 4328, 1546, 350, 11031, 29896, 29953, 322, 27842, 29896, 29953, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:37186 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:07:58 [engine.py:331] Added request cmpl-b2d86500f4af479892bf9efe9af9bd97-0.
INFO 08-09 13:08:00 [metrics.py:417] Avg prompt throughput: 3.0 tokens/s, Avg generation throughput: 100.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:08:05 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 100.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 08-09 13:08:08 [logger.py:43] Received request cmpl-500f3a1d78174d378d0cecad0f08f524-0: prompt: 'What are logits and how are they interpreted?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 526, 1480, 1169, 322, 920, 526, 896, 21551, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:60514 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:08:08 [engine.py:331] Added request cmpl-500f3a1d78174d378d0cecad0f08f524-0.
INFO 08-09 13:08:10 [metrics.py:417] Avg prompt throughput: 2.2 tokens/s, Avg generation throughput: 98.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:08:15 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 100.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 08-09 13:08:18 [logger.py:43] Received request cmpl-e66467099dc9464eba486eff2a742a5e-0: prompt: 'What is log probability in token prediction?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 1480, 6976, 297, 5993, 18988, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:51936 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:08:18 [engine.py:331] Added request cmpl-e66467099dc9464eba486eff2a742a5e-0.
INFO 08-09 13:08:20 [metrics.py:417] Avg prompt throughput: 1.8 tokens/s, Avg generation throughput: 98.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:08:25 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 102.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 08-09 13:08:28 [logger.py:43] Received request cmpl-e4df8d0db339487a862cdbd085ec7dd3-0: prompt: 'How to use `TextStreamer` for streaming output?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 304, 671, 421, 1626, 3835, 261, 29952, 363, 24820, 1962, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:41296 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:08:28 [engine.py:331] Added request cmpl-e4df8d0db339487a862cdbd085ec7dd3-0.
INFO 08-09 13:08:30 [metrics.py:417] Avg prompt throughput: 2.6 tokens/s, Avg generation throughput: 101.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:08:35 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 102.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 08-09 13:08:38 [logger.py:43] Received request cmpl-9a9d2c91fd3d43069ea668097730e80d-0: prompt: 'How to tokenize and detokenize a prompt manually?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 304, 5993, 675, 322, 1439, 4476, 675, 263, 9508, 7522, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:37744 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:08:38 [engine.py:331] Added request cmpl-9a9d2c91fd3d43069ea668097730e80d-0.
INFO 08-09 13:08:40 [metrics.py:417] Avg prompt throughput: 2.6 tokens/s, Avg generation throughput: 101.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-09 13:08:45 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 103.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.4%, CPU KV cache usage: 0.0%.
INFO 08-09 13:08:48 [logger.py:43] Received request cmpl-78e244196e584eb9a3e29e79324e1553-0: prompt: 'Describe how a prompt flows through the transformer layers.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 20355, 915, 920, 263, 9508, 24536, 1549, 278, 4327, 261, 15359, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:56330 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:08:48 [engine.py:331] Added request cmpl-78e244196e584eb9a3e29e79324e1553-0.
INFO 08-09 13:08:50 [metrics.py:417] Avg prompt throughput: 2.6 tokens/s, Avg generation throughput: 102.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-09 13:08:55 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 100.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.4%, CPU KV cache usage: 0.0%.
INFO 08-09 13:08:58 [logger.py:43] Received request cmpl-9c9128c951a346469bae493ef3ea1ea0-0: prompt: 'How to handle EOS in streaming responses?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 304, 4386, 382, 3267, 297, 24820, 20890, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:49016 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:08:58 [engine.py:331] Added request cmpl-9c9128c951a346469bae493ef3ea1ea0-0.
INFO 08-09 13:09:00 [logger.py:43] Received request cmpl-88aea63c808c409780bfb8cf37b78ef8-0: prompt: 'What is a model config JSON in HF models?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 263, 1904, 2295, 4663, 297, 379, 29943, 4733, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:49022 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:09:00 [engine.py:331] Added request cmpl-88aea63c808c409780bfb8cf37b78ef8-0.
INFO 08-09 13:09:00 [metrics.py:417] Avg prompt throughput: 4.4 tokens/s, Avg generation throughput: 99.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:09:05 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 98.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 08-09 13:09:10 [logger.py:43] Received request cmpl-fcc7a9410a984214b1890202485baa95-0: prompt: 'What is kv_cache reuse in decoder models?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 10908, 29918, 8173, 24270, 297, 1602, 6119, 4733, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:34636 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:09:10 [engine.py:331] Added request cmpl-fcc7a9410a984214b1890202485baa95-0.
INFO 08-09 13:09:10 [metrics.py:417] Avg prompt throughput: 2.4 tokens/s, Avg generation throughput: 104.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:09:15 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 101.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 08-09 13:09:19 [logger.py:43] Received request cmpl-81b8e1e1e5a244d19cca0db8678803f2-0: prompt: 'Describe a scenario of real-time chatbot streaming.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 20355, 915, 263, 10483, 310, 1855, 29899, 2230, 13563, 7451, 24820, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:46482 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:09:19 [engine.py:331] Added request cmpl-81b8e1e1e5a244d19cca0db8678803f2-0.
INFO 08-09 13:09:20 [metrics.py:417] Avg prompt throughput: 2.6 tokens/s, Avg generation throughput: 107.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:09:25 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 105.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 08-09 13:09:29 [logger.py:43] Received request cmpl-c675bb3b3fbc45389e888dd5d836a72d-0: prompt: 'How to compute throughput with variable prompts?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 304, 10272, 1549, 649, 411, 2286, 9508, 29879, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:35674 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:09:29 [engine.py:331] Added request cmpl-c675bb3b3fbc45389e888dd5d836a72d-0.
INFO 08-09 13:09:30 [metrics.py:417] Avg prompt throughput: 2.2 tokens/s, Avg generation throughput: 101.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:09:35 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 102.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 08-09 13:09:39 [logger.py:43] Received request cmpl-1cc7d14df8ff447c86fb291ef04bc0fb-0: prompt: 'How does `max_new_tokens` differ from `max_length`?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 947, 421, 3317, 29918, 1482, 29918, 517, 12360, 29952, 1163, 515, 421, 3317, 29918, 2848, 6522], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:58664 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:09:39 [engine.py:331] Added request cmpl-1cc7d14df8ff447c86fb291ef04bc0fb-0.
INFO 08-09 13:09:40 [metrics.py:417] Avg prompt throughput: 3.6 tokens/s, Avg generation throughput: 102.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:09:45 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 107.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 08-09 13:09:48 [logger.py:43] Received request cmpl-46c0dbccb3ac45768c1503ee31b0e833-0: prompt: 'Explain the difference between loss and perplexity.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 12027, 7420, 278, 4328, 1546, 6410, 322, 639, 10709, 537, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:44506 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:09:48 [engine.py:331] Added request cmpl-46c0dbccb3ac45768c1503ee31b0e833-0.
INFO 08-09 13:09:49 [logger.py:43] Received request cmpl-e8a1e80455c74a0e9f9fcf3ffff5a792-0: prompt: 'How to log benchmark results in a structured CSV?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 304, 1480, 23513, 2582, 297, 263, 2281, 2955, 16874, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:44518 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:09:49 [engine.py:331] Added request cmpl-e8a1e80455c74a0e9f9fcf3ffff5a792-0.
INFO 08-09 13:09:50 [metrics.py:417] Avg prompt throughput: 4.8 tokens/s, Avg generation throughput: 101.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:09:55 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 98.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 08-09 13:09:57 [logger.py:43] Received request cmpl-dec7320d9ec34304a1b742a3d3767f32-0: prompt: 'What are attention heads and how do they specialize?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 526, 8570, 15883, 322, 920, 437, 896, 4266, 675, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:36562 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:09:57 [engine.py:331] Added request cmpl-dec7320d9ec34304a1b742a3d3767f32-0.
INFO 08-09 13:10:00 [metrics.py:417] Avg prompt throughput: 2.4 tokens/s, Avg generation throughput: 101.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-09 13:10:05 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 90.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.4%, CPU KV cache usage: 0.0%.
INFO 08-09 13:10:07 [logger.py:43] Received request cmpl-b6f1018fd1904b379d61ac92ae999738-0: prompt: 'How to ensure reproducible benchmarking?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 304, 9801, 9483, 15520, 23513, 292, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:60390 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:10:07 [engine.py:331] Added request cmpl-b6f1018fd1904b379d61ac92ae999738-0.
INFO 08-09 13:10:10 [metrics.py:417] Avg prompt throughput: 1.8 tokens/s, Avg generation throughput: 98.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-09 13:10:15 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 101.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.4%, CPU KV cache usage: 0.0%.
INFO 08-09 13:10:17 [logger.py:43] Received request cmpl-6738dfb5713a40e998f5cdf217eba40a-0: prompt: 'What are model shards and when are they used?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 526, 1904, 528, 3163, 322, 746, 526, 896, 1304, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:49892 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:10:17 [engine.py:331] Added request cmpl-6738dfb5713a40e998f5cdf217eba40a-0.
INFO 08-09 13:10:20 [metrics.py:417] Avg prompt throughput: 2.4 tokens/s, Avg generation throughput: 100.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-09 13:10:25 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 99.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.4%, CPU KV cache usage: 0.0%.
INFO 08-09 13:10:27 [logger.py:43] Received request cmpl-a70d6aa90cfe469daa83d935e4341979-0: prompt: 'Explain the relationship between prompt length and latency.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 12027, 7420, 278, 9443, 1546, 9508, 3309, 322, 23316, 1270, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:52512 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:10:27 [engine.py:331] Added request cmpl-a70d6aa90cfe469daa83d935e4341979-0.
INFO 08-09 13:10:30 [metrics.py:417] Avg prompt throughput: 2.4 tokens/s, Avg generation throughput: 99.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-09 13:10:35 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 100.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.4%, CPU KV cache usage: 0.0%.
INFO 08-09 13:10:37 [logger.py:43] Received request cmpl-2d0fba9e2d0f4453a33dcf9a913dbde5-0: prompt: 'What is batch inference and why is it faster?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 9853, 27262, 322, 2020, 338, 372, 8473, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:50978 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:10:37 [engine.py:331] Added request cmpl-2d0fba9e2d0f4453a33dcf9a913dbde5-0.
INFO 08-09 13:10:40 [metrics.py:417] Avg prompt throughput: 2.2 tokens/s, Avg generation throughput: 99.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-09 13:10:45 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 99.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.4%, CPU KV cache usage: 0.0%.
INFO 08-09 13:10:47 [logger.py:43] Received request cmpl-9a6792af6f8f4a3eae47b4ec0b946e33-0: prompt: 'What are rotary sin-cos embeddings?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 526, 5731, 653, 4457, 29899, 3944, 8297, 29881, 886, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:56594 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:10:47 [engine.py:331] Added request cmpl-9a6792af6f8f4a3eae47b4ec0b946e33-0.
INFO 08-09 13:10:50 [metrics.py:417] Avg prompt throughput: 2.4 tokens/s, Avg generation throughput: 94.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-09 13:10:55 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 105.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.4%, CPU KV cache usage: 0.0%.
INFO 08-09 13:10:57 [logger.py:43] Received request cmpl-85c269c0cc9241c3881eded662870cc5-0: prompt: "Describe HuggingFace's `AutoTokenizer` features.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 20355, 915, 379, 688, 3460, 23360, 29915, 29879, 421, 12300, 6066, 3950, 29952, 5680, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:51758 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:10:57 [engine.py:331] Added request cmpl-85c269c0cc9241c3881eded662870cc5-0.
INFO 08-09 13:11:00 [metrics.py:417] Avg prompt throughput: 3.2 tokens/s, Avg generation throughput: 103.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-09 13:11:05 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 105.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.4%, CPU KV cache usage: 0.0%.
INFO 08-09 13:11:07 [logger.py:43] Received request cmpl-5ec6544762fa4d03b202a12ee62b138c-0: prompt: 'How does positional encoding interact with attention?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 947, 2602, 284, 8025, 16254, 411, 8570, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:45650 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:11:07 [engine.py:331] Added request cmpl-5ec6544762fa4d03b202a12ee62b138c-0.
INFO 08-09 13:11:10 [metrics.py:417] Avg prompt throughput: 2.0 tokens/s, Avg generation throughput: 103.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-09 13:11:15 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 101.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.4%, CPU KV cache usage: 0.0%.
INFO 08-09 13:11:17 [logger.py:43] Received request cmpl-c17b5412ac98499aadfd21970973d478-0: prompt: 'What is a streaming callback in Transformers?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 263, 24820, 6939, 297, 4103, 689, 414, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:48932 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:11:17 [engine.py:331] Added request cmpl-c17b5412ac98499aadfd21970973d478-0.
INFO 08-09 13:11:20 [metrics.py:417] Avg prompt throughput: 2.2 tokens/s, Avg generation throughput: 101.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-09 13:11:25 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 100.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.4%, CPU KV cache usage: 0.0%.
INFO 08-09 13:11:27 [logger.py:43] Received request cmpl-11c30ff6d91e4e5bb8c6c6a370ef3bc7-0: prompt: 'How to benchmark models using curl and Python?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 304, 23513, 4733, 773, 11051, 322, 5132, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:38312 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:11:27 [engine.py:331] Added request cmpl-11c30ff6d91e4e5bb8c6c6a370ef3bc7-0.
INFO 08-09 13:11:30 [metrics.py:417] Avg prompt throughput: 2.0 tokens/s, Avg generation throughput: 98.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-09 13:11:32 [logger.py:43] Received request cmpl-080c6b283a324bc3b064e88112045886-0: prompt: 'How to parse streaming responses from vLLM?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 304, 6088, 24820, 20890, 515, 325, 2208, 29924, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:48632 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:11:32 [engine.py:331] Added request cmpl-080c6b283a324bc3b064e88112045886-0.
INFO 08-09 13:11:33 [logger.py:43] Received request cmpl-958f7e19ffc4489ea67efdd79834e9be-0: prompt: 'How does deep caching work in inference?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 947, 6483, 22488, 664, 297, 27262, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:48646 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:11:33 [engine.py:331] Added request cmpl-958f7e19ffc4489ea67efdd79834e9be-0.
INFO 08-09 13:11:35 [metrics.py:417] Avg prompt throughput: 4.0 tokens/s, Avg generation throughput: 99.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:11:40 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 98.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 08-09 13:11:44 [logger.py:43] Received request cmpl-29b709502d0248b2bddd59e82b5f8ac2-0: prompt: 'What is a prompt-truncation error?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 263, 9508, 29899, 509, 4661, 362, 1059, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:34468 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:11:44 [engine.py:331] Added request cmpl-29b709502d0248b2bddd59e82b5f8ac2-0.
INFO 08-09 13:11:45 [metrics.py:417] Avg prompt throughput: 2.2 tokens/s, Avg generation throughput: 90.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:11:46 [logger.py:43] Received request cmpl-fb3103f79e874030a7796fecb5288920-0: prompt: 'Explain the role of tokenizer config files.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 12027, 7420, 278, 6297, 310, 5993, 3950, 2295, 2066, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:34478 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:11:46 [engine.py:331] Added request cmpl-fb3103f79e874030a7796fecb5288920-0.
INFO 08-09 13:11:50 [metrics.py:417] Avg prompt throughput: 2.2 tokens/s, Avg generation throughput: 96.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-09 13:11:55 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 101.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.4%, CPU KV cache usage: 0.0%.
INFO 08-09 13:11:56 [logger.py:43] Received request cmpl-a5c82122174b400dbacc01282f34b85c-0: prompt: 'How to batch multiple prompts into one request?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 304, 9853, 2999, 9508, 29879, 964, 697, 2009, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:53192 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:11:56 [engine.py:331] Added request cmpl-a5c82122174b400dbacc01282f34b85c-0.
INFO 08-09 13:12:00 [metrics.py:417] Avg prompt throughput: 2.2 tokens/s, Avg generation throughput: 98.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-09 13:12:05 [logger.py:43] Received request cmpl-58829c83501843779d2e80fe0737609b-0: prompt: 'What is a chat template in HuggingFace?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 263, 13563, 4472, 297, 379, 688, 3460, 23360, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:45006 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:12:05 [engine.py:331] Added request cmpl-58829c83501843779d2e80fe0737609b-0.
INFO 08-09 13:12:05 [metrics.py:417] Avg prompt throughput: 2.4 tokens/s, Avg generation throughput: 95.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:12:10 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 92.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 08-09 13:12:15 [logger.py:43] Received request cmpl-b3f899178168449d857e0a5d959e611b-0: prompt: 'What is inference latency and why is it important?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 27262, 23316, 1270, 322, 2020, 338, 372, 4100, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:59754 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:12:15 [engine.py:331] Added request cmpl-b3f899178168449d857e0a5d959e611b-0.
INFO 08-09 13:12:15 [metrics.py:417] Avg prompt throughput: 2.4 tokens/s, Avg generation throughput: 97.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:12:20 [logger.py:43] Received request cmpl-83911b0d5fef4aa781a53635ee8fe71b-0: prompt: 'How to reduce memory footprint of LLaMA models?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 304, 10032, 3370, 3661, 2158, 310, 365, 5661, 1529, 4733, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:59768 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:12:20 [engine.py:331] Added request cmpl-83911b0d5fef4aa781a53635ee8fe71b-0.
INFO 08-09 13:12:20 [metrics.py:417] Avg prompt throughput: 2.6 tokens/s, Avg generation throughput: 98.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:12:25 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 101.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 08-09 13:12:30 [logger.py:43] Received request cmpl-b013404da03b4c63a4a44f670fa7b5af-0: prompt: 'What is the difference between `generation_config.json` and `config.json`?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 278, 4328, 1546, 421, 4738, 362, 29918, 2917, 29889, 3126, 29952, 322, 421, 2917, 29889, 3126, 6522], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:60404 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:12:30 [engine.py:331] Added request cmpl-b013404da03b4c63a4a44f670fa7b5af-0.
INFO 08-09 13:12:30 [metrics.py:417] Avg prompt throughput: 4.0 tokens/s, Avg generation throughput: 100.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:12:33 [logger.py:43] Received request cmpl-465a3b0182ed4a93822e1aa23a771092-0: prompt: 'What is a quantized model and how is it served?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 263, 4323, 1891, 1904, 322, 920, 338, 372, 6766, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:53212 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:12:33 [engine.py:331] Added request cmpl-465a3b0182ed4a93822e1aa23a771092-0.
INFO 08-09 13:12:35 [metrics.py:417] Avg prompt throughput: 2.6 tokens/s, Avg generation throughput: 101.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-09 13:12:40 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 99.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.4%, CPU KV cache usage: 0.0%.
INFO 08-09 13:12:43 [logger.py:43] Received request cmpl-d94351dbd3564e548dc6c270e692bb57-0: prompt: 'How to handle prompt overflow in generation?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 304, 4386, 9508, 11969, 297, 12623, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:51002 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:12:43 [engine.py:331] Added request cmpl-d94351dbd3564e548dc6c270e692bb57-0.
INFO 08-09 13:12:45 [metrics.py:417] Avg prompt throughput: 1.8 tokens/s, Avg generation throughput: 95.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-09 13:12:50 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 94.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 08-09 13:12:53 [logger.py:43] Received request cmpl-cee9a37377fc483987918b31288bf3df-0: prompt: 'What is warmup time in inference pipelines?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 14294, 786, 931, 297, 27262, 8450, 24210, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:46176 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:12:53 [engine.py:331] Added request cmpl-cee9a37377fc483987918b31288bf3df-0.
INFO 08-09 13:12:55 [metrics.py:417] Avg prompt throughput: 2.2 tokens/s, Avg generation throughput: 105.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-09 13:13:00 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 103.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.4%, CPU KV cache usage: 0.0%.
INFO 08-09 13:13:03 [logger.py:43] Received request cmpl-48b5a139f0a240c3bcf9187784798bee-0: prompt: 'How to stream JSON tokens using curl?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 304, 4840, 4663, 18897, 773, 11051, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:45974 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:13:03 [engine.py:331] Added request cmpl-48b5a139f0a240c3bcf9187784798bee-0.
INFO 08-09 13:13:05 [metrics.py:417] Avg prompt throughput: 1.8 tokens/s, Avg generation throughput: 97.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-09 13:13:11 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 98.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 08-09 13:13:13 [logger.py:43] Received request cmpl-091b3f3c348143e6be2b7926c0aa3906-0: prompt: 'What is a `generate_with_streaming()` wrapper?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 263, 421, 17158, 29918, 2541, 29918, 5461, 292, 2555, 14476, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:42188 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:13:13 [engine.py:331] Added request cmpl-091b3f3c348143e6be2b7926c0aa3906-0.
INFO 08-09 13:13:16 [metrics.py:417] Avg prompt throughput: 2.8 tokens/s, Avg generation throughput: 97.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:13:21 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 90.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 08-09 13:13:24 [logger.py:43] Received request cmpl-9a19e2b67b764f698f0b0de856018bef-0: prompt: 'How to evaluate TTFT and ITL for multiple prompts?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 304, 14707, 323, 29911, 7818, 322, 13315, 29931, 363, 2999, 9508, 29879, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:43000 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:13:24 [engine.py:331] Added request cmpl-9a19e2b67b764f698f0b0de856018bef-0.
INFO 08-09 13:13:26 [metrics.py:417] Avg prompt throughput: 3.0 tokens/s, Avg generation throughput: 92.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:13:26 [logger.py:43] Received request cmpl-763ff27515f148bbbd6e866bd777859e-0: prompt: 'What are server cold starts in model serving?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 526, 1923, 11220, 8665, 297, 1904, 16330, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:43008 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:13:26 [engine.py:331] Added request cmpl-763ff27515f148bbbd6e866bd777859e-0.
INFO 08-09 13:13:31 [metrics.py:417] Avg prompt throughput: 2.0 tokens/s, Avg generation throughput: 92.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-09 13:13:36 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 100.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.4%, CPU KV cache usage: 0.0%.
INFO 08-09 13:13:36 [logger.py:43] Received request cmpl-a763a05fdeaa4d9d9ee97343c172d1fa-0: prompt: 'Explain how to avoid padding inefficiencies.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 12027, 7420, 920, 304, 4772, 7164, 297, 29872, 2416, 819, 2478, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:48274 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:13:36 [engine.py:331] Added request cmpl-a763a05fdeaa4d9d9ee97343c172d1fa-0.
INFO 08-09 13:13:41 [metrics.py:417] Avg prompt throughput: 2.6 tokens/s, Avg generation throughput: 97.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-09 13:13:46 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 100.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.4%, CPU KV cache usage: 0.0%.
INFO 08-09 13:13:46 [logger.py:43] Received request cmpl-b86f8fe045f34653a58cabf8a0e67a32-0: prompt: 'How does tokenizer pre-processing affect TTFT?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 947, 5993, 3950, 758, 29899, 19170, 6602, 323, 29911, 7818, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:50042 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:13:46 [engine.py:331] Added request cmpl-b86f8fe045f34653a58cabf8a0e67a32-0.
INFO 08-09 13:13:51 [metrics.py:417] Avg prompt throughput: 2.6 tokens/s, Avg generation throughput: 98.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-09 13:13:56 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 96.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.4%, CPU KV cache usage: 0.0%.
INFO 08-09 13:13:56 [logger.py:43] Received request cmpl-9f53520042b94a6e8923d445e32bae17-0: prompt: 'How to track per-prompt latency in logs?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 304, 5702, 639, 29899, 14032, 415, 23316, 1270, 297, 10748, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:33866 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:13:56 [engine.py:331] Added request cmpl-9f53520042b94a6e8923d445e32bae17-0.
INFO 08-09 13:14:01 [metrics.py:417] Avg prompt throughput: 2.6 tokens/s, Avg generation throughput: 105.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-09 13:14:06 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 102.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.4%, CPU KV cache usage: 0.0%.
INFO 08-09 13:14:06 [logger.py:43] Received request cmpl-14faf34e1e6d4e74afb0ee6507dd87fb-0: prompt: 'Explain streaming benchmarks with high concurrency.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 12027, 7420, 24820, 23513, 29879, 411, 1880, 3022, 10880, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:55474 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:14:06 [engine.py:331] Added request cmpl-14faf34e1e6d4e74afb0ee6507dd87fb-0.
INFO 08-09 13:14:11 [metrics.py:417] Avg prompt throughput: 2.2 tokens/s, Avg generation throughput: 104.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-09 13:14:16 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 102.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.4%, CPU KV cache usage: 0.0%.
INFO 08-09 13:14:16 [logger.py:43] Received request cmpl-705c6b78395c4d838c709f4040d6e2dc-0: prompt: 'How to calculate power usage per prompt?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 304, 8147, 3081, 8744, 639, 9508, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:51944 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:14:16 [engine.py:331] Added request cmpl-705c6b78395c4d838c709f4040d6e2dc-0.
INFO 08-09 13:14:21 [metrics.py:417] Avg prompt throughput: 1.8 tokens/s, Avg generation throughput: 101.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 08-09 13:14:25 [logger.py:43] Received request cmpl-fe605b974efb4ecb8ac53c01e1d36871-0: prompt: 'How does the tokenizer handle emojis and symbols?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 947, 278, 5993, 3950, 4386, 953, 3848, 275, 322, 15072, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:44102 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:14:25 [engine.py:331] Added request cmpl-fe605b974efb4ecb8ac53c01e1d36871-0.
INFO 08-09 13:14:26 [metrics.py:417] Avg prompt throughput: 2.6 tokens/s, Avg generation throughput: 101.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:14:31 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 100.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 08-09 13:14:35 [logger.py:43] Received request cmpl-7fbf62b98c184a92bc72040d83f94bfb-0: prompt: 'How does model size affect inference latency?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 947, 1904, 2159, 6602, 27262, 23316, 1270, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:33882 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:14:35 [engine.py:331] Added request cmpl-7fbf62b98c184a92bc72040d83f94bfb-0.
INFO 08-09 13:14:36 [metrics.py:417] Avg prompt throughput: 2.0 tokens/s, Avg generation throughput: 102.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:14:41 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 97.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 08-09 13:14:46 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 87.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.4%, CPU KV cache usage: 0.0%.
INFO 08-09 13:14:46 [logger.py:43] Received request cmpl-e7e1aaeddc1b49a4b469e6946de2d25f-0: prompt: 'How to debug slow streaming responses?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 304, 4744, 5232, 24820, 20890, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:56584 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:14:46 [engine.py:331] Added request cmpl-e7e1aaeddc1b49a4b469e6946de2d25f-0.
INFO 08-09 13:14:51 [metrics.py:417] Avg prompt throughput: 1.6 tokens/s, Avg generation throughput: 99.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-09 13:14:56 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 98.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.4%, CPU KV cache usage: 0.0%.
INFO 08-09 13:14:56 [logger.py:43] Received request cmpl-18042e0bfa0040a0ad7d4850f82b81a6-0: prompt: 'What causes generation to stall midway?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 9946, 12623, 304, 380, 497, 7145, 1582, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:38290 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:14:56 [engine.py:331] Added request cmpl-18042e0bfa0040a0ad7d4850f82b81a6-0.
INFO 08-09 13:15:01 [metrics.py:417] Avg prompt throughput: 2.0 tokens/s, Avg generation throughput: 97.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-09 13:15:06 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 94.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.4%, CPU KV cache usage: 0.0%.
INFO 08-09 13:15:07 [logger.py:43] Received request cmpl-9f254016142949f4b19b2aa55b8ba86f-0: prompt: 'How to load a model on HPU using HF Transformers?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 304, 2254, 263, 1904, 373, 379, 7056, 773, 379, 29943, 4103, 689, 414, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:42002 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:15:07 [engine.py:331] Added request cmpl-9f254016142949f4b19b2aa55b8ba86f-0.
INFO 08-09 13:15:11 [metrics.py:417] Avg prompt throughput: 3.2 tokens/s, Avg generation throughput: 93.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-09 13:15:11 [logger.py:43] Received request cmpl-2748e46a8e7e4bcaae88c4c472d8422c-0: prompt: 'How to pass prompt + history into a chat model?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 304, 1209, 9508, 718, 4955, 964, 263, 13563, 1904, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:42010 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:15:11 [engine.py:331] Added request cmpl-2748e46a8e7e4bcaae88c4c472d8422c-0.
INFO 08-09 13:15:16 [metrics.py:417] Avg prompt throughput: 2.4 tokens/s, Avg generation throughput: 91.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-09 13:15:21 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 88.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.4%, CPU KV cache usage: 0.0%.
INFO 08-09 13:15:22 [logger.py:43] Received request cmpl-b088395cb7f049a89fa14558f53b609e-0: prompt: 'What is an OpenAI-compatible inference API?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 385, 4673, 23869, 29899, 23712, 27262, 3450, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:52908 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:15:22 [engine.py:331] Added request cmpl-b088395cb7f049a89fa14558f53b609e-0.
INFO 08-09 13:15:26 [metrics.py:417] Avg prompt throughput: 2.2 tokens/s, Avg generation throughput: 93.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-09 13:15:31 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 93.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.4%, CPU KV cache usage: 0.0%.
INFO 08-09 13:15:33 [logger.py:43] Received request cmpl-473354f7cee74cb6a20b0f6e846fd4de-0: prompt: 'How to handle decoding errors in stream?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 304, 4386, 1602, 3689, 4436, 297, 4840, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:56570 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:15:33 [engine.py:331] Added request cmpl-473354f7cee74cb6a20b0f6e846fd4de-0.
INFO 08-09 13:15:36 [metrics.py:417] Avg prompt throughput: 2.0 tokens/s, Avg generation throughput: 103.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-09 13:15:37 [logger.py:43] Received request cmpl-87282dd5710845788e5874b91761a79a-0: prompt: 'What are temperature vs randomness in sampling?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 526, 10430, 7186, 4036, 2264, 297, 23460, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:56580 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:15:37 [engine.py:331] Added request cmpl-87282dd5710845788e5874b91761a79a-0.
INFO 08-09 13:15:41 [metrics.py:417] Avg prompt throughput: 2.0 tokens/s, Avg generation throughput: 106.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-09 13:15:46 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 102.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.4%, CPU KV cache usage: 0.0%.
INFO 08-09 13:15:47 [logger.py:43] Received request cmpl-78de08607b674a79b3fe9b1b98b65884-0: prompt: 'Explain token logprobs in output.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 12027, 7420, 5993, 1480, 771, 5824, 297, 1962, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:50354 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:15:47 [engine.py:331] Added request cmpl-78de08607b674a79b3fe9b1b98b65884-0.
INFO 08-09 13:15:51 [metrics.py:417] Avg prompt throughput: 2.0 tokens/s, Avg generation throughput: 101.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-09 13:15:56 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 106.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.4%, CPU KV cache usage: 0.0%.
INFO 08-09 13:15:57 [logger.py:43] Received request cmpl-9e273031b9e743339fe2f8abe9d19329-0: prompt: 'How to measure and reduce TTFT on Gaudi?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 304, 5645, 322, 10032, 323, 29911, 7818, 373, 10415, 4749, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:35140 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:15:57 [engine.py:331] Added request cmpl-9e273031b9e743339fe2f8abe9d19329-0.
INFO 08-09 13:16:01 [metrics.py:417] Avg prompt throughput: 2.6 tokens/s, Avg generation throughput: 107.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-09 13:16:06 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 106.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.4%, CPU KV cache usage: 0.0%.
INFO 08-09 13:16:06 [logger.py:43] Received request cmpl-d37aafdd8cbd46f7afec9f86daa5d766-0: prompt: 'Explain the internals of HF `generate()` loop.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 12027, 7420, 278, 2836, 1338, 310, 379, 29943, 421, 17158, 2555, 2425, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:36144 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:16:06 [engine.py:331] Added request cmpl-d37aafdd8cbd46f7afec9f86daa5d766-0.
INFO 08-09 13:16:11 [metrics.py:417] Avg prompt throughput: 2.8 tokens/s, Avg generation throughput: 105.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 08-09 13:16:16 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 94.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.4%, CPU KV cache usage: 0.0%.
INFO 08-09 13:16:16 [logger.py:43] Received request cmpl-6a50e6fd74014dfe81fe1f405a8a8c4a-0: prompt: 'What is speculative token generation?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 1580, 28524, 5993, 12623, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:55112 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:16:16 [engine.py:331] Added request cmpl-6a50e6fd74014dfe81fe1f405a8a8c4a-0.
INFO 08-09 13:16:21 [metrics.py:417] Avg prompt throughput: 1.6 tokens/s, Avg generation throughput: 98.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-09 13:16:26 [logger.py:43] Received request cmpl-40a36a3aa4414cb9a0cf6a0dc2305035-0: prompt: 'What are possible failure points in vLLM serving?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 526, 1950, 10672, 3291, 297, 325, 2208, 29924, 16330, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:58138 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:16:26 [engine.py:331] Added request cmpl-40a36a3aa4414cb9a0cf6a0dc2305035-0.
INFO 08-09 13:16:26 [metrics.py:417] Avg prompt throughput: 2.4 tokens/s, Avg generation throughput: 108.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:16:31 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 106.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 08-09 13:16:35 [logger.py:43] Received request cmpl-0865f585402240b6b6eb11aedab80398-0: prompt: 'How to catch stream decoding errors gracefully?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 304, 4380, 4840, 1602, 3689, 4436, 17659, 3730, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:45830 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:16:35 [engine.py:331] Added request cmpl-0865f585402240b6b6eb11aedab80398-0.
INFO 08-09 13:16:36 [metrics.py:417] Avg prompt throughput: 2.2 tokens/s, Avg generation throughput: 103.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:16:41 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 92.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 08-09 13:16:46 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 90.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.4%, CPU KV cache usage: 0.0%.
INFO 08-09 13:16:46 [logger.py:43] Received request cmpl-8f72c646b15a4e199247adc0994266a6-0: prompt: 'How to enable/disable kv caching in transformers?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 304, 9025, 29914, 20472, 10908, 22488, 297, 4327, 414, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:36910 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:16:46 [engine.py:331] Added request cmpl-8f72c646b15a4e199247adc0994266a6-0.
INFO 08-09 13:16:48 [logger.py:43] Received request cmpl-e7b107d8be6343cbb721044a8c22369d-0: prompt: 'What are challenges of multilingual streaming inference?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 526, 18066, 267, 310, 1773, 6504, 950, 24820, 27262, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:36918 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:16:48 [engine.py:331] Added request cmpl-e7b107d8be6343cbb721044a8c22369d-0.
INFO 08-09 13:16:51 [metrics.py:417] Avg prompt throughput: 4.8 tokens/s, Avg generation throughput: 100.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-09 13:16:52 [logger.py:43] Received request cmpl-82a021933c4247eb8e4533b320d22ea8-0: prompt: 'How to benchmark with different model precisions?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 304, 23513, 411, 1422, 1904, 12132, 1080, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:36932 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:16:52 [engine.py:331] Added request cmpl-82a021933c4247eb8e4533b320d22ea8-0.
INFO 08-09 13:16:56 [metrics.py:417] Avg prompt throughput: 2.0 tokens/s, Avg generation throughput: 99.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-09 13:17:01 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 97.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.4%, CPU KV cache usage: 0.0%.
INFO 08-09 13:17:02 [logger.py:43] Received request cmpl-6a9c0abb8db6466fb299c328e232afd0-0: prompt: 'What is the tradeoff between token latency and batch size?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 278, 11302, 2696, 1546, 5993, 23316, 1270, 322, 9853, 2159, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:53452 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:17:02 [engine.py:331] Added request cmpl-6a9c0abb8db6466fb299c328e232afd0-0.
INFO 08-09 13:17:06 [metrics.py:417] Avg prompt throughput: 2.8 tokens/s, Avg generation throughput: 92.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-09 13:17:11 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 95.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.4%, CPU KV cache usage: 0.0%.
INFO 08-09 13:17:12 [logger.py:43] Received request cmpl-c4598ab022474c3abb86d855c69e22f8-0: prompt: 'Describe the advantages and limitations of transformer-based neural networks.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 20355, 915, 278, 25486, 322, 27028, 310, 4327, 261, 29899, 6707, 19677, 14379, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:35418 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:17:12 [engine.py:331] Added request cmpl-c4598ab022474c3abb86d855c69e22f8-0.
INFO 08-09 13:17:16 [metrics.py:417] Avg prompt throughput: 3.0 tokens/s, Avg generation throughput: 94.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-09 13:17:21 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 99.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.4%, CPU KV cache usage: 0.0%.
INFO 08-09 13:17:23 [logger.py:43] Received request cmpl-8230b0bf3b804d7d867ebf2d1831cc50-0: prompt: 'Explain the concept of overfitting in machine learning and how to prevent it.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 12027, 7420, 278, 6964, 310, 975, 29888, 5367, 297, 4933, 6509, 322, 920, 304, 5557, 372, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:35966 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:17:23 [engine.py:331] Added request cmpl-8230b0bf3b804d7d867ebf2d1831cc50-0.
INFO 08-09 13:17:26 [metrics.py:417] Avg prompt throughput: 3.6 tokens/s, Avg generation throughput: 104.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-09 13:17:31 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 102.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.4%, CPU KV cache usage: 0.0%.
INFO 08-09 13:17:32 [logger.py:43] Received request cmpl-bb11a01c20c64bc7911670564c8ba1da-0: prompt: 'How does the attention mechanism work in transformer models?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 947, 278, 8570, 13336, 664, 297, 4327, 261, 4733, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:33260 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:17:32 [engine.py:331] Added request cmpl-bb11a01c20c64bc7911670564c8ba1da-0.
INFO 08-09 13:17:36 [metrics.py:417] Avg prompt throughput: 2.4 tokens/s, Avg generation throughput: 98.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-09 13:17:41 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 101.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.4%, CPU KV cache usage: 0.0%.
INFO 08-09 13:17:42 [logger.py:43] Received request cmpl-0c51600d57c240e385a1c4d8bd29a8bc-0: prompt: 'Summarize the main differences between supervised and unsupervised learning.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 6991, 3034, 675, 278, 1667, 12651, 1546, 2428, 11292, 322, 443, 9136, 11292, 6509, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:39440 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:17:42 [engine.py:331] Added request cmpl-0c51600d57c240e385a1c4d8bd29a8bc-0.
INFO 08-09 13:17:46 [metrics.py:417] Avg prompt throughput: 3.2 tokens/s, Avg generation throughput: 100.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-09 13:17:46 [logger.py:43] Received request cmpl-0d533f534694484db4e8394064fb2870-0: prompt: 'What are the ethical concerns associated with deploying large language models?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 526, 278, 11314, 936, 21838, 6942, 411, 7246, 292, 2919, 4086, 4733, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:39450 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:17:46 [engine.py:331] Added request cmpl-0d533f534694484db4e8394064fb2870-0.
INFO 08-09 13:17:51 [metrics.py:417] Avg prompt throughput: 3.0 tokens/s, Avg generation throughput: 100.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-09 13:17:56 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 100.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.4%, CPU KV cache usage: 0.0%.
INFO 08-09 13:17:56 [logger.py:43] Received request cmpl-1f6f0b17d79741219d3495d03249d5c2-0: prompt: 'Explain how gradient descent works in training deep neural networks.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 12027, 7420, 920, 16030, 26815, 1736, 297, 6694, 6483, 19677, 14379, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:46564 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:17:56 [engine.py:331] Added request cmpl-1f6f0b17d79741219d3495d03249d5c2-0.
INFO 08-09 13:18:01 [metrics.py:417] Avg prompt throughput: 2.6 tokens/s, Avg generation throughput: 99.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-09 13:18:06 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 97.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.4%, CPU KV cache usage: 0.0%.
INFO 08-09 13:18:06 [logger.py:43] Received request cmpl-5992f70264e445658602aeeffcd1fc95-0: prompt: 'Describe the role of activation functions in deep learning.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 20355, 915, 278, 6297, 310, 26229, 3168, 297, 6483, 6509, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:44580 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:18:06 [engine.py:331] Added request cmpl-5992f70264e445658602aeeffcd1fc95-0.
INFO 08-09 13:18:11 [metrics.py:417] Avg prompt throughput: 2.4 tokens/s, Avg generation throughput: 98.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-09 13:18:16 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 99.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.4%, CPU KV cache usage: 0.0%.
INFO 08-09 13:18:17 [logger.py:43] Received request cmpl-c6f4ccdd233d4217af0fe71496922cf7-0: prompt: 'How can transfer learning improve model performance on small datasets?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 508, 6782, 6509, 11157, 1904, 4180, 373, 2319, 20035, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:38364 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:18:17 [engine.py:331] Added request cmpl-c6f4ccdd233d4217af0fe71496922cf7-0.
INFO 08-09 13:18:21 [metrics.py:417] Avg prompt throughput: 2.4 tokens/s, Avg generation throughput: 97.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-09 13:18:26 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 101.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.4%, CPU KV cache usage: 0.0%.
INFO 08-09 13:18:27 [logger.py:43] Received request cmpl-7bfcf0926de74862957430fe7020ee43-0: prompt: 'Explain the concept of reinforcement learning with an example.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 12027, 7420, 278, 6964, 310, 15561, 1454, 13561, 6509, 411, 385, 1342, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:42506 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:18:27 [engine.py:331] Added request cmpl-7bfcf0926de74862957430fe7020ee43-0.
INFO 08-09 13:18:31 [metrics.py:417] Avg prompt throughput: 2.8 tokens/s, Avg generation throughput: 97.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-09 13:18:36 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 98.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.4%, CPU KV cache usage: 0.0%.
INFO 08-09 13:18:37 [logger.py:43] Received request cmpl-a424129d0e674ab9896d96cced8ecae5-0: prompt: 'What are the main components of a convolutional neural network (CNN)?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 526, 278, 1667, 7117, 310, 263, 26851, 284, 19677, 3564, 313, 29907, 10262, 6877], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:34204 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:18:37 [engine.py:331] Added request cmpl-a424129d0e674ab9896d96cced8ecae5-0.
INFO 08-09 13:18:41 [metrics.py:417] Avg prompt throughput: 3.2 tokens/s, Avg generation throughput: 103.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-09 13:18:46 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 105.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.4%, CPU KV cache usage: 0.0%.
INFO 08-09 13:18:46 [logger.py:43] Received request cmpl-97a641a3e03f497fbed6b920c761607a-0: prompt: 'Describe how large language models like GPT or LLaMA are trained and fine-tuned.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 20355, 915, 920, 2919, 4086, 4733, 763, 402, 7982, 470, 365, 5661, 1529, 526, 16370, 322, 2691, 29899, 29873, 348, 287, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:60954 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:18:46 [engine.py:331] Added request cmpl-97a641a3e03f497fbed6b920c761607a-0.
INFO 08-09 13:18:51 [metrics.py:417] Avg prompt throughput: 4.6 tokens/s, Avg generation throughput: 95.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-09 13:18:56 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 105.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.4%, CPU KV cache usage: 0.0%.
INFO 08-09 13:18:56 [logger.py:43] Received request cmpl-f81afd5cb95049709d49e7372303b510-0: prompt: 'Compare and contrast BERT and GPT in terms of architecture and use cases.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 3831, 598, 322, 12814, 350, 20161, 322, 402, 7982, 297, 4958, 310, 11258, 322, 671, 4251, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:53060 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:18:56 [engine.py:331] Added request cmpl-f81afd5cb95049709d49e7372303b510-0.
INFO 08-09 13:19:01 [metrics.py:417] Avg prompt throughput: 3.6 tokens/s, Avg generation throughput: 104.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-09 13:19:06 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 101.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.4%, CPU KV cache usage: 0.0%.
INFO 08-09 13:19:16 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 1.2 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 13:19:26 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
Received Interrupt
Received Interrupt
INFO 08-09 13:25:34 [launcher.py:80] Shutting down FastAPI HTTP server.
INFO:     Shutting down
INFO:     Waiting for application shutdown.
INFO:     Application shutdown complete.
/usr/lib/python3.10/multiprocessing/resource_tracker.py:224: UserWarning: resource_tracker: There appear to be 8 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
/usr/lib/python3.10/multiprocessing/resource_tracker.py:224: UserWarning: resource_tracker: There appear to be 1 leaked shared_memory objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
