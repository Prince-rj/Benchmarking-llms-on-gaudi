Calling add_step_closure function does not have any effect. It's lazy mode only functionality. (warning logged once)
Calling mark_step function does not have any effect. It's lazy mode only functionality. (warning logged once)
Calling iter_mark_step function does not have any effect. It's lazy mode only functionality. (warning logged once)
INFO 07-29 20:57:56 [__init__.py:254] Automatically detected platform hpu.
INFO 07-29 20:57:59 [config.py:1975] Disabled the custom all-reduce kernel because it is not supported on current platform.
INFO 07-29 20:58:00 [config.py:1975] Disabled the custom all-reduce kernel because it is not supported on current platform.
INFO 07-29 20:58:00 [config.py:1975] Disabled the custom all-reduce kernel because it is not supported on current platform.
INFO 07-29 20:58:01 [config.py:1975] Disabled the custom all-reduce kernel because it is not supported on current platform.
INFO 07-29 20:58:01 [api_server.py:1289] vLLM API server version 0.6.3.dev5453+ge9c83fca1
INFO 07-29 20:58:02 [config.py:1975] Disabled the custom all-reduce kernel because it is not supported on current platform.
INFO 07-29 20:58:02 [cli_args.py:309] non-default args: {'model': '/software/data/llama_inference/Llama-2-7b-hf/'}
INFO 07-29 20:58:10 [config.py:822] This model supports multiple tasks: {'embed', 'score', 'generate', 'reward', 'classify'}. Defaulting to 'generate'.
WARNING 07-29 20:58:10 [config.py:3236] Your device 'hpu' doesn't support torch.float16. Falling back to torch.bfloat16 for compatibility.
WARNING 07-29 20:58:10 [config.py:3287] Casting torch.float16 to torch.bfloat16.
INFO 07-29 20:58:10 [arg_utils.py:1706] hpu is experimental on VLLM_USE_V1=1. Falling back to V0 Engine.
INFO 07-29 20:58:10 [config.py:1975] Disabled the custom all-reduce kernel because it is not supported on current platform.
INFO 07-29 20:58:10 [api_server.py:267] Started engine process with PID 19192
Calling add_step_closure function does not have any effect. It's lazy mode only functionality. (warning logged once)
Calling mark_step function does not have any effect. It's lazy mode only functionality. (warning logged once)
Calling iter_mark_step function does not have any effect. It's lazy mode only functionality. (warning logged once)
WARNING 07-29 20:58:12 [env_override.py:17] NCCL_CUMEM_ENABLE is set to 0, skipping override. This may increase memory overhead with cudagraph+allreduce: https://github.com/NVIDIA/nccl/issues/1234
INFO 07-29 20:58:14 [__init__.py:254] Automatically detected platform hpu.
INFO 07-29 20:58:16 [llm_engine.py:239] Initializing a V0 LLM engine (v0.6.3.dev5453+ge9c83fca1) with config: model='/software/data/llama_inference/Llama-2-7b-hf/', speculative_config=None, tokenizer='/software/data/llama_inference/Llama-2-7b-hf/', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=hpu, decoding_config=DecodingConfig(backend='xgrammar', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=None, served_model_name=/software/data/llama_inference/Llama-2-7b-hf/, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, pooler_config=None, compilation_config={"level":0,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":[],"splitting_ops":[],"use_inductor":true,"compile_sizes":[],"inductor_compile_config":{"enable_auto_functionalized_v2":false},"inductor_passes":{},"use_cudagraph":false,"cudagraph_num_of_warmups":0,"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"max_capture_size":256,"local_cache_dir":null}, use_cached_outputs=True, 
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
WARNING 07-29 20:58:17 [rocm.py:28] Failed to import from amdsmi with ModuleNotFoundError("No module named 'amdsmi'")
WARNING 07-29 20:58:17 [rocm.py:33] Failed to import from vllm._C with ModuleNotFoundError("No module named 'vllm._C'")
WARNING 07-29 20:58:17 [rocm.py:39] Failed to import from vllm._rocm_C with ModuleNotFoundError("No module named 'vllm._rocm_C'")
WARNING 07-29 20:58:17 [utils.py:2850] Methods add_prompt_adapter,cache_config,compilation_config,current_platform,list_prompt_adapters,load_config,pin_prompt_adapter,remove_prompt_adapter,scheduler_config not implemented in <vllm.worker.hpu_worker.HPUWorker object at 0x773faed08550>
INFO 07-29 20:58:18 [runtime.py:26] Environment:
INFO 07-29 20:58:18 [runtime.py:30]     hw: gaudi2
INFO 07-29 20:58:18 [runtime.py:30]     build: 1.22.0.425
INFO 07-29 20:58:18 [runtime.py:30]     engine_version: v0
INFO 07-29 20:58:18 [runtime.py:30]     bridge_mode: eager
INFO 07-29 20:58:18 [runtime.py:30]     model_type: llama
INFO 07-29 20:58:18 [runtime.py:26] Features:
INFO 07-29 20:58:18 [runtime.py:30]     fp32_alibi_biases: True
INFO 07-29 20:58:18 [runtime.py:30]     fp32_softmax: False
INFO 07-29 20:58:18 [runtime.py:30]     fused_block_softmax_adjustment: False
INFO 07-29 20:58:18 [runtime.py:30]     fused_block_softmax: False
INFO 07-29 20:58:18 [runtime.py:30]     prompt_attn_impl: fsdpa_impl
INFO 07-29 20:58:18 [runtime.py:30]     skip_warmup: False
INFO 07-29 20:58:18 [runtime.py:30]     merged_prefill: False
INFO 07-29 20:58:18 [runtime.py:30]     use_contiguous_pa: True
INFO 07-29 20:58:18 [runtime.py:30]     use_delayed_sampling: True
INFO 07-29 20:58:18 [runtime.py:30]     use_bucketing: True
INFO 07-29 20:58:18 [runtime.py:30]     bucketing_strategy: exponential_bucketing
INFO 07-29 20:58:18 [runtime.py:26] User flags:
INFO 07-29 20:58:18 [runtime.py:30]     VLLM_USE_V1: False
WARNING 07-29 20:58:18 [hpu.py:135] Pin memory is not supported on HPU.
INFO 07-29 20:58:18 [hpu.py:60] Using HPUAttention backend.
============================= HPU SW VERSION ====================================== 
 HB_BUILD_VER = 1.22.0-425
============================= HPU PT BRIDGE CONFIGURATION ON RANK = 0 ============= 
 PT_HPU_LAZY_MODE = 0
 PT_HPU_RECIPE_CACHE_CONFIG = ,false,1024,false
 PT_HPU_MAX_COMPOUND_OP_SIZE = 9223372036854775807
 PT_HPU_LAZY_ACC_PAR_MODE = 1
 PT_HPU_ENABLE_REFINE_DYNAMIC_SHAPES = 0
 PT_HPU_EAGER_PIPELINE_ENABLE = 1
 PT_HPU_EAGER_COLLECTIVE_PIPELINE_ENABLE = 1
 PT_HPU_ENABLE_LAZY_COLLECTIVES = 1
---------------------------: System Configuration :---------------------------
Num CPU Cores : 160
CPU RAM       : 1007 GB
------------------------------------------------------------------------------
INFO 07-29 20:58:20 [parallel_state.py:1065] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.39it/s]
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:02<00:00,  1.51s/it]
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:02<00:00,  1.39s/it]

INFO 07-29 20:58:24 [default_loader.py:272] Loading weights took 3.05 seconds
INFO 07-29 20:58:24 [hpu_model_runner.py:1184] Pre-loading model weights on hpu:0 took 12.55 GiB of device memory (12.56 GiB/94.62 GiB used) and 572.5 MiB of host memory (415.1 GiB/1007 GiB used)
INFO 07-29 20:58:25 [hpu_model_runner.py:1266] Wrapping in HPU Graph took 0 B of device memory (12.56 GiB/94.62 GiB used) and 24.63 MiB of host memory (415.2 GiB/1007 GiB used)
INFO 07-29 20:58:25 [hpu_model_runner.py:1270] Compiling took 0 B of device memory (12.56 GiB/94.62 GiB used) and 46.12 MiB of host memory (415.3 GiB/1007 GiB used)
INFO 07-29 20:58:25 [hpu_model_runner.py:1274] Loading model weights took in total 12.55 GiB of device memory (12.56 GiB/94.62 GiB used) and 694 MiB of host memory (415.3 GiB/1007 GiB used)
WARNING 07-29 20:58:25 [common.py:139] Prompt bucket for (1, 4096, 0) was not prepared. Adding new bucket: (1, 4096, 0)
INFO 07-29 20:58:27 [hpu_worker.py:289] Model profiling run took 476 MiB of device memory (13.02 GiB/94.62 GiB used) and 435.4 MiB of host memory (415.7 GiB/1007 GiB used)
INFO 07-29 20:58:27 [hpu_worker.py:313] Free device memory: 81.6 GiB, 73.44 GiB usable (gpu_memory_utilization=0.9), 7.344 GiB reserved for HPUGraphs (VLLM_GRAPH_RESERVED_MEM=0.1), 66.1 GiB reserved for KV cache
INFO 07-29 20:58:27 [executor_base.py:113] # hpu blocks: 1057, # CPU blocks: 64
INFO 07-29 20:58:27 [executor_base.py:118] Maximum concurrency for 4096 tokens per request: 33.03x
INFO 07-29 20:58:27 [exponential.py:49] Prompt bucket config (min, step, max_warmup, limit) bs:[1, 2, 256, 9], seq:[128, 128, 4096, 13]
INFO 07-29 20:58:27 [common.py:117] Generated 38 prompt buckets [bs, query, num_blocks]: [(1, 128, 0), (1, 256, 0), (1, 384, 0), (1, 512, 0), (1, 640, 0), (1, 768, 0), (1, 896, 0), (1, 1024, 0), (1, 1408, 0), (1, 1792, 0), (1, 2304, 0), (1, 3072, 0), (1, 4096, 0), (2, 128, 0), (2, 256, 0), (2, 384, 0), (2, 512, 0), (2, 640, 0), (2, 768, 0), (2, 896, 0), (2, 1024, 0), (2, 1408, 0), (2, 1792, 0), (4, 128, 0), (4, 256, 0), (4, 384, 0), (4, 512, 0), (4, 640, 0), (4, 768, 0), (4, 896, 0), (4, 1024, 0), (8, 128, 0), (8, 256, 0), (8, 384, 0), (8, 512, 0), (16, 128, 0), (16, 256, 0), (32, 128, 0)]
INFO 07-29 20:58:27 [exponential.py:79] Decode bucket config (min, step, max_warmup, limit) bs:[1, 2, 256, 9], block:[128, 128, 1057, 12]
INFO 07-29 20:58:27 [common.py:117] Generated 72 decode buckets [bs, query, num_blocks]: [(1, 1, 128), (1, 1, 256), (1, 1, 384), (1, 1, 512), (1, 1, 640), (1, 1, 768), (1, 1, 896), (1, 1, 1024), (2, 1, 128), (2, 1, 256), (2, 1, 384), (2, 1, 512), (2, 1, 640), (2, 1, 768), (2, 1, 896), (2, 1, 1024), (4, 1, 128), (4, 1, 256), (4, 1, 384), (4, 1, 512), (4, 1, 640), (4, 1, 768), (4, 1, 896), (4, 1, 1024), (8, 1, 128), (8, 1, 256), (8, 1, 384), (8, 1, 512), (8, 1, 640), (8, 1, 768), (8, 1, 896), (8, 1, 1024), (16, 1, 128), (16, 1, 256), (16, 1, 384), (16, 1, 512), (16, 1, 640), (16, 1, 768), (16, 1, 896), (16, 1, 1024), (32, 1, 128), (32, 1, 256), (32, 1, 384), (32, 1, 512), (32, 1, 640), (32, 1, 768), (32, 1, 896), (32, 1, 1024), (64, 1, 128), (64, 1, 256), (64, 1, 384), (64, 1, 512), (64, 1, 640), (64, 1, 768), (64, 1, 896), (64, 1, 1024), (128, 1, 128), (128, 1, 256), (128, 1, 384), (128, 1, 512), (128, 1, 640), (128, 1, 768), (128, 1, 896), (128, 1, 1024), (256, 1, 128), (256, 1, 256), (256, 1, 384), (256, 1, 512), (256, 1, 640), (256, 1, 768), (256, 1, 896), (256, 1, 1024)]
INFO 07-29 20:58:28 [hpu_worker.py:350] Initializing cache engine took 66.06 GiB of device memory (79.08 GiB/94.62 GiB used) and 3.972 GiB of host memory (419.7 GiB/1007 GiB used)
INFO 07-29 20:58:28 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][1/38] batch_size:32 query_len:128 num_blocks:0 free_mem:15.54 GiB
INFO 07-29 20:58:30 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][2/38] batch_size:16 query_len:256 num_blocks:0 free_mem:15.54 GiB
INFO 07-29 20:58:31 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][3/38] batch_size:16 query_len:128 num_blocks:0 free_mem:15.54 GiB
INFO 07-29 20:58:33 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][4/38] batch_size:8 query_len:512 num_blocks:0 free_mem:15.54 GiB
INFO 07-29 20:58:35 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][5/38] batch_size:8 query_len:384 num_blocks:0 free_mem:15.54 GiB
INFO 07-29 20:58:36 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][6/38] batch_size:8 query_len:256 num_blocks:0 free_mem:15.54 GiB
INFO 07-29 20:58:38 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][7/38] batch_size:8 query_len:128 num_blocks:0 free_mem:15.54 GiB
INFO 07-29 20:58:40 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][8/38] batch_size:4 query_len:1024 num_blocks:0 free_mem:15.54 GiB
INFO 07-29 20:58:42 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][9/38] batch_size:4 query_len:896 num_blocks:0 free_mem:15.54 GiB
INFO 07-29 20:58:43 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][10/38] batch_size:4 query_len:768 num_blocks:0 free_mem:15.54 GiB
INFO 07-29 20:58:45 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][11/38] batch_size:4 query_len:640 num_blocks:0 free_mem:15.54 GiB
INFO 07-29 20:58:47 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][12/38] batch_size:4 query_len:512 num_blocks:0 free_mem:15.54 GiB
INFO 07-29 20:58:48 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][13/38] batch_size:4 query_len:384 num_blocks:0 free_mem:15.54 GiB
INFO 07-29 20:58:50 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][14/38] batch_size:4 query_len:256 num_blocks:0 free_mem:15.54 GiB
INFO 07-29 20:58:52 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][15/38] batch_size:4 query_len:128 num_blocks:0 free_mem:15.54 GiB
INFO 07-29 20:58:54 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][16/38] batch_size:2 query_len:1792 num_blocks:0 free_mem:15.54 GiB
INFO 07-29 20:58:55 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][17/38] batch_size:2 query_len:1408 num_blocks:0 free_mem:15.54 GiB
INFO 07-29 20:58:57 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][18/38] batch_size:2 query_len:1024 num_blocks:0 free_mem:15.54 GiB
INFO 07-29 20:58:59 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][19/38] batch_size:2 query_len:896 num_blocks:0 free_mem:15.54 GiB
INFO 07-29 20:59:01 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][20/38] batch_size:2 query_len:768 num_blocks:0 free_mem:15.54 GiB
INFO 07-29 20:59:03 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][21/38] batch_size:2 query_len:640 num_blocks:0 free_mem:15.54 GiB
INFO 07-29 20:59:04 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][22/38] batch_size:2 query_len:512 num_blocks:0 free_mem:15.54 GiB
INFO 07-29 20:59:06 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][23/38] batch_size:2 query_len:384 num_blocks:0 free_mem:15.54 GiB
INFO 07-29 20:59:08 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][24/38] batch_size:2 query_len:256 num_blocks:0 free_mem:15.54 GiB
INFO 07-29 20:59:10 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][25/38] batch_size:2 query_len:128 num_blocks:0 free_mem:15.54 GiB
INFO 07-29 20:59:12 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][26/38] batch_size:1 query_len:4096 num_blocks:0 free_mem:15.54 GiB
INFO 07-29 20:59:14 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][27/38] batch_size:1 query_len:3072 num_blocks:0 free_mem:15.54 GiB
INFO 07-29 20:59:16 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][28/38] batch_size:1 query_len:2304 num_blocks:0 free_mem:15.54 GiB
INFO 07-29 20:59:18 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][29/38] batch_size:1 query_len:1792 num_blocks:0 free_mem:15.54 GiB
INFO 07-29 20:59:20 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][30/38] batch_size:1 query_len:1408 num_blocks:0 free_mem:15.54 GiB
INFO 07-29 20:59:22 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][31/38] batch_size:1 query_len:1024 num_blocks:0 free_mem:15.54 GiB
INFO 07-29 20:59:24 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][32/38] batch_size:1 query_len:896 num_blocks:0 free_mem:15.54 GiB
INFO 07-29 20:59:26 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][33/38] batch_size:1 query_len:768 num_blocks:0 free_mem:15.54 GiB
INFO 07-29 20:59:28 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][34/38] batch_size:1 query_len:640 num_blocks:0 free_mem:15.54 GiB
INFO 07-29 20:59:30 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][35/38] batch_size:1 query_len:512 num_blocks:0 free_mem:15.54 GiB
INFO 07-29 20:59:32 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][36/38] batch_size:1 query_len:384 num_blocks:0 free_mem:15.54 GiB
INFO 07-29 20:59:34 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][37/38] batch_size:1 query_len:256 num_blocks:0 free_mem:15.54 GiB
INFO 07-29 20:59:36 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][38/38] batch_size:1 query_len:128 num_blocks:0 free_mem:15.54 GiB
INFO 07-29 20:59:39 [hpu_model_runner.py:3029] [Warmup][Graph/decode][1/72] batch_size:256 query_len:1 num_blocks:1024 free_mem:15.54 GiB
INFO 07-29 20:59:41 [hpu_model_runner.py:3029] [Warmup][Graph/decode][2/72] batch_size:256 query_len:1 num_blocks:896 free_mem:15.54 GiB
INFO 07-29 20:59:44 [hpu_model_runner.py:3029] [Warmup][Graph/decode][3/72] batch_size:256 query_len:1 num_blocks:768 free_mem:15.54 GiB
INFO 07-29 20:59:46 [hpu_model_runner.py:3029] [Warmup][Graph/decode][4/72] batch_size:256 query_len:1 num_blocks:640 free_mem:15.54 GiB
INFO 07-29 20:59:48 [hpu_model_runner.py:3029] [Warmup][Graph/decode][5/72] batch_size:256 query_len:1 num_blocks:512 free_mem:15.54 GiB
INFO 07-29 20:59:51 [hpu_model_runner.py:3029] [Warmup][Graph/decode][6/72] batch_size:256 query_len:1 num_blocks:384 free_mem:15.54 GiB
INFO 07-29 20:59:53 [hpu_model_runner.py:3029] [Warmup][Graph/decode][7/72] batch_size:256 query_len:1 num_blocks:256 free_mem:15.54 GiB
INFO 07-29 20:59:56 [hpu_model_runner.py:3029] [Warmup][Graph/decode][8/72] batch_size:256 query_len:1 num_blocks:128 free_mem:15.54 GiB
INFO 07-29 20:59:59 [hpu_model_runner.py:3029] [Warmup][Graph/decode][9/72] batch_size:128 query_len:1 num_blocks:1024 free_mem:15.54 GiB
INFO 07-29 21:00:01 [hpu_model_runner.py:3029] [Warmup][Graph/decode][10/72] batch_size:128 query_len:1 num_blocks:896 free_mem:15.54 GiB
INFO 07-29 21:00:04 [hpu_model_runner.py:3029] [Warmup][Graph/decode][11/72] batch_size:128 query_len:1 num_blocks:768 free_mem:15.54 GiB
INFO 07-29 21:00:06 [hpu_model_runner.py:3029] [Warmup][Graph/decode][12/72] batch_size:128 query_len:1 num_blocks:640 free_mem:15.54 GiB
INFO 07-29 21:00:09 [hpu_model_runner.py:3029] [Warmup][Graph/decode][13/72] batch_size:128 query_len:1 num_blocks:512 free_mem:15.54 GiB
INFO 07-29 21:00:12 [hpu_model_runner.py:3029] [Warmup][Graph/decode][14/72] batch_size:128 query_len:1 num_blocks:384 free_mem:15.54 GiB
INFO 07-29 21:00:14 [hpu_model_runner.py:3029] [Warmup][Graph/decode][15/72] batch_size:128 query_len:1 num_blocks:256 free_mem:15.54 GiB
INFO 07-29 21:00:17 [hpu_model_runner.py:3029] [Warmup][Graph/decode][16/72] batch_size:128 query_len:1 num_blocks:128 free_mem:15.54 GiB
INFO 07-29 21:00:20 [hpu_model_runner.py:3029] [Warmup][Graph/decode][17/72] batch_size:64 query_len:1 num_blocks:1024 free_mem:15.54 GiB
INFO 07-29 21:00:23 [hpu_model_runner.py:3029] [Warmup][Graph/decode][18/72] batch_size:64 query_len:1 num_blocks:896 free_mem:15.54 GiB
INFO 07-29 21:00:25 [hpu_model_runner.py:3029] [Warmup][Graph/decode][19/72] batch_size:64 query_len:1 num_blocks:768 free_mem:15.54 GiB
INFO 07-29 21:00:28 [hpu_model_runner.py:3029] [Warmup][Graph/decode][20/72] batch_size:64 query_len:1 num_blocks:640 free_mem:15.54 GiB
INFO 07-29 21:00:31 [hpu_model_runner.py:3029] [Warmup][Graph/decode][21/72] batch_size:64 query_len:1 num_blocks:512 free_mem:15.54 GiB
INFO 07-29 21:00:34 [hpu_model_runner.py:3029] [Warmup][Graph/decode][22/72] batch_size:64 query_len:1 num_blocks:384 free_mem:15.54 GiB
INFO 07-29 21:00:36 [hpu_model_runner.py:3029] [Warmup][Graph/decode][23/72] batch_size:64 query_len:1 num_blocks:256 free_mem:15.54 GiB
INFO 07-29 21:00:39 [hpu_model_runner.py:3029] [Warmup][Graph/decode][24/72] batch_size:64 query_len:1 num_blocks:128 free_mem:15.54 GiB
INFO 07-29 21:00:42 [hpu_model_runner.py:3029] [Warmup][Graph/decode][25/72] batch_size:32 query_len:1 num_blocks:1024 free_mem:15.54 GiB
INFO 07-29 21:00:45 [hpu_model_runner.py:3029] [Warmup][Graph/decode][26/72] batch_size:32 query_len:1 num_blocks:896 free_mem:15.54 GiB
INFO 07-29 21:00:48 [hpu_model_runner.py:3029] [Warmup][Graph/decode][27/72] batch_size:32 query_len:1 num_blocks:768 free_mem:15.54 GiB
INFO 07-29 21:00:51 [hpu_model_runner.py:3029] [Warmup][Graph/decode][28/72] batch_size:32 query_len:1 num_blocks:640 free_mem:15.54 GiB
INFO 07-29 21:00:54 [hpu_model_runner.py:3029] [Warmup][Graph/decode][29/72] batch_size:32 query_len:1 num_blocks:512 free_mem:15.54 GiB
INFO 07-29 21:00:57 [hpu_model_runner.py:3029] [Warmup][Graph/decode][30/72] batch_size:32 query_len:1 num_blocks:384 free_mem:15.54 GiB
INFO 07-29 21:01:00 [hpu_model_runner.py:3029] [Warmup][Graph/decode][31/72] batch_size:32 query_len:1 num_blocks:256 free_mem:15.54 GiB
INFO 07-29 21:01:03 [hpu_model_runner.py:3029] [Warmup][Graph/decode][32/72] batch_size:32 query_len:1 num_blocks:128 free_mem:15.54 GiB
INFO 07-29 21:01:06 [hpu_model_runner.py:3029] [Warmup][Graph/decode][33/72] batch_size:16 query_len:1 num_blocks:1024 free_mem:15.54 GiB
INFO 07-29 21:01:09 [hpu_model_runner.py:3029] [Warmup][Graph/decode][34/72] batch_size:16 query_len:1 num_blocks:896 free_mem:15.54 GiB
INFO 07-29 21:01:12 [hpu_model_runner.py:3029] [Warmup][Graph/decode][35/72] batch_size:16 query_len:1 num_blocks:768 free_mem:15.54 GiB
INFO 07-29 21:01:15 [hpu_model_runner.py:3029] [Warmup][Graph/decode][36/72] batch_size:16 query_len:1 num_blocks:640 free_mem:15.54 GiB
INFO 07-29 21:01:18 [hpu_model_runner.py:3029] [Warmup][Graph/decode][37/72] batch_size:16 query_len:1 num_blocks:512 free_mem:15.54 GiB
INFO 07-29 21:01:21 [hpu_model_runner.py:3029] [Warmup][Graph/decode][38/72] batch_size:16 query_len:1 num_blocks:384 free_mem:15.54 GiB
INFO 07-29 21:01:24 [hpu_model_runner.py:3029] [Warmup][Graph/decode][39/72] batch_size:16 query_len:1 num_blocks:256 free_mem:15.54 GiB
INFO 07-29 21:01:27 [hpu_model_runner.py:3029] [Warmup][Graph/decode][40/72] batch_size:16 query_len:1 num_blocks:128 free_mem:15.54 GiB
INFO 07-29 21:01:30 [hpu_model_runner.py:3029] [Warmup][Graph/decode][41/72] batch_size:8 query_len:1 num_blocks:1024 free_mem:15.54 GiB
INFO 07-29 21:01:33 [hpu_model_runner.py:3029] [Warmup][Graph/decode][42/72] batch_size:8 query_len:1 num_blocks:896 free_mem:15.54 GiB
INFO 07-29 21:01:36 [hpu_model_runner.py:3029] [Warmup][Graph/decode][43/72] batch_size:8 query_len:1 num_blocks:768 free_mem:15.54 GiB
INFO 07-29 21:01:39 [hpu_model_runner.py:3029] [Warmup][Graph/decode][44/72] batch_size:8 query_len:1 num_blocks:640 free_mem:15.54 GiB
INFO 07-29 21:01:42 [hpu_model_runner.py:3029] [Warmup][Graph/decode][45/72] batch_size:8 query_len:1 num_blocks:512 free_mem:15.54 GiB
INFO 07-29 21:01:46 [hpu_model_runner.py:3029] [Warmup][Graph/decode][46/72] batch_size:8 query_len:1 num_blocks:384 free_mem:15.54 GiB
INFO 07-29 21:01:49 [hpu_model_runner.py:3029] [Warmup][Graph/decode][47/72] batch_size:8 query_len:1 num_blocks:256 free_mem:15.54 GiB
INFO 07-29 21:01:52 [hpu_model_runner.py:3029] [Warmup][Graph/decode][48/72] batch_size:8 query_len:1 num_blocks:128 free_mem:15.54 GiB
INFO 07-29 21:01:55 [hpu_model_runner.py:3029] [Warmup][Graph/decode][49/72] batch_size:4 query_len:1 num_blocks:1024 free_mem:15.54 GiB
INFO 07-29 21:01:58 [hpu_model_runner.py:3029] [Warmup][Graph/decode][50/72] batch_size:4 query_len:1 num_blocks:896 free_mem:15.54 GiB
INFO 07-29 21:02:02 [hpu_model_runner.py:3029] [Warmup][Graph/decode][51/72] batch_size:4 query_len:1 num_blocks:768 free_mem:15.54 GiB
INFO 07-29 21:02:05 [hpu_model_runner.py:3029] [Warmup][Graph/decode][52/72] batch_size:4 query_len:1 num_blocks:640 free_mem:15.54 GiB
INFO 07-29 21:02:08 [hpu_model_runner.py:3029] [Warmup][Graph/decode][53/72] batch_size:4 query_len:1 num_blocks:512 free_mem:15.54 GiB
INFO 07-29 21:02:11 [hpu_model_runner.py:3029] [Warmup][Graph/decode][54/72] batch_size:4 query_len:1 num_blocks:384 free_mem:15.54 GiB
INFO 07-29 21:02:14 [hpu_model_runner.py:3029] [Warmup][Graph/decode][55/72] batch_size:4 query_len:1 num_blocks:256 free_mem:15.54 GiB
INFO 07-29 21:02:18 [hpu_model_runner.py:3029] [Warmup][Graph/decode][56/72] batch_size:4 query_len:1 num_blocks:128 free_mem:15.54 GiB
INFO 07-29 21:02:21 [hpu_model_runner.py:3029] [Warmup][Graph/decode][57/72] batch_size:2 query_len:1 num_blocks:1024 free_mem:15.54 GiB
INFO 07-29 21:02:24 [hpu_model_runner.py:3029] [Warmup][Graph/decode][58/72] batch_size:2 query_len:1 num_blocks:896 free_mem:15.54 GiB
INFO 07-29 21:02:28 [hpu_model_runner.py:3029] [Warmup][Graph/decode][59/72] batch_size:2 query_len:1 num_blocks:768 free_mem:15.54 GiB
INFO 07-29 21:02:31 [hpu_model_runner.py:3029] [Warmup][Graph/decode][60/72] batch_size:2 query_len:1 num_blocks:640 free_mem:15.54 GiB
INFO 07-29 21:02:35 [hpu_model_runner.py:3029] [Warmup][Graph/decode][61/72] batch_size:2 query_len:1 num_blocks:512 free_mem:15.54 GiB
INFO 07-29 21:02:38 [hpu_model_runner.py:3029] [Warmup][Graph/decode][62/72] batch_size:2 query_len:1 num_blocks:384 free_mem:15.54 GiB
INFO 07-29 21:02:41 [hpu_model_runner.py:3029] [Warmup][Graph/decode][63/72] batch_size:2 query_len:1 num_blocks:256 free_mem:15.54 GiB
INFO 07-29 21:02:45 [hpu_model_runner.py:3029] [Warmup][Graph/decode][64/72] batch_size:2 query_len:1 num_blocks:128 free_mem:15.54 GiB
INFO 07-29 21:02:48 [hpu_model_runner.py:3029] [Warmup][Graph/decode][65/72] batch_size:1 query_len:1 num_blocks:1024 free_mem:15.54 GiB
INFO 07-29 21:02:52 [hpu_model_runner.py:3029] [Warmup][Graph/decode][66/72] batch_size:1 query_len:1 num_blocks:896 free_mem:15.54 GiB
INFO 07-29 21:02:55 [hpu_model_runner.py:3029] [Warmup][Graph/decode][67/72] batch_size:1 query_len:1 num_blocks:768 free_mem:15.54 GiB
INFO 07-29 21:02:59 [hpu_model_runner.py:3029] [Warmup][Graph/decode][68/72] batch_size:1 query_len:1 num_blocks:640 free_mem:15.54 GiB
INFO 07-29 21:03:02 [hpu_model_runner.py:3029] [Warmup][Graph/decode][69/72] batch_size:1 query_len:1 num_blocks:512 free_mem:15.54 GiB
INFO 07-29 21:03:06 [hpu_model_runner.py:3029] [Warmup][Graph/decode][70/72] batch_size:1 query_len:1 num_blocks:384 free_mem:15.54 GiB
INFO 07-29 21:03:10 [hpu_model_runner.py:3029] [Warmup][Graph/decode][71/72] batch_size:1 query_len:1 num_blocks:256 free_mem:15.54 GiB
INFO 07-29 21:03:14 [hpu_model_runner.py:3029] [Warmup][Graph/decode][72/72] batch_size:1 query_len:1 num_blocks:128 free_mem:15.54 GiB
INFO 07-29 21:03:17 [hpu_model_runner.py:3152] Prompt captured:38 (100.0%) used_mem:31.75 KiB
INFO 07-29 21:03:17 [hpu_model_runner.py:3152] Decode captured:72 (100.0%) used_mem:0 B
INFO 07-29 21:03:17 [hpu_model_runner.py:3280] Warmup finished in 289 secs, allocated 31.75 KiB of device memory
INFO 07-29 21:03:17 [llm_engine.py:431] init engine (profile, create kv cache, warmup model) took 292.23 seconds
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
WARNING 07-29 21:03:18 [config.py:1364] Default sampling parameters have been overridden by the model's Hugging Face generation config recommended from the model creator. If this is not intended, please relaunch vLLM instance with `--generation-config vllm`.
INFO 07-29 21:03:18 [serving_chat.py:118] Using default chat sampling params from model: {'temperature': 0.9, 'top_p': 0.6}
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
INFO 07-29 21:03:18 [serving_completion.py:66] Using default completion sampling params from model: {'temperature': 0.9, 'top_p': 0.6}
INFO 07-29 21:03:18 [api_server.py:1351] Starting vLLM API server 0 on http://0.0.0.0:8000
INFO 07-29 21:03:18 [launcher.py:29] Available routes are:
INFO 07-29 21:03:18 [launcher.py:37] Route: /openapi.json, Methods: GET, HEAD
INFO 07-29 21:03:18 [launcher.py:37] Route: /docs, Methods: GET, HEAD
INFO 07-29 21:03:18 [launcher.py:37] Route: /docs/oauth2-redirect, Methods: GET, HEAD
INFO 07-29 21:03:18 [launcher.py:37] Route: /redoc, Methods: GET, HEAD
INFO 07-29 21:03:18 [launcher.py:37] Route: /health, Methods: GET
INFO 07-29 21:03:18 [launcher.py:37] Route: /load, Methods: GET
INFO 07-29 21:03:18 [launcher.py:37] Route: /ping, Methods: POST
INFO 07-29 21:03:18 [launcher.py:37] Route: /ping, Methods: GET
INFO 07-29 21:03:18 [launcher.py:37] Route: /tokenize, Methods: POST
INFO 07-29 21:03:18 [launcher.py:37] Route: /detokenize, Methods: POST
INFO 07-29 21:03:18 [launcher.py:37] Route: /v1/models, Methods: GET
INFO 07-29 21:03:18 [launcher.py:37] Route: /version, Methods: GET
INFO 07-29 21:03:18 [launcher.py:37] Route: /v1/chat/completions, Methods: POST
INFO 07-29 21:03:18 [launcher.py:37] Route: /v1/completions, Methods: POST
INFO 07-29 21:03:18 [launcher.py:37] Route: /v1/embeddings, Methods: POST
INFO 07-29 21:03:18 [launcher.py:37] Route: /pooling, Methods: POST
INFO 07-29 21:03:18 [launcher.py:37] Route: /classify, Methods: POST
INFO 07-29 21:03:18 [launcher.py:37] Route: /score, Methods: POST
INFO 07-29 21:03:18 [launcher.py:37] Route: /v1/score, Methods: POST
INFO 07-29 21:03:18 [launcher.py:37] Route: /v1/audio/transcriptions, Methods: POST
INFO 07-29 21:03:18 [launcher.py:37] Route: /rerank, Methods: POST
INFO 07-29 21:03:18 [launcher.py:37] Route: /v1/rerank, Methods: POST
INFO 07-29 21:03:18 [launcher.py:37] Route: /v2/rerank, Methods: POST
INFO 07-29 21:03:18 [launcher.py:37] Route: /invocations, Methods: POST
INFO 07-29 21:03:18 [launcher.py:37] Route: /metrics, Methods: GET
INFO:     Started server process [19110]
INFO:     Waiting for application startup.
INFO:     Application startup complete.
INFO 07-29 21:04:54 [logger.py:43] Received request cmpl-8efb6d41556044388009cfea1cb90f0d-0: prompt: 'You are llama2, a large-language model and AI assistant created by Facebook.\nDescribe what is LLaMA model and its architecture in detail.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 887, 526, 11148, 3304, 29906, 29892, 263, 2919, 29899, 11675, 1904, 322, 319, 29902, 20255, 2825, 491, 13327, 29889, 13, 4002, 29581, 825, 338, 365, 5661, 1529, 1904, 322, 967, 11258, 297, 9493, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO 07-29 21:04:54 [engine.py:331] Added request cmpl-8efb6d41556044388009cfea1cb90f0d-0.
INFO 07-29 21:04:55 [metrics.py:417] Avg prompt throughput: 5.1 tokens/s, Avg generation throughput: 0.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 07-29 21:05:00 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 79.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.5%, CPU KV cache usage: 0.0%.
INFO:     127.0.0.1:53666 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 07-29 21:05:12 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 13.8 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 07-29 21:05:22 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
Received Interrupt
INFO 07-29 21:05:28 [launcher.py:80] Shutting down FastAPI HTTP server.
INFO:     Shutting down
INFO:     Waiting for application shutdown.
INFO:     Application shutdown complete.
/usr/lib/python3.10/multiprocessing/resource_tracker.py:224: UserWarning: resource_tracker: There appear to be 1 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
