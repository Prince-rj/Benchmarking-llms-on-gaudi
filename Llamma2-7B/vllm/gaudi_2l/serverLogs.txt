Calling add_step_closure function does not have any effect. It's lazy mode only functionality. (warning logged once)
Calling mark_step function does not have any effect. It's lazy mode only functionality. (warning logged once)
Calling iter_mark_step function does not have any effect. It's lazy mode only functionality. (warning logged once)
INFO 08-09 12:33:29 [__init__.py:254] Automatically detected platform hpu.
INFO 08-09 12:33:33 [config.py:1975] Disabled the custom all-reduce kernel because it is not supported on current platform.
INFO 08-09 12:33:33 [config.py:1975] Disabled the custom all-reduce kernel because it is not supported on current platform.
INFO 08-09 12:33:34 [config.py:1975] Disabled the custom all-reduce kernel because it is not supported on current platform.
INFO 08-09 12:33:35 [config.py:1975] Disabled the custom all-reduce kernel because it is not supported on current platform.
INFO 08-09 12:33:35 [api_server.py:1289] vLLM API server version 0.6.3.dev5453+ge9c83fca1
INFO 08-09 12:33:35 [config.py:1975] Disabled the custom all-reduce kernel because it is not supported on current platform.
INFO 08-09 12:33:35 [cli_args.py:309] non-default args: {'model': '/software/data/llama_inference/Llama-2-7b-hf/', 'tensor_parallel_size': 8}
INFO 08-09 12:33:43 [config.py:822] This model supports multiple tasks: {'embed', 'classify', 'generate', 'reward', 'score'}. Defaulting to 'generate'.
WARNING 08-09 12:33:43 [config.py:3236] Your device 'hpu' doesn't support torch.float16. Falling back to torch.bfloat16 for compatibility.
WARNING 08-09 12:33:43 [config.py:3287] Casting torch.float16 to torch.bfloat16.
INFO 08-09 12:33:43 [arg_utils.py:1706] hpu is experimental on VLLM_USE_V1=1. Falling back to V0 Engine.
INFO 08-09 12:33:43 [config.py:1941] Defaulting to use mp for distributed inference
INFO 08-09 12:33:43 [config.py:1975] Disabled the custom all-reduce kernel because it is not supported on current platform.
INFO 08-09 12:33:43 [api_server.py:267] Started engine process with PID 54795
Calling add_step_closure function does not have any effect. It's lazy mode only functionality. (warning logged once)
Calling mark_step function does not have any effect. It's lazy mode only functionality. (warning logged once)
Calling iter_mark_step function does not have any effect. It's lazy mode only functionality. (warning logged once)
WARNING 08-09 12:33:46 [env_override.py:17] NCCL_CUMEM_ENABLE is set to 0, skipping override. This may increase memory overhead with cudagraph+allreduce: https://github.com/NVIDIA/nccl/issues/1234
INFO 08-09 12:33:47 [__init__.py:254] Automatically detected platform hpu.
INFO 08-09 12:33:50 [llm_engine.py:239] Initializing a V0 LLM engine (v0.6.3.dev5453+ge9c83fca1) with config: model='/software/data/llama_inference/Llama-2-7b-hf/', speculative_config=None, tokenizer='/software/data/llama_inference/Llama-2-7b-hf/', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=8, pipeline_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=hpu, decoding_config=DecodingConfig(backend='xgrammar', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=None, served_model_name=/software/data/llama_inference/Llama-2-7b-hf/, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, pooler_config=None, compilation_config={"level":0,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":[],"splitting_ops":[],"use_inductor":true,"compile_sizes":[],"inductor_compile_config":{"enable_auto_functionalized_v2":false},"inductor_passes":{},"use_cudagraph":false,"cudagraph_num_of_warmups":0,"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"max_capture_size":256,"local_cache_dir":null}, use_cached_outputs=True, 
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
WARNING 08-09 12:33:50 [multiproc_worker_utils.py:325] Reducing Torch parallelism from 80 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
WARNING 08-09 12:33:50 [rocm.py:28] Failed to import from amdsmi with ModuleNotFoundError("No module named 'amdsmi'")
WARNING 08-09 12:33:50 [rocm.py:33] Failed to import from vllm._C with ModuleNotFoundError("No module named 'vllm._C'")
WARNING 08-09 12:33:50 [rocm.py:39] Failed to import from vllm._rocm_C with ModuleNotFoundError("No module named 'vllm._rocm_C'")
WARNING 08-09 12:33:50 [utils.py:2850] Methods add_prompt_adapter,cache_config,compilation_config,current_platform,list_prompt_adapters,load_config,pin_prompt_adapter,remove_prompt_adapter,scheduler_config not implemented in <vllm.worker.hpu_worker.HPUWorker object at 0x7d2a118a36d0>
INFO 08-09 12:33:51 [runtime.py:26] Environment:
INFO 08-09 12:33:51 [runtime.py:30]     hw: gaudi2
INFO 08-09 12:33:51 [runtime.py:30]     build: 1.22.0.425
INFO 08-09 12:33:51 [runtime.py:30]     engine_version: v0
INFO 08-09 12:33:51 [runtime.py:30]     bridge_mode: eager
INFO 08-09 12:33:51 [runtime.py:30]     model_type: llama
INFO 08-09 12:33:51 [runtime.py:26] Features:
INFO 08-09 12:33:51 [runtime.py:30]     fp32_alibi_biases: True
INFO 08-09 12:33:51 [runtime.py:30]     fp32_softmax: False
INFO 08-09 12:33:51 [runtime.py:30]     fused_block_softmax_adjustment: False
INFO 08-09 12:33:51 [runtime.py:30]     fused_block_softmax: False
INFO 08-09 12:33:51 [runtime.py:30]     prompt_attn_impl: fsdpa_impl
INFO 08-09 12:33:51 [runtime.py:30]     skip_warmup: False
INFO 08-09 12:33:51 [runtime.py:30]     merged_prefill: False
INFO 08-09 12:33:51 [runtime.py:30]     use_contiguous_pa: True
INFO 08-09 12:33:51 [runtime.py:30]     use_delayed_sampling: True
INFO 08-09 12:33:51 [runtime.py:30]     use_bucketing: True
INFO 08-09 12:33:51 [runtime.py:30]     bucketing_strategy: exponential_bucketing
INFO 08-09 12:33:51 [runtime.py:26] User flags:
INFO 08-09 12:33:51 [runtime.py:30]     VLLM_USE_V1: False
WARNING 08-09 12:33:51 [hpu.py:135] Pin memory is not supported on HPU.
INFO 08-09 12:33:51 [hpu.py:60] Using HPUAttention backend.
Calling add_step_closure function does not have any effect. It's lazy mode only functionality. (warning logged once)
Calling add_step_closure function does not have any effect. It's lazy mode only functionality. (warning logged once)
Calling add_step_closure function does not have any effect. It's lazy mode only functionality. (warning logged once)
Calling mark_step function does not have any effect. It's lazy mode only functionality. (warning logged once)
Calling mark_step function does not have any effect. It's lazy mode only functionality. (warning logged once)
Calling mark_step function does not have any effect. It's lazy mode only functionality. (warning logged once)
Calling iter_mark_step function does not have any effect. It's lazy mode only functionality. (warning logged once)
Calling iter_mark_step function does not have any effect. It's lazy mode only functionality. (warning logged once)
Calling iter_mark_step function does not have any effect. It's lazy mode only functionality. (warning logged once)
Calling add_step_closure function does not have any effect. It's lazy mode only functionality. (warning logged once)
Calling mark_step function does not have any effect. It's lazy mode only functionality. (warning logged once)
Calling iter_mark_step function does not have any effect. It's lazy mode only functionality. (warning logged once)
Calling add_step_closure function does not have any effect. It's lazy mode only functionality. (warning logged once)
Calling mark_step function does not have any effect. It's lazy mode only functionality. (warning logged once)
Calling iter_mark_step function does not have any effect. It's lazy mode only functionality. (warning logged once)
Calling add_step_closure function does not have any effect. It's lazy mode only functionality. (warning logged once)
Calling add_step_closure function does not have any effect. It's lazy mode only functionality. (warning logged once)
Calling mark_step function does not have any effect. It's lazy mode only functionality. (warning logged once)
Calling mark_step function does not have any effect. It's lazy mode only functionality. (warning logged once)
Calling iter_mark_step function does not have any effect. It's lazy mode only functionality. (warning logged once)
Calling iter_mark_step function does not have any effect. It's lazy mode only functionality. (warning logged once)
WARNING 08-09 12:33:53 [env_override.py:17] NCCL_CUMEM_ENABLE is set to 0, skipping override. This may increase memory overhead with cudagraph+allreduce: https://github.com/NVIDIA/nccl/issues/1234
WARNING 08-09 12:33:53 [env_override.py:17] NCCL_CUMEM_ENABLE is set to 0, skipping override. This may increase memory overhead with cudagraph+allreduce: https://github.com/NVIDIA/nccl/issues/1234
WARNING 08-09 12:33:53 [env_override.py:17] NCCL_CUMEM_ENABLE is set to 0, skipping override. This may increase memory overhead with cudagraph+allreduce: https://github.com/NVIDIA/nccl/issues/1234
WARNING 08-09 12:33:53 [env_override.py:17] NCCL_CUMEM_ENABLE is set to 0, skipping override. This may increase memory overhead with cudagraph+allreduce: https://github.com/NVIDIA/nccl/issues/1234
WARNING 08-09 12:33:53 [env_override.py:17] NCCL_CUMEM_ENABLE is set to 0, skipping override. This may increase memory overhead with cudagraph+allreduce: https://github.com/NVIDIA/nccl/issues/1234
WARNING 08-09 12:33:53 [env_override.py:17] NCCL_CUMEM_ENABLE is set to 0, skipping override. This may increase memory overhead with cudagraph+allreduce: https://github.com/NVIDIA/nccl/issues/1234
WARNING 08-09 12:33:53 [env_override.py:17] NCCL_CUMEM_ENABLE is set to 0, skipping override. This may increase memory overhead with cudagraph+allreduce: https://github.com/NVIDIA/nccl/issues/1234
INFO 08-09 12:33:54 [__init__.py:254] Automatically detected platform hpu.
INFO 08-09 12:33:54 [__init__.py:254] Automatically detected platform hpu.
INFO 08-09 12:33:54 [__init__.py:254] Automatically detected platform hpu.
INFO 08-09 12:33:54 [__init__.py:254] Automatically detected platform hpu.
INFO 08-09 12:33:54 [__init__.py:254] Automatically detected platform hpu.
INFO 08-09 12:33:54 [__init__.py:254] Automatically detected platform hpu.
INFO 08-09 12:33:54 [__init__.py:254] Automatically detected platform hpu.
[1;36m(VllmWorkerProcess pid=55081)[0;0m INFO 08-09 12:33:57 [multiproc_worker_utils.py:228] Worker ready; awaiting tasks
[1;36m(VllmWorkerProcess pid=55079)[0;0m INFO 08-09 12:33:57 [multiproc_worker_utils.py:228] Worker ready; awaiting tasks
[1;36m(VllmWorkerProcess pid=55080)[0;0m INFO 08-09 12:33:57 [multiproc_worker_utils.py:228] Worker ready; awaiting tasks
[1;36m(VllmWorkerProcess pid=55077)[0;0m INFO 08-09 12:33:57 [multiproc_worker_utils.py:228] Worker ready; awaiting tasks
[1;36m(VllmWorkerProcess pid=55076)[0;0m INFO 08-09 12:33:57 [multiproc_worker_utils.py:228] Worker ready; awaiting tasks
[1;36m(VllmWorkerProcess pid=55078)[0;0m INFO 08-09 12:33:57 [multiproc_worker_utils.py:228] Worker ready; awaiting tasks
[1;36m(VllmWorkerProcess pid=55081)[0;0m WARNING 08-09 12:33:57 [rocm.py:28] Failed to import from amdsmi with ModuleNotFoundError("No module named 'amdsmi'")
[1;36m(VllmWorkerProcess pid=55081)[0;0m WARNING 08-09 12:33:57 [rocm.py:33] Failed to import from vllm._C with ModuleNotFoundError("No module named 'vllm._C'")
[1;36m(VllmWorkerProcess pid=55081)[0;0m WARNING 08-09 12:33:57 [rocm.py:39] Failed to import from vllm._rocm_C with ModuleNotFoundError("No module named 'vllm._rocm_C'")
[1;36m(VllmWorkerProcess pid=55081)[0;0m WARNING 08-09 12:33:57 [utils.py:2850] Methods add_prompt_adapter,cache_config,compilation_config,current_platform,list_prompt_adapters,load_config,pin_prompt_adapter,remove_prompt_adapter,scheduler_config not implemented in <vllm.worker.hpu_worker.HPUWorker object at 0x73c030568220>
[1;36m(VllmWorkerProcess pid=55075)[0;0m INFO 08-09 12:33:57 [multiproc_worker_utils.py:228] Worker ready; awaiting tasks
[1;36m(VllmWorkerProcess pid=55079)[0;0m WARNING 08-09 12:33:57 [rocm.py:28] Failed to import from amdsmi with ModuleNotFoundError("No module named 'amdsmi'")
[1;36m(VllmWorkerProcess pid=55079)[0;0m WARNING 08-09 12:33:57 [rocm.py:33] Failed to import from vllm._C with ModuleNotFoundError("No module named 'vllm._C'")
[1;36m(VllmWorkerProcess pid=55080)[0;0m WARNING 08-09 12:33:57 [rocm.py:28] Failed to import from amdsmi with ModuleNotFoundError("No module named 'amdsmi'")
[1;36m(VllmWorkerProcess pid=55079)[0;0m WARNING 08-09 12:33:57 [rocm.py:39] Failed to import from vllm._rocm_C with ModuleNotFoundError("No module named 'vllm._rocm_C'")
[1;36m(VllmWorkerProcess pid=55080)[0;0m WARNING 08-09 12:33:57 [rocm.py:33] Failed to import from vllm._C with ModuleNotFoundError("No module named 'vllm._C'")
[1;36m(VllmWorkerProcess pid=55080)[0;0m WARNING 08-09 12:33:57 [rocm.py:39] Failed to import from vllm._rocm_C with ModuleNotFoundError("No module named 'vllm._rocm_C'")
[1;36m(VllmWorkerProcess pid=55077)[0;0m WARNING 08-09 12:33:57 [rocm.py:28] Failed to import from amdsmi with ModuleNotFoundError("No module named 'amdsmi'")
[1;36m(VllmWorkerProcess pid=55077)[0;0m WARNING 08-09 12:33:57 [rocm.py:33] Failed to import from vllm._C with ModuleNotFoundError("No module named 'vllm._C'")
[1;36m(VllmWorkerProcess pid=55077)[0;0m WARNING 08-09 12:33:57 [rocm.py:39] Failed to import from vllm._rocm_C with ModuleNotFoundError("No module named 'vllm._rocm_C'")
[1;36m(VllmWorkerProcess pid=55079)[0;0m WARNING 08-09 12:33:57 [utils.py:2850] Methods add_prompt_adapter,cache_config,compilation_config,current_platform,list_prompt_adapters,load_config,pin_prompt_adapter,remove_prompt_adapter,scheduler_config not implemented in <vllm.worker.hpu_worker.HPUWorker object at 0x7ebb95e081f0>
[1;36m(VllmWorkerProcess pid=55080)[0;0m WARNING 08-09 12:33:57 [utils.py:2850] Methods add_prompt_adapter,cache_config,compilation_config,current_platform,list_prompt_adapters,load_config,pin_prompt_adapter,remove_prompt_adapter,scheduler_config not implemented in <vllm.worker.hpu_worker.HPUWorker object at 0x7443579000d0>
[1;36m(VllmWorkerProcess pid=55077)[0;0m WARNING 08-09 12:33:57 [utils.py:2850] Methods add_prompt_adapter,cache_config,compilation_config,current_platform,list_prompt_adapters,load_config,pin_prompt_adapter,remove_prompt_adapter,scheduler_config not implemented in <vllm.worker.hpu_worker.HPUWorker object at 0x7df5b22a8160>
[1;36m(VllmWorkerProcess pid=55076)[0;0m WARNING 08-09 12:33:57 [rocm.py:28] Failed to import from amdsmi with ModuleNotFoundError("No module named 'amdsmi'")
[1;36m(VllmWorkerProcess pid=55076)[0;0m WARNING 08-09 12:33:57 [rocm.py:33] Failed to import from vllm._C with ModuleNotFoundError("No module named 'vllm._C'")
[1;36m(VllmWorkerProcess pid=55076)[0;0m WARNING 08-09 12:33:57 [rocm.py:39] Failed to import from vllm._rocm_C with ModuleNotFoundError("No module named 'vllm._rocm_C'")
[1;36m(VllmWorkerProcess pid=55078)[0;0m WARNING 08-09 12:33:57 [rocm.py:28] Failed to import from amdsmi with ModuleNotFoundError("No module named 'amdsmi'")
[1;36m(VllmWorkerProcess pid=55078)[0;0m WARNING 08-09 12:33:57 [rocm.py:33] Failed to import from vllm._C with ModuleNotFoundError("No module named 'vllm._C'")
[1;36m(VllmWorkerProcess pid=55078)[0;0m WARNING 08-09 12:33:57 [rocm.py:39] Failed to import from vllm._rocm_C with ModuleNotFoundError("No module named 'vllm._rocm_C'")
[1;36m(VllmWorkerProcess pid=55076)[0;0m WARNING 08-09 12:33:57 [utils.py:2850] Methods add_prompt_adapter,cache_config,compilation_config,current_platform,list_prompt_adapters,load_config,pin_prompt_adapter,remove_prompt_adapter,scheduler_config not implemented in <vllm.worker.hpu_worker.HPUWorker object at 0x71eb03044220>
[1;36m(VllmWorkerProcess pid=55078)[0;0m WARNING 08-09 12:33:57 [utils.py:2850] Methods add_prompt_adapter,cache_config,compilation_config,current_platform,list_prompt_adapters,load_config,pin_prompt_adapter,remove_prompt_adapter,scheduler_config not implemented in <vllm.worker.hpu_worker.HPUWorker object at 0x7febfbd6c220>
[1;36m(VllmWorkerProcess pid=55075)[0;0m WARNING 08-09 12:33:57 [rocm.py:28] Failed to import from amdsmi with ModuleNotFoundError("No module named 'amdsmi'")
[1;36m(VllmWorkerProcess pid=55075)[0;0m WARNING 08-09 12:33:57 [rocm.py:33] Failed to import from vllm._C with ModuleNotFoundError("No module named 'vllm._C'")
[1;36m(VllmWorkerProcess pid=55075)[0;0m WARNING 08-09 12:33:57 [rocm.py:39] Failed to import from vllm._rocm_C with ModuleNotFoundError("No module named 'vllm._rocm_C'")
[1;36m(VllmWorkerProcess pid=55075)[0;0m WARNING 08-09 12:33:57 [utils.py:2850] Methods add_prompt_adapter,cache_config,compilation_config,current_platform,list_prompt_adapters,load_config,pin_prompt_adapter,remove_prompt_adapter,scheduler_config not implemented in <vllm.worker.hpu_worker.HPUWorker object at 0x78c6a7b60250>
[1;36m(VllmWorkerProcess pid=55081)[0;0m INFO 08-09 12:33:58 [runtime.py:26] Environment:
[1;36m(VllmWorkerProcess pid=55081)[0;0m INFO 08-09 12:33:58 [runtime.py:30]     hw: gaudi2
[1;36m(VllmWorkerProcess pid=55081)[0;0m INFO 08-09 12:33:58 [runtime.py:30]     build: 1.22.0.425
[1;36m(VllmWorkerProcess pid=55081)[0;0m INFO 08-09 12:33:58 [runtime.py:30]     engine_version: v0
[1;36m(VllmWorkerProcess pid=55081)[0;0m INFO 08-09 12:33:58 [runtime.py:30]     bridge_mode: eager
[1;36m(VllmWorkerProcess pid=55081)[0;0m INFO 08-09 12:33:58 [runtime.py:30]     model_type: llama
[1;36m(VllmWorkerProcess pid=55081)[0;0m INFO 08-09 12:33:58 [runtime.py:26] Features:
[1;36m(VllmWorkerProcess pid=55081)[0;0m INFO 08-09 12:33:58 [runtime.py:30]     fp32_alibi_biases: True
[1;36m(VllmWorkerProcess pid=55081)[0;0m INFO 08-09 12:33:58 [runtime.py:30]     fp32_softmax: False
[1;36m(VllmWorkerProcess pid=55081)[0;0m INFO 08-09 12:33:58 [runtime.py:30]     fused_block_softmax_adjustment: False
[1;36m(VllmWorkerProcess pid=55081)[0;0m INFO 08-09 12:33:58 [runtime.py:30]     fused_block_softmax: False
[1;36m(VllmWorkerProcess pid=55081)[0;0m INFO 08-09 12:33:58 [runtime.py:30]     prompt_attn_impl: fsdpa_impl
[1;36m(VllmWorkerProcess pid=55081)[0;0m INFO 08-09 12:33:58 [runtime.py:30]     skip_warmup: False
[1;36m(VllmWorkerProcess pid=55081)[0;0m INFO 08-09 12:33:58 [runtime.py:30]     merged_prefill: False
[1;36m(VllmWorkerProcess pid=55081)[0;0m INFO 08-09 12:33:58 [runtime.py:30]     use_contiguous_pa: True
[1;36m(VllmWorkerProcess pid=55081)[0;0m INFO 08-09 12:33:58 [runtime.py:30]     use_delayed_sampling: True
[1;36m(VllmWorkerProcess pid=55081)[0;0m INFO 08-09 12:33:58 [runtime.py:30]     use_bucketing: True
[1;36m(VllmWorkerProcess pid=55081)[0;0m INFO 08-09 12:33:58 [runtime.py:30]     bucketing_strategy: exponential_bucketing
[1;36m(VllmWorkerProcess pid=55081)[0;0m INFO 08-09 12:33:58 [runtime.py:26] User flags:
[1;36m(VllmWorkerProcess pid=55081)[0;0m INFO 08-09 12:33:58 [runtime.py:30]     VLLM_USE_V1: False
[1;36m(VllmWorkerProcess pid=55081)[0;0m WARNING 08-09 12:33:58 [hpu.py:135] Pin memory is not supported on HPU.
[1;36m(VllmWorkerProcess pid=55081)[0;0m INFO 08-09 12:33:58 [hpu.py:60] Using HPUAttention backend.
[1;36m(VllmWorkerProcess pid=55080)[0;0m INFO 08-09 12:33:58 [runtime.py:26] Environment:
[1;36m(VllmWorkerProcess pid=55079)[0;0m INFO 08-09 12:33:58 [runtime.py:26] Environment:
[1;36m(VllmWorkerProcess pid=55080)[0;0m INFO 08-09 12:33:58 [runtime.py:30]     hw: gaudi2
[1;36m(VllmWorkerProcess pid=55080)[0;0m INFO 08-09 12:33:58 [runtime.py:30]     build: 1.22.0.425
[1;36m(VllmWorkerProcess pid=55079)[0;0m INFO 08-09 12:33:58 [runtime.py:30]     hw: gaudi2
[1;36m(VllmWorkerProcess pid=55080)[0;0m INFO 08-09 12:33:58 [runtime.py:30]     engine_version: v0
[1;36m(VllmWorkerProcess pid=55079)[0;0m INFO 08-09 12:33:58 [runtime.py:30]     build: 1.22.0.425
[1;36m(VllmWorkerProcess pid=55080)[0;0m INFO 08-09 12:33:58 [runtime.py:30]     bridge_mode: eager
[1;36m(VllmWorkerProcess pid=55080)[0;0m INFO 08-09 12:33:58 [runtime.py:30]     model_type: llama
[1;36m(VllmWorkerProcess pid=55079)[0;0m INFO 08-09 12:33:58 [runtime.py:30]     engine_version: v0
[1;36m(VllmWorkerProcess pid=55080)[0;0m INFO 08-09 12:33:58 [runtime.py:26] Features:
[1;36m(VllmWorkerProcess pid=55079)[0;0m INFO 08-09 12:33:58 [runtime.py:30]     bridge_mode: eager
[1;36m(VllmWorkerProcess pid=55080)[0;0m INFO 08-09 12:33:58 [runtime.py:30]     fp32_alibi_biases: True
[1;36m(VllmWorkerProcess pid=55080)[0;0m INFO 08-09 12:33:58 [runtime.py:30]     fp32_softmax: False
[1;36m(VllmWorkerProcess pid=55079)[0;0m INFO 08-09 12:33:58 [runtime.py:30]     model_type: llama
[1;36m(VllmWorkerProcess pid=55080)[0;0m INFO 08-09 12:33:58 [runtime.py:30]     fused_block_softmax_adjustment: False
[1;36m(VllmWorkerProcess pid=55080)[0;0m INFO 08-09 12:33:58 [runtime.py:30]     fused_block_softmax: False
[1;36m(VllmWorkerProcess pid=55079)[0;0m INFO 08-09 12:33:58 [runtime.py:26] Features:
[1;36m(VllmWorkerProcess pid=55080)[0;0m INFO 08-09 12:33:58 [runtime.py:30]     prompt_attn_impl: fsdpa_impl
[1;36m(VllmWorkerProcess pid=55080)[0;0m INFO 08-09 12:33:58 [runtime.py:30]     skip_warmup: False
[1;36m(VllmWorkerProcess pid=55079)[0;0m INFO 08-09 12:33:58 [runtime.py:30]     fp32_alibi_biases: True
[1;36m(VllmWorkerProcess pid=55080)[0;0m INFO 08-09 12:33:58 [runtime.py:30]     merged_prefill: False
[1;36m(VllmWorkerProcess pid=55080)[0;0m INFO 08-09 12:33:58 [runtime.py:30]     use_contiguous_pa: True
[1;36m(VllmWorkerProcess pid=55079)[0;0m INFO 08-09 12:33:58 [runtime.py:30]     fp32_softmax: False
[1;36m(VllmWorkerProcess pid=55080)[0;0m INFO 08-09 12:33:58 [runtime.py:30]     use_delayed_sampling: True
[1;36m(VllmWorkerProcess pid=55080)[0;0m INFO 08-09 12:33:58 [runtime.py:30]     use_bucketing: True
[1;36m(VllmWorkerProcess pid=55079)[0;0m INFO 08-09 12:33:58 [runtime.py:30]     fused_block_softmax_adjustment: False
[1;36m(VllmWorkerProcess pid=55080)[0;0m INFO 08-09 12:33:58 [runtime.py:30]     bucketing_strategy: exponential_bucketing
[1;36m(VllmWorkerProcess pid=55080)[0;0m INFO 08-09 12:33:58 [runtime.py:26] User flags:
[1;36m(VllmWorkerProcess pid=55079)[0;0m INFO 08-09 12:33:58 [runtime.py:30]     fused_block_softmax: False
[1;36m(VllmWorkerProcess pid=55080)[0;0m INFO 08-09 12:33:58 [runtime.py:30]     VLLM_USE_V1: False
[1;36m(VllmWorkerProcess pid=55079)[0;0m INFO 08-09 12:33:58 [runtime.py:30]     prompt_attn_impl: fsdpa_impl
[1;36m(VllmWorkerProcess pid=55079)[0;0m INFO 08-09 12:33:58 [runtime.py:30]     skip_warmup: False
[1;36m(VllmWorkerProcess pid=55079)[0;0m INFO 08-09 12:33:58 [runtime.py:30]     merged_prefill: False
[1;36m(VllmWorkerProcess pid=55080)[0;0m WARNING 08-09 12:33:58 [hpu.py:135] Pin memory is not supported on HPU.
[1;36m(VllmWorkerProcess pid=55079)[0;0m INFO 08-09 12:33:58 [runtime.py:30]     use_contiguous_pa: True
[1;36m(VllmWorkerProcess pid=55079)[0;0m INFO 08-09 12:33:58 [runtime.py:30]     use_delayed_sampling: True
[1;36m(VllmWorkerProcess pid=55080)[0;0m INFO 08-09 12:33:58 [hpu.py:60] Using HPUAttention backend.
[1;36m(VllmWorkerProcess pid=55079)[0;0m INFO 08-09 12:33:58 [runtime.py:30]     use_bucketing: True
[1;36m(VllmWorkerProcess pid=55079)[0;0m INFO 08-09 12:33:58 [runtime.py:30]     bucketing_strategy: exponential_bucketing
[1;36m(VllmWorkerProcess pid=55079)[0;0m INFO 08-09 12:33:58 [runtime.py:26] User flags:
[1;36m(VllmWorkerProcess pid=55079)[0;0m INFO 08-09 12:33:58 [runtime.py:30]     VLLM_USE_V1: False
[1;36m(VllmWorkerProcess pid=55079)[0;0m WARNING 08-09 12:33:58 [hpu.py:135] Pin memory is not supported on HPU.
[1;36m(VllmWorkerProcess pid=55079)[0;0m INFO 08-09 12:33:58 [hpu.py:60] Using HPUAttention backend.
[1;36m(VllmWorkerProcess pid=55077)[0;0m INFO 08-09 12:33:58 [runtime.py:26] Environment:
[1;36m(VllmWorkerProcess pid=55077)[0;0m INFO 08-09 12:33:58 [runtime.py:30]     hw: gaudi2
[1;36m(VllmWorkerProcess pid=55077)[0;0m INFO 08-09 12:33:58 [runtime.py:30]     build: 1.22.0.425
[1;36m(VllmWorkerProcess pid=55077)[0;0m INFO 08-09 12:33:58 [runtime.py:30]     engine_version: v0
[1;36m(VllmWorkerProcess pid=55077)[0;0m INFO 08-09 12:33:58 [runtime.py:30]     bridge_mode: eager
[1;36m(VllmWorkerProcess pid=55077)[0;0m INFO 08-09 12:33:58 [runtime.py:30]     model_type: llama
[1;36m(VllmWorkerProcess pid=55077)[0;0m INFO 08-09 12:33:58 [runtime.py:26] Features:
[1;36m(VllmWorkerProcess pid=55077)[0;0m INFO 08-09 12:33:58 [runtime.py:30]     fp32_alibi_biases: True
[1;36m(VllmWorkerProcess pid=55077)[0;0m INFO 08-09 12:33:58 [runtime.py:30]     fp32_softmax: False
[1;36m(VllmWorkerProcess pid=55077)[0;0m INFO 08-09 12:33:58 [runtime.py:30]     fused_block_softmax_adjustment: False
[1;36m(VllmWorkerProcess pid=55077)[0;0m INFO 08-09 12:33:58 [runtime.py:30]     fused_block_softmax: False
[1;36m(VllmWorkerProcess pid=55077)[0;0m INFO 08-09 12:33:58 [runtime.py:30]     prompt_attn_impl: fsdpa_impl
[1;36m(VllmWorkerProcess pid=55077)[0;0m INFO 08-09 12:33:58 [runtime.py:30]     skip_warmup: False
[1;36m(VllmWorkerProcess pid=55077)[0;0m INFO 08-09 12:33:58 [runtime.py:30]     merged_prefill: False
[1;36m(VllmWorkerProcess pid=55077)[0;0m INFO 08-09 12:33:58 [runtime.py:30]     use_contiguous_pa: True
[1;36m(VllmWorkerProcess pid=55077)[0;0m INFO 08-09 12:33:58 [runtime.py:30]     use_delayed_sampling: True
[1;36m(VllmWorkerProcess pid=55077)[0;0m INFO 08-09 12:33:58 [runtime.py:30]     use_bucketing: True
[1;36m(VllmWorkerProcess pid=55077)[0;0m INFO 08-09 12:33:58 [runtime.py:30]     bucketing_strategy: exponential_bucketing
[1;36m(VllmWorkerProcess pid=55077)[0;0m INFO 08-09 12:33:58 [runtime.py:26] User flags:
[1;36m(VllmWorkerProcess pid=55077)[0;0m INFO 08-09 12:33:58 [runtime.py:30]     VLLM_USE_V1: False
[1;36m(VllmWorkerProcess pid=55077)[0;0m WARNING 08-09 12:33:58 [hpu.py:135] Pin memory is not supported on HPU.
[1;36m(VllmWorkerProcess pid=55077)[0;0m INFO 08-09 12:33:58 [hpu.py:60] Using HPUAttention backend.
[1;36m(VllmWorkerProcess pid=55076)[0;0m INFO 08-09 12:33:58 [runtime.py:26] Environment:
[1;36m(VllmWorkerProcess pid=55076)[0;0m INFO 08-09 12:33:58 [runtime.py:30]     hw: gaudi2
[1;36m(VllmWorkerProcess pid=55076)[0;0m INFO 08-09 12:33:58 [runtime.py:30]     build: 1.22.0.425
[1;36m(VllmWorkerProcess pid=55076)[0;0m INFO 08-09 12:33:58 [runtime.py:30]     engine_version: v0
[1;36m(VllmWorkerProcess pid=55076)[0;0m INFO 08-09 12:33:58 [runtime.py:30]     bridge_mode: eager
[1;36m(VllmWorkerProcess pid=55076)[0;0m INFO 08-09 12:33:58 [runtime.py:30]     model_type: llama
[1;36m(VllmWorkerProcess pid=55076)[0;0m INFO 08-09 12:33:58 [runtime.py:26] Features:
[1;36m(VllmWorkerProcess pid=55076)[0;0m INFO 08-09 12:33:58 [runtime.py:30]     fp32_alibi_biases: True
[1;36m(VllmWorkerProcess pid=55076)[0;0m INFO 08-09 12:33:58 [runtime.py:30]     fp32_softmax: False
[1;36m(VllmWorkerProcess pid=55076)[0;0m INFO 08-09 12:33:58 [runtime.py:30]     fused_block_softmax_adjustment: False
[1;36m(VllmWorkerProcess pid=55076)[0;0m INFO 08-09 12:33:58 [runtime.py:30]     fused_block_softmax: False
[1;36m(VllmWorkerProcess pid=55076)[0;0m INFO 08-09 12:33:58 [runtime.py:30]     prompt_attn_impl: fsdpa_impl
[1;36m(VllmWorkerProcess pid=55076)[0;0m INFO 08-09 12:33:58 [runtime.py:30]     skip_warmup: False
[1;36m(VllmWorkerProcess pid=55076)[0;0m INFO 08-09 12:33:58 [runtime.py:30]     merged_prefill: False
[1;36m(VllmWorkerProcess pid=55076)[0;0m INFO 08-09 12:33:58 [runtime.py:30]     use_contiguous_pa: True
[1;36m(VllmWorkerProcess pid=55076)[0;0m INFO 08-09 12:33:58 [runtime.py:30]     use_delayed_sampling: True
[1;36m(VllmWorkerProcess pid=55076)[0;0m INFO 08-09 12:33:58 [runtime.py:30]     use_bucketing: True
[1;36m(VllmWorkerProcess pid=55076)[0;0m INFO 08-09 12:33:58 [runtime.py:30]     bucketing_strategy: exponential_bucketing
[1;36m(VllmWorkerProcess pid=55076)[0;0m INFO 08-09 12:33:58 [runtime.py:26] User flags:
[1;36m(VllmWorkerProcess pid=55076)[0;0m INFO 08-09 12:33:58 [runtime.py:30]     VLLM_USE_V1: False
[1;36m(VllmWorkerProcess pid=55076)[0;0m WARNING 08-09 12:33:58 [hpu.py:135] Pin memory is not supported on HPU.
[1;36m(VllmWorkerProcess pid=55076)[0;0m INFO 08-09 12:33:58 [hpu.py:60] Using HPUAttention backend.
[1;36m(VllmWorkerProcess pid=55078)[0;0m INFO 08-09 12:33:58 [runtime.py:26] Environment:
[1;36m(VllmWorkerProcess pid=55078)[0;0m INFO 08-09 12:33:58 [runtime.py:30]     hw: gaudi2
[1;36m(VllmWorkerProcess pid=55078)[0;0m INFO 08-09 12:33:58 [runtime.py:30]     build: 1.22.0.425
[1;36m(VllmWorkerProcess pid=55078)[0;0m INFO 08-09 12:33:58 [runtime.py:30]     engine_version: v0
[1;36m(VllmWorkerProcess pid=55078)[0;0m INFO 08-09 12:33:58 [runtime.py:30]     bridge_mode: eager
[1;36m(VllmWorkerProcess pid=55078)[0;0m INFO 08-09 12:33:58 [runtime.py:30]     model_type: llama
[1;36m(VllmWorkerProcess pid=55078)[0;0m INFO 08-09 12:33:58 [runtime.py:26] Features:
[1;36m(VllmWorkerProcess pid=55078)[0;0m INFO 08-09 12:33:58 [runtime.py:30]     fp32_alibi_biases: True
[1;36m(VllmWorkerProcess pid=55078)[0;0m INFO 08-09 12:33:58 [runtime.py:30]     fp32_softmax: False
[1;36m(VllmWorkerProcess pid=55078)[0;0m INFO 08-09 12:33:58 [runtime.py:30]     fused_block_softmax_adjustment: False
[1;36m(VllmWorkerProcess pid=55078)[0;0m INFO 08-09 12:33:58 [runtime.py:30]     fused_block_softmax: False
[1;36m(VllmWorkerProcess pid=55078)[0;0m INFO 08-09 12:33:58 [runtime.py:30]     prompt_attn_impl: fsdpa_impl
[1;36m(VllmWorkerProcess pid=55078)[0;0m INFO 08-09 12:33:58 [runtime.py:30]     skip_warmup: False
[1;36m(VllmWorkerProcess pid=55078)[0;0m INFO 08-09 12:33:58 [runtime.py:30]     merged_prefill: False
[1;36m(VllmWorkerProcess pid=55078)[0;0m INFO 08-09 12:33:58 [runtime.py:30]     use_contiguous_pa: True
[1;36m(VllmWorkerProcess pid=55078)[0;0m INFO 08-09 12:33:58 [runtime.py:30]     use_delayed_sampling: True
[1;36m(VllmWorkerProcess pid=55078)[0;0m INFO 08-09 12:33:58 [runtime.py:30]     use_bucketing: True
[1;36m(VllmWorkerProcess pid=55078)[0;0m INFO 08-09 12:33:58 [runtime.py:30]     bucketing_strategy: exponential_bucketing
[1;36m(VllmWorkerProcess pid=55078)[0;0m INFO 08-09 12:33:58 [runtime.py:26] User flags:
[1;36m(VllmWorkerProcess pid=55078)[0;0m INFO 08-09 12:33:58 [runtime.py:30]     VLLM_USE_V1: False
[1;36m(VllmWorkerProcess pid=55078)[0;0m WARNING 08-09 12:33:58 [hpu.py:135] Pin memory is not supported on HPU.
[1;36m(VllmWorkerProcess pid=55078)[0;0m INFO 08-09 12:33:58 [hpu.py:60] Using HPUAttention backend.
[1;36m(VllmWorkerProcess pid=55075)[0;0m INFO 08-09 12:33:58 [runtime.py:26] Environment:
[1;36m(VllmWorkerProcess pid=55075)[0;0m INFO 08-09 12:33:58 [runtime.py:30]     hw: gaudi2
[1;36m(VllmWorkerProcess pid=55075)[0;0m INFO 08-09 12:33:58 [runtime.py:30]     build: 1.22.0.425
[1;36m(VllmWorkerProcess pid=55075)[0;0m INFO 08-09 12:33:58 [runtime.py:30]     engine_version: v0
[1;36m(VllmWorkerProcess pid=55075)[0;0m INFO 08-09 12:33:58 [runtime.py:30]     bridge_mode: eager
[1;36m(VllmWorkerProcess pid=55075)[0;0m INFO 08-09 12:33:58 [runtime.py:30]     model_type: llama
[1;36m(VllmWorkerProcess pid=55075)[0;0m INFO 08-09 12:33:58 [runtime.py:26] Features:
[1;36m(VllmWorkerProcess pid=55075)[0;0m INFO 08-09 12:33:58 [runtime.py:30]     fp32_alibi_biases: True
[1;36m(VllmWorkerProcess pid=55075)[0;0m INFO 08-09 12:33:58 [runtime.py:30]     fp32_softmax: False
[1;36m(VllmWorkerProcess pid=55075)[0;0m INFO 08-09 12:33:58 [runtime.py:30]     fused_block_softmax_adjustment: False
[1;36m(VllmWorkerProcess pid=55075)[0;0m INFO 08-09 12:33:58 [runtime.py:30]     fused_block_softmax: False
[1;36m(VllmWorkerProcess pid=55075)[0;0m INFO 08-09 12:33:58 [runtime.py:30]     prompt_attn_impl: fsdpa_impl
[1;36m(VllmWorkerProcess pid=55075)[0;0m INFO 08-09 12:33:58 [runtime.py:30]     skip_warmup: False
[1;36m(VllmWorkerProcess pid=55075)[0;0m INFO 08-09 12:33:58 [runtime.py:30]     merged_prefill: False
[1;36m(VllmWorkerProcess pid=55075)[0;0m INFO 08-09 12:33:58 [runtime.py:30]     use_contiguous_pa: True
[1;36m(VllmWorkerProcess pid=55075)[0;0m INFO 08-09 12:33:58 [runtime.py:30]     use_delayed_sampling: True
[1;36m(VllmWorkerProcess pid=55075)[0;0m INFO 08-09 12:33:58 [runtime.py:30]     use_bucketing: True
[1;36m(VllmWorkerProcess pid=55075)[0;0m INFO 08-09 12:33:58 [runtime.py:30]     bucketing_strategy: exponential_bucketing
[1;36m(VllmWorkerProcess pid=55075)[0;0m INFO 08-09 12:33:58 [runtime.py:26] User flags:
[1;36m(VllmWorkerProcess pid=55075)[0;0m INFO 08-09 12:33:58 [runtime.py:30]     VLLM_USE_V1: False
[1;36m(VllmWorkerProcess pid=55075)[0;0m WARNING 08-09 12:33:58 [hpu.py:135] Pin memory is not supported on HPU.
[1;36m(VllmWorkerProcess pid=55075)[0;0m INFO 08-09 12:33:58 [hpu.py:60] Using HPUAttention backend.
============================= HPU SW VERSION ====================================== 
 HB_BUILD_VER = 1.22.0-425
============================= HPU PT BRIDGE CONFIGURATION ON RANK = 0 ============= 
 PT_HPU_LAZY_MODE = 0
 PT_HPU_RECIPE_CACHE_CONFIG = ,false,1024,false
 PT_HPU_MAX_COMPOUND_OP_SIZE = 9223372036854775807
 PT_HPU_LAZY_ACC_PAR_MODE = 1
 PT_HPU_ENABLE_REFINE_DYNAMIC_SHAPES = 0
 PT_HPU_EAGER_PIPELINE_ENABLE = 1
 PT_HPU_EAGER_COLLECTIVE_PIPELINE_ENABLE = 1
 PT_HPU_ENABLE_LAZY_COLLECTIVES = 1
---------------------------: System Configuration :---------------------------
Num CPU Cores : 160
CPU RAM       : 1007 GB
------------------------------------------------------------------------------
============================= HPU SW VERSION ====================================== 
 HB_BUILD_VER = 1.22.0-425
============================= HPU PT BRIDGE CONFIGURATION ON RANK = 0 ============= 
 PT_HPU_LAZY_MODE = 0
 PT_HPU_RECIPE_CACHE_CONFIG = ,false,1024,false
 PT_HPU_MAX_COMPOUND_OP_SIZE = 9223372036854775807
 PT_HPU_LAZY_ACC_PAR_MODE = 1
 PT_HPU_ENABLE_REFINE_DYNAMIC_SHAPES = 0
 PT_HPU_EAGER_PIPELINE_ENABLE = 1
 PT_HPU_EAGER_COLLECTIVE_PIPELINE_ENABLE = 1
 PT_HPU_ENABLE_LAZY_COLLECTIVES = 1
---------------------------: System Configuration :---------------------------
Num CPU Cores : 160
CPU RAM       : 1007 GB
------------------------------------------------------------------------------
============================= HPU SW VERSION ====================================== 
 HB_BUILD_VER = 1.22.0-425
============================= HPU PT BRIDGE CONFIGURATION ON RANK = 0 ============= 
 PT_HPU_LAZY_MODE = 0
 PT_HPU_RECIPE_CACHE_CONFIG = ,false,1024,false
 PT_HPU_MAX_COMPOUND_OP_SIZE = 9223372036854775807
 PT_HPU_LAZY_ACC_PAR_MODE = 1
 PT_HPU_ENABLE_REFINE_DYNAMIC_SHAPES = 0
 PT_HPU_EAGER_PIPELINE_ENABLE = 1
 PT_HPU_EAGER_COLLECTIVE_PIPELINE_ENABLE = 1
 PT_HPU_ENABLE_LAZY_COLLECTIVES = 1
---------------------------: System Configuration :---------------------------
Num CPU Cores : 160
CPU RAM       : 1007 GB
------------------------------------------------------------------------------
============================= HPU SW VERSION ====================================== 
 HB_BUILD_VER = 1.22.0-425
============================= HPU PT BRIDGE CONFIGURATION ON RANK = 0 ============= 
 PT_HPU_LAZY_MODE = 0
 PT_HPU_RECIPE_CACHE_CONFIG = ,false,1024,false
 PT_HPU_MAX_COMPOUND_OP_SIZE = 9223372036854775807
 PT_HPU_LAZY_ACC_PAR_MODE = 1
 PT_HPU_ENABLE_REFINE_DYNAMIC_SHAPES = 0
 PT_HPU_EAGER_PIPELINE_ENABLE = 1
 PT_HPU_EAGER_COLLECTIVE_PIPELINE_ENABLE = 1
 PT_HPU_ENABLE_LAZY_COLLECTIVES = 1
---------------------------: System Configuration :---------------------------
Num CPU Cores : 160
CPU RAM       : 1007 GB
------------------------------------------------------------------------------
============================= HPU SW VERSION ====================================== 
 HB_BUILD_VER = 1.22.0-425
============================= HPU PT BRIDGE CONFIGURATION ON RANK = 0 ============= 
 PT_HPU_LAZY_MODE = 0
 PT_HPU_RECIPE_CACHE_CONFIG = ,false,1024,false
 PT_HPU_MAX_COMPOUND_OP_SIZE = 9223372036854775807
 PT_HPU_LAZY_ACC_PAR_MODE = 1
 PT_HPU_ENABLE_REFINE_DYNAMIC_SHAPES = 0
 PT_HPU_EAGER_PIPELINE_ENABLE = 1
 PT_HPU_EAGER_COLLECTIVE_PIPELINE_ENABLE = 1
 PT_HPU_ENABLE_LAZY_COLLECTIVES = 1
---------------------------: System Configuration :---------------------------
Num CPU Cores : 160
CPU RAM       : 1007 GB
------------------------------------------------------------------------------
============================= HPU SW VERSION ====================================== 
 HB_BUILD_VER = 1.22.0-425
============================= HPU PT BRIDGE CONFIGURATION ON RANK = 0 ============= 
 PT_HPU_LAZY_MODE = 0
 PT_HPU_RECIPE_CACHE_CONFIG = ,false,1024,false
 PT_HPU_MAX_COMPOUND_OP_SIZE = 9223372036854775807
 PT_HPU_LAZY_ACC_PAR_MODE = 1
 PT_HPU_ENABLE_REFINE_DYNAMIC_SHAPES = 0
 PT_HPU_EAGER_PIPELINE_ENABLE = 1
 PT_HPU_EAGER_COLLECTIVE_PIPELINE_ENABLE = 1
 PT_HPU_ENABLE_LAZY_COLLECTIVES = 1
---------------------------: System Configuration :---------------------------
Num CPU Cores : 160
CPU RAM       : 1007 GB
------------------------------------------------------------------------------
============================= HPU SW VERSION ====================================== 
 HB_BUILD_VER = 1.22.0-425
============================= HPU PT BRIDGE CONFIGURATION ON RANK = 0 ============= 
 PT_HPU_LAZY_MODE = 0
 PT_HPU_RECIPE_CACHE_CONFIG = ,false,1024,false
 PT_HPU_MAX_COMPOUND_OP_SIZE = 9223372036854775807
 PT_HPU_LAZY_ACC_PAR_MODE = 1
 PT_HPU_ENABLE_REFINE_DYNAMIC_SHAPES = 0
 PT_HPU_EAGER_PIPELINE_ENABLE = 1
 PT_HPU_EAGER_COLLECTIVE_PIPELINE_ENABLE = 1
 PT_HPU_ENABLE_LAZY_COLLECTIVES = 1
---------------------------: System Configuration :---------------------------
Num CPU Cores : 160
CPU RAM       : 1007 GB
------------------------------------------------------------------------------
============================= HPU SW VERSION ====================================== 
 HB_BUILD_VER = 1.22.0-425
============================= HPU PT BRIDGE CONFIGURATION ON RANK = 0 ============= 
 PT_HPU_LAZY_MODE = 0
 PT_HPU_RECIPE_CACHE_CONFIG = ,false,1024,false
 PT_HPU_MAX_COMPOUND_OP_SIZE = 9223372036854775807
 PT_HPU_LAZY_ACC_PAR_MODE = 1
 PT_HPU_ENABLE_REFINE_DYNAMIC_SHAPES = 0
 PT_HPU_EAGER_PIPELINE_ENABLE = 1
 PT_HPU_EAGER_COLLECTIVE_PIPELINE_ENABLE = 1
 PT_HPU_ENABLE_LAZY_COLLECTIVES = 1
---------------------------: System Configuration :---------------------------
Num CPU Cores : 160
CPU RAM       : 1007 GB
------------------------------------------------------------------------------
INFO 08-09 12:34:03 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[1, 2, 3, 4, 5, 6, 7], buffer_handle=(7, 4194304, 6, 'psm_69612eaf'), local_subscribe_addr='ipc:///tmp/3753ff2f-1239-44fb-ac2d-a2a573a97237', remote_subscribe_addr=None, remote_addr_ipv6=False)
INFO 08-09 12:34:03 [parallel_state.py:1065] rank 0 in world size 8 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(VllmWorkerProcess pid=55080)[0;0m INFO 08-09 12:34:03 [parallel_state.py:1065] rank 6 in world size 8 is assigned as DP rank 0, PP rank 0, TP rank 6, EP rank 6
[1;36m(VllmWorkerProcess pid=55079)[0;0m INFO 08-09 12:34:03 [parallel_state.py:1065] rank 5 in world size 8 is assigned as DP rank 0, PP rank 0, TP rank 5, EP rank 5
[1;36m(VllmWorkerProcess pid=55075)[0;0m INFO 08-09 12:34:03 [parallel_state.py:1065] rank 1 in world size 8 is assigned as DP rank 0, PP rank 0, TP rank 1, EP rank 1
[1;36m(VllmWorkerProcess pid=55077)[0;0m INFO 08-09 12:34:03 [parallel_state.py:1065] rank 3 in world size 8 is assigned as DP rank 0, PP rank 0, TP rank 3, EP rank 3
[1;36m(VllmWorkerProcess pid=55076)[0;0m INFO 08-09 12:34:03 [parallel_state.py:1065] rank 2 in world size 8 is assigned as DP rank 0, PP rank 0, TP rank 2, EP rank 2
[1;36m(VllmWorkerProcess pid=55081)[0;0m INFO 08-09 12:34:03 [parallel_state.py:1065] rank 7 in world size 8 is assigned as DP rank 0, PP rank 0, TP rank 7, EP rank 7
[1;36m(VllmWorkerProcess pid=55078)[0;0m INFO 08-09 12:34:03 [parallel_state.py:1065] rank 4 in world size 8 is assigned as DP rank 0, PP rank 0, TP rank 4, EP rank 4
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  5.79it/s]
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:00<00:00,  2.63it/s]
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:00<00:00,  2.87it/s]

INFO 08-09 12:34:05 [default_loader.py:272] Loading weights took 0.78 seconds
[1;36m(VllmWorkerProcess pid=55077)[0;0m INFO 08-09 12:34:05 [default_loader.py:272] Loading weights took 0.75 seconds
[1;36m(VllmWorkerProcess pid=55076)[0;0m INFO 08-09 12:34:05 [default_loader.py:272] Loading weights took 0.76 seconds
[1;36m(VllmWorkerProcess pid=55079)[0;0m INFO 08-09 12:34:05 [default_loader.py:272] Loading weights took 0.76 seconds
[1;36m(VllmWorkerProcess pid=55080)[0;0m INFO 08-09 12:34:05 [default_loader.py:272] Loading weights took 0.77 seconds
[1;36m(VllmWorkerProcess pid=55075)[0;0m INFO 08-09 12:34:05 [default_loader.py:272] Loading weights took 0.76 seconds
INFO 08-09 12:34:05 [hpu_model_runner.py:1184] Pre-loading model weights on hpu:0 took 1.57 GiB of device memory (1.576 GiB/94.62 GiB used) and 272.4 MiB of host memory (241.7 GiB/1007 GiB used)
[1;36m(VllmWorkerProcess pid=55078)[0;0m INFO 08-09 12:34:05 [default_loader.py:272] Loading weights took 0.74 seconds
[1;36m(VllmWorkerProcess pid=55081)[0;0m INFO 08-09 12:34:05 [default_loader.py:272] Loading weights took 0.80 seconds
[1;36m(VllmWorkerProcess pid=55079)[0;0m INFO 08-09 12:34:05 [hpu_model_runner.py:1184] Pre-loading model weights on hpu:0 took 1.57 GiB of device memory (1.576 GiB/94.62 GiB used) and 263.3 MiB of host memory (241.7 GiB/1007 GiB used)
[1;36m(VllmWorkerProcess pid=55076)[0;0m INFO 08-09 12:34:05 [hpu_model_runner.py:1184] Pre-loading model weights on hpu:0 took 1.57 GiB of device memory (1.576 GiB/94.62 GiB used) and 223.5 MiB of host memory (241.7 GiB/1007 GiB used)
[1;36m(VllmWorkerProcess pid=55080)[0;0m INFO 08-09 12:34:05 [hpu_model_runner.py:1184] Pre-loading model weights on hpu:0 took 1.57 GiB of device memory (1.576 GiB/94.62 GiB used) and 251.1 MiB of host memory (241.7 GiB/1007 GiB used)
[1;36m(VllmWorkerProcess pid=55077)[0;0m INFO 08-09 12:34:05 [hpu_model_runner.py:1184] Pre-loading model weights on hpu:0 took 1.57 GiB of device memory (1.576 GiB/94.62 GiB used) and 240.7 MiB of host memory (241.7 GiB/1007 GiB used)
[1;36m(VllmWorkerProcess pid=55075)[0;0m INFO 08-09 12:34:05 [hpu_model_runner.py:1184] Pre-loading model weights on hpu:0 took 1.57 GiB of device memory (1.576 GiB/94.62 GiB used) and 224.8 MiB of host memory (241.7 GiB/1007 GiB used)
[1;36m(VllmWorkerProcess pid=55078)[0;0m INFO 08-09 12:34:05 [hpu_model_runner.py:1184] Pre-loading model weights on hpu:0 took 1.57 GiB of device memory (1.576 GiB/94.62 GiB used) and 209.7 MiB of host memory (241.7 GiB/1007 GiB used)
[1;36m(VllmWorkerProcess pid=55081)[0;0m INFO 08-09 12:34:05 [hpu_model_runner.py:1184] Pre-loading model weights on hpu:0 took 1.57 GiB of device memory (1.576 GiB/94.62 GiB used) and 209 MiB of host memory (241.7 GiB/1007 GiB used)
INFO 08-09 12:34:05 [hpu_model_runner.py:1266] Wrapping in HPU Graph took 0 B of device memory (1.576 GiB/94.62 GiB used) and -640 KiB of host memory (241.7 GiB/1007 GiB used)
[1;36m(VllmWorkerProcess pid=55079)[0;0m INFO 08-09 12:34:05 [hpu_model_runner.py:1266] Wrapping in HPU Graph took 0 B of device memory (1.576 GiB/94.62 GiB used) and -3.895 MiB of host memory (241.7 GiB/1007 GiB used)
[1;36m(VllmWorkerProcess pid=55076)[0;0m INFO 08-09 12:34:05 [hpu_model_runner.py:1266] Wrapping in HPU Graph took 0 B of device memory (1.576 GiB/94.62 GiB used) and -3.895 MiB of host memory (241.7 GiB/1007 GiB used)
[1;36m(VllmWorkerProcess pid=55080)[0;0m INFO 08-09 12:34:06 [hpu_model_runner.py:1266] Wrapping in HPU Graph took 0 B of device memory (1.576 GiB/94.62 GiB used) and -3.895 MiB of host memory (241.7 GiB/1007 GiB used)
[1;36m(VllmWorkerProcess pid=55077)[0;0m INFO 08-09 12:34:06 [hpu_model_runner.py:1266] Wrapping in HPU Graph took 0 B of device memory (1.576 GiB/94.62 GiB used) and -3.895 MiB of host memory (241.7 GiB/1007 GiB used)
[1;36m(VllmWorkerProcess pid=55075)[0;0m INFO 08-09 12:34:06 [hpu_model_runner.py:1266] Wrapping in HPU Graph took 0 B of device memory (1.576 GiB/94.62 GiB used) and -4.633 MiB of host memory (241.7 GiB/1007 GiB used)
[1;36m(VllmWorkerProcess pid=55081)[0;0m INFO 08-09 12:34:06 [hpu_model_runner.py:1266] Wrapping in HPU Graph took 0 B of device memory (1.576 GiB/94.62 GiB used) and -2.766 MiB of host memory (241.7 GiB/1007 GiB used)
[1;36m(VllmWorkerProcess pid=55078)[0;0m INFO 08-09 12:34:06 [hpu_model_runner.py:1266] Wrapping in HPU Graph took 0 B of device memory (1.576 GiB/94.62 GiB used) and -816 KiB of host memory (241.7 GiB/1007 GiB used)
INFO 08-09 12:34:06 [hpu_model_runner.py:1270] Compiling took 0 B of device memory (1.576 GiB/94.62 GiB used) and 2.457 MiB of host memory (241.7 GiB/1007 GiB used)
[1;36m(VllmWorkerProcess pid=55076)[0;0m INFO 08-09 12:34:06 [hpu_model_runner.py:1270] Compiling took 0 B of device memory (1.576 GiB/94.62 GiB used) and -4.371 MiB of host memory (241.7 GiB/1007 GiB used)
[1;36m(VllmWorkerProcess pid=55079)[0;0m INFO 08-09 12:34:06 [hpu_model_runner.py:1270] Compiling took 0 B of device memory (1.576 GiB/94.62 GiB used) and -4.422 MiB of host memory (241.7 GiB/1007 GiB used)
[1;36m(VllmWorkerProcess pid=55081)[0;0m INFO 08-09 12:34:06 [hpu_model_runner.py:1270] Compiling took 0 B of device memory (1.576 GiB/94.62 GiB used) and -4.422 MiB of host memory (241.7 GiB/1007 GiB used)
[1;36m(VllmWorkerProcess pid=55080)[0;0m INFO 08-09 12:34:06 [hpu_model_runner.py:1270] Compiling took 0 B of device memory (1.576 GiB/94.62 GiB used) and -4.422 MiB of host memory (241.7 GiB/1007 GiB used)
[1;36m(VllmWorkerProcess pid=55077)[0;0m INFO 08-09 12:34:06 [hpu_model_runner.py:1270] Compiling took 0 B of device memory (1.576 GiB/94.62 GiB used) and -4.422 MiB of host memory (241.7 GiB/1007 GiB used)
[1;36m(VllmWorkerProcess pid=55075)[0;0m INFO 08-09 12:34:06 [hpu_model_runner.py:1270] Compiling took 0 B of device memory (1.576 GiB/94.62 GiB used) and -4.422 MiB of host memory (241.7 GiB/1007 GiB used)
INFO 08-09 12:34:06 [hpu_model_runner.py:1274] Loading model weights took in total 1.57 GiB of device memory (1.576 GiB/94.62 GiB used) and 271.9 MiB of host memory (241.7 GiB/1007 GiB used)
[1;36m(VllmWorkerProcess pid=55078)[0;0m INFO 08-09 12:34:06 [hpu_model_runner.py:1270] Compiling took 0 B of device memory (1.576 GiB/94.62 GiB used) and 6.598 MiB of host memory (241.7 GiB/1007 GiB used)
[1;36m(VllmWorkerProcess pid=55076)[0;0m INFO 08-09 12:34:06 [hpu_model_runner.py:1274] Loading model weights took in total 1.57 GiB of device memory (1.576 GiB/94.62 GiB used) and 280.2 MiB of host memory (241.7 GiB/1007 GiB used)
[1;36m(VllmWorkerProcess pid=55081)[0;0m INFO 08-09 12:34:06 [hpu_model_runner.py:1274] Loading model weights took in total 1.57 GiB of device memory (1.576 GiB/94.62 GiB used) and 264.5 MiB of host memory (241.7 GiB/1007 GiB used)
[1;36m(VllmWorkerProcess pid=55079)[0;0m INFO 08-09 12:34:06 [hpu_model_runner.py:1274] Loading model weights took in total 1.57 GiB of device memory (1.576 GiB/94.62 GiB used) and 287.5 MiB of host memory (241.7 GiB/1007 GiB used)
[1;36m(VllmWorkerProcess pid=55080)[0;0m INFO 08-09 12:34:06 [hpu_model_runner.py:1274] Loading model weights took in total 1.57 GiB of device memory (1.576 GiB/94.62 GiB used) and 285.2 MiB of host memory (241.7 GiB/1007 GiB used)
[1;36m(VllmWorkerProcess pid=55077)[0;0m INFO 08-09 12:34:06 [hpu_model_runner.py:1274] Loading model weights took in total 1.57 GiB of device memory (1.576 GiB/94.62 GiB used) and 280.9 MiB of host memory (241.7 GiB/1007 GiB used)
[1;36m(VllmWorkerProcess pid=55075)[0;0m INFO 08-09 12:34:06 [hpu_model_runner.py:1274] Loading model weights took in total 1.57 GiB of device memory (1.576 GiB/94.62 GiB used) and 280.2 MiB of host memory (241.7 GiB/1007 GiB used)
[1;36m(VllmWorkerProcess pid=55078)[0;0m INFO 08-09 12:34:06 [hpu_model_runner.py:1274] Loading model weights took in total 1.57 GiB of device memory (1.576 GiB/94.62 GiB used) and 279.1 MiB of host memory (241.7 GiB/1007 GiB used)
WARNING 08-09 12:34:07 [common.py:139] Prompt bucket for (1, 4096, 0) was not prepared. Adding new bucket: (1, 4096, 0)
[1;36m(VllmWorkerProcess pid=55081)[0;0m WARNING 08-09 12:34:07 [common.py:139] Prompt bucket for (1, 4096, 0) was not prepared. Adding new bucket: (1, 4096, 0)
[1;36m(VllmWorkerProcess pid=55078)[0;0m WARNING 08-09 12:34:07 [common.py:139] Prompt bucket for (1, 4096, 0) was not prepared. Adding new bucket: (1, 4096, 0)
[1;36m(VllmWorkerProcess pid=55079)[0;0m WARNING 08-09 12:34:07 [common.py:139] Prompt bucket for (1, 4096, 0) was not prepared. Adding new bucket: (1, 4096, 0)
[1;36m(VllmWorkerProcess pid=55080)[0;0m WARNING 08-09 12:34:07 [common.py:139] Prompt bucket for (1, 4096, 0) was not prepared. Adding new bucket: (1, 4096, 0)
[1;36m(VllmWorkerProcess pid=55077)[0;0m WARNING 08-09 12:34:07 [common.py:139] Prompt bucket for (1, 4096, 0) was not prepared. Adding new bucket: (1, 4096, 0)
[1;36m(VllmWorkerProcess pid=55076)[0;0m WARNING 08-09 12:34:07 [common.py:139] Prompt bucket for (1, 4096, 0) was not prepared. Adding new bucket: (1, 4096, 0)
[1;36m(VllmWorkerProcess pid=55075)[0;0m WARNING 08-09 12:34:07 [common.py:139] Prompt bucket for (1, 4096, 0) was not prepared. Adding new bucket: (1, 4096, 0)
[1;36m(VllmWorkerProcess pid=55081)[0;0m INFO 08-09 12:34:09 [hpu_worker.py:289] Model profiling run took 72 MiB of device memory (1.646 GiB/94.62 GiB used) and 765.3 MiB of host memory (242.5 GiB/1007 GiB used)
[1;36m(VllmWorkerProcess pid=55081)[0;0m INFO 08-09 12:34:09 [hpu_worker.py:313] Free device memory: 92.98 GiB, 83.68 GiB usable (gpu_memory_utilization=0.9), 8.368 GiB reserved for HPUGraphs (VLLM_GRAPH_RESERVED_MEM=0.1), 75.31 GiB reserved for KV cache
[1;36m(VllmWorkerProcess pid=55078)[0;0m INFO 08-09 12:34:09 [hpu_worker.py:289] Model profiling run took 72 MiB of device memory (1.646 GiB/94.62 GiB used) and 765.3 MiB of host memory (242.5 GiB/1007 GiB used)
[1;36m(VllmWorkerProcess pid=55078)[0;0m INFO 08-09 12:34:09 [hpu_worker.py:313] Free device memory: 92.98 GiB, 83.68 GiB usable (gpu_memory_utilization=0.9), 8.368 GiB reserved for HPUGraphs (VLLM_GRAPH_RESERVED_MEM=0.1), 75.31 GiB reserved for KV cache
[1;36m(VllmWorkerProcess pid=55075)[0;0m INFO 08-09 12:34:09 [hpu_worker.py:289] Model profiling run took 72 MiB of device memory (1.646 GiB/94.62 GiB used) and 758.2 MiB of host memory (242.5 GiB/1007 GiB used)
[1;36m(VllmWorkerProcess pid=55075)[0;0m INFO 08-09 12:34:09 [hpu_worker.py:313] Free device memory: 92.98 GiB, 83.68 GiB usable (gpu_memory_utilization=0.9), 8.368 GiB reserved for HPUGraphs (VLLM_GRAPH_RESERVED_MEM=0.1), 75.31 GiB reserved for KV cache
[1;36m(VllmWorkerProcess pid=55077)[0;0m INFO 08-09 12:34:09 [hpu_worker.py:289] Model profiling run took 72 MiB of device memory (1.646 GiB/94.62 GiB used) and 765.6 MiB of host memory (242.5 GiB/1007 GiB used)
[1;36m(VllmWorkerProcess pid=55080)[0;0m INFO 08-09 12:34:09 [hpu_worker.py:289] Model profiling run took 72 MiB of device memory (1.646 GiB/94.62 GiB used) and 765.6 MiB of host memory (242.5 GiB/1007 GiB used)
[1;36m(VllmWorkerProcess pid=55077)[0;0m INFO 08-09 12:34:09 [hpu_worker.py:313] Free device memory: 92.98 GiB, 83.68 GiB usable (gpu_memory_utilization=0.9), 8.368 GiB reserved for HPUGraphs (VLLM_GRAPH_RESERVED_MEM=0.1), 75.31 GiB reserved for KV cache
[1;36m(VllmWorkerProcess pid=55080)[0;0m INFO 08-09 12:34:09 [hpu_worker.py:313] Free device memory: 92.98 GiB, 83.68 GiB usable (gpu_memory_utilization=0.9), 8.368 GiB reserved for HPUGraphs (VLLM_GRAPH_RESERVED_MEM=0.1), 75.31 GiB reserved for KV cache
[1;36m(VllmWorkerProcess pid=55079)[0;0m INFO 08-09 12:34:09 [hpu_worker.py:289] Model profiling run took 72 MiB of device memory (1.646 GiB/94.62 GiB used) and 765.6 MiB of host memory (242.5 GiB/1007 GiB used)
[1;36m(VllmWorkerProcess pid=55079)[0;0m INFO 08-09 12:34:09 [hpu_worker.py:313] Free device memory: 92.98 GiB, 83.68 GiB usable (gpu_memory_utilization=0.9), 8.368 GiB reserved for HPUGraphs (VLLM_GRAPH_RESERVED_MEM=0.1), 75.31 GiB reserved for KV cache
INFO 08-09 12:34:09 [hpu_worker.py:289] Model profiling run took 72 MiB of device memory (1.646 GiB/94.62 GiB used) and 797.1 MiB of host memory (242.5 GiB/1007 GiB used)
INFO 08-09 12:34:09 [hpu_worker.py:313] Free device memory: 92.98 GiB, 83.68 GiB usable (gpu_memory_utilization=0.9), 8.368 GiB reserved for HPUGraphs (VLLM_GRAPH_RESERVED_MEM=0.1), 75.31 GiB reserved for KV cache
[1;36m(VllmWorkerProcess pid=55076)[0;0m INFO 08-09 12:34:09 [hpu_worker.py:289] Model profiling run took 72 MiB of device memory (1.646 GiB/94.62 GiB used) and 765.6 MiB of host memory (242.5 GiB/1007 GiB used)
[1;36m(VllmWorkerProcess pid=55076)[0;0m INFO 08-09 12:34:09 [hpu_worker.py:313] Free device memory: 92.98 GiB, 83.68 GiB usable (gpu_memory_utilization=0.9), 8.368 GiB reserved for HPUGraphs (VLLM_GRAPH_RESERVED_MEM=0.1), 75.31 GiB reserved for KV cache
INFO 08-09 12:34:09 [executor_base.py:113] # hpu blocks: 9639, # CPU blocks: 512
INFO 08-09 12:34:09 [executor_base.py:118] Maximum concurrency for 4096 tokens per request: 301.22x
[1;36m(VllmWorkerProcess pid=55075)[0;0m INFO 08-09 12:34:09 [exponential.py:49] Prompt bucket config (min, step, max_warmup, limit) bs:[1, 2, 256, 9], seq:[128, 128, 4096, 13]
[1;36m(VllmWorkerProcess pid=55077)[0;0m INFO 08-09 12:34:09 [exponential.py:49] Prompt bucket config (min, step, max_warmup, limit) bs:[1, 2, 256, 9], seq:[128, 128, 4096, 13]
[1;36m(VllmWorkerProcess pid=55079)[0;0m INFO 08-09 12:34:09 [exponential.py:49] Prompt bucket config (min, step, max_warmup, limit) bs:[1, 2, 256, 9], seq:[128, 128, 4096, 13]
INFO 08-09 12:34:09 [exponential.py:49] Prompt bucket config (min, step, max_warmup, limit) bs:[1, 2, 256, 9], seq:[128, 128, 4096, 13]
[1;36m(VllmWorkerProcess pid=55078)[0;0m INFO 08-09 12:34:09 [exponential.py:49] Prompt bucket config (min, step, max_warmup, limit) bs:[1, 2, 256, 9], seq:[128, 128, 4096, 13]
[1;36m(VllmWorkerProcess pid=55080)[0;0m INFO 08-09 12:34:09 [exponential.py:49] Prompt bucket config (min, step, max_warmup, limit) bs:[1, 2, 256, 9], seq:[128, 128, 4096, 13]
[1;36m(VllmWorkerProcess pid=55075)[0;0m INFO 08-09 12:34:09 [common.py:117] Generated 38 prompt buckets [bs, query, num_blocks]: [(1, 128, 0), (1, 256, 0), (1, 384, 0), (1, 512, 0), (1, 640, 0), (1, 768, 0), (1, 896, 0), (1, 1024, 0), (1, 1408, 0), (1, 1792, 0), (1, 2304, 0), (1, 3072, 0), (1, 4096, 0), (2, 128, 0), (2, 256, 0), (2, 384, 0), (2, 512, 0), (2, 640, 0), (2, 768, 0), (2, 896, 0), (2, 1024, 0), (2, 1408, 0), (2, 1792, 0), (4, 128, 0), (4, 256, 0), (4, 384, 0), (4, 512, 0), (4, 640, 0), (4, 768, 0), (4, 896, 0), (4, 1024, 0), (8, 128, 0), (8, 256, 0), (8, 384, 0), (8, 512, 0), (16, 128, 0), (16, 256, 0), (32, 128, 0)]
[1;36m(VllmWorkerProcess pid=55075)[0;0m INFO 08-09 12:34:09 [exponential.py:79] Decode bucket config (min, step, max_warmup, limit) bs:[1, 2, 256, 9], block:[128, 128, 9639, 15]
[1;36m(VllmWorkerProcess pid=55076)[0;0m INFO 08-09 12:34:09 [exponential.py:49] Prompt bucket config (min, step, max_warmup, limit) bs:[1, 2, 256, 9], seq:[128, 128, 4096, 13]
[1;36m(VllmWorkerProcess pid=55077)[0;0m INFO 08-09 12:34:09 [common.py:117] Generated 38 prompt buckets [bs, query, num_blocks]: [(1, 128, 0), (1, 256, 0), (1, 384, 0), (1, 512, 0), (1, 640, 0), (1, 768, 0), (1, 896, 0), (1, 1024, 0), (1, 1408, 0), (1, 1792, 0), (1, 2304, 0), (1, 3072, 0), (1, 4096, 0), (2, 128, 0), (2, 256, 0), (2, 384, 0), (2, 512, 0), (2, 640, 0), (2, 768, 0), (2, 896, 0), (2, 1024, 0), (2, 1408, 0), (2, 1792, 0), (4, 128, 0), (4, 256, 0), (4, 384, 0), (4, 512, 0), (4, 640, 0), (4, 768, 0), (4, 896, 0), (4, 1024, 0), (8, 128, 0), (8, 256, 0), (8, 384, 0), (8, 512, 0), (16, 128, 0), (16, 256, 0), (32, 128, 0)]
[1;36m(VllmWorkerProcess pid=55079)[0;0m INFO 08-09 12:34:09 [common.py:117] Generated 38 prompt buckets [bs, query, num_blocks]: [(1, 128, 0), (1, 256, 0), (1, 384, 0), (1, 512, 0), (1, 640, 0), (1, 768, 0), (1, 896, 0), (1, 1024, 0), (1, 1408, 0), (1, 1792, 0), (1, 2304, 0), (1, 3072, 0), (1, 4096, 0), (2, 128, 0), (2, 256, 0), (2, 384, 0), (2, 512, 0), (2, 640, 0), (2, 768, 0), (2, 896, 0), (2, 1024, 0), (2, 1408, 0), (2, 1792, 0), (4, 128, 0), (4, 256, 0), (4, 384, 0), (4, 512, 0), (4, 640, 0), (4, 768, 0), (4, 896, 0), (4, 1024, 0), (8, 128, 0), (8, 256, 0), (8, 384, 0), (8, 512, 0), (16, 128, 0), (16, 256, 0), (32, 128, 0)]
INFO 08-09 12:34:09 [common.py:117] Generated 38 prompt buckets [bs, query, num_blocks]: [(1, 128, 0), (1, 256, 0), (1, 384, 0), (1, 512, 0), (1, 640, 0), (1, 768, 0), (1, 896, 0), (1, 1024, 0), (1, 1408, 0), (1, 1792, 0), (1, 2304, 0), (1, 3072, 0), (1, 4096, 0), (2, 128, 0), (2, 256, 0), (2, 384, 0), (2, 512, 0), (2, 640, 0), (2, 768, 0), (2, 896, 0), (2, 1024, 0), (2, 1408, 0), (2, 1792, 0), (4, 128, 0), (4, 256, 0), (4, 384, 0), (4, 512, 0), (4, 640, 0), (4, 768, 0), (4, 896, 0), (4, 1024, 0), (8, 128, 0), (8, 256, 0), (8, 384, 0), (8, 512, 0), (16, 128, 0), (16, 256, 0), (32, 128, 0)]
[1;36m(VllmWorkerProcess pid=55077)[0;0m INFO 08-09 12:34:09 [exponential.py:79] Decode bucket config (min, step, max_warmup, limit) bs:[1, 2, 256, 9], block:[128, 128, 9639, 15]
[1;36m(VllmWorkerProcess pid=55078)[0;0m INFO 08-09 12:34:09 [common.py:117] Generated 38 prompt buckets [bs, query, num_blocks]: [(1, 128, 0), (1, 256, 0), (1, 384, 0), (1, 512, 0), (1, 640, 0), (1, 768, 0), (1, 896, 0), (1, 1024, 0), (1, 1408, 0), (1, 1792, 0), (1, 2304, 0), (1, 3072, 0), (1, 4096, 0), (2, 128, 0), (2, 256, 0), (2, 384, 0), (2, 512, 0), (2, 640, 0), (2, 768, 0), (2, 896, 0), (2, 1024, 0), (2, 1408, 0), (2, 1792, 0), (4, 128, 0), (4, 256, 0), (4, 384, 0), (4, 512, 0), (4, 640, 0), (4, 768, 0), (4, 896, 0), (4, 1024, 0), (8, 128, 0), (8, 256, 0), (8, 384, 0), (8, 512, 0), (16, 128, 0), (16, 256, 0), (32, 128, 0)]
[1;36m(VllmWorkerProcess pid=55080)[0;0m INFO 08-09 12:34:09 [common.py:117] Generated 38 prompt buckets [bs, query, num_blocks]: [(1, 128, 0), (1, 256, 0), (1, 384, 0), (1, 512, 0), (1, 640, 0), (1, 768, 0), (1, 896, 0), (1, 1024, 0), (1, 1408, 0), (1, 1792, 0), (1, 2304, 0), (1, 3072, 0), (1, 4096, 0), (2, 128, 0), (2, 256, 0), (2, 384, 0), (2, 512, 0), (2, 640, 0), (2, 768, 0), (2, 896, 0), (2, 1024, 0), (2, 1408, 0), (2, 1792, 0), (4, 128, 0), (4, 256, 0), (4, 384, 0), (4, 512, 0), (4, 640, 0), (4, 768, 0), (4, 896, 0), (4, 1024, 0), (8, 128, 0), (8, 256, 0), (8, 384, 0), (8, 512, 0), (16, 128, 0), (16, 256, 0), (32, 128, 0)]
INFO 08-09 12:34:09 [exponential.py:79] Decode bucket config (min, step, max_warmup, limit) bs:[1, 2, 256, 9], block:[128, 128, 9639, 15]
[1;36m(VllmWorkerProcess pid=55079)[0;0m INFO 08-09 12:34:09 [exponential.py:79] Decode bucket config (min, step, max_warmup, limit) bs:[1, 2, 256, 9], block:[128, 128, 9639, 15]
[1;36m(VllmWorkerProcess pid=55078)[0;0m INFO 08-09 12:34:09 [exponential.py:79] Decode bucket config (min, step, max_warmup, limit) bs:[1, 2, 256, 9], block:[128, 128, 9639, 15]
[1;36m(VllmWorkerProcess pid=55080)[0;0m INFO 08-09 12:34:09 [exponential.py:79] Decode bucket config (min, step, max_warmup, limit) bs:[1, 2, 256, 9], block:[128, 128, 9639, 15]
[1;36m(VllmWorkerProcess pid=55075)[0;0m INFO 08-09 12:34:09 [common.py:117] Generated 135 decode buckets [bs, query, num_blocks]: [(1, 1, 128), (1, 1, 256), (1, 1, 384), (1, 1, 512), (1, 1, 640), (1, 1, 768), (1, 1, 896), (1, 1, 1152), (1, 1, 1536), (1, 1, 2176), (1, 1, 2816), (1, 1, 3840), (1, 1, 5248), (1, 1, 7168), (1, 1, 9639), (2, 1, 128), (2, 1, 256), (2, 1, 384), (2, 1, 512), (2, 1, 640), (2, 1, 768), (2, 1, 896), (2, 1, 1152), (2, 1, 1536), (2, 1, 2176), (2, 1, 2816), (2, 1, 3840), (2, 1, 5248), (2, 1, 7168), (2, 1, 9639), (4, 1, 128), (4, 1, 256), (4, 1, 384), (4, 1, 512), (4, 1, 640), (4, 1, 768), (4, 1, 896), (4, 1, 1152), (4, 1, 1536), (4, 1, 2176), (4, 1, 2816), (4, 1, 3840), (4, 1, 5248), (4, 1, 7168), (4, 1, 9639), (8, 1, 128), (8, 1, 256), (8, 1, 384), (8, 1, 512), (8, 1, 640), (8, 1, 768), (8, 1, 896), (8, 1, 1152), (8, 1, 1536), (8, 1, 2176), (8, 1, 2816), (8, 1, 3840), (8, 1, 5248), (8, 1, 7168), (8, 1, 9639), (16, 1, 128), (16, 1, 256), (16, 1, 384), (16, 1, 512), (16, 1, 640), (16, 1, 768), (16, 1, 896), (16, 1, 1152), (16, 1, 1536), (16, 1, 2176), (16, 1, 2816), (16, 1, 3840), (16, 1, 5248), (16, 1, 7168), (16, 1, 9639), (32, 1, 128), (32, 1, 256), (32, 1, 384), (32, 1, 512), (32, 1, 640), (32, 1, 768), (32, 1, 896), (32, 1, 1152), (32, 1, 1536), (32, 1, 2176), (32, 1, 2816), (32, 1, 3840), (32, 1, 5248), (32, 1, 7168), (32, 1, 9639), (64, 1, 128), (64, 1, 256), (64, 1, 384), (64, 1, 512), (64, 1, 640), (64, 1, 768), (64, 1, 896), (64, 1, 1152), (64, 1, 1536), (64, 1, 2176), (64, 1, 2816), (64, 1, 3840), (64, 1, 5248), (64, 1, 7168), (64, 1, 9639), (128, 1, 128), (128, 1, 256), (128, 1, 384), (128, 1, 512), (128, 1, 640), (128, 1, 768), (128, 1, 896), (128, 1, 1152), (128, 1, 1536), (128, 1, 2176), (128, 1, 2816), (128, 1, 3840), (128, 1, 5248), (128, 1, 7168), (128, 1, 9639), (256, 1, 128), (256, 1, 256), (256, 1, 384), (256, 1, 512), (256, 1, 640), (256, 1, 768), (256, 1, 896), (256, 1, 1152), (256, 1, 1536), (256, 1, 2176), (256, 1, 2816), (256, 1, 3840), (256, 1, 5248), (256, 1, 7168), (256, 1, 9639)]
[1;36m(VllmWorkerProcess pid=55076)[0;0m INFO 08-09 12:34:09 [common.py:117] Generated 38 prompt buckets [bs, query, num_blocks]: [(1, 128, 0), (1, 256, 0), (1, 384, 0), (1, 512, 0), (1, 640, 0), (1, 768, 0), (1, 896, 0), (1, 1024, 0), (1, 1408, 0), (1, 1792, 0), (1, 2304, 0), (1, 3072, 0), (1, 4096, 0), (2, 128, 0), (2, 256, 0), (2, 384, 0), (2, 512, 0), (2, 640, 0), (2, 768, 0), (2, 896, 0), (2, 1024, 0), (2, 1408, 0), (2, 1792, 0), (4, 128, 0), (4, 256, 0), (4, 384, 0), (4, 512, 0), (4, 640, 0), (4, 768, 0), (4, 896, 0), (4, 1024, 0), (8, 128, 0), (8, 256, 0), (8, 384, 0), (8, 512, 0), (16, 128, 0), (16, 256, 0), (32, 128, 0)]
[1;36m(VllmWorkerProcess pid=55081)[0;0m INFO 08-09 12:34:09 [exponential.py:49] Prompt bucket config (min, step, max_warmup, limit) bs:[1, 2, 256, 9], seq:[128, 128, 4096, 13]
[1;36m(VllmWorkerProcess pid=55076)[0;0m INFO 08-09 12:34:09 [exponential.py:79] Decode bucket config (min, step, max_warmup, limit) bs:[1, 2, 256, 9], block:[128, 128, 9639, 15]
[1;36m(VllmWorkerProcess pid=55077)[0;0m INFO 08-09 12:34:09 [common.py:117] Generated 135 decode buckets [bs, query, num_blocks]: [(1, 1, 128), (1, 1, 256), (1, 1, 384), (1, 1, 512), (1, 1, 640), (1, 1, 768), (1, 1, 896), (1, 1, 1152), (1, 1, 1536), (1, 1, 2176), (1, 1, 2816), (1, 1, 3840), (1, 1, 5248), (1, 1, 7168), (1, 1, 9639), (2, 1, 128), (2, 1, 256), (2, 1, 384), (2, 1, 512), (2, 1, 640), (2, 1, 768), (2, 1, 896), (2, 1, 1152), (2, 1, 1536), (2, 1, 2176), (2, 1, 2816), (2, 1, 3840), (2, 1, 5248), (2, 1, 7168), (2, 1, 9639), (4, 1, 128), (4, 1, 256), (4, 1, 384), (4, 1, 512), (4, 1, 640), (4, 1, 768), (4, 1, 896), (4, 1, 1152), (4, 1, 1536), (4, 1, 2176), (4, 1, 2816), (4, 1, 3840), (4, 1, 5248), (4, 1, 7168), (4, 1, 9639), (8, 1, 128), (8, 1, 256), (8, 1, 384), (8, 1, 512), (8, 1, 640), (8, 1, 768), (8, 1, 896), (8, 1, 1152), (8, 1, 1536), (8, 1, 2176), (8, 1, 2816), (8, 1, 3840), (8, 1, 5248), (8, 1, 7168), (8, 1, 9639), (16, 1, 128), (16, 1, 256), (16, 1, 384), (16, 1, 512), (16, 1, 640), (16, 1, 768), (16, 1, 896), (16, 1, 1152), (16, 1, 1536), (16, 1, 2176), (16, 1, 2816), (16, 1, 3840), (16, 1, 5248), (16, 1, 7168), (16, 1, 9639), (32, 1, 128), (32, 1, 256), (32, 1, 384), (32, 1, 512), (32, 1, 640), (32, 1, 768), (32, 1, 896), (32, 1, 1152), (32, 1, 1536), (32, 1, 2176), (32, 1, 2816), (32, 1, 3840), (32, 1, 5248), (32, 1, 7168), (32, 1, 9639), (64, 1, 128), (64, 1, 256), (64, 1, 384), (64, 1, 512), (64, 1, 640), (64, 1, 768), (64, 1, 896), (64, 1, 1152), (64, 1, 1536), (64, 1, 2176), (64, 1, 2816), (64, 1, 3840), (64, 1, 5248), (64, 1, 7168), (64, 1, 9639), (128, 1, 128), (128, 1, 256), (128, 1, 384), (128, 1, 512), (128, 1, 640), (128, 1, 768), (128, 1, 896), (128, 1, 1152), (128, 1, 1536), (128, 1, 2176), (128, 1, 2816), (128, 1, 3840), (128, 1, 5248), (128, 1, 7168), (128, 1, 9639), (256, 1, 128), (256, 1, 256), (256, 1, 384), (256, 1, 512), (256, 1, 640), (256, 1, 768), (256, 1, 896), (256, 1, 1152), (256, 1, 1536), (256, 1, 2176), (256, 1, 2816), (256, 1, 3840), (256, 1, 5248), (256, 1, 7168), (256, 1, 9639)]
INFO 08-09 12:34:09 [common.py:117] Generated 135 decode buckets [bs, query, num_blocks]: [(1, 1, 128), (1, 1, 256), (1, 1, 384), (1, 1, 512), (1, 1, 640), (1, 1, 768), (1, 1, 896), (1, 1, 1152), (1, 1, 1536), (1, 1, 2176), (1, 1, 2816), (1, 1, 3840), (1, 1, 5248), (1, 1, 7168), (1, 1, 9639), (2, 1, 128), (2, 1, 256), (2, 1, 384), (2, 1, 512), (2, 1, 640), (2, 1, 768), (2, 1, 896), (2, 1, 1152), (2, 1, 1536), (2, 1, 2176), (2, 1, 2816), (2, 1, 3840), (2, 1, 5248), (2, 1, 7168), (2, 1, 9639), (4, 1, 128), (4, 1, 256), (4, 1, 384), (4, 1, 512), (4, 1, 640), (4, 1, 768), (4, 1, 896), (4, 1, 1152), (4, 1, 1536), (4, 1, 2176), (4, 1, 2816), (4, 1, 3840), (4, 1, 5248), (4, 1, 7168), (4, 1, 9639), (8, 1, 128), (8, 1, 256), (8, 1, 384), (8, 1, 512), (8, 1, 640), (8, 1, 768), (8, 1, 896), (8, 1, 1152), (8, 1, 1536), (8, 1, 2176), (8, 1, 2816), (8, 1, 3840), (8, 1, 5248), (8, 1, 7168), (8, 1, 9639), (16, 1, 128), (16, 1, 256), (16, 1, 384), (16, 1, 512), (16, 1, 640), (16, 1, 768), (16, 1, 896), (16, 1, 1152), (16, 1, 1536), (16, 1, 2176), (16, 1, 2816), (16, 1, 3840), (16, 1, 5248), (16, 1, 7168), (16, 1, 9639), (32, 1, 128), (32, 1, 256), (32, 1, 384), (32, 1, 512), (32, 1, 640), (32, 1, 768), (32, 1, 896), (32, 1, 1152), (32, 1, 1536), (32, 1, 2176), (32, 1, 2816), (32, 1, 3840), (32, 1, 5248), (32, 1, 7168), (32, 1, 9639), (64, 1, 128), (64, 1, 256), (64, 1, 384), (64, 1, 512), (64, 1, 640), (64, 1, 768), (64, 1, 896), (64, 1, 1152), (64, 1, 1536), (64, 1, 2176), (64, 1, 2816), (64, 1, 3840), (64, 1, 5248), (64, 1, 7168), (64, 1, 9639), (128, 1, 128), (128, 1, 256), (128, 1, 384), (128, 1, 512), (128, 1, 640), (128, 1, 768), (128, 1, 896), (128, 1, 1152), (128, 1, 1536), (128, 1, 2176), (128, 1, 2816), (128, 1, 3840), (128, 1, 5248), (128, 1, 7168), (128, 1, 9639), (256, 1, 128), (256, 1, 256), (256, 1, 384), (256, 1, 512), (256, 1, 640), (256, 1, 768), (256, 1, 896), (256, 1, 1152), (256, 1, 1536), (256, 1, 2176), (256, 1, 2816), (256, 1, 3840), (256, 1, 5248), (256, 1, 7168), (256, 1, 9639)]
[1;36m(VllmWorkerProcess pid=55079)[0;0m INFO 08-09 12:34:09 [common.py:117] Generated 135 decode buckets [bs, query, num_blocks]: [(1, 1, 128), (1, 1, 256), (1, 1, 384), (1, 1, 512), (1, 1, 640), (1, 1, 768), (1, 1, 896), (1, 1, 1152), (1, 1, 1536), (1, 1, 2176), (1, 1, 2816), (1, 1, 3840), (1, 1, 5248), (1, 1, 7168), (1, 1, 9639), (2, 1, 128), (2, 1, 256), (2, 1, 384), (2, 1, 512), (2, 1, 640), (2, 1, 768), (2, 1, 896), (2, 1, 1152), (2, 1, 1536), (2, 1, 2176), (2, 1, 2816), (2, 1, 3840), (2, 1, 5248), (2, 1, 7168), (2, 1, 9639), (4, 1, 128), (4, 1, 256), (4, 1, 384), (4, 1, 512), (4, 1, 640), (4, 1, 768), (4, 1, 896), (4, 1, 1152), (4, 1, 1536), (4, 1, 2176), (4, 1, 2816), (4, 1, 3840), (4, 1, 5248), (4, 1, 7168), (4, 1, 9639), (8, 1, 128), (8, 1, 256), (8, 1, 384), (8, 1, 512), (8, 1, 640), (8, 1, 768), (8, 1, 896), (8, 1, 1152), (8, 1, 1536), (8, 1, 2176), (8, 1, 2816), (8, 1, 3840), (8, 1, 5248), (8, 1, 7168), (8, 1, 9639), (16, 1, 128), (16, 1, 256), (16, 1, 384), (16, 1, 512), (16, 1, 640), (16, 1, 768), (16, 1, 896), (16, 1, 1152), (16, 1, 1536), (16, 1, 2176), (16, 1, 2816), (16, 1, 3840), (16, 1, 5248), (16, 1, 7168), (16, 1, 9639), (32, 1, 128), (32, 1, 256), (32, 1, 384), (32, 1, 512), (32, 1, 640), (32, 1, 768), (32, 1, 896), (32, 1, 1152), (32, 1, 1536), (32, 1, 2176), (32, 1, 2816), (32, 1, 3840), (32, 1, 5248), (32, 1, 7168), (32, 1, 9639), (64, 1, 128), (64, 1, 256), (64, 1, 384), (64, 1, 512), (64, 1, 640), (64, 1, 768), (64, 1, 896), (64, 1, 1152), (64, 1, 1536), (64, 1, 2176), (64, 1, 2816), (64, 1, 3840), (64, 1, 5248), (64, 1, 7168), (64, 1, 9639), (128, 1, 128), (128, 1, 256), (128, 1, 384), (128, 1, 512), (128, 1, 640), (128, 1, 768), (128, 1, 896), (128, 1, 1152), (128, 1, 1536), (128, 1, 2176), (128, 1, 2816), (128, 1, 3840), (128, 1, 5248), (128, 1, 7168), (128, 1, 9639), (256, 1, 128), (256, 1, 256), (256, 1, 384), (256, 1, 512), (256, 1, 640), (256, 1, 768), (256, 1, 896), (256, 1, 1152), (256, 1, 1536), (256, 1, 2176), (256, 1, 2816), (256, 1, 3840), (256, 1, 5248), (256, 1, 7168), (256, 1, 9639)]
[1;36m(VllmWorkerProcess pid=55078)[0;0m INFO 08-09 12:34:09 [common.py:117] Generated 135 decode buckets [bs, query, num_blocks]: [(1, 1, 128), (1, 1, 256), (1, 1, 384), (1, 1, 512), (1, 1, 640), (1, 1, 768), (1, 1, 896), (1, 1, 1152), (1, 1, 1536), (1, 1, 2176), (1, 1, 2816), (1, 1, 3840), (1, 1, 5248), (1, 1, 7168), (1, 1, 9639), (2, 1, 128), (2, 1, 256), (2, 1, 384), (2, 1, 512), (2, 1, 640), (2, 1, 768), (2, 1, 896), (2, 1, 1152), (2, 1, 1536), (2, 1, 2176), (2, 1, 2816), (2, 1, 3840), (2, 1, 5248), (2, 1, 7168), (2, 1, 9639), (4, 1, 128), (4, 1, 256), (4, 1, 384), (4, 1, 512), (4, 1, 640), (4, 1, 768), (4, 1, 896), (4, 1, 1152), (4, 1, 1536), (4, 1, 2176), (4, 1, 2816), (4, 1, 3840), (4, 1, 5248), (4, 1, 7168), (4, 1, 9639), (8, 1, 128), (8, 1, 256), (8, 1, 384), (8, 1, 512), (8, 1, 640), (8, 1, 768), (8, 1, 896), (8, 1, 1152), (8, 1, 1536), (8, 1, 2176), (8, 1, 2816), (8, 1, 3840), (8, 1, 5248), (8, 1, 7168), (8, 1, 9639), (16, 1, 128), (16, 1, 256), (16, 1, 384), (16, 1, 512), (16, 1, 640), (16, 1, 768), (16, 1, 896), (16, 1, 1152), (16, 1, 1536), (16, 1, 2176), (16, 1, 2816), (16, 1, 3840), (16, 1, 5248), (16, 1, 7168), (16, 1, 9639), (32, 1, 128), (32, 1, 256), (32, 1, 384), (32, 1, 512), (32, 1, 640), (32, 1, 768), (32, 1, 896), (32, 1, 1152), (32, 1, 1536), (32, 1, 2176), (32, 1, 2816), (32, 1, 3840), (32, 1, 5248), (32, 1, 7168), (32, 1, 9639), (64, 1, 128), (64, 1, 256), (64, 1, 384), (64, 1, 512), (64, 1, 640), (64, 1, 768), (64, 1, 896), (64, 1, 1152), (64, 1, 1536), (64, 1, 2176), (64, 1, 2816), (64, 1, 3840), (64, 1, 5248), (64, 1, 7168), (64, 1, 9639), (128, 1, 128), (128, 1, 256), (128, 1, 384), (128, 1, 512), (128, 1, 640), (128, 1, 768), (128, 1, 896), (128, 1, 1152), (128, 1, 1536), (128, 1, 2176), (128, 1, 2816), (128, 1, 3840), (128, 1, 5248), (128, 1, 7168), (128, 1, 9639), (256, 1, 128), (256, 1, 256), (256, 1, 384), (256, 1, 512), (256, 1, 640), (256, 1, 768), (256, 1, 896), (256, 1, 1152), (256, 1, 1536), (256, 1, 2176), (256, 1, 2816), (256, 1, 3840), (256, 1, 5248), (256, 1, 7168), (256, 1, 9639)]
[1;36m(VllmWorkerProcess pid=55080)[0;0m INFO 08-09 12:34:09 [common.py:117] Generated 135 decode buckets [bs, query, num_blocks]: [(1, 1, 128), (1, 1, 256), (1, 1, 384), (1, 1, 512), (1, 1, 640), (1, 1, 768), (1, 1, 896), (1, 1, 1152), (1, 1, 1536), (1, 1, 2176), (1, 1, 2816), (1, 1, 3840), (1, 1, 5248), (1, 1, 7168), (1, 1, 9639), (2, 1, 128), (2, 1, 256), (2, 1, 384), (2, 1, 512), (2, 1, 640), (2, 1, 768), (2, 1, 896), (2, 1, 1152), (2, 1, 1536), (2, 1, 2176), (2, 1, 2816), (2, 1, 3840), (2, 1, 5248), (2, 1, 7168), (2, 1, 9639), (4, 1, 128), (4, 1, 256), (4, 1, 384), (4, 1, 512), (4, 1, 640), (4, 1, 768), (4, 1, 896), (4, 1, 1152), (4, 1, 1536), (4, 1, 2176), (4, 1, 2816), (4, 1, 3840), (4, 1, 5248), (4, 1, 7168), (4, 1, 9639), (8, 1, 128), (8, 1, 256), (8, 1, 384), (8, 1, 512), (8, 1, 640), (8, 1, 768), (8, 1, 896), (8, 1, 1152), (8, 1, 1536), (8, 1, 2176), (8, 1, 2816), (8, 1, 3840), (8, 1, 5248), (8, 1, 7168), (8, 1, 9639), (16, 1, 128), (16, 1, 256), (16, 1, 384), (16, 1, 512), (16, 1, 640), (16, 1, 768), (16, 1, 896), (16, 1, 1152), (16, 1, 1536), (16, 1, 2176), (16, 1, 2816), (16, 1, 3840), (16, 1, 5248), (16, 1, 7168), (16, 1, 9639), (32, 1, 128), (32, 1, 256), (32, 1, 384), (32, 1, 512), (32, 1, 640), (32, 1, 768), (32, 1, 896), (32, 1, 1152), (32, 1, 1536), (32, 1, 2176), (32, 1, 2816), (32, 1, 3840), (32, 1, 5248), (32, 1, 7168), (32, 1, 9639), (64, 1, 128), (64, 1, 256), (64, 1, 384), (64, 1, 512), (64, 1, 640), (64, 1, 768), (64, 1, 896), (64, 1, 1152), (64, 1, 1536), (64, 1, 2176), (64, 1, 2816), (64, 1, 3840), (64, 1, 5248), (64, 1, 7168), (64, 1, 9639), (128, 1, 128), (128, 1, 256), (128, 1, 384), (128, 1, 512), (128, 1, 640), (128, 1, 768), (128, 1, 896), (128, 1, 1152), (128, 1, 1536), (128, 1, 2176), (128, 1, 2816), (128, 1, 3840), (128, 1, 5248), (128, 1, 7168), (128, 1, 9639), (256, 1, 128), (256, 1, 256), (256, 1, 384), (256, 1, 512), (256, 1, 640), (256, 1, 768), (256, 1, 896), (256, 1, 1152), (256, 1, 1536), (256, 1, 2176), (256, 1, 2816), (256, 1, 3840), (256, 1, 5248), (256, 1, 7168), (256, 1, 9639)]
[1;36m(VllmWorkerProcess pid=55081)[0;0m INFO 08-09 12:34:09 [common.py:117] Generated 38 prompt buckets [bs, query, num_blocks]: [(1, 128, 0), (1, 256, 0), (1, 384, 0), (1, 512, 0), (1, 640, 0), (1, 768, 0), (1, 896, 0), (1, 1024, 0), (1, 1408, 0), (1, 1792, 0), (1, 2304, 0), (1, 3072, 0), (1, 4096, 0), (2, 128, 0), (2, 256, 0), (2, 384, 0), (2, 512, 0), (2, 640, 0), (2, 768, 0), (2, 896, 0), (2, 1024, 0), (2, 1408, 0), (2, 1792, 0), (4, 128, 0), (4, 256, 0), (4, 384, 0), (4, 512, 0), (4, 640, 0), (4, 768, 0), (4, 896, 0), (4, 1024, 0), (8, 128, 0), (8, 256, 0), (8, 384, 0), (8, 512, 0), (16, 128, 0), (16, 256, 0), (32, 128, 0)]
[1;36m(VllmWorkerProcess pid=55076)[0;0m INFO 08-09 12:34:09 [common.py:117] Generated 135 decode buckets [bs, query, num_blocks]: [(1, 1, 128), (1, 1, 256), (1, 1, 384), (1, 1, 512), (1, 1, 640), (1, 1, 768), (1, 1, 896), (1, 1, 1152), (1, 1, 1536), (1, 1, 2176), (1, 1, 2816), (1, 1, 3840), (1, 1, 5248), (1, 1, 7168), (1, 1, 9639), (2, 1, 128), (2, 1, 256), (2, 1, 384), (2, 1, 512), (2, 1, 640), (2, 1, 768), (2, 1, 896), (2, 1, 1152), (2, 1, 1536), (2, 1, 2176), (2, 1, 2816), (2, 1, 3840), (2, 1, 5248), (2, 1, 7168), (2, 1, 9639), (4, 1, 128), (4, 1, 256), (4, 1, 384), (4, 1, 512), (4, 1, 640), (4, 1, 768), (4, 1, 896), (4, 1, 1152), (4, 1, 1536), (4, 1, 2176), (4, 1, 2816), (4, 1, 3840), (4, 1, 5248), (4, 1, 7168), (4, 1, 9639), (8, 1, 128), (8, 1, 256), (8, 1, 384), (8, 1, 512), (8, 1, 640), (8, 1, 768), (8, 1, 896), (8, 1, 1152), (8, 1, 1536), (8, 1, 2176), (8, 1, 2816), (8, 1, 3840), (8, 1, 5248), (8, 1, 7168), (8, 1, 9639), (16, 1, 128), (16, 1, 256), (16, 1, 384), (16, 1, 512), (16, 1, 640), (16, 1, 768), (16, 1, 896), (16, 1, 1152), (16, 1, 1536), (16, 1, 2176), (16, 1, 2816), (16, 1, 3840), (16, 1, 5248), (16, 1, 7168), (16, 1, 9639), (32, 1, 128), (32, 1, 256), (32, 1, 384), (32, 1, 512), (32, 1, 640), (32, 1, 768), (32, 1, 896), (32, 1, 1152), (32, 1, 1536), (32, 1, 2176), (32, 1, 2816), (32, 1, 3840), (32, 1, 5248), (32, 1, 7168), (32, 1, 9639), (64, 1, 128), (64, 1, 256), (64, 1, 384), (64, 1, 512), (64, 1, 640), (64, 1, 768), (64, 1, 896), (64, 1, 1152), (64, 1, 1536), (64, 1, 2176), (64, 1, 2816), (64, 1, 3840), (64, 1, 5248), (64, 1, 7168), (64, 1, 9639), (128, 1, 128), (128, 1, 256), (128, 1, 384), (128, 1, 512), (128, 1, 640), (128, 1, 768), (128, 1, 896), (128, 1, 1152), (128, 1, 1536), (128, 1, 2176), (128, 1, 2816), (128, 1, 3840), (128, 1, 5248), (128, 1, 7168), (128, 1, 9639), (256, 1, 128), (256, 1, 256), (256, 1, 384), (256, 1, 512), (256, 1, 640), (256, 1, 768), (256, 1, 896), (256, 1, 1152), (256, 1, 1536), (256, 1, 2176), (256, 1, 2816), (256, 1, 3840), (256, 1, 5248), (256, 1, 7168), (256, 1, 9639)]
[1;36m(VllmWorkerProcess pid=55081)[0;0m INFO 08-09 12:34:09 [exponential.py:79] Decode bucket config (min, step, max_warmup, limit) bs:[1, 2, 256, 9], block:[128, 128, 9639, 15]
[1;36m(VllmWorkerProcess pid=55081)[0;0m INFO 08-09 12:34:09 [common.py:117] Generated 135 decode buckets [bs, query, num_blocks]: [(1, 1, 128), (1, 1, 256), (1, 1, 384), (1, 1, 512), (1, 1, 640), (1, 1, 768), (1, 1, 896), (1, 1, 1152), (1, 1, 1536), (1, 1, 2176), (1, 1, 2816), (1, 1, 3840), (1, 1, 5248), (1, 1, 7168), (1, 1, 9639), (2, 1, 128), (2, 1, 256), (2, 1, 384), (2, 1, 512), (2, 1, 640), (2, 1, 768), (2, 1, 896), (2, 1, 1152), (2, 1, 1536), (2, 1, 2176), (2, 1, 2816), (2, 1, 3840), (2, 1, 5248), (2, 1, 7168), (2, 1, 9639), (4, 1, 128), (4, 1, 256), (4, 1, 384), (4, 1, 512), (4, 1, 640), (4, 1, 768), (4, 1, 896), (4, 1, 1152), (4, 1, 1536), (4, 1, 2176), (4, 1, 2816), (4, 1, 3840), (4, 1, 5248), (4, 1, 7168), (4, 1, 9639), (8, 1, 128), (8, 1, 256), (8, 1, 384), (8, 1, 512), (8, 1, 640), (8, 1, 768), (8, 1, 896), (8, 1, 1152), (8, 1, 1536), (8, 1, 2176), (8, 1, 2816), (8, 1, 3840), (8, 1, 5248), (8, 1, 7168), (8, 1, 9639), (16, 1, 128), (16, 1, 256), (16, 1, 384), (16, 1, 512), (16, 1, 640), (16, 1, 768), (16, 1, 896), (16, 1, 1152), (16, 1, 1536), (16, 1, 2176), (16, 1, 2816), (16, 1, 3840), (16, 1, 5248), (16, 1, 7168), (16, 1, 9639), (32, 1, 128), (32, 1, 256), (32, 1, 384), (32, 1, 512), (32, 1, 640), (32, 1, 768), (32, 1, 896), (32, 1, 1152), (32, 1, 1536), (32, 1, 2176), (32, 1, 2816), (32, 1, 3840), (32, 1, 5248), (32, 1, 7168), (32, 1, 9639), (64, 1, 128), (64, 1, 256), (64, 1, 384), (64, 1, 512), (64, 1, 640), (64, 1, 768), (64, 1, 896), (64, 1, 1152), (64, 1, 1536), (64, 1, 2176), (64, 1, 2816), (64, 1, 3840), (64, 1, 5248), (64, 1, 7168), (64, 1, 9639), (128, 1, 128), (128, 1, 256), (128, 1, 384), (128, 1, 512), (128, 1, 640), (128, 1, 768), (128, 1, 896), (128, 1, 1152), (128, 1, 1536), (128, 1, 2176), (128, 1, 2816), (128, 1, 3840), (128, 1, 5248), (128, 1, 7168), (128, 1, 9639), (256, 1, 128), (256, 1, 256), (256, 1, 384), (256, 1, 512), (256, 1, 640), (256, 1, 768), (256, 1, 896), (256, 1, 1152), (256, 1, 1536), (256, 1, 2176), (256, 1, 2816), (256, 1, 3840), (256, 1, 5248), (256, 1, 7168), (256, 1, 9639)]
[1;36m(VllmWorkerProcess pid=55078)[0;0m INFO 08-09 12:34:11 [hpu_worker.py:350] Initializing cache engine took 75.3 GiB of device memory (76.95 GiB/94.62 GiB used) and 32 GiB of host memory (274.5 GiB/1007 GiB used)
[1;36m(VllmWorkerProcess pid=55081)[0;0m INFO 08-09 12:34:11 [hpu_worker.py:350] Initializing cache engine took 75.3 GiB of device memory (76.95 GiB/94.62 GiB used) and 32.01 GiB of host memory (274.5 GiB/1007 GiB used)
[1;36m(VllmWorkerProcess pid=55076)[0;0m INFO 08-09 12:34:11 [hpu_worker.py:350] Initializing cache engine took 75.3 GiB of device memory (76.95 GiB/94.62 GiB used) and 31.97 GiB of host memory (274.5 GiB/1007 GiB used)
[1;36m(VllmWorkerProcess pid=55075)[0;0m INFO 08-09 12:34:11 [hpu_worker.py:350] Initializing cache engine took 75.3 GiB of device memory (76.95 GiB/94.62 GiB used) and 31.97 GiB of host memory (274.5 GiB/1007 GiB used)
[1;36m(VllmWorkerProcess pid=55080)[0;0m INFO 08-09 12:34:11 [hpu_worker.py:350] Initializing cache engine took 75.3 GiB of device memory (76.95 GiB/94.62 GiB used) and 31.96 GiB of host memory (274.5 GiB/1007 GiB used)
[1;36m(VllmWorkerProcess pid=55077)[0;0m INFO 08-09 12:34:11 [hpu_worker.py:350] Initializing cache engine took 75.3 GiB of device memory (76.95 GiB/94.62 GiB used) and 31.96 GiB of host memory (274.5 GiB/1007 GiB used)
INFO 08-09 12:34:11 [hpu_worker.py:350] Initializing cache engine took 75.3 GiB of device memory (76.95 GiB/94.62 GiB used) and 31.91 GiB of host memory (274.5 GiB/1007 GiB used)
[1;36m(VllmWorkerProcess pid=55079)[0;0m INFO 08-09 12:34:11 [hpu_worker.py:350] Initializing cache engine took 75.3 GiB of device memory (76.95 GiB/94.62 GiB used) and 31.96 GiB of host memory (274.5 GiB/1007 GiB used)
[1;36m(VllmWorkerProcess pid=55081)[0;0m INFO 08-09 12:34:11 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][1/38] batch_size:32 query_len:128 num_blocks:0 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55079)[0;0m INFO 08-09 12:34:11 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][1/38] batch_size:32 query_len:128 num_blocks:0 free_mem:17.67 GiB
INFO 08-09 12:34:11 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][1/38] batch_size:32 query_len:128 num_blocks:0 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55078)[0;0m INFO 08-09 12:34:11 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][1/38] batch_size:32 query_len:128 num_blocks:0 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55076)[0;0m INFO 08-09 12:34:11 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][1/38] batch_size:32 query_len:128 num_blocks:0 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55075)[0;0m INFO 08-09 12:34:11 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][1/38] batch_size:32 query_len:128 num_blocks:0 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55077)[0;0m INFO 08-09 12:34:11 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][1/38] batch_size:32 query_len:128 num_blocks:0 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55080)[0;0m INFO 08-09 12:34:11 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][1/38] batch_size:32 query_len:128 num_blocks:0 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55081)[0;0m INFO 08-09 12:34:13 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][2/38] batch_size:16 query_len:256 num_blocks:0 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55079)[0;0m INFO 08-09 12:34:13 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][2/38] batch_size:16 query_len:256 num_blocks:0 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55077)[0;0m INFO 08-09 12:34:13 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][2/38] batch_size:16 query_len:256 num_blocks:0 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55076)[0;0m INFO 08-09 12:34:13 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][2/38] batch_size:16 query_len:256 num_blocks:0 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55075)[0;0m INFO 08-09 12:34:13 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][2/38] batch_size:16 query_len:256 num_blocks:0 free_mem:17.67 GiB
INFO 08-09 12:34:13 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][2/38] batch_size:16 query_len:256 num_blocks:0 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55080)[0;0m INFO 08-09 12:34:13 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][2/38] batch_size:16 query_len:256 num_blocks:0 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55078)[0;0m INFO 08-09 12:34:13 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][2/38] batch_size:16 query_len:256 num_blocks:0 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55081)[0;0m INFO 08-09 12:34:15 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][3/38] batch_size:16 query_len:128 num_blocks:0 free_mem:17.67 GiB
INFO 08-09 12:34:15 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][3/38] batch_size:16 query_len:128 num_blocks:0 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55080)[0;0m INFO 08-09 12:34:15 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][3/38] batch_size:16 query_len:128 num_blocks:0 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55079)[0;0m INFO 08-09 12:34:15 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][3/38] batch_size:16 query_len:128 num_blocks:0 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55078)[0;0m INFO 08-09 12:34:15 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][3/38] batch_size:16 query_len:128 num_blocks:0 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55077)[0;0m INFO 08-09 12:34:15 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][3/38] batch_size:16 query_len:128 num_blocks:0 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55075)[0;0m INFO 08-09 12:34:15 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][3/38] batch_size:16 query_len:128 num_blocks:0 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55076)[0;0m INFO 08-09 12:34:15 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][3/38] batch_size:16 query_len:128 num_blocks:0 free_mem:17.67 GiB
INFO 08-09 12:34:17 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][4/38] batch_size:8 query_len:512 num_blocks:0 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55081)[0;0m INFO 08-09 12:34:17 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][4/38] batch_size:8 query_len:512 num_blocks:0 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55079)[0;0m INFO 08-09 12:34:17 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][4/38] batch_size:8 query_len:512 num_blocks:0 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55077)[0;0m INFO 08-09 12:34:17 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][4/38] batch_size:8 query_len:512 num_blocks:0 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55075)[0;0m INFO 08-09 12:34:17 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][4/38] batch_size:8 query_len:512 num_blocks:0 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55078)[0;0m INFO 08-09 12:34:17 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][4/38] batch_size:8 query_len:512 num_blocks:0 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55080)[0;0m INFO 08-09 12:34:17 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][4/38] batch_size:8 query_len:512 num_blocks:0 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55076)[0;0m INFO 08-09 12:34:17 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][4/38] batch_size:8 query_len:512 num_blocks:0 free_mem:17.67 GiB
INFO 08-09 12:34:19 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][5/38] batch_size:8 query_len:384 num_blocks:0 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55081)[0;0m INFO 08-09 12:34:19 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][5/38] batch_size:8 query_len:384 num_blocks:0 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55075)[0;0m INFO 08-09 12:34:19 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][5/38] batch_size:8 query_len:384 num_blocks:0 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55080)[0;0m INFO 08-09 12:34:19 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][5/38] batch_size:8 query_len:384 num_blocks:0 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55078)[0;0m INFO 08-09 12:34:19 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][5/38] batch_size:8 query_len:384 num_blocks:0 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55079)[0;0m INFO 08-09 12:34:19 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][5/38] batch_size:8 query_len:384 num_blocks:0 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55076)[0;0m INFO 08-09 12:34:19 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][5/38] batch_size:8 query_len:384 num_blocks:0 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55077)[0;0m INFO 08-09 12:34:19 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][5/38] batch_size:8 query_len:384 num_blocks:0 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55078)[0;0m INFO 08-09 12:34:22 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][6/38] batch_size:8 query_len:256 num_blocks:0 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55077)[0;0m INFO 08-09 12:34:22 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][6/38] batch_size:8 query_len:256 num_blocks:0 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55075)[0;0m INFO 08-09 12:34:22 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][6/38] batch_size:8 query_len:256 num_blocks:0 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55081)[0;0m INFO 08-09 12:34:22 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][6/38] batch_size:8 query_len:256 num_blocks:0 free_mem:17.67 GiB
INFO 08-09 12:34:22 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][6/38] batch_size:8 query_len:256 num_blocks:0 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55079)[0;0m INFO 08-09 12:34:22 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][6/38] batch_size:8 query_len:256 num_blocks:0 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55080)[0;0m INFO 08-09 12:34:22 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][6/38] batch_size:8 query_len:256 num_blocks:0 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55076)[0;0m INFO 08-09 12:34:22 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][6/38] batch_size:8 query_len:256 num_blocks:0 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55079)[0;0m INFO 08-09 12:34:24 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][7/38] batch_size:8 query_len:128 num_blocks:0 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55078)[0;0m INFO 08-09 12:34:24 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][7/38] batch_size:8 query_len:128 num_blocks:0 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55075)[0;0m INFO 08-09 12:34:24 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][7/38] batch_size:8 query_len:128 num_blocks:0 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55081)[0;0m INFO 08-09 12:34:24 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][7/38] batch_size:8 query_len:128 num_blocks:0 free_mem:17.67 GiB
INFO 08-09 12:34:24 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][7/38] batch_size:8 query_len:128 num_blocks:0 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55080)[0;0m INFO 08-09 12:34:24 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][7/38] batch_size:8 query_len:128 num_blocks:0 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55077)[0;0m INFO 08-09 12:34:24 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][7/38] batch_size:8 query_len:128 num_blocks:0 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55076)[0;0m INFO 08-09 12:34:24 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][7/38] batch_size:8 query_len:128 num_blocks:0 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55079)[0;0m INFO 08-09 12:34:26 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][8/38] batch_size:4 query_len:1024 num_blocks:0 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55077)[0;0m INFO 08-09 12:34:26 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][8/38] batch_size:4 query_len:1024 num_blocks:0 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55078)[0;0m INFO 08-09 12:34:26 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][8/38] batch_size:4 query_len:1024 num_blocks:0 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55075)[0;0m INFO 08-09 12:34:26 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][8/38] batch_size:4 query_len:1024 num_blocks:0 free_mem:17.67 GiB
INFO 08-09 12:34:26 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][8/38] batch_size:4 query_len:1024 num_blocks:0 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55081)[0;0m INFO 08-09 12:34:26 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][8/38] batch_size:4 query_len:1024 num_blocks:0 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55080)[0;0m INFO 08-09 12:34:26 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][8/38] batch_size:4 query_len:1024 num_blocks:0 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55076)[0;0m INFO 08-09 12:34:26 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][8/38] batch_size:4 query_len:1024 num_blocks:0 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55078)[0;0m INFO 08-09 12:34:28 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][9/38] batch_size:4 query_len:896 num_blocks:0 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55075)[0;0m INFO 08-09 12:34:28 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][9/38] batch_size:4 query_len:896 num_blocks:0 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55081)[0;0m INFO 08-09 12:34:28 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][9/38] batch_size:4 query_len:896 num_blocks:0 free_mem:17.67 GiB
INFO 08-09 12:34:28 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][9/38] batch_size:4 query_len:896 num_blocks:0 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55080)[0;0m INFO 08-09 12:34:28 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][9/38] batch_size:4 query_len:896 num_blocks:0 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55077)[0;0m INFO 08-09 12:34:28 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][9/38] batch_size:4 query_len:896 num_blocks:0 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55076)[0;0m INFO 08-09 12:34:28 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][9/38] batch_size:4 query_len:896 num_blocks:0 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55079)[0;0m INFO 08-09 12:34:28 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][9/38] batch_size:4 query_len:896 num_blocks:0 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55078)[0;0m INFO 08-09 12:34:30 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][10/38] batch_size:4 query_len:768 num_blocks:0 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55077)[0;0m INFO 08-09 12:34:30 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][10/38] batch_size:4 query_len:768 num_blocks:0 free_mem:17.67 GiB
INFO 08-09 12:34:30 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][10/38] batch_size:4 query_len:768 num_blocks:0 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55076)[0;0m INFO 08-09 12:34:30 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][10/38] batch_size:4 query_len:768 num_blocks:0 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55075)[0;0m INFO 08-09 12:34:30 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][10/38] batch_size:4 query_len:768 num_blocks:0 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55079)[0;0m INFO 08-09 12:34:30 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][10/38] batch_size:4 query_len:768 num_blocks:0 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55080)[0;0m INFO 08-09 12:34:30 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][10/38] batch_size:4 query_len:768 num_blocks:0 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55081)[0;0m INFO 08-09 12:34:30 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][10/38] batch_size:4 query_len:768 num_blocks:0 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55080)[0;0m INFO 08-09 12:34:33 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][11/38] batch_size:4 query_len:640 num_blocks:0 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55078)[0;0m INFO 08-09 12:34:33 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][11/38] batch_size:4 query_len:640 num_blocks:0 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55075)[0;0m INFO 08-09 12:34:33 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][11/38] batch_size:4 query_len:640 num_blocks:0 free_mem:17.67 GiB
INFO 08-09 12:34:33 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][11/38] batch_size:4 query_len:640 num_blocks:0 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55081)[0;0m INFO 08-09 12:34:33 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][11/38] batch_size:4 query_len:640 num_blocks:0 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55077)[0;0m INFO 08-09 12:34:33 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][11/38] batch_size:4 query_len:640 num_blocks:0 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55079)[0;0m INFO 08-09 12:34:33 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][11/38] batch_size:4 query_len:640 num_blocks:0 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55076)[0;0m INFO 08-09 12:34:33 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][11/38] batch_size:4 query_len:640 num_blocks:0 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55080)[0;0m INFO 08-09 12:34:35 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][12/38] batch_size:4 query_len:512 num_blocks:0 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55077)[0;0m INFO 08-09 12:34:35 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][12/38] batch_size:4 query_len:512 num_blocks:0 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55078)[0;0m INFO 08-09 12:34:35 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][12/38] batch_size:4 query_len:512 num_blocks:0 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55075)[0;0m INFO 08-09 12:34:35 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][12/38] batch_size:4 query_len:512 num_blocks:0 free_mem:17.67 GiB
INFO 08-09 12:34:35 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][12/38] batch_size:4 query_len:512 num_blocks:0 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55079)[0;0m INFO 08-09 12:34:35 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][12/38] batch_size:4 query_len:512 num_blocks:0 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55081)[0;0m INFO 08-09 12:34:35 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][12/38] batch_size:4 query_len:512 num_blocks:0 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55076)[0;0m INFO 08-09 12:34:35 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][12/38] batch_size:4 query_len:512 num_blocks:0 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55081)[0;0m INFO 08-09 12:34:37 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][13/38] batch_size:4 query_len:384 num_blocks:0 free_mem:17.67 GiB
INFO 08-09 12:34:37 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][13/38] batch_size:4 query_len:384 num_blocks:0 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55080)[0;0m INFO 08-09 12:34:37 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][13/38] batch_size:4 query_len:384 num_blocks:0 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55078)[0;0m INFO 08-09 12:34:37 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][13/38] batch_size:4 query_len:384 num_blocks:0 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55079)[0;0m INFO 08-09 12:34:37 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][13/38] batch_size:4 query_len:384 num_blocks:0 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55075)[0;0m INFO 08-09 12:34:37 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][13/38] batch_size:4 query_len:384 num_blocks:0 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55077)[0;0m INFO 08-09 12:34:37 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][13/38] batch_size:4 query_len:384 num_blocks:0 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55076)[0;0m INFO 08-09 12:34:37 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][13/38] batch_size:4 query_len:384 num_blocks:0 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55079)[0;0m INFO 08-09 12:34:40 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][14/38] batch_size:4 query_len:256 num_blocks:0 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55078)[0;0m INFO 08-09 12:34:40 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][14/38] batch_size:4 query_len:256 num_blocks:0 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55081)[0;0m INFO 08-09 12:34:40 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][14/38] batch_size:4 query_len:256 num_blocks:0 free_mem:17.67 GiB
INFO 08-09 12:34:40 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][14/38] batch_size:4 query_len:256 num_blocks:0 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55080)[0;0m INFO 08-09 12:34:40 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][14/38] batch_size:4 query_len:256 num_blocks:0 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55077)[0;0m INFO 08-09 12:34:40 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][14/38] batch_size:4 query_len:256 num_blocks:0 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55075)[0;0m INFO 08-09 12:34:40 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][14/38] batch_size:4 query_len:256 num_blocks:0 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55076)[0;0m INFO 08-09 12:34:40 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][14/38] batch_size:4 query_len:256 num_blocks:0 free_mem:17.67 GiB
INFO 08-09 12:34:42 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][15/38] batch_size:4 query_len:128 num_blocks:0 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55080)[0;0m INFO 08-09 12:34:42 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][15/38] batch_size:4 query_len:128 num_blocks:0 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55081)[0;0m INFO 08-09 12:34:42 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][15/38] batch_size:4 query_len:128 num_blocks:0 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55079)[0;0m INFO 08-09 12:34:42 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][15/38] batch_size:4 query_len:128 num_blocks:0 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55078)[0;0m INFO 08-09 12:34:42 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][15/38] batch_size:4 query_len:128 num_blocks:0 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55075)[0;0m INFO 08-09 12:34:42 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][15/38] batch_size:4 query_len:128 num_blocks:0 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55076)[0;0m INFO 08-09 12:34:42 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][15/38] batch_size:4 query_len:128 num_blocks:0 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55077)[0;0m INFO 08-09 12:34:42 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][15/38] batch_size:4 query_len:128 num_blocks:0 free_mem:17.67 GiB
INFO 08-09 12:34:45 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][16/38] batch_size:2 query_len:1792 num_blocks:0 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55081)[0;0m INFO 08-09 12:34:45 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][16/38] batch_size:2 query_len:1792 num_blocks:0 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55080)[0;0m INFO 08-09 12:34:45 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][16/38] batch_size:2 query_len:1792 num_blocks:0 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55079)[0;0m INFO 08-09 12:34:45 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][16/38] batch_size:2 query_len:1792 num_blocks:0 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55078)[0;0m INFO 08-09 12:34:45 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][16/38] batch_size:2 query_len:1792 num_blocks:0 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55075)[0;0m INFO 08-09 12:34:45 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][16/38] batch_size:2 query_len:1792 num_blocks:0 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55077)[0;0m INFO 08-09 12:34:45 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][16/38] batch_size:2 query_len:1792 num_blocks:0 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55076)[0;0m INFO 08-09 12:34:45 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][16/38] batch_size:2 query_len:1792 num_blocks:0 free_mem:17.67 GiB
INFO 08-09 12:34:47 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][17/38] batch_size:2 query_len:1408 num_blocks:0 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55080)[0;0m INFO 08-09 12:34:47 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][17/38] batch_size:2 query_len:1408 num_blocks:0 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55081)[0;0m INFO 08-09 12:34:47 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][17/38] batch_size:2 query_len:1408 num_blocks:0 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55079)[0;0m INFO 08-09 12:34:47 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][17/38] batch_size:2 query_len:1408 num_blocks:0 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55078)[0;0m INFO 08-09 12:34:47 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][17/38] batch_size:2 query_len:1408 num_blocks:0 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55076)[0;0m INFO 08-09 12:34:47 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][17/38] batch_size:2 query_len:1408 num_blocks:0 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55077)[0;0m INFO 08-09 12:34:47 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][17/38] batch_size:2 query_len:1408 num_blocks:0 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55075)[0;0m INFO 08-09 12:34:47 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][17/38] batch_size:2 query_len:1408 num_blocks:0 free_mem:17.67 GiB
INFO 08-09 12:34:49 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][18/38] batch_size:2 query_len:1024 num_blocks:0 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55079)[0;0m INFO 08-09 12:34:49 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][18/38] batch_size:2 query_len:1024 num_blocks:0 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55078)[0;0m INFO 08-09 12:34:49 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][18/38] batch_size:2 query_len:1024 num_blocks:0 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55075)[0;0m INFO 08-09 12:34:49 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][18/38] batch_size:2 query_len:1024 num_blocks:0 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55081)[0;0m INFO 08-09 12:34:49 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][18/38] batch_size:2 query_len:1024 num_blocks:0 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55080)[0;0m INFO 08-09 12:34:49 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][18/38] batch_size:2 query_len:1024 num_blocks:0 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55077)[0;0m INFO 08-09 12:34:49 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][18/38] batch_size:2 query_len:1024 num_blocks:0 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55076)[0;0m INFO 08-09 12:34:49 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][18/38] batch_size:2 query_len:1024 num_blocks:0 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55079)[0;0m INFO 08-09 12:34:52 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][19/38] batch_size:2 query_len:896 num_blocks:0 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55078)[0;0m INFO 08-09 12:34:52 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][19/38] batch_size:2 query_len:896 num_blocks:0 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55081)[0;0m INFO 08-09 12:34:52 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][19/38] batch_size:2 query_len:896 num_blocks:0 free_mem:17.67 GiB
INFO 08-09 12:34:52 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][19/38] batch_size:2 query_len:896 num_blocks:0 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55076)[0;0m INFO 08-09 12:34:52 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][19/38] batch_size:2 query_len:896 num_blocks:0 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55077)[0;0m INFO 08-09 12:34:52 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][19/38] batch_size:2 query_len:896 num_blocks:0 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55080)[0;0m INFO 08-09 12:34:52 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][19/38] batch_size:2 query_len:896 num_blocks:0 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55075)[0;0m INFO 08-09 12:34:52 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][19/38] batch_size:2 query_len:896 num_blocks:0 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55080)[0;0m INFO 08-09 12:34:54 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][20/38] batch_size:2 query_len:768 num_blocks:0 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55078)[0;0m INFO 08-09 12:34:54 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][20/38] batch_size:2 query_len:768 num_blocks:0 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55075)[0;0m INFO 08-09 12:34:54 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][20/38] batch_size:2 query_len:768 num_blocks:0 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55081)[0;0m INFO 08-09 12:34:54 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][20/38] batch_size:2 query_len:768 num_blocks:0 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55079)[0;0m INFO 08-09 12:34:54 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][20/38] batch_size:2 query_len:768 num_blocks:0 free_mem:17.67 GiB
INFO 08-09 12:34:54 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][20/38] batch_size:2 query_len:768 num_blocks:0 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55076)[0;0m INFO 08-09 12:34:54 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][20/38] batch_size:2 query_len:768 num_blocks:0 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55077)[0;0m INFO 08-09 12:34:54 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][20/38] batch_size:2 query_len:768 num_blocks:0 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55080)[0;0m INFO 08-09 12:34:57 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][21/38] batch_size:2 query_len:640 num_blocks:0 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55081)[0;0m INFO 08-09 12:34:57 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][21/38] batch_size:2 query_len:640 num_blocks:0 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55078)[0;0m INFO 08-09 12:34:57 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][21/38] batch_size:2 query_len:640 num_blocks:0 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55076)[0;0m INFO 08-09 12:34:57 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][21/38] batch_size:2 query_len:640 num_blocks:0 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55079)[0;0m INFO 08-09 12:34:57 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][21/38] batch_size:2 query_len:640 num_blocks:0 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55077)[0;0m INFO 08-09 12:34:57 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][21/38] batch_size:2 query_len:640 num_blocks:0 free_mem:17.67 GiB
INFO 08-09 12:34:57 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][21/38] batch_size:2 query_len:640 num_blocks:0 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55075)[0;0m INFO 08-09 12:34:57 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][21/38] batch_size:2 query_len:640 num_blocks:0 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55080)[0;0m INFO 08-09 12:34:59 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][22/38] batch_size:2 query_len:512 num_blocks:0 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55075)[0;0m INFO 08-09 12:34:59 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][22/38] batch_size:2 query_len:512 num_blocks:0 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55079)[0;0m INFO 08-09 12:34:59 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][22/38] batch_size:2 query_len:512 num_blocks:0 free_mem:17.67 GiB
INFO 08-09 12:34:59 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][22/38] batch_size:2 query_len:512 num_blocks:0 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55081)[0;0m INFO 08-09 12:34:59 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][22/38] batch_size:2 query_len:512 num_blocks:0 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55078)[0;0m INFO 08-09 12:34:59 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][22/38] batch_size:2 query_len:512 num_blocks:0 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55076)[0;0m INFO 08-09 12:34:59 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][22/38] batch_size:2 query_len:512 num_blocks:0 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55077)[0;0m INFO 08-09 12:34:59 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][22/38] batch_size:2 query_len:512 num_blocks:0 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55080)[0;0m INFO 08-09 12:35:02 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][23/38] batch_size:2 query_len:384 num_blocks:0 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55078)[0;0m INFO 08-09 12:35:02 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][23/38] batch_size:2 query_len:384 num_blocks:0 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55081)[0;0m INFO 08-09 12:35:02 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][23/38] batch_size:2 query_len:384 num_blocks:0 free_mem:17.67 GiB
INFO 08-09 12:35:02 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][23/38] batch_size:2 query_len:384 num_blocks:0 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55079)[0;0m INFO 08-09 12:35:02 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][23/38] batch_size:2 query_len:384 num_blocks:0 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55077)[0;0m INFO 08-09 12:35:02 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][23/38] batch_size:2 query_len:384 num_blocks:0 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55076)[0;0m INFO 08-09 12:35:02 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][23/38] batch_size:2 query_len:384 num_blocks:0 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55075)[0;0m INFO 08-09 12:35:02 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][23/38] batch_size:2 query_len:384 num_blocks:0 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55080)[0;0m INFO 08-09 12:35:04 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][24/38] batch_size:2 query_len:256 num_blocks:0 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55081)[0;0m INFO 08-09 12:35:04 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][24/38] batch_size:2 query_len:256 num_blocks:0 free_mem:17.67 GiB
INFO 08-09 12:35:04 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][24/38] batch_size:2 query_len:256 num_blocks:0 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55079)[0;0m INFO 08-09 12:35:04 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][24/38] batch_size:2 query_len:256 num_blocks:0 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55077)[0;0m INFO 08-09 12:35:04 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][24/38] batch_size:2 query_len:256 num_blocks:0 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55078)[0;0m INFO 08-09 12:35:04 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][24/38] batch_size:2 query_len:256 num_blocks:0 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55076)[0;0m INFO 08-09 12:35:04 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][24/38] batch_size:2 query_len:256 num_blocks:0 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55075)[0;0m INFO 08-09 12:35:04 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][24/38] batch_size:2 query_len:256 num_blocks:0 free_mem:17.67 GiB
INFO 08-09 12:35:07 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][25/38] batch_size:2 query_len:128 num_blocks:0 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55075)[0;0m INFO 08-09 12:35:07 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][25/38] batch_size:2 query_len:128 num_blocks:0 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55080)[0;0m INFO 08-09 12:35:07 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][25/38] batch_size:2 query_len:128 num_blocks:0 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55079)[0;0m INFO 08-09 12:35:07 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][25/38] batch_size:2 query_len:128 num_blocks:0 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55081)[0;0m INFO 08-09 12:35:07 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][25/38] batch_size:2 query_len:128 num_blocks:0 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55078)[0;0m INFO 08-09 12:35:07 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][25/38] batch_size:2 query_len:128 num_blocks:0 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55076)[0;0m INFO 08-09 12:35:07 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][25/38] batch_size:2 query_len:128 num_blocks:0 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55077)[0;0m INFO 08-09 12:35:07 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][25/38] batch_size:2 query_len:128 num_blocks:0 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55081)[0;0m INFO 08-09 12:35:09 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][26/38] batch_size:1 query_len:4096 num_blocks:0 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55075)[0;0m INFO 08-09 12:35:09 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][26/38] batch_size:1 query_len:4096 num_blocks:0 free_mem:17.67 GiB
INFO 08-09 12:35:09 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][26/38] batch_size:1 query_len:4096 num_blocks:0 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55079)[0;0m INFO 08-09 12:35:09 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][26/38] batch_size:1 query_len:4096 num_blocks:0 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55078)[0;0m INFO 08-09 12:35:09 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][26/38] batch_size:1 query_len:4096 num_blocks:0 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55080)[0;0m INFO 08-09 12:35:09 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][26/38] batch_size:1 query_len:4096 num_blocks:0 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55077)[0;0m INFO 08-09 12:35:09 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][26/38] batch_size:1 query_len:4096 num_blocks:0 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55076)[0;0m INFO 08-09 12:35:09 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][26/38] batch_size:1 query_len:4096 num_blocks:0 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55080)[0;0m INFO 08-09 12:35:11 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][27/38] batch_size:1 query_len:3072 num_blocks:0 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55075)[0;0m INFO 08-09 12:35:11 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][27/38] batch_size:1 query_len:3072 num_blocks:0 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55081)[0;0m INFO 08-09 12:35:11 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][27/38] batch_size:1 query_len:3072 num_blocks:0 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55079)[0;0m INFO 08-09 12:35:11 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][27/38] batch_size:1 query_len:3072 num_blocks:0 free_mem:17.67 GiB
INFO 08-09 12:35:11 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][27/38] batch_size:1 query_len:3072 num_blocks:0 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55078)[0;0m INFO 08-09 12:35:11 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][27/38] batch_size:1 query_len:3072 num_blocks:0 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55076)[0;0m INFO 08-09 12:35:11 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][27/38] batch_size:1 query_len:3072 num_blocks:0 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55077)[0;0m INFO 08-09 12:35:11 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][27/38] batch_size:1 query_len:3072 num_blocks:0 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55080)[0;0m INFO 08-09 12:35:14 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][28/38] batch_size:1 query_len:2304 num_blocks:0 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55076)[0;0m INFO 08-09 12:35:14 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][28/38] batch_size:1 query_len:2304 num_blocks:0 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55079)[0;0m INFO 08-09 12:35:14 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][28/38] batch_size:1 query_len:2304 num_blocks:0 free_mem:17.67 GiB
INFO 08-09 12:35:14 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][28/38] batch_size:1 query_len:2304 num_blocks:0 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55081)[0;0m INFO 08-09 12:35:14 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][28/38] batch_size:1 query_len:2304 num_blocks:0 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55078)[0;0m INFO 08-09 12:35:14 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][28/38] batch_size:1 query_len:2304 num_blocks:0 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55077)[0;0m INFO 08-09 12:35:14 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][28/38] batch_size:1 query_len:2304 num_blocks:0 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55075)[0;0m INFO 08-09 12:35:14 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][28/38] batch_size:1 query_len:2304 num_blocks:0 free_mem:17.67 GiB
INFO 08-09 12:35:17 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][29/38] batch_size:1 query_len:1792 num_blocks:0 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55080)[0;0m INFO 08-09 12:35:17 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][29/38] batch_size:1 query_len:1792 num_blocks:0 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55081)[0;0m INFO 08-09 12:35:17 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][29/38] batch_size:1 query_len:1792 num_blocks:0 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55079)[0;0m INFO 08-09 12:35:17 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][29/38] batch_size:1 query_len:1792 num_blocks:0 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55076)[0;0m INFO 08-09 12:35:17 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][29/38] batch_size:1 query_len:1792 num_blocks:0 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55075)[0;0m INFO 08-09 12:35:17 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][29/38] batch_size:1 query_len:1792 num_blocks:0 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55078)[0;0m INFO 08-09 12:35:17 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][29/38] batch_size:1 query_len:1792 num_blocks:0 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55077)[0;0m INFO 08-09 12:35:17 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][29/38] batch_size:1 query_len:1792 num_blocks:0 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55081)[0;0m INFO 08-09 12:35:19 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][30/38] batch_size:1 query_len:1408 num_blocks:0 free_mem:17.67 GiB
INFO 08-09 12:35:19 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][30/38] batch_size:1 query_len:1408 num_blocks:0 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55076)[0;0m INFO 08-09 12:35:19 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][30/38] batch_size:1 query_len:1408 num_blocks:0 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55079)[0;0m INFO 08-09 12:35:19 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][30/38] batch_size:1 query_len:1408 num_blocks:0 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55080)[0;0m INFO 08-09 12:35:19 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][30/38] batch_size:1 query_len:1408 num_blocks:0 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55078)[0;0m INFO 08-09 12:35:19 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][30/38] batch_size:1 query_len:1408 num_blocks:0 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55077)[0;0m INFO 08-09 12:35:19 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][30/38] batch_size:1 query_len:1408 num_blocks:0 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55075)[0;0m INFO 08-09 12:35:19 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][30/38] batch_size:1 query_len:1408 num_blocks:0 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55076)[0;0m INFO 08-09 12:35:22 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][31/38] batch_size:1 query_len:1024 num_blocks:0 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55081)[0;0m INFO 08-09 12:35:22 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][31/38] batch_size:1 query_len:1024 num_blocks:0 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55078)[0;0m INFO 08-09 12:35:22 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][31/38] batch_size:1 query_len:1024 num_blocks:0 free_mem:17.67 GiB
INFO 08-09 12:35:22 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][31/38] batch_size:1 query_len:1024 num_blocks:0 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55079)[0;0m INFO 08-09 12:35:22 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][31/38] batch_size:1 query_len:1024 num_blocks:0 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55077)[0;0m INFO 08-09 12:35:22 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][31/38] batch_size:1 query_len:1024 num_blocks:0 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55080)[0;0m INFO 08-09 12:35:22 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][31/38] batch_size:1 query_len:1024 num_blocks:0 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55075)[0;0m INFO 08-09 12:35:22 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][31/38] batch_size:1 query_len:1024 num_blocks:0 free_mem:17.67 GiB
INFO 08-09 12:35:25 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][32/38] batch_size:1 query_len:896 num_blocks:0 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55081)[0;0m INFO 08-09 12:35:25 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][32/38] batch_size:1 query_len:896 num_blocks:0 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55080)[0;0m INFO 08-09 12:35:25 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][32/38] batch_size:1 query_len:896 num_blocks:0 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55079)[0;0m INFO 08-09 12:35:25 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][32/38] batch_size:1 query_len:896 num_blocks:0 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55078)[0;0m INFO 08-09 12:35:25 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][32/38] batch_size:1 query_len:896 num_blocks:0 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55075)[0;0m INFO 08-09 12:35:25 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][32/38] batch_size:1 query_len:896 num_blocks:0 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55077)[0;0m INFO 08-09 12:35:25 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][32/38] batch_size:1 query_len:896 num_blocks:0 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55076)[0;0m INFO 08-09 12:35:25 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][32/38] batch_size:1 query_len:896 num_blocks:0 free_mem:17.67 GiB
INFO 08-09 12:35:27 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][33/38] batch_size:1 query_len:768 num_blocks:0 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55081)[0;0m INFO 08-09 12:35:27 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][33/38] batch_size:1 query_len:768 num_blocks:0 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55080)[0;0m INFO 08-09 12:35:27 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][33/38] batch_size:1 query_len:768 num_blocks:0 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55079)[0;0m INFO 08-09 12:35:27 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][33/38] batch_size:1 query_len:768 num_blocks:0 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55076)[0;0m INFO 08-09 12:35:27 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][33/38] batch_size:1 query_len:768 num_blocks:0 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55075)[0;0m INFO 08-09 12:35:27 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][33/38] batch_size:1 query_len:768 num_blocks:0 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55078)[0;0m INFO 08-09 12:35:27 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][33/38] batch_size:1 query_len:768 num_blocks:0 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55077)[0;0m INFO 08-09 12:35:27 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][33/38] batch_size:1 query_len:768 num_blocks:0 free_mem:17.67 GiB
INFO 08-09 12:35:30 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][34/38] batch_size:1 query_len:640 num_blocks:0 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55079)[0;0m INFO 08-09 12:35:30 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][34/38] batch_size:1 query_len:640 num_blocks:0 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55080)[0;0m INFO 08-09 12:35:30 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][34/38] batch_size:1 query_len:640 num_blocks:0 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55081)[0;0m INFO 08-09 12:35:30 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][34/38] batch_size:1 query_len:640 num_blocks:0 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55076)[0;0m INFO 08-09 12:35:30 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][34/38] batch_size:1 query_len:640 num_blocks:0 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55078)[0;0m INFO 08-09 12:35:30 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][34/38] batch_size:1 query_len:640 num_blocks:0 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55077)[0;0m INFO 08-09 12:35:30 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][34/38] batch_size:1 query_len:640 num_blocks:0 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55075)[0;0m INFO 08-09 12:35:30 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][34/38] batch_size:1 query_len:640 num_blocks:0 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55076)[0;0m INFO 08-09 12:35:33 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][35/38] batch_size:1 query_len:512 num_blocks:0 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55079)[0;0m INFO 08-09 12:35:33 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][35/38] batch_size:1 query_len:512 num_blocks:0 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55080)[0;0m INFO 08-09 12:35:33 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][35/38] batch_size:1 query_len:512 num_blocks:0 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55081)[0;0m INFO 08-09 12:35:33 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][35/38] batch_size:1 query_len:512 num_blocks:0 free_mem:17.67 GiB
INFO 08-09 12:35:33 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][35/38] batch_size:1 query_len:512 num_blocks:0 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55075)[0;0m INFO 08-09 12:35:33 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][35/38] batch_size:1 query_len:512 num_blocks:0 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55077)[0;0m INFO 08-09 12:35:33 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][35/38] batch_size:1 query_len:512 num_blocks:0 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55078)[0;0m INFO 08-09 12:35:33 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][35/38] batch_size:1 query_len:512 num_blocks:0 free_mem:17.67 GiB
INFO 08-09 12:35:35 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][36/38] batch_size:1 query_len:384 num_blocks:0 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55079)[0;0m INFO 08-09 12:35:35 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][36/38] batch_size:1 query_len:384 num_blocks:0 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55075)[0;0m INFO 08-09 12:35:35 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][36/38] batch_size:1 query_len:384 num_blocks:0 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55076)[0;0m INFO 08-09 12:35:35 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][36/38] batch_size:1 query_len:384 num_blocks:0 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55080)[0;0m INFO 08-09 12:35:35 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][36/38] batch_size:1 query_len:384 num_blocks:0 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55081)[0;0m INFO 08-09 12:35:35 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][36/38] batch_size:1 query_len:384 num_blocks:0 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55078)[0;0m INFO 08-09 12:35:35 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][36/38] batch_size:1 query_len:384 num_blocks:0 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55077)[0;0m INFO 08-09 12:35:35 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][36/38] batch_size:1 query_len:384 num_blocks:0 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55079)[0;0m INFO 08-09 12:35:38 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][37/38] batch_size:1 query_len:256 num_blocks:0 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55078)[0;0m INFO 08-09 12:35:38 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][37/38] batch_size:1 query_len:256 num_blocks:0 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55075)[0;0m INFO 08-09 12:35:38 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][37/38] batch_size:1 query_len:256 num_blocks:0 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55080)[0;0m INFO 08-09 12:35:38 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][37/38] batch_size:1 query_len:256 num_blocks:0 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55081)[0;0m INFO 08-09 12:35:38 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][37/38] batch_size:1 query_len:256 num_blocks:0 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55077)[0;0m INFO 08-09 12:35:38 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][37/38] batch_size:1 query_len:256 num_blocks:0 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55076)[0;0m INFO 08-09 12:35:38 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][37/38] batch_size:1 query_len:256 num_blocks:0 free_mem:17.67 GiB
INFO 08-09 12:35:38 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][37/38] batch_size:1 query_len:256 num_blocks:0 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55079)[0;0m INFO 08-09 12:35:41 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][38/38] batch_size:1 query_len:128 num_blocks:0 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55080)[0;0m INFO 08-09 12:35:41 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][38/38] batch_size:1 query_len:128 num_blocks:0 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55078)[0;0m INFO 08-09 12:35:41 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][38/38] batch_size:1 query_len:128 num_blocks:0 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55076)[0;0m INFO 08-09 12:35:41 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][38/38] batch_size:1 query_len:128 num_blocks:0 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55081)[0;0m INFO 08-09 12:35:41 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][38/38] batch_size:1 query_len:128 num_blocks:0 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55077)[0;0m INFO 08-09 12:35:41 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][38/38] batch_size:1 query_len:128 num_blocks:0 free_mem:17.67 GiB
INFO 08-09 12:35:41 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][38/38] batch_size:1 query_len:128 num_blocks:0 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55075)[0;0m INFO 08-09 12:35:41 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][38/38] batch_size:1 query_len:128 num_blocks:0 free_mem:17.67 GiB
INFO 08-09 12:35:44 [hpu_model_runner.py:3029] [Warmup][Graph/decode][1/135] batch_size:256 query_len:1 num_blocks:9639 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55080)[0;0m INFO 08-09 12:35:44 [hpu_model_runner.py:3029] [Warmup][Graph/decode][1/135] batch_size:256 query_len:1 num_blocks:9639 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55081)[0;0m INFO 08-09 12:35:44 [hpu_model_runner.py:3029] [Warmup][Graph/decode][1/135] batch_size:256 query_len:1 num_blocks:9639 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55076)[0;0m INFO 08-09 12:35:44 [hpu_model_runner.py:3029] [Warmup][Graph/decode][1/135] batch_size:256 query_len:1 num_blocks:9639 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55079)[0;0m INFO 08-09 12:35:44 [hpu_model_runner.py:3029] [Warmup][Graph/decode][1/135] batch_size:256 query_len:1 num_blocks:9639 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55078)[0;0m INFO 08-09 12:35:44 [hpu_model_runner.py:3029] [Warmup][Graph/decode][1/135] batch_size:256 query_len:1 num_blocks:9639 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55077)[0;0m INFO 08-09 12:35:44 [hpu_model_runner.py:3029] [Warmup][Graph/decode][1/135] batch_size:256 query_len:1 num_blocks:9639 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55075)[0;0m INFO 08-09 12:35:44 [hpu_model_runner.py:3029] [Warmup][Graph/decode][1/135] batch_size:256 query_len:1 num_blocks:9639 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55081)[0;0m INFO 08-09 12:35:47 [hpu_model_runner.py:3029] [Warmup][Graph/decode][2/135] batch_size:256 query_len:1 num_blocks:7168 free_mem:17.67 GiB
INFO 08-09 12:35:47 [hpu_model_runner.py:3029] [Warmup][Graph/decode][2/135] batch_size:256 query_len:1 num_blocks:7168 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55080)[0;0m INFO 08-09 12:35:47 [hpu_model_runner.py:3029] [Warmup][Graph/decode][2/135] batch_size:256 query_len:1 num_blocks:7168 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55079)[0;0m INFO 08-09 12:35:47 [hpu_model_runner.py:3029] [Warmup][Graph/decode][2/135] batch_size:256 query_len:1 num_blocks:7168 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55076)[0;0m INFO 08-09 12:35:47 [hpu_model_runner.py:3029] [Warmup][Graph/decode][2/135] batch_size:256 query_len:1 num_blocks:7168 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55078)[0;0m INFO 08-09 12:35:47 [hpu_model_runner.py:3029] [Warmup][Graph/decode][2/135] batch_size:256 query_len:1 num_blocks:7168 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55075)[0;0m INFO 08-09 12:35:47 [hpu_model_runner.py:3029] [Warmup][Graph/decode][2/135] batch_size:256 query_len:1 num_blocks:7168 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55077)[0;0m INFO 08-09 12:35:47 [hpu_model_runner.py:3029] [Warmup][Graph/decode][2/135] batch_size:256 query_len:1 num_blocks:7168 free_mem:17.67 GiB
INFO 08-09 12:35:50 [hpu_model_runner.py:3029] [Warmup][Graph/decode][3/135] batch_size:256 query_len:1 num_blocks:5248 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55080)[0;0m INFO 08-09 12:35:50 [hpu_model_runner.py:3029] [Warmup][Graph/decode][3/135] batch_size:256 query_len:1 num_blocks:5248 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55081)[0;0m INFO 08-09 12:35:50 [hpu_model_runner.py:3029] [Warmup][Graph/decode][3/135] batch_size:256 query_len:1 num_blocks:5248 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55079)[0;0m INFO 08-09 12:35:50 [hpu_model_runner.py:3029] [Warmup][Graph/decode][3/135] batch_size:256 query_len:1 num_blocks:5248 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55078)[0;0m INFO 08-09 12:35:50 [hpu_model_runner.py:3029] [Warmup][Graph/decode][3/135] batch_size:256 query_len:1 num_blocks:5248 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55077)[0;0m INFO 08-09 12:35:50 [hpu_model_runner.py:3029] [Warmup][Graph/decode][3/135] batch_size:256 query_len:1 num_blocks:5248 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55076)[0;0m INFO 08-09 12:35:50 [hpu_model_runner.py:3029] [Warmup][Graph/decode][3/135] batch_size:256 query_len:1 num_blocks:5248 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55075)[0;0m INFO 08-09 12:35:50 [hpu_model_runner.py:3029] [Warmup][Graph/decode][3/135] batch_size:256 query_len:1 num_blocks:5248 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55080)[0;0m INFO 08-09 12:35:53 [hpu_model_runner.py:3029] [Warmup][Graph/decode][4/135] batch_size:256 query_len:1 num_blocks:3840 free_mem:17.67 GiB
INFO 08-09 12:35:53 [hpu_model_runner.py:3029] [Warmup][Graph/decode][4/135] batch_size:256 query_len:1 num_blocks:3840 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55081)[0;0m INFO 08-09 12:35:53 [hpu_model_runner.py:3029] [Warmup][Graph/decode][4/135] batch_size:256 query_len:1 num_blocks:3840 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55079)[0;0m INFO 08-09 12:35:53 [hpu_model_runner.py:3029] [Warmup][Graph/decode][4/135] batch_size:256 query_len:1 num_blocks:3840 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55077)[0;0m INFO 08-09 12:35:53 [hpu_model_runner.py:3029] [Warmup][Graph/decode][4/135] batch_size:256 query_len:1 num_blocks:3840 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55078)[0;0m INFO 08-09 12:35:53 [hpu_model_runner.py:3029] [Warmup][Graph/decode][4/135] batch_size:256 query_len:1 num_blocks:3840 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55076)[0;0m INFO 08-09 12:35:53 [hpu_model_runner.py:3029] [Warmup][Graph/decode][4/135] batch_size:256 query_len:1 num_blocks:3840 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55075)[0;0m INFO 08-09 12:35:53 [hpu_model_runner.py:3029] [Warmup][Graph/decode][4/135] batch_size:256 query_len:1 num_blocks:3840 free_mem:17.67 GiB
INFO 08-09 12:35:56 [hpu_model_runner.py:3029] [Warmup][Graph/decode][5/135] batch_size:256 query_len:1 num_blocks:2816 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55080)[0;0m INFO 08-09 12:35:56 [hpu_model_runner.py:3029] [Warmup][Graph/decode][5/135] batch_size:256 query_len:1 num_blocks:2816 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55081)[0;0m INFO 08-09 12:35:56 [hpu_model_runner.py:3029] [Warmup][Graph/decode][5/135] batch_size:256 query_len:1 num_blocks:2816 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55078)[0;0m INFO 08-09 12:35:56 [hpu_model_runner.py:3029] [Warmup][Graph/decode][5/135] batch_size:256 query_len:1 num_blocks:2816 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55079)[0;0m INFO 08-09 12:35:56 [hpu_model_runner.py:3029] [Warmup][Graph/decode][5/135] batch_size:256 query_len:1 num_blocks:2816 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55077)[0;0m INFO 08-09 12:35:56 [hpu_model_runner.py:3029] [Warmup][Graph/decode][5/135] batch_size:256 query_len:1 num_blocks:2816 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55076)[0;0m INFO 08-09 12:35:56 [hpu_model_runner.py:3029] [Warmup][Graph/decode][5/135] batch_size:256 query_len:1 num_blocks:2816 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55075)[0;0m INFO 08-09 12:35:56 [hpu_model_runner.py:3029] [Warmup][Graph/decode][5/135] batch_size:256 query_len:1 num_blocks:2816 free_mem:17.67 GiB
INFO 08-09 12:35:59 [hpu_model_runner.py:3029] [Warmup][Graph/decode][6/135] batch_size:256 query_len:1 num_blocks:2176 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55080)[0;0m INFO 08-09 12:35:59 [hpu_model_runner.py:3029] [Warmup][Graph/decode][6/135] batch_size:256 query_len:1 num_blocks:2176 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55081)[0;0m INFO 08-09 12:35:59 [hpu_model_runner.py:3029] [Warmup][Graph/decode][6/135] batch_size:256 query_len:1 num_blocks:2176 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55078)[0;0m INFO 08-09 12:35:59 [hpu_model_runner.py:3029] [Warmup][Graph/decode][6/135] batch_size:256 query_len:1 num_blocks:2176 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55079)[0;0m INFO 08-09 12:35:59 [hpu_model_runner.py:3029] [Warmup][Graph/decode][6/135] batch_size:256 query_len:1 num_blocks:2176 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55076)[0;0m INFO 08-09 12:35:59 [hpu_model_runner.py:3029] [Warmup][Graph/decode][6/135] batch_size:256 query_len:1 num_blocks:2176 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55077)[0;0m INFO 08-09 12:35:59 [hpu_model_runner.py:3029] [Warmup][Graph/decode][6/135] batch_size:256 query_len:1 num_blocks:2176 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55075)[0;0m INFO 08-09 12:35:59 [hpu_model_runner.py:3029] [Warmup][Graph/decode][6/135] batch_size:256 query_len:1 num_blocks:2176 free_mem:17.67 GiB
INFO 08-09 12:36:02 [hpu_model_runner.py:3029] [Warmup][Graph/decode][7/135] batch_size:256 query_len:1 num_blocks:1536 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55080)[0;0m INFO 08-09 12:36:02 [hpu_model_runner.py:3029] [Warmup][Graph/decode][7/135] batch_size:256 query_len:1 num_blocks:1536 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55077)[0;0m INFO 08-09 12:36:02 [hpu_model_runner.py:3029] [Warmup][Graph/decode][7/135] batch_size:256 query_len:1 num_blocks:1536 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55081)[0;0m INFO 08-09 12:36:02 [hpu_model_runner.py:3029] [Warmup][Graph/decode][7/135] batch_size:256 query_len:1 num_blocks:1536 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55079)[0;0m INFO 08-09 12:36:02 [hpu_model_runner.py:3029] [Warmup][Graph/decode][7/135] batch_size:256 query_len:1 num_blocks:1536 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55078)[0;0m INFO 08-09 12:36:02 [hpu_model_runner.py:3029] [Warmup][Graph/decode][7/135] batch_size:256 query_len:1 num_blocks:1536 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55075)[0;0m INFO 08-09 12:36:02 [hpu_model_runner.py:3029] [Warmup][Graph/decode][7/135] batch_size:256 query_len:1 num_blocks:1536 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55076)[0;0m INFO 08-09 12:36:02 [hpu_model_runner.py:3029] [Warmup][Graph/decode][7/135] batch_size:256 query_len:1 num_blocks:1536 free_mem:17.67 GiB
INFO 08-09 12:36:05 [hpu_model_runner.py:3029] [Warmup][Graph/decode][8/135] batch_size:256 query_len:1 num_blocks:1152 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55081)[0;0m INFO 08-09 12:36:05 [hpu_model_runner.py:3029] [Warmup][Graph/decode][8/135] batch_size:256 query_len:1 num_blocks:1152 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55080)[0;0m INFO 08-09 12:36:05 [hpu_model_runner.py:3029] [Warmup][Graph/decode][8/135] batch_size:256 query_len:1 num_blocks:1152 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55079)[0;0m INFO 08-09 12:36:05 [hpu_model_runner.py:3029] [Warmup][Graph/decode][8/135] batch_size:256 query_len:1 num_blocks:1152 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55076)[0;0m INFO 08-09 12:36:05 [hpu_model_runner.py:3029] [Warmup][Graph/decode][8/135] batch_size:256 query_len:1 num_blocks:1152 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55078)[0;0m INFO 08-09 12:36:05 [hpu_model_runner.py:3029] [Warmup][Graph/decode][8/135] batch_size:256 query_len:1 num_blocks:1152 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55077)[0;0m INFO 08-09 12:36:05 [hpu_model_runner.py:3029] [Warmup][Graph/decode][8/135] batch_size:256 query_len:1 num_blocks:1152 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55075)[0;0m INFO 08-09 12:36:05 [hpu_model_runner.py:3029] [Warmup][Graph/decode][8/135] batch_size:256 query_len:1 num_blocks:1152 free_mem:17.67 GiB
INFO 08-09 12:36:08 [hpu_model_runner.py:3029] [Warmup][Graph/decode][9/135] batch_size:256 query_len:1 num_blocks:896 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55081)[0;0m INFO 08-09 12:36:08 [hpu_model_runner.py:3029] [Warmup][Graph/decode][9/135] batch_size:256 query_len:1 num_blocks:896 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55080)[0;0m INFO 08-09 12:36:08 [hpu_model_runner.py:3029] [Warmup][Graph/decode][9/135] batch_size:256 query_len:1 num_blocks:896 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55079)[0;0m INFO 08-09 12:36:08 [hpu_model_runner.py:3029] [Warmup][Graph/decode][9/135] batch_size:256 query_len:1 num_blocks:896 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55076)[0;0m INFO 08-09 12:36:08 [hpu_model_runner.py:3029] [Warmup][Graph/decode][9/135] batch_size:256 query_len:1 num_blocks:896 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55078)[0;0m INFO 08-09 12:36:08 [hpu_model_runner.py:3029] [Warmup][Graph/decode][9/135] batch_size:256 query_len:1 num_blocks:896 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55077)[0;0m INFO 08-09 12:36:08 [hpu_model_runner.py:3029] [Warmup][Graph/decode][9/135] batch_size:256 query_len:1 num_blocks:896 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55075)[0;0m INFO 08-09 12:36:08 [hpu_model_runner.py:3029] [Warmup][Graph/decode][9/135] batch_size:256 query_len:1 num_blocks:896 free_mem:17.67 GiB
INFO 08-09 12:36:11 [hpu_model_runner.py:3029] [Warmup][Graph/decode][10/135] batch_size:256 query_len:1 num_blocks:768 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55081)[0;0m INFO 08-09 12:36:11 [hpu_model_runner.py:3029] [Warmup][Graph/decode][10/135] batch_size:256 query_len:1 num_blocks:768 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55080)[0;0m INFO 08-09 12:36:11 [hpu_model_runner.py:3029] [Warmup][Graph/decode][10/135] batch_size:256 query_len:1 num_blocks:768 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55079)[0;0m INFO 08-09 12:36:11 [hpu_model_runner.py:3029] [Warmup][Graph/decode][10/135] batch_size:256 query_len:1 num_blocks:768 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55075)[0;0m INFO 08-09 12:36:11 [hpu_model_runner.py:3029] [Warmup][Graph/decode][10/135] batch_size:256 query_len:1 num_blocks:768 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55078)[0;0m INFO 08-09 12:36:11 [hpu_model_runner.py:3029] [Warmup][Graph/decode][10/135] batch_size:256 query_len:1 num_blocks:768 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55077)[0;0m INFO 08-09 12:36:11 [hpu_model_runner.py:3029] [Warmup][Graph/decode][10/135] batch_size:256 query_len:1 num_blocks:768 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55076)[0;0m INFO 08-09 12:36:11 [hpu_model_runner.py:3029] [Warmup][Graph/decode][10/135] batch_size:256 query_len:1 num_blocks:768 free_mem:17.67 GiB
INFO 08-09 12:36:14 [hpu_model_runner.py:3029] [Warmup][Graph/decode][11/135] batch_size:256 query_len:1 num_blocks:640 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55081)[0;0m INFO 08-09 12:36:14 [hpu_model_runner.py:3029] [Warmup][Graph/decode][11/135] batch_size:256 query_len:1 num_blocks:640 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55079)[0;0m INFO 08-09 12:36:14 [hpu_model_runner.py:3029] [Warmup][Graph/decode][11/135] batch_size:256 query_len:1 num_blocks:640 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55077)[0;0m INFO 08-09 12:36:14 [hpu_model_runner.py:3029] [Warmup][Graph/decode][11/135] batch_size:256 query_len:1 num_blocks:640 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55080)[0;0m INFO 08-09 12:36:14 [hpu_model_runner.py:3029] [Warmup][Graph/decode][11/135] batch_size:256 query_len:1 num_blocks:640 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55075)[0;0m INFO 08-09 12:36:14 [hpu_model_runner.py:3029] [Warmup][Graph/decode][11/135] batch_size:256 query_len:1 num_blocks:640 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55078)[0;0m INFO 08-09 12:36:14 [hpu_model_runner.py:3029] [Warmup][Graph/decode][11/135] batch_size:256 query_len:1 num_blocks:640 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55076)[0;0m INFO 08-09 12:36:14 [hpu_model_runner.py:3029] [Warmup][Graph/decode][11/135] batch_size:256 query_len:1 num_blocks:640 free_mem:17.67 GiB
INFO 08-09 12:36:17 [hpu_model_runner.py:3029] [Warmup][Graph/decode][12/135] batch_size:256 query_len:1 num_blocks:512 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55081)[0;0m INFO 08-09 12:36:17 [hpu_model_runner.py:3029] [Warmup][Graph/decode][12/135] batch_size:256 query_len:1 num_blocks:512 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55080)[0;0m INFO 08-09 12:36:17 [hpu_model_runner.py:3029] [Warmup][Graph/decode][12/135] batch_size:256 query_len:1 num_blocks:512 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55079)[0;0m INFO 08-09 12:36:17 [hpu_model_runner.py:3029] [Warmup][Graph/decode][12/135] batch_size:256 query_len:1 num_blocks:512 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55078)[0;0m INFO 08-09 12:36:17 [hpu_model_runner.py:3029] [Warmup][Graph/decode][12/135] batch_size:256 query_len:1 num_blocks:512 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55077)[0;0m INFO 08-09 12:36:17 [hpu_model_runner.py:3029] [Warmup][Graph/decode][12/135] batch_size:256 query_len:1 num_blocks:512 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55076)[0;0m INFO 08-09 12:36:17 [hpu_model_runner.py:3029] [Warmup][Graph/decode][12/135] batch_size:256 query_len:1 num_blocks:512 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55075)[0;0m INFO 08-09 12:36:17 [hpu_model_runner.py:3029] [Warmup][Graph/decode][12/135] batch_size:256 query_len:1 num_blocks:512 free_mem:17.67 GiB
INFO 08-09 12:36:20 [hpu_model_runner.py:3029] [Warmup][Graph/decode][13/135] batch_size:256 query_len:1 num_blocks:384 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55080)[0;0m INFO 08-09 12:36:20 [hpu_model_runner.py:3029] [Warmup][Graph/decode][13/135] batch_size:256 query_len:1 num_blocks:384 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55081)[0;0m INFO 08-09 12:36:20 [hpu_model_runner.py:3029] [Warmup][Graph/decode][13/135] batch_size:256 query_len:1 num_blocks:384 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55077)[0;0m INFO 08-09 12:36:20 [hpu_model_runner.py:3029] [Warmup][Graph/decode][13/135] batch_size:256 query_len:1 num_blocks:384 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55078)[0;0m INFO 08-09 12:36:20 [hpu_model_runner.py:3029] [Warmup][Graph/decode][13/135] batch_size:256 query_len:1 num_blocks:384 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55075)[0;0m INFO 08-09 12:36:20 [hpu_model_runner.py:3029] [Warmup][Graph/decode][13/135] batch_size:256 query_len:1 num_blocks:384 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55076)[0;0m INFO 08-09 12:36:20 [hpu_model_runner.py:3029] [Warmup][Graph/decode][13/135] batch_size:256 query_len:1 num_blocks:384 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55079)[0;0m INFO 08-09 12:36:20 [hpu_model_runner.py:3029] [Warmup][Graph/decode][13/135] batch_size:256 query_len:1 num_blocks:384 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55081)[0;0m INFO 08-09 12:36:23 [hpu_model_runner.py:3029] [Warmup][Graph/decode][14/135] batch_size:256 query_len:1 num_blocks:256 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55080)[0;0m INFO 08-09 12:36:23 [hpu_model_runner.py:3029] [Warmup][Graph/decode][14/135] batch_size:256 query_len:1 num_blocks:256 free_mem:17.67 GiB
INFO 08-09 12:36:23 [hpu_model_runner.py:3029] [Warmup][Graph/decode][14/135] batch_size:256 query_len:1 num_blocks:256 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55079)[0;0m INFO 08-09 12:36:23 [hpu_model_runner.py:3029] [Warmup][Graph/decode][14/135] batch_size:256 query_len:1 num_blocks:256 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55077)[0;0m INFO 08-09 12:36:23 [hpu_model_runner.py:3029] [Warmup][Graph/decode][14/135] batch_size:256 query_len:1 num_blocks:256 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55078)[0;0m INFO 08-09 12:36:23 [hpu_model_runner.py:3029] [Warmup][Graph/decode][14/135] batch_size:256 query_len:1 num_blocks:256 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55076)[0;0m INFO 08-09 12:36:23 [hpu_model_runner.py:3029] [Warmup][Graph/decode][14/135] batch_size:256 query_len:1 num_blocks:256 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55075)[0;0m INFO 08-09 12:36:23 [hpu_model_runner.py:3029] [Warmup][Graph/decode][14/135] batch_size:256 query_len:1 num_blocks:256 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55081)[0;0m INFO 08-09 12:36:26 [hpu_model_runner.py:3029] [Warmup][Graph/decode][15/135] batch_size:256 query_len:1 num_blocks:128 free_mem:17.67 GiB
INFO 08-09 12:36:26 [hpu_model_runner.py:3029] [Warmup][Graph/decode][15/135] batch_size:256 query_len:1 num_blocks:128 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55080)[0;0m INFO 08-09 12:36:26 [hpu_model_runner.py:3029] [Warmup][Graph/decode][15/135] batch_size:256 query_len:1 num_blocks:128 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55078)[0;0m INFO 08-09 12:36:26 [hpu_model_runner.py:3029] [Warmup][Graph/decode][15/135] batch_size:256 query_len:1 num_blocks:128 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55075)[0;0m INFO 08-09 12:36:26 [hpu_model_runner.py:3029] [Warmup][Graph/decode][15/135] batch_size:256 query_len:1 num_blocks:128 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55079)[0;0m INFO 08-09 12:36:26 [hpu_model_runner.py:3029] [Warmup][Graph/decode][15/135] batch_size:256 query_len:1 num_blocks:128 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55077)[0;0m INFO 08-09 12:36:26 [hpu_model_runner.py:3029] [Warmup][Graph/decode][15/135] batch_size:256 query_len:1 num_blocks:128 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55076)[0;0m INFO 08-09 12:36:26 [hpu_model_runner.py:3029] [Warmup][Graph/decode][15/135] batch_size:256 query_len:1 num_blocks:128 free_mem:17.67 GiB
INFO 08-09 12:36:30 [hpu_model_runner.py:3029] [Warmup][Graph/decode][16/135] batch_size:128 query_len:1 num_blocks:9639 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55081)[0;0m INFO 08-09 12:36:30 [hpu_model_runner.py:3029] [Warmup][Graph/decode][16/135] batch_size:128 query_len:1 num_blocks:9639 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55080)[0;0m INFO 08-09 12:36:30 [hpu_model_runner.py:3029] [Warmup][Graph/decode][16/135] batch_size:128 query_len:1 num_blocks:9639 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55079)[0;0m INFO 08-09 12:36:30 [hpu_model_runner.py:3029] [Warmup][Graph/decode][16/135] batch_size:128 query_len:1 num_blocks:9639 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55078)[0;0m INFO 08-09 12:36:30 [hpu_model_runner.py:3029] [Warmup][Graph/decode][16/135] batch_size:128 query_len:1 num_blocks:9639 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55077)[0;0m INFO 08-09 12:36:30 [hpu_model_runner.py:3029] [Warmup][Graph/decode][16/135] batch_size:128 query_len:1 num_blocks:9639 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55075)[0;0m INFO 08-09 12:36:30 [hpu_model_runner.py:3029] [Warmup][Graph/decode][16/135] batch_size:128 query_len:1 num_blocks:9639 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55076)[0;0m INFO 08-09 12:36:30 [hpu_model_runner.py:3029] [Warmup][Graph/decode][16/135] batch_size:128 query_len:1 num_blocks:9639 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55080)[0;0m INFO 08-09 12:36:33 [hpu_model_runner.py:3029] [Warmup][Graph/decode][17/135] batch_size:128 query_len:1 num_blocks:7168 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55079)[0;0m INFO 08-09 12:36:33 [hpu_model_runner.py:3029] [Warmup][Graph/decode][17/135] batch_size:128 query_len:1 num_blocks:7168 free_mem:17.67 GiB
INFO 08-09 12:36:33 [hpu_model_runner.py:3029] [Warmup][Graph/decode][17/135] batch_size:128 query_len:1 num_blocks:7168 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55078)[0;0m INFO 08-09 12:36:33 [hpu_model_runner.py:3029] [Warmup][Graph/decode][17/135] batch_size:128 query_len:1 num_blocks:7168 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55081)[0;0m INFO 08-09 12:36:33 [hpu_model_runner.py:3029] [Warmup][Graph/decode][17/135] batch_size:128 query_len:1 num_blocks:7168 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55076)[0;0m INFO 08-09 12:36:33 [hpu_model_runner.py:3029] [Warmup][Graph/decode][17/135] batch_size:128 query_len:1 num_blocks:7168 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55075)[0;0m INFO 08-09 12:36:33 [hpu_model_runner.py:3029] [Warmup][Graph/decode][17/135] batch_size:128 query_len:1 num_blocks:7168 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55077)[0;0m INFO 08-09 12:36:33 [hpu_model_runner.py:3029] [Warmup][Graph/decode][17/135] batch_size:128 query_len:1 num_blocks:7168 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55081)[0;0m INFO 08-09 12:36:36 [hpu_model_runner.py:3029] [Warmup][Graph/decode][18/135] batch_size:128 query_len:1 num_blocks:5248 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55079)[0;0m INFO 08-09 12:36:36 [hpu_model_runner.py:3029] [Warmup][Graph/decode][18/135] batch_size:128 query_len:1 num_blocks:5248 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55076)[0;0m INFO 08-09 12:36:36 [hpu_model_runner.py:3029] [Warmup][Graph/decode][18/135] batch_size:128 query_len:1 num_blocks:5248 free_mem:17.67 GiB
INFO 08-09 12:36:36 [hpu_model_runner.py:3029] [Warmup][Graph/decode][18/135] batch_size:128 query_len:1 num_blocks:5248 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55080)[0;0m INFO 08-09 12:36:36 [hpu_model_runner.py:3029] [Warmup][Graph/decode][18/135] batch_size:128 query_len:1 num_blocks:5248 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55078)[0;0m INFO 08-09 12:36:36 [hpu_model_runner.py:3029] [Warmup][Graph/decode][18/135] batch_size:128 query_len:1 num_blocks:5248 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55077)[0;0m INFO 08-09 12:36:36 [hpu_model_runner.py:3029] [Warmup][Graph/decode][18/135] batch_size:128 query_len:1 num_blocks:5248 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55075)[0;0m INFO 08-09 12:36:36 [hpu_model_runner.py:3029] [Warmup][Graph/decode][18/135] batch_size:128 query_len:1 num_blocks:5248 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55081)[0;0m INFO 08-09 12:36:40 [hpu_model_runner.py:3029] [Warmup][Graph/decode][19/135] batch_size:128 query_len:1 num_blocks:3840 free_mem:17.67 GiB
INFO 08-09 12:36:40 [hpu_model_runner.py:3029] [Warmup][Graph/decode][19/135] batch_size:128 query_len:1 num_blocks:3840 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55080)[0;0m INFO 08-09 12:36:40 [hpu_model_runner.py:3029] [Warmup][Graph/decode][19/135] batch_size:128 query_len:1 num_blocks:3840 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55078)[0;0m INFO 08-09 12:36:40 [hpu_model_runner.py:3029] [Warmup][Graph/decode][19/135] batch_size:128 query_len:1 num_blocks:3840 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55079)[0;0m INFO 08-09 12:36:40 [hpu_model_runner.py:3029] [Warmup][Graph/decode][19/135] batch_size:128 query_len:1 num_blocks:3840 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55076)[0;0m INFO 08-09 12:36:40 [hpu_model_runner.py:3029] [Warmup][Graph/decode][19/135] batch_size:128 query_len:1 num_blocks:3840 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55075)[0;0m INFO 08-09 12:36:40 [hpu_model_runner.py:3029] [Warmup][Graph/decode][19/135] batch_size:128 query_len:1 num_blocks:3840 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55077)[0;0m INFO 08-09 12:36:40 [hpu_model_runner.py:3029] [Warmup][Graph/decode][19/135] batch_size:128 query_len:1 num_blocks:3840 free_mem:17.67 GiB
INFO 08-09 12:36:43 [hpu_model_runner.py:3029] [Warmup][Graph/decode][20/135] batch_size:128 query_len:1 num_blocks:2816 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55077)[0;0m INFO 08-09 12:36:43 [hpu_model_runner.py:3029] [Warmup][Graph/decode][20/135] batch_size:128 query_len:1 num_blocks:2816 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55078)[0;0m INFO 08-09 12:36:43 [hpu_model_runner.py:3029] [Warmup][Graph/decode][20/135] batch_size:128 query_len:1 num_blocks:2816 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55081)[0;0m INFO 08-09 12:36:43 [hpu_model_runner.py:3029] [Warmup][Graph/decode][20/135] batch_size:128 query_len:1 num_blocks:2816 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55080)[0;0m INFO 08-09 12:36:43 [hpu_model_runner.py:3029] [Warmup][Graph/decode][20/135] batch_size:128 query_len:1 num_blocks:2816 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55079)[0;0m INFO 08-09 12:36:43 [hpu_model_runner.py:3029] [Warmup][Graph/decode][20/135] batch_size:128 query_len:1 num_blocks:2816 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55076)[0;0m INFO 08-09 12:36:43 [hpu_model_runner.py:3029] [Warmup][Graph/decode][20/135] batch_size:128 query_len:1 num_blocks:2816 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55075)[0;0m INFO 08-09 12:36:43 [hpu_model_runner.py:3029] [Warmup][Graph/decode][20/135] batch_size:128 query_len:1 num_blocks:2816 free_mem:17.67 GiB
INFO 08-09 12:36:46 [hpu_model_runner.py:3029] [Warmup][Graph/decode][21/135] batch_size:128 query_len:1 num_blocks:2176 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55079)[0;0m INFO 08-09 12:36:46 [hpu_model_runner.py:3029] [Warmup][Graph/decode][21/135] batch_size:128 query_len:1 num_blocks:2176 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55080)[0;0m INFO 08-09 12:36:46 [hpu_model_runner.py:3029] [Warmup][Graph/decode][21/135] batch_size:128 query_len:1 num_blocks:2176 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55081)[0;0m INFO 08-09 12:36:46 [hpu_model_runner.py:3029] [Warmup][Graph/decode][21/135] batch_size:128 query_len:1 num_blocks:2176 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55078)[0;0m INFO 08-09 12:36:46 [hpu_model_runner.py:3029] [Warmup][Graph/decode][21/135] batch_size:128 query_len:1 num_blocks:2176 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55076)[0;0m INFO 08-09 12:36:46 [hpu_model_runner.py:3029] [Warmup][Graph/decode][21/135] batch_size:128 query_len:1 num_blocks:2176 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55077)[0;0m INFO 08-09 12:36:46 [hpu_model_runner.py:3029] [Warmup][Graph/decode][21/135] batch_size:128 query_len:1 num_blocks:2176 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55075)[0;0m INFO 08-09 12:36:46 [hpu_model_runner.py:3029] [Warmup][Graph/decode][21/135] batch_size:128 query_len:1 num_blocks:2176 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55080)[0;0m INFO 08-09 12:36:49 [hpu_model_runner.py:3029] [Warmup][Graph/decode][22/135] batch_size:128 query_len:1 num_blocks:1536 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55077)[0;0m INFO 08-09 12:36:49 [hpu_model_runner.py:3029] [Warmup][Graph/decode][22/135] batch_size:128 query_len:1 num_blocks:1536 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55078)[0;0m INFO 08-09 12:36:49 [hpu_model_runner.py:3029] [Warmup][Graph/decode][22/135] batch_size:128 query_len:1 num_blocks:1536 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55076)[0;0m INFO 08-09 12:36:49 [hpu_model_runner.py:3029] [Warmup][Graph/decode][22/135] batch_size:128 query_len:1 num_blocks:1536 free_mem:17.67 GiB
INFO 08-09 12:36:49 [hpu_model_runner.py:3029] [Warmup][Graph/decode][22/135] batch_size:128 query_len:1 num_blocks:1536 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55081)[0;0m INFO 08-09 12:36:49 [hpu_model_runner.py:3029] [Warmup][Graph/decode][22/135] batch_size:128 query_len:1 num_blocks:1536 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55079)[0;0m INFO 08-09 12:36:49 [hpu_model_runner.py:3029] [Warmup][Graph/decode][22/135] batch_size:128 query_len:1 num_blocks:1536 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55075)[0;0m INFO 08-09 12:36:49 [hpu_model_runner.py:3029] [Warmup][Graph/decode][22/135] batch_size:128 query_len:1 num_blocks:1536 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55080)[0;0m INFO 08-09 12:36:53 [hpu_model_runner.py:3029] [Warmup][Graph/decode][23/135] batch_size:128 query_len:1 num_blocks:1152 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55077)[0;0m INFO 08-09 12:36:53 [hpu_model_runner.py:3029] [Warmup][Graph/decode][23/135] batch_size:128 query_len:1 num_blocks:1152 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55076)[0;0m INFO 08-09 12:36:53 [hpu_model_runner.py:3029] [Warmup][Graph/decode][23/135] batch_size:128 query_len:1 num_blocks:1152 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55078)[0;0m INFO 08-09 12:36:53 [hpu_model_runner.py:3029] [Warmup][Graph/decode][23/135] batch_size:128 query_len:1 num_blocks:1152 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55075)[0;0m INFO 08-09 12:36:53 [hpu_model_runner.py:3029] [Warmup][Graph/decode][23/135] batch_size:128 query_len:1 num_blocks:1152 free_mem:17.67 GiB
INFO 08-09 12:36:53 [hpu_model_runner.py:3029] [Warmup][Graph/decode][23/135] batch_size:128 query_len:1 num_blocks:1152 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55081)[0;0m INFO 08-09 12:36:53 [hpu_model_runner.py:3029] [Warmup][Graph/decode][23/135] batch_size:128 query_len:1 num_blocks:1152 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55079)[0;0m INFO 08-09 12:36:53 [hpu_model_runner.py:3029] [Warmup][Graph/decode][23/135] batch_size:128 query_len:1 num_blocks:1152 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55080)[0;0m INFO 08-09 12:36:56 [hpu_model_runner.py:3029] [Warmup][Graph/decode][24/135] batch_size:128 query_len:1 num_blocks:896 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55078)[0;0m INFO 08-09 12:36:56 [hpu_model_runner.py:3029] [Warmup][Graph/decode][24/135] batch_size:128 query_len:1 num_blocks:896 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55076)[0;0m INFO 08-09 12:36:56 [hpu_model_runner.py:3029] [Warmup][Graph/decode][24/135] batch_size:128 query_len:1 num_blocks:896 free_mem:17.67 GiB
INFO 08-09 12:36:56 [hpu_model_runner.py:3029] [Warmup][Graph/decode][24/135] batch_size:128 query_len:1 num_blocks:896 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55081)[0;0m INFO 08-09 12:36:56 [hpu_model_runner.py:3029] [Warmup][Graph/decode][24/135] batch_size:128 query_len:1 num_blocks:896 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55079)[0;0m INFO 08-09 12:36:56 [hpu_model_runner.py:3029] [Warmup][Graph/decode][24/135] batch_size:128 query_len:1 num_blocks:896 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55077)[0;0m INFO 08-09 12:36:56 [hpu_model_runner.py:3029] [Warmup][Graph/decode][24/135] batch_size:128 query_len:1 num_blocks:896 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55075)[0;0m INFO 08-09 12:36:56 [hpu_model_runner.py:3029] [Warmup][Graph/decode][24/135] batch_size:128 query_len:1 num_blocks:896 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55080)[0;0m INFO 08-09 12:36:59 [hpu_model_runner.py:3029] [Warmup][Graph/decode][25/135] batch_size:128 query_len:1 num_blocks:768 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55079)[0;0m INFO 08-09 12:36:59 [hpu_model_runner.py:3029] [Warmup][Graph/decode][25/135] batch_size:128 query_len:1 num_blocks:768 free_mem:17.67 GiB
INFO 08-09 12:36:59 [hpu_model_runner.py:3029] [Warmup][Graph/decode][25/135] batch_size:128 query_len:1 num_blocks:768 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55078)[0;0m INFO 08-09 12:36:59 [hpu_model_runner.py:3029] [Warmup][Graph/decode][25/135] batch_size:128 query_len:1 num_blocks:768 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55081)[0;0m INFO 08-09 12:36:59 [hpu_model_runner.py:3029] [Warmup][Graph/decode][25/135] batch_size:128 query_len:1 num_blocks:768 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55076)[0;0m INFO 08-09 12:36:59 [hpu_model_runner.py:3029] [Warmup][Graph/decode][25/135] batch_size:128 query_len:1 num_blocks:768 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55077)[0;0m INFO 08-09 12:36:59 [hpu_model_runner.py:3029] [Warmup][Graph/decode][25/135] batch_size:128 query_len:1 num_blocks:768 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55075)[0;0m INFO 08-09 12:36:59 [hpu_model_runner.py:3029] [Warmup][Graph/decode][25/135] batch_size:128 query_len:1 num_blocks:768 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55081)[0;0m INFO 08-09 12:37:03 [hpu_model_runner.py:3029] [Warmup][Graph/decode][26/135] batch_size:128 query_len:1 num_blocks:640 free_mem:17.67 GiB
INFO 08-09 12:37:03 [hpu_model_runner.py:3029] [Warmup][Graph/decode][26/135] batch_size:128 query_len:1 num_blocks:640 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55080)[0;0m INFO 08-09 12:37:03 [hpu_model_runner.py:3029] [Warmup][Graph/decode][26/135] batch_size:128 query_len:1 num_blocks:640 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55078)[0;0m INFO 08-09 12:37:03 [hpu_model_runner.py:3029] [Warmup][Graph/decode][26/135] batch_size:128 query_len:1 num_blocks:640 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55079)[0;0m INFO 08-09 12:37:03 [hpu_model_runner.py:3029] [Warmup][Graph/decode][26/135] batch_size:128 query_len:1 num_blocks:640 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55076)[0;0m INFO 08-09 12:37:03 [hpu_model_runner.py:3029] [Warmup][Graph/decode][26/135] batch_size:128 query_len:1 num_blocks:640 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55077)[0;0m INFO 08-09 12:37:03 [hpu_model_runner.py:3029] [Warmup][Graph/decode][26/135] batch_size:128 query_len:1 num_blocks:640 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55075)[0;0m INFO 08-09 12:37:03 [hpu_model_runner.py:3029] [Warmup][Graph/decode][26/135] batch_size:128 query_len:1 num_blocks:640 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55081)[0;0m INFO 08-09 12:37:06 [hpu_model_runner.py:3029] [Warmup][Graph/decode][27/135] batch_size:128 query_len:1 num_blocks:512 free_mem:17.67 GiB
INFO 08-09 12:37:06 [hpu_model_runner.py:3029] [Warmup][Graph/decode][27/135] batch_size:128 query_len:1 num_blocks:512 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55080)[0;0m INFO 08-09 12:37:06 [hpu_model_runner.py:3029] [Warmup][Graph/decode][27/135] batch_size:128 query_len:1 num_blocks:512 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55078)[0;0m INFO 08-09 12:37:06 [hpu_model_runner.py:3029] [Warmup][Graph/decode][27/135] batch_size:128 query_len:1 num_blocks:512 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55077)[0;0m INFO 08-09 12:37:06 [hpu_model_runner.py:3029] [Warmup][Graph/decode][27/135] batch_size:128 query_len:1 num_blocks:512 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55079)[0;0m INFO 08-09 12:37:06 [hpu_model_runner.py:3029] [Warmup][Graph/decode][27/135] batch_size:128 query_len:1 num_blocks:512 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55075)[0;0m INFO 08-09 12:37:06 [hpu_model_runner.py:3029] [Warmup][Graph/decode][27/135] batch_size:128 query_len:1 num_blocks:512 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55076)[0;0m INFO 08-09 12:37:06 [hpu_model_runner.py:3029] [Warmup][Graph/decode][27/135] batch_size:128 query_len:1 num_blocks:512 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55078)[0;0m INFO 08-09 12:37:10 [hpu_model_runner.py:3029] [Warmup][Graph/decode][28/135] batch_size:128 query_len:1 num_blocks:384 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55075)[0;0m INFO 08-09 12:37:10 [hpu_model_runner.py:3029] [Warmup][Graph/decode][28/135] batch_size:128 query_len:1 num_blocks:384 free_mem:17.67 GiB
INFO 08-09 12:37:10 [hpu_model_runner.py:3029] [Warmup][Graph/decode][28/135] batch_size:128 query_len:1 num_blocks:384 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55081)[0;0m INFO 08-09 12:37:10 [hpu_model_runner.py:3029] [Warmup][Graph/decode][28/135] batch_size:128 query_len:1 num_blocks:384 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55076)[0;0m INFO 08-09 12:37:10 [hpu_model_runner.py:3029] [Warmup][Graph/decode][28/135] batch_size:128 query_len:1 num_blocks:384 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55080)[0;0m INFO 08-09 12:37:10 [hpu_model_runner.py:3029] [Warmup][Graph/decode][28/135] batch_size:128 query_len:1 num_blocks:384 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55079)[0;0m INFO 08-09 12:37:10 [hpu_model_runner.py:3029] [Warmup][Graph/decode][28/135] batch_size:128 query_len:1 num_blocks:384 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55077)[0;0m INFO 08-09 12:37:10 [hpu_model_runner.py:3029] [Warmup][Graph/decode][28/135] batch_size:128 query_len:1 num_blocks:384 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55081)[0;0m INFO 08-09 12:37:13 [hpu_model_runner.py:3029] [Warmup][Graph/decode][29/135] batch_size:128 query_len:1 num_blocks:256 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55078)[0;0m INFO 08-09 12:37:13 [hpu_model_runner.py:3029] [Warmup][Graph/decode][29/135] batch_size:128 query_len:1 num_blocks:256 free_mem:17.67 GiB
INFO 08-09 12:37:13 [hpu_model_runner.py:3029] [Warmup][Graph/decode][29/135] batch_size:128 query_len:1 num_blocks:256 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55080)[0;0m INFO 08-09 12:37:13 [hpu_model_runner.py:3029] [Warmup][Graph/decode][29/135] batch_size:128 query_len:1 num_blocks:256 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55079)[0;0m INFO 08-09 12:37:13 [hpu_model_runner.py:3029] [Warmup][Graph/decode][29/135] batch_size:128 query_len:1 num_blocks:256 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55076)[0;0m INFO 08-09 12:37:13 [hpu_model_runner.py:3029] [Warmup][Graph/decode][29/135] batch_size:128 query_len:1 num_blocks:256 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55077)[0;0m INFO 08-09 12:37:13 [hpu_model_runner.py:3029] [Warmup][Graph/decode][29/135] batch_size:128 query_len:1 num_blocks:256 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55075)[0;0m INFO 08-09 12:37:13 [hpu_model_runner.py:3029] [Warmup][Graph/decode][29/135] batch_size:128 query_len:1 num_blocks:256 free_mem:17.67 GiB
INFO 08-09 12:37:16 [hpu_model_runner.py:3029] [Warmup][Graph/decode][30/135] batch_size:128 query_len:1 num_blocks:128 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55081)[0;0m INFO 08-09 12:37:16 [hpu_model_runner.py:3029] [Warmup][Graph/decode][30/135] batch_size:128 query_len:1 num_blocks:128 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55080)[0;0m INFO 08-09 12:37:16 [hpu_model_runner.py:3029] [Warmup][Graph/decode][30/135] batch_size:128 query_len:1 num_blocks:128 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55078)[0;0m INFO 08-09 12:37:16 [hpu_model_runner.py:3029] [Warmup][Graph/decode][30/135] batch_size:128 query_len:1 num_blocks:128 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55079)[0;0m INFO 08-09 12:37:16 [hpu_model_runner.py:3029] [Warmup][Graph/decode][30/135] batch_size:128 query_len:1 num_blocks:128 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55077)[0;0m INFO 08-09 12:37:16 [hpu_model_runner.py:3029] [Warmup][Graph/decode][30/135] batch_size:128 query_len:1 num_blocks:128 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55076)[0;0m INFO 08-09 12:37:16 [hpu_model_runner.py:3029] [Warmup][Graph/decode][30/135] batch_size:128 query_len:1 num_blocks:128 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55075)[0;0m INFO 08-09 12:37:16 [hpu_model_runner.py:3029] [Warmup][Graph/decode][30/135] batch_size:128 query_len:1 num_blocks:128 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55081)[0;0m INFO 08-09 12:37:20 [hpu_model_runner.py:3029] [Warmup][Graph/decode][31/135] batch_size:64 query_len:1 num_blocks:9639 free_mem:17.67 GiB
INFO 08-09 12:37:20 [hpu_model_runner.py:3029] [Warmup][Graph/decode][31/135] batch_size:64 query_len:1 num_blocks:9639 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55079)[0;0m INFO 08-09 12:37:20 [hpu_model_runner.py:3029] [Warmup][Graph/decode][31/135] batch_size:64 query_len:1 num_blocks:9639 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55077)[0;0m INFO 08-09 12:37:20 [hpu_model_runner.py:3029] [Warmup][Graph/decode][31/135] batch_size:64 query_len:1 num_blocks:9639 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55080)[0;0m INFO 08-09 12:37:20 [hpu_model_runner.py:3029] [Warmup][Graph/decode][31/135] batch_size:64 query_len:1 num_blocks:9639 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55076)[0;0m INFO 08-09 12:37:20 [hpu_model_runner.py:3029] [Warmup][Graph/decode][31/135] batch_size:64 query_len:1 num_blocks:9639 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55078)[0;0m INFO 08-09 12:37:20 [hpu_model_runner.py:3029] [Warmup][Graph/decode][31/135] batch_size:64 query_len:1 num_blocks:9639 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55075)[0;0m INFO 08-09 12:37:20 [hpu_model_runner.py:3029] [Warmup][Graph/decode][31/135] batch_size:64 query_len:1 num_blocks:9639 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55081)[0;0m INFO 08-09 12:37:24 [hpu_model_runner.py:3029] [Warmup][Graph/decode][32/135] batch_size:64 query_len:1 num_blocks:7168 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55080)[0;0m INFO 08-09 12:37:24 [hpu_model_runner.py:3029] [Warmup][Graph/decode][32/135] batch_size:64 query_len:1 num_blocks:7168 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55079)[0;0m INFO 08-09 12:37:24 [hpu_model_runner.py:3029] [Warmup][Graph/decode][32/135] batch_size:64 query_len:1 num_blocks:7168 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55077)[0;0m INFO 08-09 12:37:24 [hpu_model_runner.py:3029] [Warmup][Graph/decode][32/135] batch_size:64 query_len:1 num_blocks:7168 free_mem:17.67 GiB
INFO 08-09 12:37:24 [hpu_model_runner.py:3029] [Warmup][Graph/decode][32/135] batch_size:64 query_len:1 num_blocks:7168 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55078)[0;0m INFO 08-09 12:37:24 [hpu_model_runner.py:3029] [Warmup][Graph/decode][32/135] batch_size:64 query_len:1 num_blocks:7168 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55075)[0;0m INFO 08-09 12:37:24 [hpu_model_runner.py:3029] [Warmup][Graph/decode][32/135] batch_size:64 query_len:1 num_blocks:7168 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55076)[0;0m INFO 08-09 12:37:24 [hpu_model_runner.py:3029] [Warmup][Graph/decode][32/135] batch_size:64 query_len:1 num_blocks:7168 free_mem:17.67 GiB
INFO 08-09 12:37:27 [hpu_model_runner.py:3029] [Warmup][Graph/decode][33/135] batch_size:64 query_len:1 num_blocks:5248 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55079)[0;0m INFO 08-09 12:37:27 [hpu_model_runner.py:3029] [Warmup][Graph/decode][33/135] batch_size:64 query_len:1 num_blocks:5248 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55076)[0;0m INFO 08-09 12:37:27 [hpu_model_runner.py:3029] [Warmup][Graph/decode][33/135] batch_size:64 query_len:1 num_blocks:5248 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55075)[0;0m INFO 08-09 12:37:27 [hpu_model_runner.py:3029] [Warmup][Graph/decode][33/135] batch_size:64 query_len:1 num_blocks:5248 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55081)[0;0m INFO 08-09 12:37:27 [hpu_model_runner.py:3029] [Warmup][Graph/decode][33/135] batch_size:64 query_len:1 num_blocks:5248 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55078)[0;0m INFO 08-09 12:37:27 [hpu_model_runner.py:3029] [Warmup][Graph/decode][33/135] batch_size:64 query_len:1 num_blocks:5248 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55080)[0;0m INFO 08-09 12:37:27 [hpu_model_runner.py:3029] [Warmup][Graph/decode][33/135] batch_size:64 query_len:1 num_blocks:5248 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55077)[0;0m INFO 08-09 12:37:27 [hpu_model_runner.py:3029] [Warmup][Graph/decode][33/135] batch_size:64 query_len:1 num_blocks:5248 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55081)[0;0m INFO 08-09 12:37:31 [hpu_model_runner.py:3029] [Warmup][Graph/decode][34/135] batch_size:64 query_len:1 num_blocks:3840 free_mem:17.67 GiB
INFO 08-09 12:37:31 [hpu_model_runner.py:3029] [Warmup][Graph/decode][34/135] batch_size:64 query_len:1 num_blocks:3840 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55080)[0;0m INFO 08-09 12:37:31 [hpu_model_runner.py:3029] [Warmup][Graph/decode][34/135] batch_size:64 query_len:1 num_blocks:3840 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55079)[0;0m INFO 08-09 12:37:31 [hpu_model_runner.py:3029] [Warmup][Graph/decode][34/135] batch_size:64 query_len:1 num_blocks:3840 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55077)[0;0m INFO 08-09 12:37:31 [hpu_model_runner.py:3029] [Warmup][Graph/decode][34/135] batch_size:64 query_len:1 num_blocks:3840 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55075)[0;0m INFO 08-09 12:37:31 [hpu_model_runner.py:3029] [Warmup][Graph/decode][34/135] batch_size:64 query_len:1 num_blocks:3840 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55078)[0;0m INFO 08-09 12:37:31 [hpu_model_runner.py:3029] [Warmup][Graph/decode][34/135] batch_size:64 query_len:1 num_blocks:3840 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55076)[0;0m INFO 08-09 12:37:31 [hpu_model_runner.py:3029] [Warmup][Graph/decode][34/135] batch_size:64 query_len:1 num_blocks:3840 free_mem:17.67 GiB
INFO 08-09 12:37:35 [hpu_model_runner.py:3029] [Warmup][Graph/decode][35/135] batch_size:64 query_len:1 num_blocks:2816 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55079)[0;0m INFO 08-09 12:37:35 [hpu_model_runner.py:3029] [Warmup][Graph/decode][35/135] batch_size:64 query_len:1 num_blocks:2816 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55080)[0;0m INFO 08-09 12:37:35 [hpu_model_runner.py:3029] [Warmup][Graph/decode][35/135] batch_size:64 query_len:1 num_blocks:2816 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55081)[0;0m INFO 08-09 12:37:35 [hpu_model_runner.py:3029] [Warmup][Graph/decode][35/135] batch_size:64 query_len:1 num_blocks:2816 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55077)[0;0m INFO 08-09 12:37:35 [hpu_model_runner.py:3029] [Warmup][Graph/decode][35/135] batch_size:64 query_len:1 num_blocks:2816 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55076)[0;0m INFO 08-09 12:37:35 [hpu_model_runner.py:3029] [Warmup][Graph/decode][35/135] batch_size:64 query_len:1 num_blocks:2816 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55078)[0;0m INFO 08-09 12:37:35 [hpu_model_runner.py:3029] [Warmup][Graph/decode][35/135] batch_size:64 query_len:1 num_blocks:2816 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55075)[0;0m INFO 08-09 12:37:35 [hpu_model_runner.py:3029] [Warmup][Graph/decode][35/135] batch_size:64 query_len:1 num_blocks:2816 free_mem:17.67 GiB
INFO 08-09 12:37:38 [hpu_model_runner.py:3029] [Warmup][Graph/decode][36/135] batch_size:64 query_len:1 num_blocks:2176 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55080)[0;0m INFO 08-09 12:37:38 [hpu_model_runner.py:3029] [Warmup][Graph/decode][36/135] batch_size:64 query_len:1 num_blocks:2176 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55081)[0;0m INFO 08-09 12:37:38 [hpu_model_runner.py:3029] [Warmup][Graph/decode][36/135] batch_size:64 query_len:1 num_blocks:2176 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55078)[0;0m INFO 08-09 12:37:38 [hpu_model_runner.py:3029] [Warmup][Graph/decode][36/135] batch_size:64 query_len:1 num_blocks:2176 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55079)[0;0m INFO 08-09 12:37:38 [hpu_model_runner.py:3029] [Warmup][Graph/decode][36/135] batch_size:64 query_len:1 num_blocks:2176 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55077)[0;0m INFO 08-09 12:37:38 [hpu_model_runner.py:3029] [Warmup][Graph/decode][36/135] batch_size:64 query_len:1 num_blocks:2176 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55075)[0;0m INFO 08-09 12:37:38 [hpu_model_runner.py:3029] [Warmup][Graph/decode][36/135] batch_size:64 query_len:1 num_blocks:2176 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55076)[0;0m INFO 08-09 12:37:38 [hpu_model_runner.py:3029] [Warmup][Graph/decode][36/135] batch_size:64 query_len:1 num_blocks:2176 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55081)[0;0m INFO 08-09 12:37:42 [hpu_model_runner.py:3029] [Warmup][Graph/decode][37/135] batch_size:64 query_len:1 num_blocks:1536 free_mem:17.67 GiB
INFO 08-09 12:37:42 [hpu_model_runner.py:3029] [Warmup][Graph/decode][37/135] batch_size:64 query_len:1 num_blocks:1536 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55080)[0;0m INFO 08-09 12:37:42 [hpu_model_runner.py:3029] [Warmup][Graph/decode][37/135] batch_size:64 query_len:1 num_blocks:1536 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55079)[0;0m INFO 08-09 12:37:42 [hpu_model_runner.py:3029] [Warmup][Graph/decode][37/135] batch_size:64 query_len:1 num_blocks:1536 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55078)[0;0m INFO 08-09 12:37:42 [hpu_model_runner.py:3029] [Warmup][Graph/decode][37/135] batch_size:64 query_len:1 num_blocks:1536 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55077)[0;0m INFO 08-09 12:37:42 [hpu_model_runner.py:3029] [Warmup][Graph/decode][37/135] batch_size:64 query_len:1 num_blocks:1536 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55075)[0;0m INFO 08-09 12:37:42 [hpu_model_runner.py:3029] [Warmup][Graph/decode][37/135] batch_size:64 query_len:1 num_blocks:1536 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55076)[0;0m INFO 08-09 12:37:42 [hpu_model_runner.py:3029] [Warmup][Graph/decode][37/135] batch_size:64 query_len:1 num_blocks:1536 free_mem:17.67 GiB
INFO 08-09 12:37:45 [hpu_model_runner.py:3029] [Warmup][Graph/decode][38/135] batch_size:64 query_len:1 num_blocks:1152 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55078)[0;0m INFO 08-09 12:37:45 [hpu_model_runner.py:3029] [Warmup][Graph/decode][38/135] batch_size:64 query_len:1 num_blocks:1152 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55076)[0;0m INFO 08-09 12:37:45 [hpu_model_runner.py:3029] [Warmup][Graph/decode][38/135] batch_size:64 query_len:1 num_blocks:1152 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55075)[0;0m INFO 08-09 12:37:45 [hpu_model_runner.py:3029] [Warmup][Graph/decode][38/135] batch_size:64 query_len:1 num_blocks:1152 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55081)[0;0m INFO 08-09 12:37:45 [hpu_model_runner.py:3029] [Warmup][Graph/decode][38/135] batch_size:64 query_len:1 num_blocks:1152 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55079)[0;0m INFO 08-09 12:37:45 [hpu_model_runner.py:3029] [Warmup][Graph/decode][38/135] batch_size:64 query_len:1 num_blocks:1152 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55080)[0;0m INFO 08-09 12:37:45 [hpu_model_runner.py:3029] [Warmup][Graph/decode][38/135] batch_size:64 query_len:1 num_blocks:1152 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55077)[0;0m INFO 08-09 12:37:45 [hpu_model_runner.py:3029] [Warmup][Graph/decode][38/135] batch_size:64 query_len:1 num_blocks:1152 free_mem:17.67 GiB
INFO 08-09 12:37:49 [hpu_model_runner.py:3029] [Warmup][Graph/decode][39/135] batch_size:64 query_len:1 num_blocks:896 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55080)[0;0m INFO 08-09 12:37:49 [hpu_model_runner.py:3029] [Warmup][Graph/decode][39/135] batch_size:64 query_len:1 num_blocks:896 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55079)[0;0m INFO 08-09 12:37:49 [hpu_model_runner.py:3029] [Warmup][Graph/decode][39/135] batch_size:64 query_len:1 num_blocks:896 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55078)[0;0m INFO 08-09 12:37:49 [hpu_model_runner.py:3029] [Warmup][Graph/decode][39/135] batch_size:64 query_len:1 num_blocks:896 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55075)[0;0m INFO 08-09 12:37:49 [hpu_model_runner.py:3029] [Warmup][Graph/decode][39/135] batch_size:64 query_len:1 num_blocks:896 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55081)[0;0m INFO 08-09 12:37:49 [hpu_model_runner.py:3029] [Warmup][Graph/decode][39/135] batch_size:64 query_len:1 num_blocks:896 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55077)[0;0m INFO 08-09 12:37:49 [hpu_model_runner.py:3029] [Warmup][Graph/decode][39/135] batch_size:64 query_len:1 num_blocks:896 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55076)[0;0m INFO 08-09 12:37:49 [hpu_model_runner.py:3029] [Warmup][Graph/decode][39/135] batch_size:64 query_len:1 num_blocks:896 free_mem:17.67 GiB
INFO 08-09 12:37:53 [hpu_model_runner.py:3029] [Warmup][Graph/decode][40/135] batch_size:64 query_len:1 num_blocks:768 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55081)[0;0m INFO 08-09 12:37:53 [hpu_model_runner.py:3029] [Warmup][Graph/decode][40/135] batch_size:64 query_len:1 num_blocks:768 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55080)[0;0m INFO 08-09 12:37:53 [hpu_model_runner.py:3029] [Warmup][Graph/decode][40/135] batch_size:64 query_len:1 num_blocks:768 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55079)[0;0m INFO 08-09 12:37:53 [hpu_model_runner.py:3029] [Warmup][Graph/decode][40/135] batch_size:64 query_len:1 num_blocks:768 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55078)[0;0m INFO 08-09 12:37:53 [hpu_model_runner.py:3029] [Warmup][Graph/decode][40/135] batch_size:64 query_len:1 num_blocks:768 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55077)[0;0m INFO 08-09 12:37:53 [hpu_model_runner.py:3029] [Warmup][Graph/decode][40/135] batch_size:64 query_len:1 num_blocks:768 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55076)[0;0m INFO 08-09 12:37:53 [hpu_model_runner.py:3029] [Warmup][Graph/decode][40/135] batch_size:64 query_len:1 num_blocks:768 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55075)[0;0m INFO 08-09 12:37:53 [hpu_model_runner.py:3029] [Warmup][Graph/decode][40/135] batch_size:64 query_len:1 num_blocks:768 free_mem:17.67 GiB
INFO 08-09 12:37:56 [hpu_model_runner.py:3029] [Warmup][Graph/decode][41/135] batch_size:64 query_len:1 num_blocks:640 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55081)[0;0m INFO 08-09 12:37:56 [hpu_model_runner.py:3029] [Warmup][Graph/decode][41/135] batch_size:64 query_len:1 num_blocks:640 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55079)[0;0m INFO 08-09 12:37:56 [hpu_model_runner.py:3029] [Warmup][Graph/decode][41/135] batch_size:64 query_len:1 num_blocks:640 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55080)[0;0m INFO 08-09 12:37:56 [hpu_model_runner.py:3029] [Warmup][Graph/decode][41/135] batch_size:64 query_len:1 num_blocks:640 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55078)[0;0m INFO 08-09 12:37:56 [hpu_model_runner.py:3029] [Warmup][Graph/decode][41/135] batch_size:64 query_len:1 num_blocks:640 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55076)[0;0m INFO 08-09 12:37:56 [hpu_model_runner.py:3029] [Warmup][Graph/decode][41/135] batch_size:64 query_len:1 num_blocks:640 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55077)[0;0m INFO 08-09 12:37:56 [hpu_model_runner.py:3029] [Warmup][Graph/decode][41/135] batch_size:64 query_len:1 num_blocks:640 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55075)[0;0m INFO 08-09 12:37:56 [hpu_model_runner.py:3029] [Warmup][Graph/decode][41/135] batch_size:64 query_len:1 num_blocks:640 free_mem:17.67 GiB
INFO 08-09 12:38:00 [hpu_model_runner.py:3029] [Warmup][Graph/decode][42/135] batch_size:64 query_len:1 num_blocks:512 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55081)[0;0m INFO 08-09 12:38:00 [hpu_model_runner.py:3029] [Warmup][Graph/decode][42/135] batch_size:64 query_len:1 num_blocks:512 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55080)[0;0m INFO 08-09 12:38:00 [hpu_model_runner.py:3029] [Warmup][Graph/decode][42/135] batch_size:64 query_len:1 num_blocks:512 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55079)[0;0m INFO 08-09 12:38:00 [hpu_model_runner.py:3029] [Warmup][Graph/decode][42/135] batch_size:64 query_len:1 num_blocks:512 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55078)[0;0m INFO 08-09 12:38:00 [hpu_model_runner.py:3029] [Warmup][Graph/decode][42/135] batch_size:64 query_len:1 num_blocks:512 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55076)[0;0m INFO 08-09 12:38:00 [hpu_model_runner.py:3029] [Warmup][Graph/decode][42/135] batch_size:64 query_len:1 num_blocks:512 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55077)[0;0m INFO 08-09 12:38:00 [hpu_model_runner.py:3029] [Warmup][Graph/decode][42/135] batch_size:64 query_len:1 num_blocks:512 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55075)[0;0m INFO 08-09 12:38:00 [hpu_model_runner.py:3029] [Warmup][Graph/decode][42/135] batch_size:64 query_len:1 num_blocks:512 free_mem:17.67 GiB
INFO 08-09 12:38:04 [hpu_model_runner.py:3029] [Warmup][Graph/decode][43/135] batch_size:64 query_len:1 num_blocks:384 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55081)[0;0m INFO 08-09 12:38:04 [hpu_model_runner.py:3029] [Warmup][Graph/decode][43/135] batch_size:64 query_len:1 num_blocks:384 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55079)[0;0m INFO 08-09 12:38:04 [hpu_model_runner.py:3029] [Warmup][Graph/decode][43/135] batch_size:64 query_len:1 num_blocks:384 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55080)[0;0m INFO 08-09 12:38:04 [hpu_model_runner.py:3029] [Warmup][Graph/decode][43/135] batch_size:64 query_len:1 num_blocks:384 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55078)[0;0m INFO 08-09 12:38:04 [hpu_model_runner.py:3029] [Warmup][Graph/decode][43/135] batch_size:64 query_len:1 num_blocks:384 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55077)[0;0m INFO 08-09 12:38:04 [hpu_model_runner.py:3029] [Warmup][Graph/decode][43/135] batch_size:64 query_len:1 num_blocks:384 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55075)[0;0m INFO 08-09 12:38:04 [hpu_model_runner.py:3029] [Warmup][Graph/decode][43/135] batch_size:64 query_len:1 num_blocks:384 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55076)[0;0m INFO 08-09 12:38:04 [hpu_model_runner.py:3029] [Warmup][Graph/decode][43/135] batch_size:64 query_len:1 num_blocks:384 free_mem:17.67 GiB
INFO 08-09 12:38:08 [hpu_model_runner.py:3029] [Warmup][Graph/decode][44/135] batch_size:64 query_len:1 num_blocks:256 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55081)[0;0m INFO 08-09 12:38:08 [hpu_model_runner.py:3029] [Warmup][Graph/decode][44/135] batch_size:64 query_len:1 num_blocks:256 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55079)[0;0m INFO 08-09 12:38:08 [hpu_model_runner.py:3029] [Warmup][Graph/decode][44/135] batch_size:64 query_len:1 num_blocks:256 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55076)[0;0m INFO 08-09 12:38:08 [hpu_model_runner.py:3029] [Warmup][Graph/decode][44/135] batch_size:64 query_len:1 num_blocks:256 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55078)[0;0m INFO 08-09 12:38:08 [hpu_model_runner.py:3029] [Warmup][Graph/decode][44/135] batch_size:64 query_len:1 num_blocks:256 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55077)[0;0m INFO 08-09 12:38:08 [hpu_model_runner.py:3029] [Warmup][Graph/decode][44/135] batch_size:64 query_len:1 num_blocks:256 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55080)[0;0m INFO 08-09 12:38:08 [hpu_model_runner.py:3029] [Warmup][Graph/decode][44/135] batch_size:64 query_len:1 num_blocks:256 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55075)[0;0m INFO 08-09 12:38:08 [hpu_model_runner.py:3029] [Warmup][Graph/decode][44/135] batch_size:64 query_len:1 num_blocks:256 free_mem:17.67 GiB
INFO 08-09 12:38:11 [hpu_model_runner.py:3029] [Warmup][Graph/decode][45/135] batch_size:64 query_len:1 num_blocks:128 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55081)[0;0m INFO 08-09 12:38:11 [hpu_model_runner.py:3029] [Warmup][Graph/decode][45/135] batch_size:64 query_len:1 num_blocks:128 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55080)[0;0m INFO 08-09 12:38:11 [hpu_model_runner.py:3029] [Warmup][Graph/decode][45/135] batch_size:64 query_len:1 num_blocks:128 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55079)[0;0m INFO 08-09 12:38:11 [hpu_model_runner.py:3029] [Warmup][Graph/decode][45/135] batch_size:64 query_len:1 num_blocks:128 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55076)[0;0m INFO 08-09 12:38:11 [hpu_model_runner.py:3029] [Warmup][Graph/decode][45/135] batch_size:64 query_len:1 num_blocks:128 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55078)[0;0m INFO 08-09 12:38:11 [hpu_model_runner.py:3029] [Warmup][Graph/decode][45/135] batch_size:64 query_len:1 num_blocks:128 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55077)[0;0m INFO 08-09 12:38:11 [hpu_model_runner.py:3029] [Warmup][Graph/decode][45/135] batch_size:64 query_len:1 num_blocks:128 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55075)[0;0m INFO 08-09 12:38:11 [hpu_model_runner.py:3029] [Warmup][Graph/decode][45/135] batch_size:64 query_len:1 num_blocks:128 free_mem:17.67 GiB
INFO 08-09 12:38:15 [hpu_model_runner.py:3029] [Warmup][Graph/decode][46/135] batch_size:32 query_len:1 num_blocks:9639 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55081)[0;0m INFO 08-09 12:38:15 [hpu_model_runner.py:3029] [Warmup][Graph/decode][46/135] batch_size:32 query_len:1 num_blocks:9639 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55079)[0;0m INFO 08-09 12:38:15 [hpu_model_runner.py:3029] [Warmup][Graph/decode][46/135] batch_size:32 query_len:1 num_blocks:9639 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55080)[0;0m INFO 08-09 12:38:15 [hpu_model_runner.py:3029] [Warmup][Graph/decode][46/135] batch_size:32 query_len:1 num_blocks:9639 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55078)[0;0m INFO 08-09 12:38:15 [hpu_model_runner.py:3029] [Warmup][Graph/decode][46/135] batch_size:32 query_len:1 num_blocks:9639 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55075)[0;0m INFO 08-09 12:38:15 [hpu_model_runner.py:3029] [Warmup][Graph/decode][46/135] batch_size:32 query_len:1 num_blocks:9639 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55077)[0;0m INFO 08-09 12:38:15 [hpu_model_runner.py:3029] [Warmup][Graph/decode][46/135] batch_size:32 query_len:1 num_blocks:9639 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55076)[0;0m INFO 08-09 12:38:15 [hpu_model_runner.py:3029] [Warmup][Graph/decode][46/135] batch_size:32 query_len:1 num_blocks:9639 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55079)[0;0m INFO 08-09 12:38:20 [hpu_model_runner.py:3029] [Warmup][Graph/decode][47/135] batch_size:32 query_len:1 num_blocks:7168 free_mem:17.67 GiB
INFO 08-09 12:38:20 [hpu_model_runner.py:3029] [Warmup][Graph/decode][47/135] batch_size:32 query_len:1 num_blocks:7168 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55075)[0;0m INFO 08-09 12:38:20 [hpu_model_runner.py:3029] [Warmup][Graph/decode][47/135] batch_size:32 query_len:1 num_blocks:7168 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55081)[0;0m INFO 08-09 12:38:20 [hpu_model_runner.py:3029] [Warmup][Graph/decode][47/135] batch_size:32 query_len:1 num_blocks:7168 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55080)[0;0m INFO 08-09 12:38:20 [hpu_model_runner.py:3029] [Warmup][Graph/decode][47/135] batch_size:32 query_len:1 num_blocks:7168 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55078)[0;0m INFO 08-09 12:38:20 [hpu_model_runner.py:3029] [Warmup][Graph/decode][47/135] batch_size:32 query_len:1 num_blocks:7168 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55077)[0;0m INFO 08-09 12:38:20 [hpu_model_runner.py:3029] [Warmup][Graph/decode][47/135] batch_size:32 query_len:1 num_blocks:7168 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55076)[0;0m INFO 08-09 12:38:20 [hpu_model_runner.py:3029] [Warmup][Graph/decode][47/135] batch_size:32 query_len:1 num_blocks:7168 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55081)[0;0m INFO 08-09 12:38:24 [hpu_model_runner.py:3029] [Warmup][Graph/decode][48/135] batch_size:32 query_len:1 num_blocks:5248 free_mem:17.67 GiB
INFO 08-09 12:38:24 [hpu_model_runner.py:3029] [Warmup][Graph/decode][48/135] batch_size:32 query_len:1 num_blocks:5248 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55079)[0;0m INFO 08-09 12:38:24 [hpu_model_runner.py:3029] [Warmup][Graph/decode][48/135] batch_size:32 query_len:1 num_blocks:5248 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55075)[0;0m INFO 08-09 12:38:24 [hpu_model_runner.py:3029] [Warmup][Graph/decode][48/135] batch_size:32 query_len:1 num_blocks:5248 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55077)[0;0m INFO 08-09 12:38:24 [hpu_model_runner.py:3029] [Warmup][Graph/decode][48/135] batch_size:32 query_len:1 num_blocks:5248 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55080)[0;0m INFO 08-09 12:38:24 [hpu_model_runner.py:3029] [Warmup][Graph/decode][48/135] batch_size:32 query_len:1 num_blocks:5248 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55078)[0;0m INFO 08-09 12:38:24 [hpu_model_runner.py:3029] [Warmup][Graph/decode][48/135] batch_size:32 query_len:1 num_blocks:5248 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55076)[0;0m INFO 08-09 12:38:24 [hpu_model_runner.py:3029] [Warmup][Graph/decode][48/135] batch_size:32 query_len:1 num_blocks:5248 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55080)[0;0m INFO 08-09 12:38:27 [hpu_model_runner.py:3029] [Warmup][Graph/decode][49/135] batch_size:32 query_len:1 num_blocks:3840 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55079)[0;0m INFO 08-09 12:38:27 [hpu_model_runner.py:3029] [Warmup][Graph/decode][49/135] batch_size:32 query_len:1 num_blocks:3840 free_mem:17.67 GiB
INFO 08-09 12:38:27 [hpu_model_runner.py:3029] [Warmup][Graph/decode][49/135] batch_size:32 query_len:1 num_blocks:3840 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55081)[0;0m INFO 08-09 12:38:27 [hpu_model_runner.py:3029] [Warmup][Graph/decode][49/135] batch_size:32 query_len:1 num_blocks:3840 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55077)[0;0m INFO 08-09 12:38:27 [hpu_model_runner.py:3029] [Warmup][Graph/decode][49/135] batch_size:32 query_len:1 num_blocks:3840 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55075)[0;0m INFO 08-09 12:38:27 [hpu_model_runner.py:3029] [Warmup][Graph/decode][49/135] batch_size:32 query_len:1 num_blocks:3840 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55078)[0;0m INFO 08-09 12:38:27 [hpu_model_runner.py:3029] [Warmup][Graph/decode][49/135] batch_size:32 query_len:1 num_blocks:3840 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55076)[0;0m INFO 08-09 12:38:27 [hpu_model_runner.py:3029] [Warmup][Graph/decode][49/135] batch_size:32 query_len:1 num_blocks:3840 free_mem:17.67 GiB
INFO 08-09 12:38:31 [hpu_model_runner.py:3029] [Warmup][Graph/decode][50/135] batch_size:32 query_len:1 num_blocks:2816 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55080)[0;0m INFO 08-09 12:38:31 [hpu_model_runner.py:3029] [Warmup][Graph/decode][50/135] batch_size:32 query_len:1 num_blocks:2816 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55081)[0;0m INFO 08-09 12:38:31 [hpu_model_runner.py:3029] [Warmup][Graph/decode][50/135] batch_size:32 query_len:1 num_blocks:2816 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55077)[0;0m INFO 08-09 12:38:31 [hpu_model_runner.py:3029] [Warmup][Graph/decode][50/135] batch_size:32 query_len:1 num_blocks:2816 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55078)[0;0m INFO 08-09 12:38:31 [hpu_model_runner.py:3029] [Warmup][Graph/decode][50/135] batch_size:32 query_len:1 num_blocks:2816 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55079)[0;0m INFO 08-09 12:38:31 [hpu_model_runner.py:3029] [Warmup][Graph/decode][50/135] batch_size:32 query_len:1 num_blocks:2816 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55075)[0;0m INFO 08-09 12:38:31 [hpu_model_runner.py:3029] [Warmup][Graph/decode][50/135] batch_size:32 query_len:1 num_blocks:2816 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55076)[0;0m INFO 08-09 12:38:31 [hpu_model_runner.py:3029] [Warmup][Graph/decode][50/135] batch_size:32 query_len:1 num_blocks:2816 free_mem:17.67 GiB
INFO 08-09 12:38:35 [hpu_model_runner.py:3029] [Warmup][Graph/decode][51/135] batch_size:32 query_len:1 num_blocks:2176 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55080)[0;0m INFO 08-09 12:38:35 [hpu_model_runner.py:3029] [Warmup][Graph/decode][51/135] batch_size:32 query_len:1 num_blocks:2176 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55081)[0;0m INFO 08-09 12:38:35 [hpu_model_runner.py:3029] [Warmup][Graph/decode][51/135] batch_size:32 query_len:1 num_blocks:2176 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55077)[0;0m INFO 08-09 12:38:35 [hpu_model_runner.py:3029] [Warmup][Graph/decode][51/135] batch_size:32 query_len:1 num_blocks:2176 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55078)[0;0m INFO 08-09 12:38:35 [hpu_model_runner.py:3029] [Warmup][Graph/decode][51/135] batch_size:32 query_len:1 num_blocks:2176 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55079)[0;0m INFO 08-09 12:38:35 [hpu_model_runner.py:3029] [Warmup][Graph/decode][51/135] batch_size:32 query_len:1 num_blocks:2176 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55075)[0;0m INFO 08-09 12:38:35 [hpu_model_runner.py:3029] [Warmup][Graph/decode][51/135] batch_size:32 query_len:1 num_blocks:2176 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55076)[0;0m INFO 08-09 12:38:35 [hpu_model_runner.py:3029] [Warmup][Graph/decode][51/135] batch_size:32 query_len:1 num_blocks:2176 free_mem:17.67 GiB
INFO 08-09 12:38:39 [hpu_model_runner.py:3029] [Warmup][Graph/decode][52/135] batch_size:32 query_len:1 num_blocks:1536 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55081)[0;0m INFO 08-09 12:38:39 [hpu_model_runner.py:3029] [Warmup][Graph/decode][52/135] batch_size:32 query_len:1 num_blocks:1536 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55079)[0;0m INFO 08-09 12:38:39 [hpu_model_runner.py:3029] [Warmup][Graph/decode][52/135] batch_size:32 query_len:1 num_blocks:1536 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55080)[0;0m INFO 08-09 12:38:39 [hpu_model_runner.py:3029] [Warmup][Graph/decode][52/135] batch_size:32 query_len:1 num_blocks:1536 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55078)[0;0m INFO 08-09 12:38:39 [hpu_model_runner.py:3029] [Warmup][Graph/decode][52/135] batch_size:32 query_len:1 num_blocks:1536 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55077)[0;0m INFO 08-09 12:38:39 [hpu_model_runner.py:3029] [Warmup][Graph/decode][52/135] batch_size:32 query_len:1 num_blocks:1536 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55076)[0;0m INFO 08-09 12:38:39 [hpu_model_runner.py:3029] [Warmup][Graph/decode][52/135] batch_size:32 query_len:1 num_blocks:1536 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55075)[0;0m INFO 08-09 12:38:39 [hpu_model_runner.py:3029] [Warmup][Graph/decode][52/135] batch_size:32 query_len:1 num_blocks:1536 free_mem:17.67 GiB
INFO 08-09 12:38:43 [hpu_model_runner.py:3029] [Warmup][Graph/decode][53/135] batch_size:32 query_len:1 num_blocks:1152 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55081)[0;0m INFO 08-09 12:38:43 [hpu_model_runner.py:3029] [Warmup][Graph/decode][53/135] batch_size:32 query_len:1 num_blocks:1152 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55080)[0;0m INFO 08-09 12:38:43 [hpu_model_runner.py:3029] [Warmup][Graph/decode][53/135] batch_size:32 query_len:1 num_blocks:1152 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55079)[0;0m INFO 08-09 12:38:43 [hpu_model_runner.py:3029] [Warmup][Graph/decode][53/135] batch_size:32 query_len:1 num_blocks:1152 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55075)[0;0m INFO 08-09 12:38:43 [hpu_model_runner.py:3029] [Warmup][Graph/decode][53/135] batch_size:32 query_len:1 num_blocks:1152 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55078)[0;0m INFO 08-09 12:38:43 [hpu_model_runner.py:3029] [Warmup][Graph/decode][53/135] batch_size:32 query_len:1 num_blocks:1152 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55077)[0;0m INFO 08-09 12:38:43 [hpu_model_runner.py:3029] [Warmup][Graph/decode][53/135] batch_size:32 query_len:1 num_blocks:1152 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55076)[0;0m INFO 08-09 12:38:43 [hpu_model_runner.py:3029] [Warmup][Graph/decode][53/135] batch_size:32 query_len:1 num_blocks:1152 free_mem:17.67 GiB
INFO 08-09 12:38:47 [hpu_model_runner.py:3029] [Warmup][Graph/decode][54/135] batch_size:32 query_len:1 num_blocks:896 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55081)[0;0m INFO 08-09 12:38:47 [hpu_model_runner.py:3029] [Warmup][Graph/decode][54/135] batch_size:32 query_len:1 num_blocks:896 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55079)[0;0m INFO 08-09 12:38:47 [hpu_model_runner.py:3029] [Warmup][Graph/decode][54/135] batch_size:32 query_len:1 num_blocks:896 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55077)[0;0m INFO 08-09 12:38:47 [hpu_model_runner.py:3029] [Warmup][Graph/decode][54/135] batch_size:32 query_len:1 num_blocks:896 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55080)[0;0m INFO 08-09 12:38:47 [hpu_model_runner.py:3029] [Warmup][Graph/decode][54/135] batch_size:32 query_len:1 num_blocks:896 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55078)[0;0m INFO 08-09 12:38:47 [hpu_model_runner.py:3029] [Warmup][Graph/decode][54/135] batch_size:32 query_len:1 num_blocks:896 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55075)[0;0m INFO 08-09 12:38:47 [hpu_model_runner.py:3029] [Warmup][Graph/decode][54/135] batch_size:32 query_len:1 num_blocks:896 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55076)[0;0m INFO 08-09 12:38:47 [hpu_model_runner.py:3029] [Warmup][Graph/decode][54/135] batch_size:32 query_len:1 num_blocks:896 free_mem:17.67 GiB
INFO 08-09 12:38:51 [hpu_model_runner.py:3029] [Warmup][Graph/decode][55/135] batch_size:32 query_len:1 num_blocks:768 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55081)[0;0m INFO 08-09 12:38:51 [hpu_model_runner.py:3029] [Warmup][Graph/decode][55/135] batch_size:32 query_len:1 num_blocks:768 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55080)[0;0m INFO 08-09 12:38:51 [hpu_model_runner.py:3029] [Warmup][Graph/decode][55/135] batch_size:32 query_len:1 num_blocks:768 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55079)[0;0m INFO 08-09 12:38:51 [hpu_model_runner.py:3029] [Warmup][Graph/decode][55/135] batch_size:32 query_len:1 num_blocks:768 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55078)[0;0m INFO 08-09 12:38:51 [hpu_model_runner.py:3029] [Warmup][Graph/decode][55/135] batch_size:32 query_len:1 num_blocks:768 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55076)[0;0m INFO 08-09 12:38:51 [hpu_model_runner.py:3029] [Warmup][Graph/decode][55/135] batch_size:32 query_len:1 num_blocks:768 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55077)[0;0m INFO 08-09 12:38:51 [hpu_model_runner.py:3029] [Warmup][Graph/decode][55/135] batch_size:32 query_len:1 num_blocks:768 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55075)[0;0m INFO 08-09 12:38:51 [hpu_model_runner.py:3029] [Warmup][Graph/decode][55/135] batch_size:32 query_len:1 num_blocks:768 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55080)[0;0m INFO 08-09 12:38:55 [hpu_model_runner.py:3029] [Warmup][Graph/decode][56/135] batch_size:32 query_len:1 num_blocks:640 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55076)[0;0m INFO 08-09 12:38:55 [hpu_model_runner.py:3029] [Warmup][Graph/decode][56/135] batch_size:32 query_len:1 num_blocks:640 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55081)[0;0m INFO 08-09 12:38:55 [hpu_model_runner.py:3029] [Warmup][Graph/decode][56/135] batch_size:32 query_len:1 num_blocks:640 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55079)[0;0m INFO 08-09 12:38:55 [hpu_model_runner.py:3029] [Warmup][Graph/decode][56/135] batch_size:32 query_len:1 num_blocks:640 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55078)[0;0m INFO 08-09 12:38:55 [hpu_model_runner.py:3029] [Warmup][Graph/decode][56/135] batch_size:32 query_len:1 num_blocks:640 free_mem:17.67 GiB
INFO 08-09 12:38:55 [hpu_model_runner.py:3029] [Warmup][Graph/decode][56/135] batch_size:32 query_len:1 num_blocks:640 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55077)[0;0m INFO 08-09 12:38:55 [hpu_model_runner.py:3029] [Warmup][Graph/decode][56/135] batch_size:32 query_len:1 num_blocks:640 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55075)[0;0m INFO 08-09 12:38:55 [hpu_model_runner.py:3029] [Warmup][Graph/decode][56/135] batch_size:32 query_len:1 num_blocks:640 free_mem:17.67 GiB
INFO 08-09 12:38:59 [hpu_model_runner.py:3029] [Warmup][Graph/decode][57/135] batch_size:32 query_len:1 num_blocks:512 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55080)[0;0m INFO 08-09 12:38:59 [hpu_model_runner.py:3029] [Warmup][Graph/decode][57/135] batch_size:32 query_len:1 num_blocks:512 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55077)[0;0m INFO 08-09 12:38:59 [hpu_model_runner.py:3029] [Warmup][Graph/decode][57/135] batch_size:32 query_len:1 num_blocks:512 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55076)[0;0m INFO 08-09 12:38:59 [hpu_model_runner.py:3029] [Warmup][Graph/decode][57/135] batch_size:32 query_len:1 num_blocks:512 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55078)[0;0m INFO 08-09 12:38:59 [hpu_model_runner.py:3029] [Warmup][Graph/decode][57/135] batch_size:32 query_len:1 num_blocks:512 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55081)[0;0m INFO 08-09 12:38:59 [hpu_model_runner.py:3029] [Warmup][Graph/decode][57/135] batch_size:32 query_len:1 num_blocks:512 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55079)[0;0m INFO 08-09 12:38:59 [hpu_model_runner.py:3029] [Warmup][Graph/decode][57/135] batch_size:32 query_len:1 num_blocks:512 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55075)[0;0m INFO 08-09 12:38:59 [hpu_model_runner.py:3029] [Warmup][Graph/decode][57/135] batch_size:32 query_len:1 num_blocks:512 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55080)[0;0m INFO 08-09 12:39:03 [hpu_model_runner.py:3029] [Warmup][Graph/decode][58/135] batch_size:32 query_len:1 num_blocks:384 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55078)[0;0m INFO 08-09 12:39:03 [hpu_model_runner.py:3029] [Warmup][Graph/decode][58/135] batch_size:32 query_len:1 num_blocks:384 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55081)[0;0m INFO 08-09 12:39:03 [hpu_model_runner.py:3029] [Warmup][Graph/decode][58/135] batch_size:32 query_len:1 num_blocks:384 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55076)[0;0m INFO 08-09 12:39:03 [hpu_model_runner.py:3029] [Warmup][Graph/decode][58/135] batch_size:32 query_len:1 num_blocks:384 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55079)[0;0m INFO 08-09 12:39:03 [hpu_model_runner.py:3029] [Warmup][Graph/decode][58/135] batch_size:32 query_len:1 num_blocks:384 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55075)[0;0m INFO 08-09 12:39:03 [hpu_model_runner.py:3029] [Warmup][Graph/decode][58/135] batch_size:32 query_len:1 num_blocks:384 free_mem:17.67 GiB
INFO 08-09 12:39:03 [hpu_model_runner.py:3029] [Warmup][Graph/decode][58/135] batch_size:32 query_len:1 num_blocks:384 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55077)[0;0m INFO 08-09 12:39:03 [hpu_model_runner.py:3029] [Warmup][Graph/decode][58/135] batch_size:32 query_len:1 num_blocks:384 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55078)[0;0m INFO 08-09 12:39:07 [hpu_model_runner.py:3029] [Warmup][Graph/decode][59/135] batch_size:32 query_len:1 num_blocks:256 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55076)[0;0m INFO 08-09 12:39:07 [hpu_model_runner.py:3029] [Warmup][Graph/decode][59/135] batch_size:32 query_len:1 num_blocks:256 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55075)[0;0m INFO 08-09 12:39:07 [hpu_model_runner.py:3029] [Warmup][Graph/decode][59/135] batch_size:32 query_len:1 num_blocks:256 free_mem:17.67 GiB
INFO 08-09 12:39:07 [hpu_model_runner.py:3029] [Warmup][Graph/decode][59/135] batch_size:32 query_len:1 num_blocks:256 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55081)[0;0m INFO 08-09 12:39:07 [hpu_model_runner.py:3029] [Warmup][Graph/decode][59/135] batch_size:32 query_len:1 num_blocks:256 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55079)[0;0m INFO 08-09 12:39:07 [hpu_model_runner.py:3029] [Warmup][Graph/decode][59/135] batch_size:32 query_len:1 num_blocks:256 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55080)[0;0m INFO 08-09 12:39:07 [hpu_model_runner.py:3029] [Warmup][Graph/decode][59/135] batch_size:32 query_len:1 num_blocks:256 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55077)[0;0m INFO 08-09 12:39:07 [hpu_model_runner.py:3029] [Warmup][Graph/decode][59/135] batch_size:32 query_len:1 num_blocks:256 free_mem:17.67 GiB
INFO 08-09 12:39:11 [hpu_model_runner.py:3029] [Warmup][Graph/decode][60/135] batch_size:32 query_len:1 num_blocks:128 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55081)[0;0m INFO 08-09 12:39:11 [hpu_model_runner.py:3029] [Warmup][Graph/decode][60/135] batch_size:32 query_len:1 num_blocks:128 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55080)[0;0m INFO 08-09 12:39:11 [hpu_model_runner.py:3029] [Warmup][Graph/decode][60/135] batch_size:32 query_len:1 num_blocks:128 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55078)[0;0m INFO 08-09 12:39:11 [hpu_model_runner.py:3029] [Warmup][Graph/decode][60/135] batch_size:32 query_len:1 num_blocks:128 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55075)[0;0m INFO 08-09 12:39:11 [hpu_model_runner.py:3029] [Warmup][Graph/decode][60/135] batch_size:32 query_len:1 num_blocks:128 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55077)[0;0m INFO 08-09 12:39:11 [hpu_model_runner.py:3029] [Warmup][Graph/decode][60/135] batch_size:32 query_len:1 num_blocks:128 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55079)[0;0m INFO 08-09 12:39:11 [hpu_model_runner.py:3029] [Warmup][Graph/decode][60/135] batch_size:32 query_len:1 num_blocks:128 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55076)[0;0m INFO 08-09 12:39:11 [hpu_model_runner.py:3029] [Warmup][Graph/decode][60/135] batch_size:32 query_len:1 num_blocks:128 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55078)[0;0m INFO 08-09 12:39:16 [hpu_model_runner.py:3029] [Warmup][Graph/decode][61/135] batch_size:16 query_len:1 num_blocks:9639 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55077)[0;0m INFO 08-09 12:39:16 [hpu_model_runner.py:3029] [Warmup][Graph/decode][61/135] batch_size:16 query_len:1 num_blocks:9639 free_mem:17.67 GiB
INFO 08-09 12:39:16 [hpu_model_runner.py:3029] [Warmup][Graph/decode][61/135] batch_size:16 query_len:1 num_blocks:9639 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55076)[0;0m INFO 08-09 12:39:16 [hpu_model_runner.py:3029] [Warmup][Graph/decode][61/135] batch_size:16 query_len:1 num_blocks:9639 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55080)[0;0m INFO 08-09 12:39:16 [hpu_model_runner.py:3029] [Warmup][Graph/decode][61/135] batch_size:16 query_len:1 num_blocks:9639 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55081)[0;0m INFO 08-09 12:39:16 [hpu_model_runner.py:3029] [Warmup][Graph/decode][61/135] batch_size:16 query_len:1 num_blocks:9639 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55079)[0;0m INFO 08-09 12:39:16 [hpu_model_runner.py:3029] [Warmup][Graph/decode][61/135] batch_size:16 query_len:1 num_blocks:9639 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55075)[0;0m INFO 08-09 12:39:16 [hpu_model_runner.py:3029] [Warmup][Graph/decode][61/135] batch_size:16 query_len:1 num_blocks:9639 free_mem:17.67 GiB
INFO 08-09 12:39:20 [hpu_model_runner.py:3029] [Warmup][Graph/decode][62/135] batch_size:16 query_len:1 num_blocks:7168 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55081)[0;0m INFO 08-09 12:39:20 [hpu_model_runner.py:3029] [Warmup][Graph/decode][62/135] batch_size:16 query_len:1 num_blocks:7168 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55080)[0;0m INFO 08-09 12:39:20 [hpu_model_runner.py:3029] [Warmup][Graph/decode][62/135] batch_size:16 query_len:1 num_blocks:7168 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55079)[0;0m INFO 08-09 12:39:20 [hpu_model_runner.py:3029] [Warmup][Graph/decode][62/135] batch_size:16 query_len:1 num_blocks:7168 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55077)[0;0m INFO 08-09 12:39:20 [hpu_model_runner.py:3029] [Warmup][Graph/decode][62/135] batch_size:16 query_len:1 num_blocks:7168 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55075)[0;0m INFO 08-09 12:39:20 [hpu_model_runner.py:3029] [Warmup][Graph/decode][62/135] batch_size:16 query_len:1 num_blocks:7168 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55076)[0;0m INFO 08-09 12:39:20 [hpu_model_runner.py:3029] [Warmup][Graph/decode][62/135] batch_size:16 query_len:1 num_blocks:7168 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55078)[0;0m INFO 08-09 12:39:20 [hpu_model_runner.py:3029] [Warmup][Graph/decode][62/135] batch_size:16 query_len:1 num_blocks:7168 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55081)[0;0m INFO 08-09 12:39:24 [hpu_model_runner.py:3029] [Warmup][Graph/decode][63/135] batch_size:16 query_len:1 num_blocks:5248 free_mem:17.67 GiB
INFO 08-09 12:39:24 [hpu_model_runner.py:3029] [Warmup][Graph/decode][63/135] batch_size:16 query_len:1 num_blocks:5248 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55079)[0;0m INFO 08-09 12:39:24 [hpu_model_runner.py:3029] [Warmup][Graph/decode][63/135] batch_size:16 query_len:1 num_blocks:5248 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55080)[0;0m INFO 08-09 12:39:24 [hpu_model_runner.py:3029] [Warmup][Graph/decode][63/135] batch_size:16 query_len:1 num_blocks:5248 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55078)[0;0m INFO 08-09 12:39:24 [hpu_model_runner.py:3029] [Warmup][Graph/decode][63/135] batch_size:16 query_len:1 num_blocks:5248 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55075)[0;0m INFO 08-09 12:39:24 [hpu_model_runner.py:3029] [Warmup][Graph/decode][63/135] batch_size:16 query_len:1 num_blocks:5248 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55076)[0;0m INFO 08-09 12:39:24 [hpu_model_runner.py:3029] [Warmup][Graph/decode][63/135] batch_size:16 query_len:1 num_blocks:5248 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55077)[0;0m INFO 08-09 12:39:24 [hpu_model_runner.py:3029] [Warmup][Graph/decode][63/135] batch_size:16 query_len:1 num_blocks:5248 free_mem:17.67 GiB
INFO 08-09 12:39:28 [hpu_model_runner.py:3029] [Warmup][Graph/decode][64/135] batch_size:16 query_len:1 num_blocks:3840 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55080)[0;0m INFO 08-09 12:39:28 [hpu_model_runner.py:3029] [Warmup][Graph/decode][64/135] batch_size:16 query_len:1 num_blocks:3840 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55079)[0;0m INFO 08-09 12:39:28 [hpu_model_runner.py:3029] [Warmup][Graph/decode][64/135] batch_size:16 query_len:1 num_blocks:3840 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55081)[0;0m INFO 08-09 12:39:29 [hpu_model_runner.py:3029] [Warmup][Graph/decode][64/135] batch_size:16 query_len:1 num_blocks:3840 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55078)[0;0m INFO 08-09 12:39:29 [hpu_model_runner.py:3029] [Warmup][Graph/decode][64/135] batch_size:16 query_len:1 num_blocks:3840 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55077)[0;0m INFO 08-09 12:39:29 [hpu_model_runner.py:3029] [Warmup][Graph/decode][64/135] batch_size:16 query_len:1 num_blocks:3840 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55076)[0;0m INFO 08-09 12:39:29 [hpu_model_runner.py:3029] [Warmup][Graph/decode][64/135] batch_size:16 query_len:1 num_blocks:3840 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55075)[0;0m INFO 08-09 12:39:29 [hpu_model_runner.py:3029] [Warmup][Graph/decode][64/135] batch_size:16 query_len:1 num_blocks:3840 free_mem:17.67 GiB
INFO 08-09 12:39:33 [hpu_model_runner.py:3029] [Warmup][Graph/decode][65/135] batch_size:16 query_len:1 num_blocks:2816 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55081)[0;0m INFO 08-09 12:39:33 [hpu_model_runner.py:3029] [Warmup][Graph/decode][65/135] batch_size:16 query_len:1 num_blocks:2816 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55076)[0;0m INFO 08-09 12:39:33 [hpu_model_runner.py:3029] [Warmup][Graph/decode][65/135] batch_size:16 query_len:1 num_blocks:2816 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55078)[0;0m INFO 08-09 12:39:33 [hpu_model_runner.py:3029] [Warmup][Graph/decode][65/135] batch_size:16 query_len:1 num_blocks:2816 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55080)[0;0m INFO 08-09 12:39:33 [hpu_model_runner.py:3029] [Warmup][Graph/decode][65/135] batch_size:16 query_len:1 num_blocks:2816 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55079)[0;0m INFO 08-09 12:39:33 [hpu_model_runner.py:3029] [Warmup][Graph/decode][65/135] batch_size:16 query_len:1 num_blocks:2816 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55077)[0;0m INFO 08-09 12:39:33 [hpu_model_runner.py:3029] [Warmup][Graph/decode][65/135] batch_size:16 query_len:1 num_blocks:2816 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55075)[0;0m INFO 08-09 12:39:33 [hpu_model_runner.py:3029] [Warmup][Graph/decode][65/135] batch_size:16 query_len:1 num_blocks:2816 free_mem:17.67 GiB
INFO 08-09 12:39:37 [hpu_model_runner.py:3029] [Warmup][Graph/decode][66/135] batch_size:16 query_len:1 num_blocks:2176 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55080)[0;0m INFO 08-09 12:39:37 [hpu_model_runner.py:3029] [Warmup][Graph/decode][66/135] batch_size:16 query_len:1 num_blocks:2176 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55081)[0;0m INFO 08-09 12:39:37 [hpu_model_runner.py:3029] [Warmup][Graph/decode][66/135] batch_size:16 query_len:1 num_blocks:2176 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55079)[0;0m INFO 08-09 12:39:37 [hpu_model_runner.py:3029] [Warmup][Graph/decode][66/135] batch_size:16 query_len:1 num_blocks:2176 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55078)[0;0m INFO 08-09 12:39:37 [hpu_model_runner.py:3029] [Warmup][Graph/decode][66/135] batch_size:16 query_len:1 num_blocks:2176 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55077)[0;0m INFO 08-09 12:39:37 [hpu_model_runner.py:3029] [Warmup][Graph/decode][66/135] batch_size:16 query_len:1 num_blocks:2176 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55076)[0;0m INFO 08-09 12:39:37 [hpu_model_runner.py:3029] [Warmup][Graph/decode][66/135] batch_size:16 query_len:1 num_blocks:2176 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55075)[0;0m INFO 08-09 12:39:37 [hpu_model_runner.py:3029] [Warmup][Graph/decode][66/135] batch_size:16 query_len:1 num_blocks:2176 free_mem:17.67 GiB
INFO 08-09 12:39:41 [hpu_model_runner.py:3029] [Warmup][Graph/decode][67/135] batch_size:16 query_len:1 num_blocks:1536 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55079)[0;0m INFO 08-09 12:39:41 [hpu_model_runner.py:3029] [Warmup][Graph/decode][67/135] batch_size:16 query_len:1 num_blocks:1536 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55080)[0;0m INFO 08-09 12:39:41 [hpu_model_runner.py:3029] [Warmup][Graph/decode][67/135] batch_size:16 query_len:1 num_blocks:1536 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55077)[0;0m INFO 08-09 12:39:41 [hpu_model_runner.py:3029] [Warmup][Graph/decode][67/135] batch_size:16 query_len:1 num_blocks:1536 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55081)[0;0m INFO 08-09 12:39:41 [hpu_model_runner.py:3029] [Warmup][Graph/decode][67/135] batch_size:16 query_len:1 num_blocks:1536 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55078)[0;0m INFO 08-09 12:39:41 [hpu_model_runner.py:3029] [Warmup][Graph/decode][67/135] batch_size:16 query_len:1 num_blocks:1536 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55076)[0;0m INFO 08-09 12:39:41 [hpu_model_runner.py:3029] [Warmup][Graph/decode][67/135] batch_size:16 query_len:1 num_blocks:1536 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55075)[0;0m INFO 08-09 12:39:41 [hpu_model_runner.py:3029] [Warmup][Graph/decode][67/135] batch_size:16 query_len:1 num_blocks:1536 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55079)[0;0m INFO 08-09 12:39:46 [hpu_model_runner.py:3029] [Warmup][Graph/decode][68/135] batch_size:16 query_len:1 num_blocks:1152 free_mem:17.67 GiB
INFO 08-09 12:39:46 [hpu_model_runner.py:3029] [Warmup][Graph/decode][68/135] batch_size:16 query_len:1 num_blocks:1152 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55080)[0;0m INFO 08-09 12:39:46 [hpu_model_runner.py:3029] [Warmup][Graph/decode][68/135] batch_size:16 query_len:1 num_blocks:1152 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55081)[0;0m INFO 08-09 12:39:46 [hpu_model_runner.py:3029] [Warmup][Graph/decode][68/135] batch_size:16 query_len:1 num_blocks:1152 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55077)[0;0m INFO 08-09 12:39:46 [hpu_model_runner.py:3029] [Warmup][Graph/decode][68/135] batch_size:16 query_len:1 num_blocks:1152 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55076)[0;0m INFO 08-09 12:39:46 [hpu_model_runner.py:3029] [Warmup][Graph/decode][68/135] batch_size:16 query_len:1 num_blocks:1152 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55078)[0;0m INFO 08-09 12:39:46 [hpu_model_runner.py:3029] [Warmup][Graph/decode][68/135] batch_size:16 query_len:1 num_blocks:1152 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55075)[0;0m INFO 08-09 12:39:46 [hpu_model_runner.py:3029] [Warmup][Graph/decode][68/135] batch_size:16 query_len:1 num_blocks:1152 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55080)[0;0m INFO 08-09 12:39:50 [hpu_model_runner.py:3029] [Warmup][Graph/decode][69/135] batch_size:16 query_len:1 num_blocks:896 free_mem:17.67 GiB
INFO 08-09 12:39:50 [hpu_model_runner.py:3029] [Warmup][Graph/decode][69/135] batch_size:16 query_len:1 num_blocks:896 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55081)[0;0m INFO 08-09 12:39:50 [hpu_model_runner.py:3029] [Warmup][Graph/decode][69/135] batch_size:16 query_len:1 num_blocks:896 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55079)[0;0m INFO 08-09 12:39:50 [hpu_model_runner.py:3029] [Warmup][Graph/decode][69/135] batch_size:16 query_len:1 num_blocks:896 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55078)[0;0m INFO 08-09 12:39:50 [hpu_model_runner.py:3029] [Warmup][Graph/decode][69/135] batch_size:16 query_len:1 num_blocks:896 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55076)[0;0m INFO 08-09 12:39:50 [hpu_model_runner.py:3029] [Warmup][Graph/decode][69/135] batch_size:16 query_len:1 num_blocks:896 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55075)[0;0m INFO 08-09 12:39:50 [hpu_model_runner.py:3029] [Warmup][Graph/decode][69/135] batch_size:16 query_len:1 num_blocks:896 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55077)[0;0m INFO 08-09 12:39:50 [hpu_model_runner.py:3029] [Warmup][Graph/decode][69/135] batch_size:16 query_len:1 num_blocks:896 free_mem:17.67 GiB
INFO 08-09 12:39:54 [hpu_model_runner.py:3029] [Warmup][Graph/decode][70/135] batch_size:16 query_len:1 num_blocks:768 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55081)[0;0m INFO 08-09 12:39:54 [hpu_model_runner.py:3029] [Warmup][Graph/decode][70/135] batch_size:16 query_len:1 num_blocks:768 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55077)[0;0m INFO 08-09 12:39:54 [hpu_model_runner.py:3029] [Warmup][Graph/decode][70/135] batch_size:16 query_len:1 num_blocks:768 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55076)[0;0m INFO 08-09 12:39:54 [hpu_model_runner.py:3029] [Warmup][Graph/decode][70/135] batch_size:16 query_len:1 num_blocks:768 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55080)[0;0m INFO 08-09 12:39:54 [hpu_model_runner.py:3029] [Warmup][Graph/decode][70/135] batch_size:16 query_len:1 num_blocks:768 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55079)[0;0m INFO 08-09 12:39:54 [hpu_model_runner.py:3029] [Warmup][Graph/decode][70/135] batch_size:16 query_len:1 num_blocks:768 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55078)[0;0m INFO 08-09 12:39:54 [hpu_model_runner.py:3029] [Warmup][Graph/decode][70/135] batch_size:16 query_len:1 num_blocks:768 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55075)[0;0m INFO 08-09 12:39:54 [hpu_model_runner.py:3029] [Warmup][Graph/decode][70/135] batch_size:16 query_len:1 num_blocks:768 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55081)[0;0m INFO 08-09 12:39:58 [hpu_model_runner.py:3029] [Warmup][Graph/decode][71/135] batch_size:16 query_len:1 num_blocks:640 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55077)[0;0m INFO 08-09 12:39:58 [hpu_model_runner.py:3029] [Warmup][Graph/decode][71/135] batch_size:16 query_len:1 num_blocks:640 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55075)[0;0m INFO 08-09 12:39:58 [hpu_model_runner.py:3029] [Warmup][Graph/decode][71/135] batch_size:16 query_len:1 num_blocks:640 free_mem:17.67 GiB
INFO 08-09 12:39:58 [hpu_model_runner.py:3029] [Warmup][Graph/decode][71/135] batch_size:16 query_len:1 num_blocks:640 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55078)[0;0m INFO 08-09 12:39:58 [hpu_model_runner.py:3029] [Warmup][Graph/decode][71/135] batch_size:16 query_len:1 num_blocks:640 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55076)[0;0m INFO 08-09 12:39:58 [hpu_model_runner.py:3029] [Warmup][Graph/decode][71/135] batch_size:16 query_len:1 num_blocks:640 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55080)[0;0m INFO 08-09 12:39:58 [hpu_model_runner.py:3029] [Warmup][Graph/decode][71/135] batch_size:16 query_len:1 num_blocks:640 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55079)[0;0m INFO 08-09 12:39:58 [hpu_model_runner.py:3029] [Warmup][Graph/decode][71/135] batch_size:16 query_len:1 num_blocks:640 free_mem:17.67 GiB
INFO 08-09 12:40:03 [hpu_model_runner.py:3029] [Warmup][Graph/decode][72/135] batch_size:16 query_len:1 num_blocks:512 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55081)[0;0m INFO 08-09 12:40:03 [hpu_model_runner.py:3029] [Warmup][Graph/decode][72/135] batch_size:16 query_len:1 num_blocks:512 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55080)[0;0m INFO 08-09 12:40:03 [hpu_model_runner.py:3029] [Warmup][Graph/decode][72/135] batch_size:16 query_len:1 num_blocks:512 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55075)[0;0m INFO 08-09 12:40:03 [hpu_model_runner.py:3029] [Warmup][Graph/decode][72/135] batch_size:16 query_len:1 num_blocks:512 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55079)[0;0m INFO 08-09 12:40:03 [hpu_model_runner.py:3029] [Warmup][Graph/decode][72/135] batch_size:16 query_len:1 num_blocks:512 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55077)[0;0m INFO 08-09 12:40:03 [hpu_model_runner.py:3029] [Warmup][Graph/decode][72/135] batch_size:16 query_len:1 num_blocks:512 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55078)[0;0m INFO 08-09 12:40:03 [hpu_model_runner.py:3029] [Warmup][Graph/decode][72/135] batch_size:16 query_len:1 num_blocks:512 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55076)[0;0m INFO 08-09 12:40:03 [hpu_model_runner.py:3029] [Warmup][Graph/decode][72/135] batch_size:16 query_len:1 num_blocks:512 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55080)[0;0m INFO 08-09 12:40:07 [hpu_model_runner.py:3029] [Warmup][Graph/decode][73/135] batch_size:16 query_len:1 num_blocks:384 free_mem:17.67 GiB
INFO 08-09 12:40:07 [hpu_model_runner.py:3029] [Warmup][Graph/decode][73/135] batch_size:16 query_len:1 num_blocks:384 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55081)[0;0m INFO 08-09 12:40:07 [hpu_model_runner.py:3029] [Warmup][Graph/decode][73/135] batch_size:16 query_len:1 num_blocks:384 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55079)[0;0m INFO 08-09 12:40:07 [hpu_model_runner.py:3029] [Warmup][Graph/decode][73/135] batch_size:16 query_len:1 num_blocks:384 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55077)[0;0m INFO 08-09 12:40:07 [hpu_model_runner.py:3029] [Warmup][Graph/decode][73/135] batch_size:16 query_len:1 num_blocks:384 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55075)[0;0m INFO 08-09 12:40:07 [hpu_model_runner.py:3029] [Warmup][Graph/decode][73/135] batch_size:16 query_len:1 num_blocks:384 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55078)[0;0m INFO 08-09 12:40:07 [hpu_model_runner.py:3029] [Warmup][Graph/decode][73/135] batch_size:16 query_len:1 num_blocks:384 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55076)[0;0m INFO 08-09 12:40:07 [hpu_model_runner.py:3029] [Warmup][Graph/decode][73/135] batch_size:16 query_len:1 num_blocks:384 free_mem:17.67 GiB
INFO 08-09 12:40:11 [hpu_model_runner.py:3029] [Warmup][Graph/decode][74/135] batch_size:16 query_len:1 num_blocks:256 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55081)[0;0m INFO 08-09 12:40:11 [hpu_model_runner.py:3029] [Warmup][Graph/decode][74/135] batch_size:16 query_len:1 num_blocks:256 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55080)[0;0m INFO 08-09 12:40:11 [hpu_model_runner.py:3029] [Warmup][Graph/decode][74/135] batch_size:16 query_len:1 num_blocks:256 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55077)[0;0m INFO 08-09 12:40:11 [hpu_model_runner.py:3029] [Warmup][Graph/decode][74/135] batch_size:16 query_len:1 num_blocks:256 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55079)[0;0m INFO 08-09 12:40:11 [hpu_model_runner.py:3029] [Warmup][Graph/decode][74/135] batch_size:16 query_len:1 num_blocks:256 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55078)[0;0m INFO 08-09 12:40:11 [hpu_model_runner.py:3029] [Warmup][Graph/decode][74/135] batch_size:16 query_len:1 num_blocks:256 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55076)[0;0m INFO 08-09 12:40:11 [hpu_model_runner.py:3029] [Warmup][Graph/decode][74/135] batch_size:16 query_len:1 num_blocks:256 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55075)[0;0m INFO 08-09 12:40:11 [hpu_model_runner.py:3029] [Warmup][Graph/decode][74/135] batch_size:16 query_len:1 num_blocks:256 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55080)[0;0m INFO 08-09 12:40:16 [hpu_model_runner.py:3029] [Warmup][Graph/decode][75/135] batch_size:16 query_len:1 num_blocks:128 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55078)[0;0m INFO 08-09 12:40:16 [hpu_model_runner.py:3029] [Warmup][Graph/decode][75/135] batch_size:16 query_len:1 num_blocks:128 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55077)[0;0m INFO 08-09 12:40:16 [hpu_model_runner.py:3029] [Warmup][Graph/decode][75/135] batch_size:16 query_len:1 num_blocks:128 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55075)[0;0m INFO 08-09 12:40:16 [hpu_model_runner.py:3029] [Warmup][Graph/decode][75/135] batch_size:16 query_len:1 num_blocks:128 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55081)[0;0m INFO 08-09 12:40:16 [hpu_model_runner.py:3029] [Warmup][Graph/decode][75/135] batch_size:16 query_len:1 num_blocks:128 free_mem:17.67 GiB
INFO 08-09 12:40:16 [hpu_model_runner.py:3029] [Warmup][Graph/decode][75/135] batch_size:16 query_len:1 num_blocks:128 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55079)[0;0m INFO 08-09 12:40:16 [hpu_model_runner.py:3029] [Warmup][Graph/decode][75/135] batch_size:16 query_len:1 num_blocks:128 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55076)[0;0m INFO 08-09 12:40:16 [hpu_model_runner.py:3029] [Warmup][Graph/decode][75/135] batch_size:16 query_len:1 num_blocks:128 free_mem:17.67 GiB
INFO 08-09 12:40:20 [hpu_model_runner.py:3029] [Warmup][Graph/decode][76/135] batch_size:8 query_len:1 num_blocks:9639 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55077)[0;0m INFO 08-09 12:40:20 [hpu_model_runner.py:3029] [Warmup][Graph/decode][76/135] batch_size:8 query_len:1 num_blocks:9639 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55075)[0;0m INFO 08-09 12:40:20 [hpu_model_runner.py:3029] [Warmup][Graph/decode][76/135] batch_size:8 query_len:1 num_blocks:9639 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55080)[0;0m INFO 08-09 12:40:20 [hpu_model_runner.py:3029] [Warmup][Graph/decode][76/135] batch_size:8 query_len:1 num_blocks:9639 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55081)[0;0m INFO 08-09 12:40:20 [hpu_model_runner.py:3029] [Warmup][Graph/decode][76/135] batch_size:8 query_len:1 num_blocks:9639 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55079)[0;0m INFO 08-09 12:40:20 [hpu_model_runner.py:3029] [Warmup][Graph/decode][76/135] batch_size:8 query_len:1 num_blocks:9639 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55076)[0;0m INFO 08-09 12:40:20 [hpu_model_runner.py:3029] [Warmup][Graph/decode][76/135] batch_size:8 query_len:1 num_blocks:9639 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55078)[0;0m INFO 08-09 12:40:20 [hpu_model_runner.py:3029] [Warmup][Graph/decode][76/135] batch_size:8 query_len:1 num_blocks:9639 free_mem:17.67 GiB
INFO 08-09 12:40:25 [hpu_model_runner.py:3029] [Warmup][Graph/decode][77/135] batch_size:8 query_len:1 num_blocks:7168 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55079)[0;0m INFO 08-09 12:40:25 [hpu_model_runner.py:3029] [Warmup][Graph/decode][77/135] batch_size:8 query_len:1 num_blocks:7168 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55081)[0;0m INFO 08-09 12:40:25 [hpu_model_runner.py:3029] [Warmup][Graph/decode][77/135] batch_size:8 query_len:1 num_blocks:7168 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55080)[0;0m INFO 08-09 12:40:25 [hpu_model_runner.py:3029] [Warmup][Graph/decode][77/135] batch_size:8 query_len:1 num_blocks:7168 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55078)[0;0m INFO 08-09 12:40:25 [hpu_model_runner.py:3029] [Warmup][Graph/decode][77/135] batch_size:8 query_len:1 num_blocks:7168 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55077)[0;0m INFO 08-09 12:40:25 [hpu_model_runner.py:3029] [Warmup][Graph/decode][77/135] batch_size:8 query_len:1 num_blocks:7168 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55076)[0;0m INFO 08-09 12:40:25 [hpu_model_runner.py:3029] [Warmup][Graph/decode][77/135] batch_size:8 query_len:1 num_blocks:7168 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55075)[0;0m INFO 08-09 12:40:25 [hpu_model_runner.py:3029] [Warmup][Graph/decode][77/135] batch_size:8 query_len:1 num_blocks:7168 free_mem:17.67 GiB
INFO 08-09 12:40:30 [hpu_model_runner.py:3029] [Warmup][Graph/decode][78/135] batch_size:8 query_len:1 num_blocks:5248 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55079)[0;0m INFO 08-09 12:40:30 [hpu_model_runner.py:3029] [Warmup][Graph/decode][78/135] batch_size:8 query_len:1 num_blocks:5248 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55081)[0;0m INFO 08-09 12:40:30 [hpu_model_runner.py:3029] [Warmup][Graph/decode][78/135] batch_size:8 query_len:1 num_blocks:5248 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55080)[0;0m INFO 08-09 12:40:30 [hpu_model_runner.py:3029] [Warmup][Graph/decode][78/135] batch_size:8 query_len:1 num_blocks:5248 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55075)[0;0m INFO 08-09 12:40:30 [hpu_model_runner.py:3029] [Warmup][Graph/decode][78/135] batch_size:8 query_len:1 num_blocks:5248 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55078)[0;0m INFO 08-09 12:40:30 [hpu_model_runner.py:3029] [Warmup][Graph/decode][78/135] batch_size:8 query_len:1 num_blocks:5248 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55076)[0;0m INFO 08-09 12:40:30 [hpu_model_runner.py:3029] [Warmup][Graph/decode][78/135] batch_size:8 query_len:1 num_blocks:5248 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55077)[0;0m INFO 08-09 12:40:30 [hpu_model_runner.py:3029] [Warmup][Graph/decode][78/135] batch_size:8 query_len:1 num_blocks:5248 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55080)[0;0m INFO 08-09 12:40:34 [hpu_model_runner.py:3029] [Warmup][Graph/decode][79/135] batch_size:8 query_len:1 num_blocks:3840 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55081)[0;0m INFO 08-09 12:40:34 [hpu_model_runner.py:3029] [Warmup][Graph/decode][79/135] batch_size:8 query_len:1 num_blocks:3840 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55076)[0;0m INFO 08-09 12:40:34 [hpu_model_runner.py:3029] [Warmup][Graph/decode][79/135] batch_size:8 query_len:1 num_blocks:3840 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55075)[0;0m INFO 08-09 12:40:34 [hpu_model_runner.py:3029] [Warmup][Graph/decode][79/135] batch_size:8 query_len:1 num_blocks:3840 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55078)[0;0m INFO 08-09 12:40:34 [hpu_model_runner.py:3029] [Warmup][Graph/decode][79/135] batch_size:8 query_len:1 num_blocks:3840 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55077)[0;0m INFO 08-09 12:40:34 [hpu_model_runner.py:3029] [Warmup][Graph/decode][79/135] batch_size:8 query_len:1 num_blocks:3840 free_mem:17.67 GiB
INFO 08-09 12:40:34 [hpu_model_runner.py:3029] [Warmup][Graph/decode][79/135] batch_size:8 query_len:1 num_blocks:3840 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55079)[0;0m INFO 08-09 12:40:34 [hpu_model_runner.py:3029] [Warmup][Graph/decode][79/135] batch_size:8 query_len:1 num_blocks:3840 free_mem:17.67 GiB
INFO 08-09 12:40:39 [hpu_model_runner.py:3029] [Warmup][Graph/decode][80/135] batch_size:8 query_len:1 num_blocks:2816 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55079)[0;0m INFO 08-09 12:40:39 [hpu_model_runner.py:3029] [Warmup][Graph/decode][80/135] batch_size:8 query_len:1 num_blocks:2816 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55076)[0;0m INFO 08-09 12:40:39 [hpu_model_runner.py:3029] [Warmup][Graph/decode][80/135] batch_size:8 query_len:1 num_blocks:2816 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55081)[0;0m INFO 08-09 12:40:39 [hpu_model_runner.py:3029] [Warmup][Graph/decode][80/135] batch_size:8 query_len:1 num_blocks:2816 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55080)[0;0m INFO 08-09 12:40:39 [hpu_model_runner.py:3029] [Warmup][Graph/decode][80/135] batch_size:8 query_len:1 num_blocks:2816 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55078)[0;0m INFO 08-09 12:40:39 [hpu_model_runner.py:3029] [Warmup][Graph/decode][80/135] batch_size:8 query_len:1 num_blocks:2816 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55077)[0;0m INFO 08-09 12:40:39 [hpu_model_runner.py:3029] [Warmup][Graph/decode][80/135] batch_size:8 query_len:1 num_blocks:2816 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55075)[0;0m INFO 08-09 12:40:39 [hpu_model_runner.py:3029] [Warmup][Graph/decode][80/135] batch_size:8 query_len:1 num_blocks:2816 free_mem:17.67 GiB
INFO 08-09 12:40:43 [hpu_model_runner.py:3029] [Warmup][Graph/decode][81/135] batch_size:8 query_len:1 num_blocks:2176 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55081)[0;0m INFO 08-09 12:40:43 [hpu_model_runner.py:3029] [Warmup][Graph/decode][81/135] batch_size:8 query_len:1 num_blocks:2176 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55079)[0;0m INFO 08-09 12:40:43 [hpu_model_runner.py:3029] [Warmup][Graph/decode][81/135] batch_size:8 query_len:1 num_blocks:2176 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55078)[0;0m INFO 08-09 12:40:43 [hpu_model_runner.py:3029] [Warmup][Graph/decode][81/135] batch_size:8 query_len:1 num_blocks:2176 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55080)[0;0m INFO 08-09 12:40:43 [hpu_model_runner.py:3029] [Warmup][Graph/decode][81/135] batch_size:8 query_len:1 num_blocks:2176 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55075)[0;0m INFO 08-09 12:40:43 [hpu_model_runner.py:3029] [Warmup][Graph/decode][81/135] batch_size:8 query_len:1 num_blocks:2176 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55077)[0;0m INFO 08-09 12:40:43 [hpu_model_runner.py:3029] [Warmup][Graph/decode][81/135] batch_size:8 query_len:1 num_blocks:2176 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55076)[0;0m INFO 08-09 12:40:43 [hpu_model_runner.py:3029] [Warmup][Graph/decode][81/135] batch_size:8 query_len:1 num_blocks:2176 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55080)[0;0m INFO 08-09 12:40:48 [hpu_model_runner.py:3029] [Warmup][Graph/decode][82/135] batch_size:8 query_len:1 num_blocks:1536 free_mem:17.67 GiB
INFO 08-09 12:40:48 [hpu_model_runner.py:3029] [Warmup][Graph/decode][82/135] batch_size:8 query_len:1 num_blocks:1536 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55081)[0;0m INFO 08-09 12:40:48 [hpu_model_runner.py:3029] [Warmup][Graph/decode][82/135] batch_size:8 query_len:1 num_blocks:1536 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55077)[0;0m INFO 08-09 12:40:48 [hpu_model_runner.py:3029] [Warmup][Graph/decode][82/135] batch_size:8 query_len:1 num_blocks:1536 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55075)[0;0m INFO 08-09 12:40:48 [hpu_model_runner.py:3029] [Warmup][Graph/decode][82/135] batch_size:8 query_len:1 num_blocks:1536 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55079)[0;0m INFO 08-09 12:40:48 [hpu_model_runner.py:3029] [Warmup][Graph/decode][82/135] batch_size:8 query_len:1 num_blocks:1536 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55078)[0;0m INFO 08-09 12:40:48 [hpu_model_runner.py:3029] [Warmup][Graph/decode][82/135] batch_size:8 query_len:1 num_blocks:1536 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55076)[0;0m INFO 08-09 12:40:48 [hpu_model_runner.py:3029] [Warmup][Graph/decode][82/135] batch_size:8 query_len:1 num_blocks:1536 free_mem:17.67 GiB
INFO 08-09 12:40:53 [hpu_model_runner.py:3029] [Warmup][Graph/decode][83/135] batch_size:8 query_len:1 num_blocks:1152 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55080)[0;0m INFO 08-09 12:40:53 [hpu_model_runner.py:3029] [Warmup][Graph/decode][83/135] batch_size:8 query_len:1 num_blocks:1152 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55081)[0;0m INFO 08-09 12:40:53 [hpu_model_runner.py:3029] [Warmup][Graph/decode][83/135] batch_size:8 query_len:1 num_blocks:1152 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55079)[0;0m INFO 08-09 12:40:53 [hpu_model_runner.py:3029] [Warmup][Graph/decode][83/135] batch_size:8 query_len:1 num_blocks:1152 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55078)[0;0m INFO 08-09 12:40:53 [hpu_model_runner.py:3029] [Warmup][Graph/decode][83/135] batch_size:8 query_len:1 num_blocks:1152 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55077)[0;0m INFO 08-09 12:40:53 [hpu_model_runner.py:3029] [Warmup][Graph/decode][83/135] batch_size:8 query_len:1 num_blocks:1152 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55076)[0;0m INFO 08-09 12:40:53 [hpu_model_runner.py:3029] [Warmup][Graph/decode][83/135] batch_size:8 query_len:1 num_blocks:1152 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55075)[0;0m INFO 08-09 12:40:53 [hpu_model_runner.py:3029] [Warmup][Graph/decode][83/135] batch_size:8 query_len:1 num_blocks:1152 free_mem:17.67 GiB
INFO 08-09 12:40:57 [hpu_model_runner.py:3029] [Warmup][Graph/decode][84/135] batch_size:8 query_len:1 num_blocks:896 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55081)[0;0m INFO 08-09 12:40:57 [hpu_model_runner.py:3029] [Warmup][Graph/decode][84/135] batch_size:8 query_len:1 num_blocks:896 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55080)[0;0m INFO 08-09 12:40:57 [hpu_model_runner.py:3029] [Warmup][Graph/decode][84/135] batch_size:8 query_len:1 num_blocks:896 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55079)[0;0m INFO 08-09 12:40:57 [hpu_model_runner.py:3029] [Warmup][Graph/decode][84/135] batch_size:8 query_len:1 num_blocks:896 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55078)[0;0m INFO 08-09 12:40:57 [hpu_model_runner.py:3029] [Warmup][Graph/decode][84/135] batch_size:8 query_len:1 num_blocks:896 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55077)[0;0m INFO 08-09 12:40:57 [hpu_model_runner.py:3029] [Warmup][Graph/decode][84/135] batch_size:8 query_len:1 num_blocks:896 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55075)[0;0m INFO 08-09 12:40:57 [hpu_model_runner.py:3029] [Warmup][Graph/decode][84/135] batch_size:8 query_len:1 num_blocks:896 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55076)[0;0m INFO 08-09 12:40:57 [hpu_model_runner.py:3029] [Warmup][Graph/decode][84/135] batch_size:8 query_len:1 num_blocks:896 free_mem:17.67 GiB
INFO 08-09 12:41:02 [hpu_model_runner.py:3029] [Warmup][Graph/decode][85/135] batch_size:8 query_len:1 num_blocks:768 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55080)[0;0m INFO 08-09 12:41:02 [hpu_model_runner.py:3029] [Warmup][Graph/decode][85/135] batch_size:8 query_len:1 num_blocks:768 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55081)[0;0m INFO 08-09 12:41:02 [hpu_model_runner.py:3029] [Warmup][Graph/decode][85/135] batch_size:8 query_len:1 num_blocks:768 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55079)[0;0m INFO 08-09 12:41:02 [hpu_model_runner.py:3029] [Warmup][Graph/decode][85/135] batch_size:8 query_len:1 num_blocks:768 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55077)[0;0m INFO 08-09 12:41:02 [hpu_model_runner.py:3029] [Warmup][Graph/decode][85/135] batch_size:8 query_len:1 num_blocks:768 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55078)[0;0m INFO 08-09 12:41:02 [hpu_model_runner.py:3029] [Warmup][Graph/decode][85/135] batch_size:8 query_len:1 num_blocks:768 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55076)[0;0m INFO 08-09 12:41:02 [hpu_model_runner.py:3029] [Warmup][Graph/decode][85/135] batch_size:8 query_len:1 num_blocks:768 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55075)[0;0m INFO 08-09 12:41:02 [hpu_model_runner.py:3029] [Warmup][Graph/decode][85/135] batch_size:8 query_len:1 num_blocks:768 free_mem:17.67 GiB
INFO 08-09 12:41:07 [hpu_model_runner.py:3029] [Warmup][Graph/decode][86/135] batch_size:8 query_len:1 num_blocks:640 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55079)[0;0m INFO 08-09 12:41:07 [hpu_model_runner.py:3029] [Warmup][Graph/decode][86/135] batch_size:8 query_len:1 num_blocks:640 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55077)[0;0m INFO 08-09 12:41:07 [hpu_model_runner.py:3029] [Warmup][Graph/decode][86/135] batch_size:8 query_len:1 num_blocks:640 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55078)[0;0m INFO 08-09 12:41:07 [hpu_model_runner.py:3029] [Warmup][Graph/decode][86/135] batch_size:8 query_len:1 num_blocks:640 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55080)[0;0m INFO 08-09 12:41:07 [hpu_model_runner.py:3029] [Warmup][Graph/decode][86/135] batch_size:8 query_len:1 num_blocks:640 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55081)[0;0m INFO 08-09 12:41:07 [hpu_model_runner.py:3029] [Warmup][Graph/decode][86/135] batch_size:8 query_len:1 num_blocks:640 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55076)[0;0m INFO 08-09 12:41:07 [hpu_model_runner.py:3029] [Warmup][Graph/decode][86/135] batch_size:8 query_len:1 num_blocks:640 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55075)[0;0m INFO 08-09 12:41:07 [hpu_model_runner.py:3029] [Warmup][Graph/decode][86/135] batch_size:8 query_len:1 num_blocks:640 free_mem:17.67 GiB
INFO 08-09 12:41:11 [hpu_model_runner.py:3029] [Warmup][Graph/decode][87/135] batch_size:8 query_len:1 num_blocks:512 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55080)[0;0m INFO 08-09 12:41:11 [hpu_model_runner.py:3029] [Warmup][Graph/decode][87/135] batch_size:8 query_len:1 num_blocks:512 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55078)[0;0m INFO 08-09 12:41:11 [hpu_model_runner.py:3029] [Warmup][Graph/decode][87/135] batch_size:8 query_len:1 num_blocks:512 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55077)[0;0m INFO 08-09 12:41:11 [hpu_model_runner.py:3029] [Warmup][Graph/decode][87/135] batch_size:8 query_len:1 num_blocks:512 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55075)[0;0m INFO 08-09 12:41:11 [hpu_model_runner.py:3029] [Warmup][Graph/decode][87/135] batch_size:8 query_len:1 num_blocks:512 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55079)[0;0m INFO 08-09 12:41:11 [hpu_model_runner.py:3029] [Warmup][Graph/decode][87/135] batch_size:8 query_len:1 num_blocks:512 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55081)[0;0m INFO 08-09 12:41:11 [hpu_model_runner.py:3029] [Warmup][Graph/decode][87/135] batch_size:8 query_len:1 num_blocks:512 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55076)[0;0m INFO 08-09 12:41:11 [hpu_model_runner.py:3029] [Warmup][Graph/decode][87/135] batch_size:8 query_len:1 num_blocks:512 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55081)[0;0m INFO 08-09 12:41:16 [hpu_model_runner.py:3029] [Warmup][Graph/decode][88/135] batch_size:8 query_len:1 num_blocks:384 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55079)[0;0m INFO 08-09 12:41:16 [hpu_model_runner.py:3029] [Warmup][Graph/decode][88/135] batch_size:8 query_len:1 num_blocks:384 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55077)[0;0m INFO 08-09 12:41:16 [hpu_model_runner.py:3029] [Warmup][Graph/decode][88/135] batch_size:8 query_len:1 num_blocks:384 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55076)[0;0m INFO 08-09 12:41:16 [hpu_model_runner.py:3029] [Warmup][Graph/decode][88/135] batch_size:8 query_len:1 num_blocks:384 free_mem:17.67 GiB
INFO 08-09 12:41:16 [hpu_model_runner.py:3029] [Warmup][Graph/decode][88/135] batch_size:8 query_len:1 num_blocks:384 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55078)[0;0m INFO 08-09 12:41:16 [hpu_model_runner.py:3029] [Warmup][Graph/decode][88/135] batch_size:8 query_len:1 num_blocks:384 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55075)[0;0m INFO 08-09 12:41:16 [hpu_model_runner.py:3029] [Warmup][Graph/decode][88/135] batch_size:8 query_len:1 num_blocks:384 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55080)[0;0m INFO 08-09 12:41:16 [hpu_model_runner.py:3029] [Warmup][Graph/decode][88/135] batch_size:8 query_len:1 num_blocks:384 free_mem:17.67 GiB
INFO 08-09 12:41:21 [hpu_model_runner.py:3029] [Warmup][Graph/decode][89/135] batch_size:8 query_len:1 num_blocks:256 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55079)[0;0m INFO 08-09 12:41:21 [hpu_model_runner.py:3029] [Warmup][Graph/decode][89/135] batch_size:8 query_len:1 num_blocks:256 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55080)[0;0m INFO 08-09 12:41:21 [hpu_model_runner.py:3029] [Warmup][Graph/decode][89/135] batch_size:8 query_len:1 num_blocks:256 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55081)[0;0m INFO 08-09 12:41:21 [hpu_model_runner.py:3029] [Warmup][Graph/decode][89/135] batch_size:8 query_len:1 num_blocks:256 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55076)[0;0m INFO 08-09 12:41:21 [hpu_model_runner.py:3029] [Warmup][Graph/decode][89/135] batch_size:8 query_len:1 num_blocks:256 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55075)[0;0m INFO 08-09 12:41:21 [hpu_model_runner.py:3029] [Warmup][Graph/decode][89/135] batch_size:8 query_len:1 num_blocks:256 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55078)[0;0m INFO 08-09 12:41:21 [hpu_model_runner.py:3029] [Warmup][Graph/decode][89/135] batch_size:8 query_len:1 num_blocks:256 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55077)[0;0m INFO 08-09 12:41:21 [hpu_model_runner.py:3029] [Warmup][Graph/decode][89/135] batch_size:8 query_len:1 num_blocks:256 free_mem:17.67 GiB
INFO 08-09 12:41:25 [hpu_model_runner.py:3029] [Warmup][Graph/decode][90/135] batch_size:8 query_len:1 num_blocks:128 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55078)[0;0m INFO 08-09 12:41:25 [hpu_model_runner.py:3029] [Warmup][Graph/decode][90/135] batch_size:8 query_len:1 num_blocks:128 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55081)[0;0m INFO 08-09 12:41:25 [hpu_model_runner.py:3029] [Warmup][Graph/decode][90/135] batch_size:8 query_len:1 num_blocks:128 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55080)[0;0m INFO 08-09 12:41:25 [hpu_model_runner.py:3029] [Warmup][Graph/decode][90/135] batch_size:8 query_len:1 num_blocks:128 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55077)[0;0m INFO 08-09 12:41:25 [hpu_model_runner.py:3029] [Warmup][Graph/decode][90/135] batch_size:8 query_len:1 num_blocks:128 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55076)[0;0m INFO 08-09 12:41:25 [hpu_model_runner.py:3029] [Warmup][Graph/decode][90/135] batch_size:8 query_len:1 num_blocks:128 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55079)[0;0m INFO 08-09 12:41:25 [hpu_model_runner.py:3029] [Warmup][Graph/decode][90/135] batch_size:8 query_len:1 num_blocks:128 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55075)[0;0m INFO 08-09 12:41:25 [hpu_model_runner.py:3029] [Warmup][Graph/decode][90/135] batch_size:8 query_len:1 num_blocks:128 free_mem:17.67 GiB
INFO 08-09 12:41:30 [hpu_model_runner.py:3029] [Warmup][Graph/decode][91/135] batch_size:4 query_len:1 num_blocks:9639 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55080)[0;0m INFO 08-09 12:41:30 [hpu_model_runner.py:3029] [Warmup][Graph/decode][91/135] batch_size:4 query_len:1 num_blocks:9639 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55079)[0;0m INFO 08-09 12:41:30 [hpu_model_runner.py:3029] [Warmup][Graph/decode][91/135] batch_size:4 query_len:1 num_blocks:9639 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55077)[0;0m INFO 08-09 12:41:30 [hpu_model_runner.py:3029] [Warmup][Graph/decode][91/135] batch_size:4 query_len:1 num_blocks:9639 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55075)[0;0m INFO 08-09 12:41:30 [hpu_model_runner.py:3029] [Warmup][Graph/decode][91/135] batch_size:4 query_len:1 num_blocks:9639 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55076)[0;0m INFO 08-09 12:41:30 [hpu_model_runner.py:3029] [Warmup][Graph/decode][91/135] batch_size:4 query_len:1 num_blocks:9639 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55081)[0;0m INFO 08-09 12:41:30 [hpu_model_runner.py:3029] [Warmup][Graph/decode][91/135] batch_size:4 query_len:1 num_blocks:9639 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55078)[0;0m INFO 08-09 12:41:30 [hpu_model_runner.py:3029] [Warmup][Graph/decode][91/135] batch_size:4 query_len:1 num_blocks:9639 free_mem:17.67 GiB
INFO 08-09 12:41:35 [hpu_model_runner.py:3029] [Warmup][Graph/decode][92/135] batch_size:4 query_len:1 num_blocks:7168 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55081)[0;0m INFO 08-09 12:41:35 [hpu_model_runner.py:3029] [Warmup][Graph/decode][92/135] batch_size:4 query_len:1 num_blocks:7168 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55079)[0;0m INFO 08-09 12:41:35 [hpu_model_runner.py:3029] [Warmup][Graph/decode][92/135] batch_size:4 query_len:1 num_blocks:7168 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55076)[0;0m INFO 08-09 12:41:35 [hpu_model_runner.py:3029] [Warmup][Graph/decode][92/135] batch_size:4 query_len:1 num_blocks:7168 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55077)[0;0m INFO 08-09 12:41:35 [hpu_model_runner.py:3029] [Warmup][Graph/decode][92/135] batch_size:4 query_len:1 num_blocks:7168 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55078)[0;0m INFO 08-09 12:41:35 [hpu_model_runner.py:3029] [Warmup][Graph/decode][92/135] batch_size:4 query_len:1 num_blocks:7168 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55080)[0;0m INFO 08-09 12:41:35 [hpu_model_runner.py:3029] [Warmup][Graph/decode][92/135] batch_size:4 query_len:1 num_blocks:7168 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55075)[0;0m INFO 08-09 12:41:35 [hpu_model_runner.py:3029] [Warmup][Graph/decode][92/135] batch_size:4 query_len:1 num_blocks:7168 free_mem:17.67 GiB
INFO 08-09 12:41:41 [hpu_model_runner.py:3029] [Warmup][Graph/decode][93/135] batch_size:4 query_len:1 num_blocks:5248 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55081)[0;0m INFO 08-09 12:41:41 [hpu_model_runner.py:3029] [Warmup][Graph/decode][93/135] batch_size:4 query_len:1 num_blocks:5248 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55079)[0;0m INFO 08-09 12:41:41 [hpu_model_runner.py:3029] [Warmup][Graph/decode][93/135] batch_size:4 query_len:1 num_blocks:5248 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55078)[0;0m INFO 08-09 12:41:41 [hpu_model_runner.py:3029] [Warmup][Graph/decode][93/135] batch_size:4 query_len:1 num_blocks:5248 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55075)[0;0m INFO 08-09 12:41:41 [hpu_model_runner.py:3029] [Warmup][Graph/decode][93/135] batch_size:4 query_len:1 num_blocks:5248 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55080)[0;0m INFO 08-09 12:41:41 [hpu_model_runner.py:3029] [Warmup][Graph/decode][93/135] batch_size:4 query_len:1 num_blocks:5248 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55076)[0;0m INFO 08-09 12:41:41 [hpu_model_runner.py:3029] [Warmup][Graph/decode][93/135] batch_size:4 query_len:1 num_blocks:5248 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55077)[0;0m INFO 08-09 12:41:41 [hpu_model_runner.py:3029] [Warmup][Graph/decode][93/135] batch_size:4 query_len:1 num_blocks:5248 free_mem:17.67 GiB
INFO 08-09 12:41:46 [hpu_model_runner.py:3029] [Warmup][Graph/decode][94/135] batch_size:4 query_len:1 num_blocks:3840 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55081)[0;0m INFO 08-09 12:41:46 [hpu_model_runner.py:3029] [Warmup][Graph/decode][94/135] batch_size:4 query_len:1 num_blocks:3840 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55076)[0;0m INFO 08-09 12:41:46 [hpu_model_runner.py:3029] [Warmup][Graph/decode][94/135] batch_size:4 query_len:1 num_blocks:3840 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55077)[0;0m INFO 08-09 12:41:46 [hpu_model_runner.py:3029] [Warmup][Graph/decode][94/135] batch_size:4 query_len:1 num_blocks:3840 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55079)[0;0m INFO 08-09 12:41:46 [hpu_model_runner.py:3029] [Warmup][Graph/decode][94/135] batch_size:4 query_len:1 num_blocks:3840 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55075)[0;0m INFO 08-09 12:41:46 [hpu_model_runner.py:3029] [Warmup][Graph/decode][94/135] batch_size:4 query_len:1 num_blocks:3840 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55078)[0;0m INFO 08-09 12:41:46 [hpu_model_runner.py:3029] [Warmup][Graph/decode][94/135] batch_size:4 query_len:1 num_blocks:3840 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55080)[0;0m INFO 08-09 12:41:46 [hpu_model_runner.py:3029] [Warmup][Graph/decode][94/135] batch_size:4 query_len:1 num_blocks:3840 free_mem:17.67 GiB
INFO 08-09 12:41:51 [hpu_model_runner.py:3029] [Warmup][Graph/decode][95/135] batch_size:4 query_len:1 num_blocks:2816 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55079)[0;0m INFO 08-09 12:41:51 [hpu_model_runner.py:3029] [Warmup][Graph/decode][95/135] batch_size:4 query_len:1 num_blocks:2816 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55075)[0;0m INFO 08-09 12:41:51 [hpu_model_runner.py:3029] [Warmup][Graph/decode][95/135] batch_size:4 query_len:1 num_blocks:2816 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55080)[0;0m INFO 08-09 12:41:51 [hpu_model_runner.py:3029] [Warmup][Graph/decode][95/135] batch_size:4 query_len:1 num_blocks:2816 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55081)[0;0m INFO 08-09 12:41:51 [hpu_model_runner.py:3029] [Warmup][Graph/decode][95/135] batch_size:4 query_len:1 num_blocks:2816 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55077)[0;0m INFO 08-09 12:41:51 [hpu_model_runner.py:3029] [Warmup][Graph/decode][95/135] batch_size:4 query_len:1 num_blocks:2816 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55076)[0;0m INFO 08-09 12:41:51 [hpu_model_runner.py:3029] [Warmup][Graph/decode][95/135] batch_size:4 query_len:1 num_blocks:2816 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55078)[0;0m INFO 08-09 12:41:51 [hpu_model_runner.py:3029] [Warmup][Graph/decode][95/135] batch_size:4 query_len:1 num_blocks:2816 free_mem:17.67 GiB
INFO 08-09 12:41:56 [hpu_model_runner.py:3029] [Warmup][Graph/decode][96/135] batch_size:4 query_len:1 num_blocks:2176 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55080)[0;0m INFO 08-09 12:41:56 [hpu_model_runner.py:3029] [Warmup][Graph/decode][96/135] batch_size:4 query_len:1 num_blocks:2176 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55075)[0;0m INFO 08-09 12:41:56 [hpu_model_runner.py:3029] [Warmup][Graph/decode][96/135] batch_size:4 query_len:1 num_blocks:2176 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55079)[0;0m INFO 08-09 12:41:56 [hpu_model_runner.py:3029] [Warmup][Graph/decode][96/135] batch_size:4 query_len:1 num_blocks:2176 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55081)[0;0m INFO 08-09 12:41:56 [hpu_model_runner.py:3029] [Warmup][Graph/decode][96/135] batch_size:4 query_len:1 num_blocks:2176 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55076)[0;0m INFO 08-09 12:41:56 [hpu_model_runner.py:3029] [Warmup][Graph/decode][96/135] batch_size:4 query_len:1 num_blocks:2176 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55078)[0;0m INFO 08-09 12:41:56 [hpu_model_runner.py:3029] [Warmup][Graph/decode][96/135] batch_size:4 query_len:1 num_blocks:2176 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55077)[0;0m INFO 08-09 12:41:56 [hpu_model_runner.py:3029] [Warmup][Graph/decode][96/135] batch_size:4 query_len:1 num_blocks:2176 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55079)[0;0m INFO 08-09 12:42:01 [hpu_model_runner.py:3029] [Warmup][Graph/decode][97/135] batch_size:4 query_len:1 num_blocks:1536 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55077)[0;0m INFO 08-09 12:42:01 [hpu_model_runner.py:3029] [Warmup][Graph/decode][97/135] batch_size:4 query_len:1 num_blocks:1536 free_mem:17.67 GiB
INFO 08-09 12:42:01 [hpu_model_runner.py:3029] [Warmup][Graph/decode][97/135] batch_size:4 query_len:1 num_blocks:1536 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55076)[0;0m INFO 08-09 12:42:01 [hpu_model_runner.py:3029] [Warmup][Graph/decode][97/135] batch_size:4 query_len:1 num_blocks:1536 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55075)[0;0m INFO 08-09 12:42:01 [hpu_model_runner.py:3029] [Warmup][Graph/decode][97/135] batch_size:4 query_len:1 num_blocks:1536 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55081)[0;0m INFO 08-09 12:42:01 [hpu_model_runner.py:3029] [Warmup][Graph/decode][97/135] batch_size:4 query_len:1 num_blocks:1536 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55078)[0;0m INFO 08-09 12:42:01 [hpu_model_runner.py:3029] [Warmup][Graph/decode][97/135] batch_size:4 query_len:1 num_blocks:1536 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55080)[0;0m INFO 08-09 12:42:01 [hpu_model_runner.py:3029] [Warmup][Graph/decode][97/135] batch_size:4 query_len:1 num_blocks:1536 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55080)[0;0m INFO 08-09 12:42:06 [hpu_model_runner.py:3029] [Warmup][Graph/decode][98/135] batch_size:4 query_len:1 num_blocks:1152 free_mem:17.67 GiB
INFO 08-09 12:42:06 [hpu_model_runner.py:3029] [Warmup][Graph/decode][98/135] batch_size:4 query_len:1 num_blocks:1152 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55079)[0;0m INFO 08-09 12:42:06 [hpu_model_runner.py:3029] [Warmup][Graph/decode][98/135] batch_size:4 query_len:1 num_blocks:1152 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55081)[0;0m INFO 08-09 12:42:06 [hpu_model_runner.py:3029] [Warmup][Graph/decode][98/135] batch_size:4 query_len:1 num_blocks:1152 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55078)[0;0m INFO 08-09 12:42:06 [hpu_model_runner.py:3029] [Warmup][Graph/decode][98/135] batch_size:4 query_len:1 num_blocks:1152 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55077)[0;0m INFO 08-09 12:42:06 [hpu_model_runner.py:3029] [Warmup][Graph/decode][98/135] batch_size:4 query_len:1 num_blocks:1152 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55076)[0;0m INFO 08-09 12:42:06 [hpu_model_runner.py:3029] [Warmup][Graph/decode][98/135] batch_size:4 query_len:1 num_blocks:1152 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55075)[0;0m INFO 08-09 12:42:06 [hpu_model_runner.py:3029] [Warmup][Graph/decode][98/135] batch_size:4 query_len:1 num_blocks:1152 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55079)[0;0m INFO 08-09 12:42:11 [hpu_model_runner.py:3029] [Warmup][Graph/decode][99/135] batch_size:4 query_len:1 num_blocks:896 free_mem:17.67 GiB
INFO 08-09 12:42:11 [hpu_model_runner.py:3029] [Warmup][Graph/decode][99/135] batch_size:4 query_len:1 num_blocks:896 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55081)[0;0m INFO 08-09 12:42:11 [hpu_model_runner.py:3029] [Warmup][Graph/decode][99/135] batch_size:4 query_len:1 num_blocks:896 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55075)[0;0m INFO 08-09 12:42:11 [hpu_model_runner.py:3029] [Warmup][Graph/decode][99/135] batch_size:4 query_len:1 num_blocks:896 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55080)[0;0m INFO 08-09 12:42:11 [hpu_model_runner.py:3029] [Warmup][Graph/decode][99/135] batch_size:4 query_len:1 num_blocks:896 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55076)[0;0m INFO 08-09 12:42:11 [hpu_model_runner.py:3029] [Warmup][Graph/decode][99/135] batch_size:4 query_len:1 num_blocks:896 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55078)[0;0m INFO 08-09 12:42:11 [hpu_model_runner.py:3029] [Warmup][Graph/decode][99/135] batch_size:4 query_len:1 num_blocks:896 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55077)[0;0m INFO 08-09 12:42:11 [hpu_model_runner.py:3029] [Warmup][Graph/decode][99/135] batch_size:4 query_len:1 num_blocks:896 free_mem:17.67 GiB
INFO 08-09 12:42:16 [hpu_model_runner.py:3029] [Warmup][Graph/decode][100/135] batch_size:4 query_len:1 num_blocks:768 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55081)[0;0m INFO 08-09 12:42:16 [hpu_model_runner.py:3029] [Warmup][Graph/decode][100/135] batch_size:4 query_len:1 num_blocks:768 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55079)[0;0m INFO 08-09 12:42:16 [hpu_model_runner.py:3029] [Warmup][Graph/decode][100/135] batch_size:4 query_len:1 num_blocks:768 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55080)[0;0m INFO 08-09 12:42:16 [hpu_model_runner.py:3029] [Warmup][Graph/decode][100/135] batch_size:4 query_len:1 num_blocks:768 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55077)[0;0m INFO 08-09 12:42:16 [hpu_model_runner.py:3029] [Warmup][Graph/decode][100/135] batch_size:4 query_len:1 num_blocks:768 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55078)[0;0m INFO 08-09 12:42:16 [hpu_model_runner.py:3029] [Warmup][Graph/decode][100/135] batch_size:4 query_len:1 num_blocks:768 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55075)[0;0m INFO 08-09 12:42:16 [hpu_model_runner.py:3029] [Warmup][Graph/decode][100/135] batch_size:4 query_len:1 num_blocks:768 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55076)[0;0m INFO 08-09 12:42:16 [hpu_model_runner.py:3029] [Warmup][Graph/decode][100/135] batch_size:4 query_len:1 num_blocks:768 free_mem:17.67 GiB
INFO 08-09 12:42:20 [hpu_model_runner.py:3029] [Warmup][Graph/decode][101/135] batch_size:4 query_len:1 num_blocks:640 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55079)[0;0m INFO 08-09 12:42:20 [hpu_model_runner.py:3029] [Warmup][Graph/decode][101/135] batch_size:4 query_len:1 num_blocks:640 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55075)[0;0m INFO 08-09 12:42:20 [hpu_model_runner.py:3029] [Warmup][Graph/decode][101/135] batch_size:4 query_len:1 num_blocks:640 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55076)[0;0m INFO 08-09 12:42:20 [hpu_model_runner.py:3029] [Warmup][Graph/decode][101/135] batch_size:4 query_len:1 num_blocks:640 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55081)[0;0m INFO 08-09 12:42:20 [hpu_model_runner.py:3029] [Warmup][Graph/decode][101/135] batch_size:4 query_len:1 num_blocks:640 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55077)[0;0m INFO 08-09 12:42:20 [hpu_model_runner.py:3029] [Warmup][Graph/decode][101/135] batch_size:4 query_len:1 num_blocks:640 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55080)[0;0m INFO 08-09 12:42:20 [hpu_model_runner.py:3029] [Warmup][Graph/decode][101/135] batch_size:4 query_len:1 num_blocks:640 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55078)[0;0m INFO 08-09 12:42:20 [hpu_model_runner.py:3029] [Warmup][Graph/decode][101/135] batch_size:4 query_len:1 num_blocks:640 free_mem:17.67 GiB
INFO 08-09 12:42:26 [hpu_model_runner.py:3029] [Warmup][Graph/decode][102/135] batch_size:4 query_len:1 num_blocks:512 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55075)[0;0m INFO 08-09 12:42:26 [hpu_model_runner.py:3029] [Warmup][Graph/decode][102/135] batch_size:4 query_len:1 num_blocks:512 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55080)[0;0m INFO 08-09 12:42:26 [hpu_model_runner.py:3029] [Warmup][Graph/decode][102/135] batch_size:4 query_len:1 num_blocks:512 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55079)[0;0m INFO 08-09 12:42:26 [hpu_model_runner.py:3029] [Warmup][Graph/decode][102/135] batch_size:4 query_len:1 num_blocks:512 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55081)[0;0m INFO 08-09 12:42:26 [hpu_model_runner.py:3029] [Warmup][Graph/decode][102/135] batch_size:4 query_len:1 num_blocks:512 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55078)[0;0m INFO 08-09 12:42:26 [hpu_model_runner.py:3029] [Warmup][Graph/decode][102/135] batch_size:4 query_len:1 num_blocks:512 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55077)[0;0m INFO 08-09 12:42:26 [hpu_model_runner.py:3029] [Warmup][Graph/decode][102/135] batch_size:4 query_len:1 num_blocks:512 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55076)[0;0m INFO 08-09 12:42:26 [hpu_model_runner.py:3029] [Warmup][Graph/decode][102/135] batch_size:4 query_len:1 num_blocks:512 free_mem:17.67 GiB
INFO 08-09 12:42:31 [hpu_model_runner.py:3029] [Warmup][Graph/decode][103/135] batch_size:4 query_len:1 num_blocks:384 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55079)[0;0m INFO 08-09 12:42:31 [hpu_model_runner.py:3029] [Warmup][Graph/decode][103/135] batch_size:4 query_len:1 num_blocks:384 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55080)[0;0m INFO 08-09 12:42:31 [hpu_model_runner.py:3029] [Warmup][Graph/decode][103/135] batch_size:4 query_len:1 num_blocks:384 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55078)[0;0m INFO 08-09 12:42:31 [hpu_model_runner.py:3029] [Warmup][Graph/decode][103/135] batch_size:4 query_len:1 num_blocks:384 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55075)[0;0m INFO 08-09 12:42:31 [hpu_model_runner.py:3029] [Warmup][Graph/decode][103/135] batch_size:4 query_len:1 num_blocks:384 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55081)[0;0m INFO 08-09 12:42:31 [hpu_model_runner.py:3029] [Warmup][Graph/decode][103/135] batch_size:4 query_len:1 num_blocks:384 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55077)[0;0m INFO 08-09 12:42:31 [hpu_model_runner.py:3029] [Warmup][Graph/decode][103/135] batch_size:4 query_len:1 num_blocks:384 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55076)[0;0m INFO 08-09 12:42:31 [hpu_model_runner.py:3029] [Warmup][Graph/decode][103/135] batch_size:4 query_len:1 num_blocks:384 free_mem:17.67 GiB
INFO 08-09 12:42:36 [hpu_model_runner.py:3029] [Warmup][Graph/decode][104/135] batch_size:4 query_len:1 num_blocks:256 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55080)[0;0m INFO 08-09 12:42:36 [hpu_model_runner.py:3029] [Warmup][Graph/decode][104/135] batch_size:4 query_len:1 num_blocks:256 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55079)[0;0m INFO 08-09 12:42:36 [hpu_model_runner.py:3029] [Warmup][Graph/decode][104/135] batch_size:4 query_len:1 num_blocks:256 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55077)[0;0m INFO 08-09 12:42:36 [hpu_model_runner.py:3029] [Warmup][Graph/decode][104/135] batch_size:4 query_len:1 num_blocks:256 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55081)[0;0m INFO 08-09 12:42:36 [hpu_model_runner.py:3029] [Warmup][Graph/decode][104/135] batch_size:4 query_len:1 num_blocks:256 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55078)[0;0m INFO 08-09 12:42:36 [hpu_model_runner.py:3029] [Warmup][Graph/decode][104/135] batch_size:4 query_len:1 num_blocks:256 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55076)[0;0m INFO 08-09 12:42:36 [hpu_model_runner.py:3029] [Warmup][Graph/decode][104/135] batch_size:4 query_len:1 num_blocks:256 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55075)[0;0m INFO 08-09 12:42:36 [hpu_model_runner.py:3029] [Warmup][Graph/decode][104/135] batch_size:4 query_len:1 num_blocks:256 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55081)[0;0m INFO 08-09 12:42:41 [hpu_model_runner.py:3029] [Warmup][Graph/decode][105/135] batch_size:4 query_len:1 num_blocks:128 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55079)[0;0m INFO 08-09 12:42:41 [hpu_model_runner.py:3029] [Warmup][Graph/decode][105/135] batch_size:4 query_len:1 num_blocks:128 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55076)[0;0m INFO 08-09 12:42:41 [hpu_model_runner.py:3029] [Warmup][Graph/decode][105/135] batch_size:4 query_len:1 num_blocks:128 free_mem:17.67 GiB
INFO 08-09 12:42:41 [hpu_model_runner.py:3029] [Warmup][Graph/decode][105/135] batch_size:4 query_len:1 num_blocks:128 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55080)[0;0m INFO 08-09 12:42:41 [hpu_model_runner.py:3029] [Warmup][Graph/decode][105/135] batch_size:4 query_len:1 num_blocks:128 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55078)[0;0m INFO 08-09 12:42:41 [hpu_model_runner.py:3029] [Warmup][Graph/decode][105/135] batch_size:4 query_len:1 num_blocks:128 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55077)[0;0m INFO 08-09 12:42:41 [hpu_model_runner.py:3029] [Warmup][Graph/decode][105/135] batch_size:4 query_len:1 num_blocks:128 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55075)[0;0m INFO 08-09 12:42:41 [hpu_model_runner.py:3029] [Warmup][Graph/decode][105/135] batch_size:4 query_len:1 num_blocks:128 free_mem:17.67 GiB
INFO 08-09 12:42:46 [hpu_model_runner.py:3029] [Warmup][Graph/decode][106/135] batch_size:2 query_len:1 num_blocks:9639 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55076)[0;0m INFO 08-09 12:42:46 [hpu_model_runner.py:3029] [Warmup][Graph/decode][106/135] batch_size:2 query_len:1 num_blocks:9639 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55075)[0;0m INFO 08-09 12:42:46 [hpu_model_runner.py:3029] [Warmup][Graph/decode][106/135] batch_size:2 query_len:1 num_blocks:9639 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55081)[0;0m INFO 08-09 12:42:46 [hpu_model_runner.py:3029] [Warmup][Graph/decode][106/135] batch_size:2 query_len:1 num_blocks:9639 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55079)[0;0m INFO 08-09 12:42:46 [hpu_model_runner.py:3029] [Warmup][Graph/decode][106/135] batch_size:2 query_len:1 num_blocks:9639 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55080)[0;0m INFO 08-09 12:42:46 [hpu_model_runner.py:3029] [Warmup][Graph/decode][106/135] batch_size:2 query_len:1 num_blocks:9639 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55078)[0;0m INFO 08-09 12:42:46 [hpu_model_runner.py:3029] [Warmup][Graph/decode][106/135] batch_size:2 query_len:1 num_blocks:9639 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55077)[0;0m INFO 08-09 12:42:46 [hpu_model_runner.py:3029] [Warmup][Graph/decode][106/135] batch_size:2 query_len:1 num_blocks:9639 free_mem:17.67 GiB
INFO 08-09 12:42:52 [hpu_model_runner.py:3029] [Warmup][Graph/decode][107/135] batch_size:2 query_len:1 num_blocks:7168 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55081)[0;0m INFO 08-09 12:42:52 [hpu_model_runner.py:3029] [Warmup][Graph/decode][107/135] batch_size:2 query_len:1 num_blocks:7168 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55079)[0;0m INFO 08-09 12:42:52 [hpu_model_runner.py:3029] [Warmup][Graph/decode][107/135] batch_size:2 query_len:1 num_blocks:7168 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55080)[0;0m INFO 08-09 12:42:52 [hpu_model_runner.py:3029] [Warmup][Graph/decode][107/135] batch_size:2 query_len:1 num_blocks:7168 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55077)[0;0m INFO 08-09 12:42:52 [hpu_model_runner.py:3029] [Warmup][Graph/decode][107/135] batch_size:2 query_len:1 num_blocks:7168 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55075)[0;0m INFO 08-09 12:42:52 [hpu_model_runner.py:3029] [Warmup][Graph/decode][107/135] batch_size:2 query_len:1 num_blocks:7168 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55076)[0;0m INFO 08-09 12:42:52 [hpu_model_runner.py:3029] [Warmup][Graph/decode][107/135] batch_size:2 query_len:1 num_blocks:7168 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55078)[0;0m INFO 08-09 12:42:52 [hpu_model_runner.py:3029] [Warmup][Graph/decode][107/135] batch_size:2 query_len:1 num_blocks:7168 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55080)[0;0m INFO 08-09 12:42:57 [hpu_model_runner.py:3029] [Warmup][Graph/decode][108/135] batch_size:2 query_len:1 num_blocks:5248 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55076)[0;0m INFO 08-09 12:42:57 [hpu_model_runner.py:3029] [Warmup][Graph/decode][108/135] batch_size:2 query_len:1 num_blocks:5248 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55079)[0;0m INFO 08-09 12:42:57 [hpu_model_runner.py:3029] [Warmup][Graph/decode][108/135] batch_size:2 query_len:1 num_blocks:5248 free_mem:17.67 GiB
INFO 08-09 12:42:57 [hpu_model_runner.py:3029] [Warmup][Graph/decode][108/135] batch_size:2 query_len:1 num_blocks:5248 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55081)[0;0m INFO 08-09 12:42:57 [hpu_model_runner.py:3029] [Warmup][Graph/decode][108/135] batch_size:2 query_len:1 num_blocks:5248 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55078)[0;0m INFO 08-09 12:42:57 [hpu_model_runner.py:3029] [Warmup][Graph/decode][108/135] batch_size:2 query_len:1 num_blocks:5248 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55075)[0;0m INFO 08-09 12:42:57 [hpu_model_runner.py:3029] [Warmup][Graph/decode][108/135] batch_size:2 query_len:1 num_blocks:5248 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55077)[0;0m INFO 08-09 12:42:57 [hpu_model_runner.py:3029] [Warmup][Graph/decode][108/135] batch_size:2 query_len:1 num_blocks:5248 free_mem:17.67 GiB
INFO 08-09 12:43:02 [hpu_model_runner.py:3029] [Warmup][Graph/decode][109/135] batch_size:2 query_len:1 num_blocks:3840 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55080)[0;0m INFO 08-09 12:43:02 [hpu_model_runner.py:3029] [Warmup][Graph/decode][109/135] batch_size:2 query_len:1 num_blocks:3840 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55075)[0;0m INFO 08-09 12:43:02 [hpu_model_runner.py:3029] [Warmup][Graph/decode][109/135] batch_size:2 query_len:1 num_blocks:3840 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55077)[0;0m INFO 08-09 12:43:02 [hpu_model_runner.py:3029] [Warmup][Graph/decode][109/135] batch_size:2 query_len:1 num_blocks:3840 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55081)[0;0m INFO 08-09 12:43:02 [hpu_model_runner.py:3029] [Warmup][Graph/decode][109/135] batch_size:2 query_len:1 num_blocks:3840 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55076)[0;0m INFO 08-09 12:43:02 [hpu_model_runner.py:3029] [Warmup][Graph/decode][109/135] batch_size:2 query_len:1 num_blocks:3840 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55079)[0;0m INFO 08-09 12:43:02 [hpu_model_runner.py:3029] [Warmup][Graph/decode][109/135] batch_size:2 query_len:1 num_blocks:3840 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55078)[0;0m INFO 08-09 12:43:02 [hpu_model_runner.py:3029] [Warmup][Graph/decode][109/135] batch_size:2 query_len:1 num_blocks:3840 free_mem:17.67 GiB
INFO 08-09 12:43:07 [hpu_model_runner.py:3029] [Warmup][Graph/decode][110/135] batch_size:2 query_len:1 num_blocks:2816 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55080)[0;0m INFO 08-09 12:43:07 [hpu_model_runner.py:3029] [Warmup][Graph/decode][110/135] batch_size:2 query_len:1 num_blocks:2816 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55081)[0;0m INFO 08-09 12:43:07 [hpu_model_runner.py:3029] [Warmup][Graph/decode][110/135] batch_size:2 query_len:1 num_blocks:2816 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55077)[0;0m INFO 08-09 12:43:07 [hpu_model_runner.py:3029] [Warmup][Graph/decode][110/135] batch_size:2 query_len:1 num_blocks:2816 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55076)[0;0m INFO 08-09 12:43:07 [hpu_model_runner.py:3029] [Warmup][Graph/decode][110/135] batch_size:2 query_len:1 num_blocks:2816 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55075)[0;0m INFO 08-09 12:43:07 [hpu_model_runner.py:3029] [Warmup][Graph/decode][110/135] batch_size:2 query_len:1 num_blocks:2816 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55079)[0;0m INFO 08-09 12:43:07 [hpu_model_runner.py:3029] [Warmup][Graph/decode][110/135] batch_size:2 query_len:1 num_blocks:2816 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55078)[0;0m INFO 08-09 12:43:07 [hpu_model_runner.py:3029] [Warmup][Graph/decode][110/135] batch_size:2 query_len:1 num_blocks:2816 free_mem:17.67 GiB
INFO 08-09 12:43:13 [hpu_model_runner.py:3029] [Warmup][Graph/decode][111/135] batch_size:2 query_len:1 num_blocks:2176 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55080)[0;0m INFO 08-09 12:43:13 [hpu_model_runner.py:3029] [Warmup][Graph/decode][111/135] batch_size:2 query_len:1 num_blocks:2176 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55075)[0;0m INFO 08-09 12:43:13 [hpu_model_runner.py:3029] [Warmup][Graph/decode][111/135] batch_size:2 query_len:1 num_blocks:2176 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55081)[0;0m INFO 08-09 12:43:13 [hpu_model_runner.py:3029] [Warmup][Graph/decode][111/135] batch_size:2 query_len:1 num_blocks:2176 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55077)[0;0m INFO 08-09 12:43:13 [hpu_model_runner.py:3029] [Warmup][Graph/decode][111/135] batch_size:2 query_len:1 num_blocks:2176 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55076)[0;0m INFO 08-09 12:43:13 [hpu_model_runner.py:3029] [Warmup][Graph/decode][111/135] batch_size:2 query_len:1 num_blocks:2176 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55079)[0;0m INFO 08-09 12:43:13 [hpu_model_runner.py:3029] [Warmup][Graph/decode][111/135] batch_size:2 query_len:1 num_blocks:2176 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55078)[0;0m INFO 08-09 12:43:13 [hpu_model_runner.py:3029] [Warmup][Graph/decode][111/135] batch_size:2 query_len:1 num_blocks:2176 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55080)[0;0m INFO 08-09 12:43:18 [hpu_model_runner.py:3029] [Warmup][Graph/decode][112/135] batch_size:2 query_len:1 num_blocks:1536 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55081)[0;0m INFO 08-09 12:43:18 [hpu_model_runner.py:3029] [Warmup][Graph/decode][112/135] batch_size:2 query_len:1 num_blocks:1536 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55076)[0;0m INFO 08-09 12:43:18 [hpu_model_runner.py:3029] [Warmup][Graph/decode][112/135] batch_size:2 query_len:1 num_blocks:1536 free_mem:17.67 GiB
INFO 08-09 12:43:18 [hpu_model_runner.py:3029] [Warmup][Graph/decode][112/135] batch_size:2 query_len:1 num_blocks:1536 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55078)[0;0m INFO 08-09 12:43:18 [hpu_model_runner.py:3029] [Warmup][Graph/decode][112/135] batch_size:2 query_len:1 num_blocks:1536 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55079)[0;0m INFO 08-09 12:43:18 [hpu_model_runner.py:3029] [Warmup][Graph/decode][112/135] batch_size:2 query_len:1 num_blocks:1536 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55077)[0;0m INFO 08-09 12:43:18 [hpu_model_runner.py:3029] [Warmup][Graph/decode][112/135] batch_size:2 query_len:1 num_blocks:1536 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55075)[0;0m INFO 08-09 12:43:18 [hpu_model_runner.py:3029] [Warmup][Graph/decode][112/135] batch_size:2 query_len:1 num_blocks:1536 free_mem:17.67 GiB
INFO 08-09 12:43:23 [hpu_model_runner.py:3029] [Warmup][Graph/decode][113/135] batch_size:2 query_len:1 num_blocks:1152 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55079)[0;0m INFO 08-09 12:43:23 [hpu_model_runner.py:3029] [Warmup][Graph/decode][113/135] batch_size:2 query_len:1 num_blocks:1152 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55075)[0;0m INFO 08-09 12:43:23 [hpu_model_runner.py:3029] [Warmup][Graph/decode][113/135] batch_size:2 query_len:1 num_blocks:1152 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55080)[0;0m INFO 08-09 12:43:23 [hpu_model_runner.py:3029] [Warmup][Graph/decode][113/135] batch_size:2 query_len:1 num_blocks:1152 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55081)[0;0m INFO 08-09 12:43:23 [hpu_model_runner.py:3029] [Warmup][Graph/decode][113/135] batch_size:2 query_len:1 num_blocks:1152 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55077)[0;0m INFO 08-09 12:43:23 [hpu_model_runner.py:3029] [Warmup][Graph/decode][113/135] batch_size:2 query_len:1 num_blocks:1152 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55076)[0;0m INFO 08-09 12:43:23 [hpu_model_runner.py:3029] [Warmup][Graph/decode][113/135] batch_size:2 query_len:1 num_blocks:1152 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55078)[0;0m INFO 08-09 12:43:23 [hpu_model_runner.py:3029] [Warmup][Graph/decode][113/135] batch_size:2 query_len:1 num_blocks:1152 free_mem:17.67 GiB
INFO 08-09 12:43:29 [hpu_model_runner.py:3029] [Warmup][Graph/decode][114/135] batch_size:2 query_len:1 num_blocks:896 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55081)[0;0m INFO 08-09 12:43:29 [hpu_model_runner.py:3029] [Warmup][Graph/decode][114/135] batch_size:2 query_len:1 num_blocks:896 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55080)[0;0m INFO 08-09 12:43:29 [hpu_model_runner.py:3029] [Warmup][Graph/decode][114/135] batch_size:2 query_len:1 num_blocks:896 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55079)[0;0m INFO 08-09 12:43:29 [hpu_model_runner.py:3029] [Warmup][Graph/decode][114/135] batch_size:2 query_len:1 num_blocks:896 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55078)[0;0m INFO 08-09 12:43:29 [hpu_model_runner.py:3029] [Warmup][Graph/decode][114/135] batch_size:2 query_len:1 num_blocks:896 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55077)[0;0m INFO 08-09 12:43:29 [hpu_model_runner.py:3029] [Warmup][Graph/decode][114/135] batch_size:2 query_len:1 num_blocks:896 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55076)[0;0m INFO 08-09 12:43:29 [hpu_model_runner.py:3029] [Warmup][Graph/decode][114/135] batch_size:2 query_len:1 num_blocks:896 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55075)[0;0m INFO 08-09 12:43:29 [hpu_model_runner.py:3029] [Warmup][Graph/decode][114/135] batch_size:2 query_len:1 num_blocks:896 free_mem:17.67 GiB
INFO 08-09 12:43:34 [hpu_model_runner.py:3029] [Warmup][Graph/decode][115/135] batch_size:2 query_len:1 num_blocks:768 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55079)[0;0m INFO 08-09 12:43:34 [hpu_model_runner.py:3029] [Warmup][Graph/decode][115/135] batch_size:2 query_len:1 num_blocks:768 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55075)[0;0m INFO 08-09 12:43:34 [hpu_model_runner.py:3029] [Warmup][Graph/decode][115/135] batch_size:2 query_len:1 num_blocks:768 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55081)[0;0m INFO 08-09 12:43:34 [hpu_model_runner.py:3029] [Warmup][Graph/decode][115/135] batch_size:2 query_len:1 num_blocks:768 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55080)[0;0m INFO 08-09 12:43:34 [hpu_model_runner.py:3029] [Warmup][Graph/decode][115/135] batch_size:2 query_len:1 num_blocks:768 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55078)[0;0m INFO 08-09 12:43:34 [hpu_model_runner.py:3029] [Warmup][Graph/decode][115/135] batch_size:2 query_len:1 num_blocks:768 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55077)[0;0m INFO 08-09 12:43:34 [hpu_model_runner.py:3029] [Warmup][Graph/decode][115/135] batch_size:2 query_len:1 num_blocks:768 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55076)[0;0m INFO 08-09 12:43:34 [hpu_model_runner.py:3029] [Warmup][Graph/decode][115/135] batch_size:2 query_len:1 num_blocks:768 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55081)[0;0m INFO 08-09 12:43:39 [hpu_model_runner.py:3029] [Warmup][Graph/decode][116/135] batch_size:2 query_len:1 num_blocks:640 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55076)[0;0m INFO 08-09 12:43:39 [hpu_model_runner.py:3029] [Warmup][Graph/decode][116/135] batch_size:2 query_len:1 num_blocks:640 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55080)[0;0m INFO 08-09 12:43:39 [hpu_model_runner.py:3029] [Warmup][Graph/decode][116/135] batch_size:2 query_len:1 num_blocks:640 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55078)[0;0m INFO 08-09 12:43:39 [hpu_model_runner.py:3029] [Warmup][Graph/decode][116/135] batch_size:2 query_len:1 num_blocks:640 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55079)[0;0m INFO 08-09 12:43:39 [hpu_model_runner.py:3029] [Warmup][Graph/decode][116/135] batch_size:2 query_len:1 num_blocks:640 free_mem:17.67 GiB
INFO 08-09 12:43:39 [hpu_model_runner.py:3029] [Warmup][Graph/decode][116/135] batch_size:2 query_len:1 num_blocks:640 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55077)[0;0m INFO 08-09 12:43:39 [hpu_model_runner.py:3029] [Warmup][Graph/decode][116/135] batch_size:2 query_len:1 num_blocks:640 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55075)[0;0m INFO 08-09 12:43:39 [hpu_model_runner.py:3029] [Warmup][Graph/decode][116/135] batch_size:2 query_len:1 num_blocks:640 free_mem:17.67 GiB
INFO 08-09 12:43:45 [hpu_model_runner.py:3029] [Warmup][Graph/decode][117/135] batch_size:2 query_len:1 num_blocks:512 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55076)[0;0m INFO 08-09 12:43:45 [hpu_model_runner.py:3029] [Warmup][Graph/decode][117/135] batch_size:2 query_len:1 num_blocks:512 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55075)[0;0m INFO 08-09 12:43:45 [hpu_model_runner.py:3029] [Warmup][Graph/decode][117/135] batch_size:2 query_len:1 num_blocks:512 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55077)[0;0m INFO 08-09 12:43:45 [hpu_model_runner.py:3029] [Warmup][Graph/decode][117/135] batch_size:2 query_len:1 num_blocks:512 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55078)[0;0m INFO 08-09 12:43:45 [hpu_model_runner.py:3029] [Warmup][Graph/decode][117/135] batch_size:2 query_len:1 num_blocks:512 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55079)[0;0m INFO 08-09 12:43:45 [hpu_model_runner.py:3029] [Warmup][Graph/decode][117/135] batch_size:2 query_len:1 num_blocks:512 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55080)[0;0m INFO 08-09 12:43:45 [hpu_model_runner.py:3029] [Warmup][Graph/decode][117/135] batch_size:2 query_len:1 num_blocks:512 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55081)[0;0m INFO 08-09 12:43:45 [hpu_model_runner.py:3029] [Warmup][Graph/decode][117/135] batch_size:2 query_len:1 num_blocks:512 free_mem:17.67 GiB
INFO 08-09 12:43:50 [hpu_model_runner.py:3029] [Warmup][Graph/decode][118/135] batch_size:2 query_len:1 num_blocks:384 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55079)[0;0m INFO 08-09 12:43:50 [hpu_model_runner.py:3029] [Warmup][Graph/decode][118/135] batch_size:2 query_len:1 num_blocks:384 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55076)[0;0m INFO 08-09 12:43:50 [hpu_model_runner.py:3029] [Warmup][Graph/decode][118/135] batch_size:2 query_len:1 num_blocks:384 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55075)[0;0m INFO 08-09 12:43:50 [hpu_model_runner.py:3029] [Warmup][Graph/decode][118/135] batch_size:2 query_len:1 num_blocks:384 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55081)[0;0m INFO 08-09 12:43:50 [hpu_model_runner.py:3029] [Warmup][Graph/decode][118/135] batch_size:2 query_len:1 num_blocks:384 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55080)[0;0m INFO 08-09 12:43:50 [hpu_model_runner.py:3029] [Warmup][Graph/decode][118/135] batch_size:2 query_len:1 num_blocks:384 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55077)[0;0m INFO 08-09 12:43:50 [hpu_model_runner.py:3029] [Warmup][Graph/decode][118/135] batch_size:2 query_len:1 num_blocks:384 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55078)[0;0m INFO 08-09 12:43:50 [hpu_model_runner.py:3029] [Warmup][Graph/decode][118/135] batch_size:2 query_len:1 num_blocks:384 free_mem:17.67 GiB
INFO 08-09 12:43:55 [hpu_model_runner.py:3029] [Warmup][Graph/decode][119/135] batch_size:2 query_len:1 num_blocks:256 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55079)[0;0m INFO 08-09 12:43:55 [hpu_model_runner.py:3029] [Warmup][Graph/decode][119/135] batch_size:2 query_len:1 num_blocks:256 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55075)[0;0m INFO 08-09 12:43:55 [hpu_model_runner.py:3029] [Warmup][Graph/decode][119/135] batch_size:2 query_len:1 num_blocks:256 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55080)[0;0m INFO 08-09 12:43:55 [hpu_model_runner.py:3029] [Warmup][Graph/decode][119/135] batch_size:2 query_len:1 num_blocks:256 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55081)[0;0m INFO 08-09 12:43:55 [hpu_model_runner.py:3029] [Warmup][Graph/decode][119/135] batch_size:2 query_len:1 num_blocks:256 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55078)[0;0m INFO 08-09 12:43:55 [hpu_model_runner.py:3029] [Warmup][Graph/decode][119/135] batch_size:2 query_len:1 num_blocks:256 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55076)[0;0m INFO 08-09 12:43:55 [hpu_model_runner.py:3029] [Warmup][Graph/decode][119/135] batch_size:2 query_len:1 num_blocks:256 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55077)[0;0m INFO 08-09 12:43:55 [hpu_model_runner.py:3029] [Warmup][Graph/decode][119/135] batch_size:2 query_len:1 num_blocks:256 free_mem:17.67 GiB
INFO 08-09 12:44:01 [hpu_model_runner.py:3029] [Warmup][Graph/decode][120/135] batch_size:2 query_len:1 num_blocks:128 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55081)[0;0m INFO 08-09 12:44:01 [hpu_model_runner.py:3029] [Warmup][Graph/decode][120/135] batch_size:2 query_len:1 num_blocks:128 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55079)[0;0m INFO 08-09 12:44:01 [hpu_model_runner.py:3029] [Warmup][Graph/decode][120/135] batch_size:2 query_len:1 num_blocks:128 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55078)[0;0m INFO 08-09 12:44:01 [hpu_model_runner.py:3029] [Warmup][Graph/decode][120/135] batch_size:2 query_len:1 num_blocks:128 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55077)[0;0m INFO 08-09 12:44:01 [hpu_model_runner.py:3029] [Warmup][Graph/decode][120/135] batch_size:2 query_len:1 num_blocks:128 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55075)[0;0m INFO 08-09 12:44:01 [hpu_model_runner.py:3029] [Warmup][Graph/decode][120/135] batch_size:2 query_len:1 num_blocks:128 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55080)[0;0m INFO 08-09 12:44:01 [hpu_model_runner.py:3029] [Warmup][Graph/decode][120/135] batch_size:2 query_len:1 num_blocks:128 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55076)[0;0m INFO 08-09 12:44:01 [hpu_model_runner.py:3029] [Warmup][Graph/decode][120/135] batch_size:2 query_len:1 num_blocks:128 free_mem:17.67 GiB
INFO 08-09 12:44:06 [hpu_model_runner.py:3029] [Warmup][Graph/decode][121/135] batch_size:1 query_len:1 num_blocks:9639 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55079)[0;0m INFO 08-09 12:44:06 [hpu_model_runner.py:3029] [Warmup][Graph/decode][121/135] batch_size:1 query_len:1 num_blocks:9639 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55081)[0;0m INFO 08-09 12:44:06 [hpu_model_runner.py:3029] [Warmup][Graph/decode][121/135] batch_size:1 query_len:1 num_blocks:9639 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55078)[0;0m INFO 08-09 12:44:06 [hpu_model_runner.py:3029] [Warmup][Graph/decode][121/135] batch_size:1 query_len:1 num_blocks:9639 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55080)[0;0m INFO 08-09 12:44:06 [hpu_model_runner.py:3029] [Warmup][Graph/decode][121/135] batch_size:1 query_len:1 num_blocks:9639 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55077)[0;0m INFO 08-09 12:44:06 [hpu_model_runner.py:3029] [Warmup][Graph/decode][121/135] batch_size:1 query_len:1 num_blocks:9639 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55075)[0;0m INFO 08-09 12:44:06 [hpu_model_runner.py:3029] [Warmup][Graph/decode][121/135] batch_size:1 query_len:1 num_blocks:9639 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55076)[0;0m INFO 08-09 12:44:06 [hpu_model_runner.py:3029] [Warmup][Graph/decode][121/135] batch_size:1 query_len:1 num_blocks:9639 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55079)[0;0m INFO 08-09 12:44:12 [hpu_model_runner.py:3029] [Warmup][Graph/decode][122/135] batch_size:1 query_len:1 num_blocks:7168 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55077)[0;0m INFO 08-09 12:44:12 [hpu_model_runner.py:3029] [Warmup][Graph/decode][122/135] batch_size:1 query_len:1 num_blocks:7168 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55080)[0;0m INFO 08-09 12:44:12 [hpu_model_runner.py:3029] [Warmup][Graph/decode][122/135] batch_size:1 query_len:1 num_blocks:7168 free_mem:17.67 GiB
INFO 08-09 12:44:12 [hpu_model_runner.py:3029] [Warmup][Graph/decode][122/135] batch_size:1 query_len:1 num_blocks:7168 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55075)[0;0m INFO 08-09 12:44:12 [hpu_model_runner.py:3029] [Warmup][Graph/decode][122/135] batch_size:1 query_len:1 num_blocks:7168 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55078)[0;0m INFO 08-09 12:44:12 [hpu_model_runner.py:3029] [Warmup][Graph/decode][122/135] batch_size:1 query_len:1 num_blocks:7168 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55081)[0;0m INFO 08-09 12:44:12 [hpu_model_runner.py:3029] [Warmup][Graph/decode][122/135] batch_size:1 query_len:1 num_blocks:7168 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55076)[0;0m INFO 08-09 12:44:12 [hpu_model_runner.py:3029] [Warmup][Graph/decode][122/135] batch_size:1 query_len:1 num_blocks:7168 free_mem:17.67 GiB
INFO 08-09 12:44:18 [hpu_model_runner.py:3029] [Warmup][Graph/decode][123/135] batch_size:1 query_len:1 num_blocks:5248 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55080)[0;0m INFO 08-09 12:44:18 [hpu_model_runner.py:3029] [Warmup][Graph/decode][123/135] batch_size:1 query_len:1 num_blocks:5248 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55076)[0;0m INFO 08-09 12:44:18 [hpu_model_runner.py:3029] [Warmup][Graph/decode][123/135] batch_size:1 query_len:1 num_blocks:5248 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55081)[0;0m INFO 08-09 12:44:18 [hpu_model_runner.py:3029] [Warmup][Graph/decode][123/135] batch_size:1 query_len:1 num_blocks:5248 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55075)[0;0m INFO 08-09 12:44:18 [hpu_model_runner.py:3029] [Warmup][Graph/decode][123/135] batch_size:1 query_len:1 num_blocks:5248 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55077)[0;0m INFO 08-09 12:44:18 [hpu_model_runner.py:3029] [Warmup][Graph/decode][123/135] batch_size:1 query_len:1 num_blocks:5248 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55079)[0;0m INFO 08-09 12:44:18 [hpu_model_runner.py:3029] [Warmup][Graph/decode][123/135] batch_size:1 query_len:1 num_blocks:5248 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55078)[0;0m INFO 08-09 12:44:18 [hpu_model_runner.py:3029] [Warmup][Graph/decode][123/135] batch_size:1 query_len:1 num_blocks:5248 free_mem:17.67 GiB
INFO 08-09 12:44:23 [hpu_model_runner.py:3029] [Warmup][Graph/decode][124/135] batch_size:1 query_len:1 num_blocks:3840 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55080)[0;0m INFO 08-09 12:44:23 [hpu_model_runner.py:3029] [Warmup][Graph/decode][124/135] batch_size:1 query_len:1 num_blocks:3840 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55081)[0;0m INFO 08-09 12:44:23 [hpu_model_runner.py:3029] [Warmup][Graph/decode][124/135] batch_size:1 query_len:1 num_blocks:3840 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55079)[0;0m INFO 08-09 12:44:23 [hpu_model_runner.py:3029] [Warmup][Graph/decode][124/135] batch_size:1 query_len:1 num_blocks:3840 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55078)[0;0m INFO 08-09 12:44:23 [hpu_model_runner.py:3029] [Warmup][Graph/decode][124/135] batch_size:1 query_len:1 num_blocks:3840 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55076)[0;0m INFO 08-09 12:44:23 [hpu_model_runner.py:3029] [Warmup][Graph/decode][124/135] batch_size:1 query_len:1 num_blocks:3840 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55075)[0;0m INFO 08-09 12:44:23 [hpu_model_runner.py:3029] [Warmup][Graph/decode][124/135] batch_size:1 query_len:1 num_blocks:3840 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55077)[0;0m INFO 08-09 12:44:23 [hpu_model_runner.py:3029] [Warmup][Graph/decode][124/135] batch_size:1 query_len:1 num_blocks:3840 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55080)[0;0m INFO 08-09 12:44:29 [hpu_model_runner.py:3029] [Warmup][Graph/decode][125/135] batch_size:1 query_len:1 num_blocks:2816 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55079)[0;0m INFO 08-09 12:44:29 [hpu_model_runner.py:3029] [Warmup][Graph/decode][125/135] batch_size:1 query_len:1 num_blocks:2816 free_mem:17.67 GiB
INFO 08-09 12:44:29 [hpu_model_runner.py:3029] [Warmup][Graph/decode][125/135] batch_size:1 query_len:1 num_blocks:2816 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55075)[0;0m INFO 08-09 12:44:29 [hpu_model_runner.py:3029] [Warmup][Graph/decode][125/135] batch_size:1 query_len:1 num_blocks:2816 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55081)[0;0m INFO 08-09 12:44:29 [hpu_model_runner.py:3029] [Warmup][Graph/decode][125/135] batch_size:1 query_len:1 num_blocks:2816 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55077)[0;0m INFO 08-09 12:44:29 [hpu_model_runner.py:3029] [Warmup][Graph/decode][125/135] batch_size:1 query_len:1 num_blocks:2816 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55076)[0;0m INFO 08-09 12:44:29 [hpu_model_runner.py:3029] [Warmup][Graph/decode][125/135] batch_size:1 query_len:1 num_blocks:2816 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55078)[0;0m INFO 08-09 12:44:29 [hpu_model_runner.py:3029] [Warmup][Graph/decode][125/135] batch_size:1 query_len:1 num_blocks:2816 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55079)[0;0m INFO 08-09 12:44:34 [hpu_model_runner.py:3029] [Warmup][Graph/decode][126/135] batch_size:1 query_len:1 num_blocks:2176 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55081)[0;0m INFO 08-09 12:44:34 [hpu_model_runner.py:3029] [Warmup][Graph/decode][126/135] batch_size:1 query_len:1 num_blocks:2176 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55080)[0;0m INFO 08-09 12:44:34 [hpu_model_runner.py:3029] [Warmup][Graph/decode][126/135] batch_size:1 query_len:1 num_blocks:2176 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55075)[0;0m INFO 08-09 12:44:34 [hpu_model_runner.py:3029] [Warmup][Graph/decode][126/135] batch_size:1 query_len:1 num_blocks:2176 free_mem:17.67 GiB
INFO 08-09 12:44:34 [hpu_model_runner.py:3029] [Warmup][Graph/decode][126/135] batch_size:1 query_len:1 num_blocks:2176 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55078)[0;0m INFO 08-09 12:44:34 [hpu_model_runner.py:3029] [Warmup][Graph/decode][126/135] batch_size:1 query_len:1 num_blocks:2176 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55077)[0;0m INFO 08-09 12:44:34 [hpu_model_runner.py:3029] [Warmup][Graph/decode][126/135] batch_size:1 query_len:1 num_blocks:2176 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55076)[0;0m INFO 08-09 12:44:34 [hpu_model_runner.py:3029] [Warmup][Graph/decode][126/135] batch_size:1 query_len:1 num_blocks:2176 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55079)[0;0m INFO 08-09 12:44:40 [hpu_model_runner.py:3029] [Warmup][Graph/decode][127/135] batch_size:1 query_len:1 num_blocks:1536 free_mem:17.67 GiB
INFO 08-09 12:44:40 [hpu_model_runner.py:3029] [Warmup][Graph/decode][127/135] batch_size:1 query_len:1 num_blocks:1536 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55077)[0;0m INFO 08-09 12:44:40 [hpu_model_runner.py:3029] [Warmup][Graph/decode][127/135] batch_size:1 query_len:1 num_blocks:1536 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55080)[0;0m INFO 08-09 12:44:40 [hpu_model_runner.py:3029] [Warmup][Graph/decode][127/135] batch_size:1 query_len:1 num_blocks:1536 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55075)[0;0m INFO 08-09 12:44:40 [hpu_model_runner.py:3029] [Warmup][Graph/decode][127/135] batch_size:1 query_len:1 num_blocks:1536 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55081)[0;0m INFO 08-09 12:44:40 [hpu_model_runner.py:3029] [Warmup][Graph/decode][127/135] batch_size:1 query_len:1 num_blocks:1536 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55076)[0;0m INFO 08-09 12:44:40 [hpu_model_runner.py:3029] [Warmup][Graph/decode][127/135] batch_size:1 query_len:1 num_blocks:1536 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55078)[0;0m INFO 08-09 12:44:40 [hpu_model_runner.py:3029] [Warmup][Graph/decode][127/135] batch_size:1 query_len:1 num_blocks:1536 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55080)[0;0m INFO 08-09 12:44:45 [hpu_model_runner.py:3029] [Warmup][Graph/decode][128/135] batch_size:1 query_len:1 num_blocks:1152 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55079)[0;0m INFO 08-09 12:44:45 [hpu_model_runner.py:3029] [Warmup][Graph/decode][128/135] batch_size:1 query_len:1 num_blocks:1152 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55081)[0;0m INFO 08-09 12:44:45 [hpu_model_runner.py:3029] [Warmup][Graph/decode][128/135] batch_size:1 query_len:1 num_blocks:1152 free_mem:17.67 GiB
INFO 08-09 12:44:45 [hpu_model_runner.py:3029] [Warmup][Graph/decode][128/135] batch_size:1 query_len:1 num_blocks:1152 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55078)[0;0m INFO 08-09 12:44:45 [hpu_model_runner.py:3029] [Warmup][Graph/decode][128/135] batch_size:1 query_len:1 num_blocks:1152 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55077)[0;0m INFO 08-09 12:44:45 [hpu_model_runner.py:3029] [Warmup][Graph/decode][128/135] batch_size:1 query_len:1 num_blocks:1152 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55076)[0;0m INFO 08-09 12:44:45 [hpu_model_runner.py:3029] [Warmup][Graph/decode][128/135] batch_size:1 query_len:1 num_blocks:1152 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55075)[0;0m INFO 08-09 12:44:45 [hpu_model_runner.py:3029] [Warmup][Graph/decode][128/135] batch_size:1 query_len:1 num_blocks:1152 free_mem:17.67 GiB
INFO 08-09 12:44:51 [hpu_model_runner.py:3029] [Warmup][Graph/decode][129/135] batch_size:1 query_len:1 num_blocks:896 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55080)[0;0m INFO 08-09 12:44:51 [hpu_model_runner.py:3029] [Warmup][Graph/decode][129/135] batch_size:1 query_len:1 num_blocks:896 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55079)[0;0m INFO 08-09 12:44:51 [hpu_model_runner.py:3029] [Warmup][Graph/decode][129/135] batch_size:1 query_len:1 num_blocks:896 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55081)[0;0m INFO 08-09 12:44:51 [hpu_model_runner.py:3029] [Warmup][Graph/decode][129/135] batch_size:1 query_len:1 num_blocks:896 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55077)[0;0m INFO 08-09 12:44:51 [hpu_model_runner.py:3029] [Warmup][Graph/decode][129/135] batch_size:1 query_len:1 num_blocks:896 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55075)[0;0m INFO 08-09 12:44:51 [hpu_model_runner.py:3029] [Warmup][Graph/decode][129/135] batch_size:1 query_len:1 num_blocks:896 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55078)[0;0m INFO 08-09 12:44:51 [hpu_model_runner.py:3029] [Warmup][Graph/decode][129/135] batch_size:1 query_len:1 num_blocks:896 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55076)[0;0m INFO 08-09 12:44:51 [hpu_model_runner.py:3029] [Warmup][Graph/decode][129/135] batch_size:1 query_len:1 num_blocks:896 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55081)[0;0m INFO 08-09 12:44:56 [hpu_model_runner.py:3029] [Warmup][Graph/decode][130/135] batch_size:1 query_len:1 num_blocks:768 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55080)[0;0m INFO 08-09 12:44:56 [hpu_model_runner.py:3029] [Warmup][Graph/decode][130/135] batch_size:1 query_len:1 num_blocks:768 free_mem:17.67 GiB
INFO 08-09 12:44:56 [hpu_model_runner.py:3029] [Warmup][Graph/decode][130/135] batch_size:1 query_len:1 num_blocks:768 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55079)[0;0m INFO 08-09 12:44:56 [hpu_model_runner.py:3029] [Warmup][Graph/decode][130/135] batch_size:1 query_len:1 num_blocks:768 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55076)[0;0m INFO 08-09 12:44:56 [hpu_model_runner.py:3029] [Warmup][Graph/decode][130/135] batch_size:1 query_len:1 num_blocks:768 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55077)[0;0m INFO 08-09 12:44:56 [hpu_model_runner.py:3029] [Warmup][Graph/decode][130/135] batch_size:1 query_len:1 num_blocks:768 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55075)[0;0m INFO 08-09 12:44:56 [hpu_model_runner.py:3029] [Warmup][Graph/decode][130/135] batch_size:1 query_len:1 num_blocks:768 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55078)[0;0m INFO 08-09 12:44:56 [hpu_model_runner.py:3029] [Warmup][Graph/decode][130/135] batch_size:1 query_len:1 num_blocks:768 free_mem:17.67 GiB
INFO 08-09 12:45:02 [hpu_model_runner.py:3029] [Warmup][Graph/decode][131/135] batch_size:1 query_len:1 num_blocks:640 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55079)[0;0m INFO 08-09 12:45:02 [hpu_model_runner.py:3029] [Warmup][Graph/decode][131/135] batch_size:1 query_len:1 num_blocks:640 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55080)[0;0m INFO 08-09 12:45:02 [hpu_model_runner.py:3029] [Warmup][Graph/decode][131/135] batch_size:1 query_len:1 num_blocks:640 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55076)[0;0m INFO 08-09 12:45:02 [hpu_model_runner.py:3029] [Warmup][Graph/decode][131/135] batch_size:1 query_len:1 num_blocks:640 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55075)[0;0m INFO 08-09 12:45:02 [hpu_model_runner.py:3029] [Warmup][Graph/decode][131/135] batch_size:1 query_len:1 num_blocks:640 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55081)[0;0m INFO 08-09 12:45:02 [hpu_model_runner.py:3029] [Warmup][Graph/decode][131/135] batch_size:1 query_len:1 num_blocks:640 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55078)[0;0m INFO 08-09 12:45:02 [hpu_model_runner.py:3029] [Warmup][Graph/decode][131/135] batch_size:1 query_len:1 num_blocks:640 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55077)[0;0m INFO 08-09 12:45:02 [hpu_model_runner.py:3029] [Warmup][Graph/decode][131/135] batch_size:1 query_len:1 num_blocks:640 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55079)[0;0m INFO 08-09 12:45:08 [hpu_model_runner.py:3029] [Warmup][Graph/decode][132/135] batch_size:1 query_len:1 num_blocks:512 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55081)[0;0m INFO 08-09 12:45:08 [hpu_model_runner.py:3029] [Warmup][Graph/decode][132/135] batch_size:1 query_len:1 num_blocks:512 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55076)[0;0m INFO 08-09 12:45:08 [hpu_model_runner.py:3029] [Warmup][Graph/decode][132/135] batch_size:1 query_len:1 num_blocks:512 free_mem:17.67 GiB
INFO 08-09 12:45:08 [hpu_model_runner.py:3029] [Warmup][Graph/decode][132/135] batch_size:1 query_len:1 num_blocks:512 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55080)[0;0m INFO 08-09 12:45:08 [hpu_model_runner.py:3029] [Warmup][Graph/decode][132/135] batch_size:1 query_len:1 num_blocks:512 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55075)[0;0m INFO 08-09 12:45:08 [hpu_model_runner.py:3029] [Warmup][Graph/decode][132/135] batch_size:1 query_len:1 num_blocks:512 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55077)[0;0m INFO 08-09 12:45:08 [hpu_model_runner.py:3029] [Warmup][Graph/decode][132/135] batch_size:1 query_len:1 num_blocks:512 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55078)[0;0m INFO 08-09 12:45:08 [hpu_model_runner.py:3029] [Warmup][Graph/decode][132/135] batch_size:1 query_len:1 num_blocks:512 free_mem:17.67 GiB
INFO 08-09 12:45:13 [hpu_model_runner.py:3029] [Warmup][Graph/decode][133/135] batch_size:1 query_len:1 num_blocks:384 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55080)[0;0m INFO 08-09 12:45:13 [hpu_model_runner.py:3029] [Warmup][Graph/decode][133/135] batch_size:1 query_len:1 num_blocks:384 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55081)[0;0m INFO 08-09 12:45:13 [hpu_model_runner.py:3029] [Warmup][Graph/decode][133/135] batch_size:1 query_len:1 num_blocks:384 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55079)[0;0m INFO 08-09 12:45:13 [hpu_model_runner.py:3029] [Warmup][Graph/decode][133/135] batch_size:1 query_len:1 num_blocks:384 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55077)[0;0m INFO 08-09 12:45:13 [hpu_model_runner.py:3029] [Warmup][Graph/decode][133/135] batch_size:1 query_len:1 num_blocks:384 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55075)[0;0m INFO 08-09 12:45:13 [hpu_model_runner.py:3029] [Warmup][Graph/decode][133/135] batch_size:1 query_len:1 num_blocks:384 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55076)[0;0m INFO 08-09 12:45:13 [hpu_model_runner.py:3029] [Warmup][Graph/decode][133/135] batch_size:1 query_len:1 num_blocks:384 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55078)[0;0m INFO 08-09 12:45:13 [hpu_model_runner.py:3029] [Warmup][Graph/decode][133/135] batch_size:1 query_len:1 num_blocks:384 free_mem:17.67 GiB
INFO 08-09 12:45:19 [hpu_model_runner.py:3029] [Warmup][Graph/decode][134/135] batch_size:1 query_len:1 num_blocks:256 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55077)[0;0m INFO 08-09 12:45:19 [hpu_model_runner.py:3029] [Warmup][Graph/decode][134/135] batch_size:1 query_len:1 num_blocks:256 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55081)[0;0m INFO 08-09 12:45:19 [hpu_model_runner.py:3029] [Warmup][Graph/decode][134/135] batch_size:1 query_len:1 num_blocks:256 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55076)[0;0m INFO 08-09 12:45:19 [hpu_model_runner.py:3029] [Warmup][Graph/decode][134/135] batch_size:1 query_len:1 num_blocks:256 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55075)[0;0m INFO 08-09 12:45:19 [hpu_model_runner.py:3029] [Warmup][Graph/decode][134/135] batch_size:1 query_len:1 num_blocks:256 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55080)[0;0m INFO 08-09 12:45:19 [hpu_model_runner.py:3029] [Warmup][Graph/decode][134/135] batch_size:1 query_len:1 num_blocks:256 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55078)[0;0m INFO 08-09 12:45:19 [hpu_model_runner.py:3029] [Warmup][Graph/decode][134/135] batch_size:1 query_len:1 num_blocks:256 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55079)[0;0m INFO 08-09 12:45:19 [hpu_model_runner.py:3029] [Warmup][Graph/decode][134/135] batch_size:1 query_len:1 num_blocks:256 free_mem:17.67 GiB
INFO 08-09 12:45:25 [hpu_model_runner.py:3029] [Warmup][Graph/decode][135/135] batch_size:1 query_len:1 num_blocks:128 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55081)[0;0m INFO 08-09 12:45:25 [hpu_model_runner.py:3029] [Warmup][Graph/decode][135/135] batch_size:1 query_len:1 num_blocks:128 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55080)[0;0m INFO 08-09 12:45:25 [hpu_model_runner.py:3029] [Warmup][Graph/decode][135/135] batch_size:1 query_len:1 num_blocks:128 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55079)[0;0m INFO 08-09 12:45:25 [hpu_model_runner.py:3029] [Warmup][Graph/decode][135/135] batch_size:1 query_len:1 num_blocks:128 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55075)[0;0m INFO 08-09 12:45:25 [hpu_model_runner.py:3029] [Warmup][Graph/decode][135/135] batch_size:1 query_len:1 num_blocks:128 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55076)[0;0m INFO 08-09 12:45:25 [hpu_model_runner.py:3029] [Warmup][Graph/decode][135/135] batch_size:1 query_len:1 num_blocks:128 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55077)[0;0m INFO 08-09 12:45:25 [hpu_model_runner.py:3029] [Warmup][Graph/decode][135/135] batch_size:1 query_len:1 num_blocks:128 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55078)[0;0m INFO 08-09 12:45:25 [hpu_model_runner.py:3029] [Warmup][Graph/decode][135/135] batch_size:1 query_len:1 num_blocks:128 free_mem:17.67 GiB
[1;36m(VllmWorkerProcess pid=55078)[0;0m INFO 08-09 12:45:30 [hpu_model_runner.py:3152] Prompt captured:38 (100.0%) used_mem:0 B
[1;36m(VllmWorkerProcess pid=55078)[0;0m INFO 08-09 12:45:30 [hpu_model_runner.py:3152] Decode captured:135 (100.0%) used_mem:4 KiB
[1;36m(VllmWorkerProcess pid=55077)[0;0m INFO 08-09 12:45:30 [hpu_model_runner.py:3152] Prompt captured:38 (100.0%) used_mem:0 B
[1;36m(VllmWorkerProcess pid=55080)[0;0m INFO 08-09 12:45:30 [hpu_model_runner.py:3152] Prompt captured:38 (100.0%) used_mem:0 B
[1;36m(VllmWorkerProcess pid=55078)[0;0m INFO 08-09 12:45:30 [hpu_model_runner.py:3280] Warmup finished in 680 secs, allocated 0 B of device memory
INFO 08-09 12:45:30 [hpu_model_runner.py:3152] Prompt captured:38 (100.0%) used_mem:0 B
[1;36m(VllmWorkerProcess pid=55077)[0;0m INFO 08-09 12:45:30 [hpu_model_runner.py:3152] Decode captured:135 (100.0%) used_mem:4 KiB
[1;36m(VllmWorkerProcess pid=55081)[0;0m INFO 08-09 12:45:30 [hpu_model_runner.py:3152] Prompt captured:38 (100.0%) used_mem:0 B
[1;36m(VllmWorkerProcess pid=55079)[0;0m INFO 08-09 12:45:30 [hpu_model_runner.py:3152] Prompt captured:38 (100.0%) used_mem:0 B
INFO 08-09 12:45:30 [hpu_model_runner.py:3152] Decode captured:135 (100.0%) used_mem:4 KiB
[1;36m(VllmWorkerProcess pid=55081)[0;0m INFO 08-09 12:45:30 [hpu_model_runner.py:3152] Decode captured:135 (100.0%) used_mem:4 KiB
[1;36m(VllmWorkerProcess pid=55079)[0;0m INFO 08-09 12:45:30 [hpu_model_runner.py:3152] Decode captured:135 (100.0%) used_mem:4 KiB
[1;36m(VllmWorkerProcess pid=55076)[0;0m INFO 08-09 12:45:30 [hpu_model_runner.py:3152] Prompt captured:38 (100.0%) used_mem:0 B
[1;36m(VllmWorkerProcess pid=55075)[0;0m INFO 08-09 12:45:30 [hpu_model_runner.py:3152] Prompt captured:38 (100.0%) used_mem:0 B
[1;36m(VllmWorkerProcess pid=55077)[0;0m INFO 08-09 12:45:30 [hpu_model_runner.py:3280] Warmup finished in 680 secs, allocated 0 B of device memory
[1;36m(VllmWorkerProcess pid=55080)[0;0m INFO 08-09 12:45:30 [hpu_model_runner.py:3152] Decode captured:135 (100.0%) used_mem:4 KiB
[1;36m(VllmWorkerProcess pid=55076)[0;0m INFO 08-09 12:45:30 [hpu_model_runner.py:3152] Decode captured:135 (100.0%) used_mem:4 KiB
[1;36m(VllmWorkerProcess pid=55075)[0;0m INFO 08-09 12:45:30 [hpu_model_runner.py:3152] Decode captured:135 (100.0%) used_mem:4 KiB
INFO 08-09 12:45:30 [hpu_model_runner.py:3280] Warmup finished in 680 secs, allocated 0 B of device memory
[1;36m(VllmWorkerProcess pid=55079)[0;0m INFO 08-09 12:45:30 [hpu_model_runner.py:3280] Warmup finished in 680 secs, allocated 0 B of device memory
[1;36m(VllmWorkerProcess pid=55081)[0;0m INFO 08-09 12:45:30 [hpu_model_runner.py:3280] Warmup finished in 680 secs, allocated 0 B of device memory
[1;36m(VllmWorkerProcess pid=55080)[0;0m INFO 08-09 12:45:30 [hpu_model_runner.py:3280] Warmup finished in 680 secs, allocated 0 B of device memory
[1;36m(VllmWorkerProcess pid=55076)[0;0m INFO 08-09 12:45:31 [hpu_model_runner.py:3280] Warmup finished in 680 secs, allocated 0 B of device memory
[1;36m(VllmWorkerProcess pid=55075)[0;0m INFO 08-09 12:45:31 [hpu_model_runner.py:3280] Warmup finished in 680 secs, allocated 0 B of device memory
INFO 08-09 12:45:31 [llm_engine.py:431] init engine (profile, create kv cache, warmup model) took 684.16 seconds
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
WARNING 08-09 12:45:31 [config.py:1364] Default sampling parameters have been overridden by the model's Hugging Face generation config recommended from the model creator. If this is not intended, please relaunch vLLM instance with `--generation-config vllm`.
INFO 08-09 12:45:31 [serving_chat.py:118] Using default chat sampling params from model: {'temperature': 0.9, 'top_p': 0.6}
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
INFO 08-09 12:45:31 [serving_completion.py:66] Using default completion sampling params from model: {'temperature': 0.9, 'top_p': 0.6}
INFO 08-09 12:45:31 [api_server.py:1351] Starting vLLM API server 0 on http://0.0.0.0:8000
INFO 08-09 12:45:31 [launcher.py:29] Available routes are:
INFO 08-09 12:45:31 [launcher.py:37] Route: /openapi.json, Methods: HEAD, GET
INFO 08-09 12:45:31 [launcher.py:37] Route: /docs, Methods: HEAD, GET
INFO 08-09 12:45:31 [launcher.py:37] Route: /docs/oauth2-redirect, Methods: HEAD, GET
INFO 08-09 12:45:31 [launcher.py:37] Route: /redoc, Methods: HEAD, GET
INFO 08-09 12:45:31 [launcher.py:37] Route: /health, Methods: GET
INFO 08-09 12:45:31 [launcher.py:37] Route: /load, Methods: GET
INFO 08-09 12:45:31 [launcher.py:37] Route: /ping, Methods: POST
INFO 08-09 12:45:31 [launcher.py:37] Route: /ping, Methods: GET
INFO 08-09 12:45:31 [launcher.py:37] Route: /tokenize, Methods: POST
INFO 08-09 12:45:31 [launcher.py:37] Route: /detokenize, Methods: POST
INFO 08-09 12:45:31 [launcher.py:37] Route: /v1/models, Methods: GET
INFO 08-09 12:45:31 [launcher.py:37] Route: /version, Methods: GET
INFO 08-09 12:45:31 [launcher.py:37] Route: /v1/chat/completions, Methods: POST
INFO 08-09 12:45:31 [launcher.py:37] Route: /v1/completions, Methods: POST
INFO 08-09 12:45:31 [launcher.py:37] Route: /v1/embeddings, Methods: POST
INFO 08-09 12:45:31 [launcher.py:37] Route: /pooling, Methods: POST
INFO 08-09 12:45:31 [launcher.py:37] Route: /classify, Methods: POST
INFO 08-09 12:45:31 [launcher.py:37] Route: /score, Methods: POST
INFO 08-09 12:45:31 [launcher.py:37] Route: /v1/score, Methods: POST
INFO 08-09 12:45:31 [launcher.py:37] Route: /v1/audio/transcriptions, Methods: POST
INFO 08-09 12:45:31 [launcher.py:37] Route: /rerank, Methods: POST
INFO 08-09 12:45:31 [launcher.py:37] Route: /v1/rerank, Methods: POST
INFO 08-09 12:45:31 [launcher.py:37] Route: /v2/rerank, Methods: POST
INFO 08-09 12:45:31 [launcher.py:37] Route: /invocations, Methods: POST
INFO 08-09 12:45:31 [launcher.py:37] Route: /metrics, Methods: GET
INFO:     Started server process [54097]
INFO:     Waiting for application startup.
INFO:     Application startup complete.
INFO 08-09 12:48:36 [logger.py:43] Received request cmpl-c6be26dcf3fb4704a81a32ba28cc3ae5-0: prompt: 'What is LLaMA 2 and how does it work?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 365, 5661, 1529, 29871, 29906, 322, 920, 947, 372, 664, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:60158 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 12:48:36 [engine.py:331] Added request cmpl-c6be26dcf3fb4704a81a32ba28cc3ae5-0.
INFO 08-09 12:48:38 [metrics.py:417] Avg prompt throughput: 2.0 tokens/s, Avg generation throughput: 0.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 12:48:43 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 52.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 12:48:48 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 82.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 12:48:51 [logger.py:43] Received request cmpl-67941c218aa34ae1bd5c42754d0bcb6d-0: prompt: 'Explain the architecture of a transformer model.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 12027, 7420, 278, 11258, 310, 263, 4327, 261, 1904, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:34346 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 12:48:51 [engine.py:331] Added request cmpl-67941c218aa34ae1bd5c42754d0bcb6d-0.
INFO 08-09 12:48:53 [metrics.py:417] Avg prompt throughput: 2.2 tokens/s, Avg generation throughput: 88.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 12:48:58 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 83.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 12:49:03 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 89.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 12:49:03 [logger.py:43] Received request cmpl-af57cee2dbf943cfb1e4b90a80412eee-0: prompt: 'What are the core differences between CNN and RNN?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 526, 278, 7136, 12651, 1546, 29696, 322, 390, 10262, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:59370 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 12:49:03 [engine.py:331] Added request cmpl-af57cee2dbf943cfb1e4b90a80412eee-0.
INFO 08-09 12:49:08 [metrics.py:417] Avg prompt throughput: 2.4 tokens/s, Avg generation throughput: 85.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 12:49:11 [logger.py:43] Received request cmpl-b85a4f1f7d374bb3a69baeaaf5078c9f-0: prompt: 'Describe how attention mechanism improves translation models.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 20355, 915, 920, 8570, 13336, 4857, 1960, 13962, 4733, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:52536 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 12:49:11 [engine.py:331] Added request cmpl-b85a4f1f7d374bb3a69baeaaf5078c9f-0.
INFO 08-09 12:49:13 [metrics.py:417] Avg prompt throughput: 2.2 tokens/s, Avg generation throughput: 87.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 12:49:18 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 88.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 12:49:23 [logger.py:43] Received request cmpl-ac22711a96c9469e8f62a680ef1dd365-0: prompt: 'What is the role of positional encoding in transformers?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 278, 6297, 310, 2602, 284, 8025, 297, 4327, 414, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:43828 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 12:49:23 [engine.py:331] Added request cmpl-ac22711a96c9469e8f62a680ef1dd365-0.
INFO 08-09 12:49:23 [metrics.py:417] Avg prompt throughput: 2.6 tokens/s, Avg generation throughput: 90.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 12:49:28 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 87.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 12:49:33 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 87.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 12:49:34 [logger.py:43] Received request cmpl-37f3457906ac45559e66062a6da571e1-0: prompt: 'How does self-attention differ from cross-attention?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 947, 1583, 29899, 1131, 2509, 1163, 515, 4891, 29899, 1131, 2509, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:57410 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 12:49:34 [engine.py:331] Added request cmpl-37f3457906ac45559e66062a6da571e1-0.
INFO 08-09 12:49:38 [metrics.py:417] Avg prompt throughput: 2.8 tokens/s, Avg generation throughput: 88.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 12:49:43 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 88.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 12:49:45 [logger.py:43] Received request cmpl-6efc5424443849a8b5411e478b6c4020-0: prompt: 'Explain the concept of tokenization in NLP.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 12027, 7420, 278, 6964, 310, 5993, 2133, 297, 405, 13208, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:55970 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 12:49:45 [engine.py:331] Added request cmpl-6efc5424443849a8b5411e478b6c4020-0.
INFO 08-09 12:49:48 [metrics.py:417] Avg prompt throughput: 2.4 tokens/s, Avg generation throughput: 88.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 12:49:53 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 87.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 12:49:57 [logger.py:43] Received request cmpl-fbc530e0df9f45adb62cd42390d3b8e1-0: prompt: 'What is the vanishing gradient problem and how to mitigate it?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 278, 1109, 14424, 16030, 1108, 322, 920, 304, 1380, 335, 403, 372, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:53442 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 12:49:57 [engine.py:331] Added request cmpl-fbc530e0df9f45adb62cd42390d3b8e1-0.
INFO 08-09 12:49:58 [metrics.py:417] Avg prompt throughput: 3.2 tokens/s, Avg generation throughput: 84.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 12:50:03 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 87.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 12:50:08 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 91.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 12:50:08 [logger.py:43] Received request cmpl-6f0369141f054329817a536d668ce061-0: prompt: 'Describe how Gaudi architecture differs from NVIDIA GPUs.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 20355, 915, 920, 10415, 4749, 11258, 2923, 414, 515, 405, 13044, 10764, 22796, 29879, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:57102 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 12:50:08 [engine.py:331] Added request cmpl-6f0369141f054329817a536d668ce061-0.
INFO 08-09 12:50:11 [logger.py:43] Received request cmpl-6e973395d60e44979a65ac958971fb40-0: prompt: 'What is model parallelism and how does it help scale large models?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 1904, 8943, 1608, 322, 920, 947, 372, 1371, 6287, 2919, 4733, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:57112 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 12:50:11 [engine.py:331] Added request cmpl-6e973395d60e44979a65ac958971fb40-0.
INFO 08-09 12:50:13 [metrics.py:417] Avg prompt throughput: 6.2 tokens/s, Avg generation throughput: 82.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 12:50:18 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 91.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 12:50:22 [logger.py:43] Received request cmpl-ff8e665aa2b445c79c330f83ac15dfbf-0: prompt: 'Explain the difference between GPT and BERT architectures.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 12027, 7420, 278, 4328, 1546, 402, 7982, 322, 350, 20161, 6956, 1973, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:33698 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 12:50:22 [engine.py:331] Added request cmpl-ff8e665aa2b445c79c330f83ac15dfbf-0.
INFO 08-09 12:50:23 [metrics.py:417] Avg prompt throughput: 2.8 tokens/s, Avg generation throughput: 89.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 12:50:28 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 89.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 12:50:33 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 89.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 12:50:33 [logger.py:43] Received request cmpl-0e87a90827ec45bba63d7fd438710152-0: prompt: 'How does prompt engineering affect language model outputs?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 947, 9508, 21639, 6602, 4086, 1904, 14391, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:56424 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 12:50:33 [engine.py:331] Added request cmpl-0e87a90827ec45bba63d7fd438710152-0.
INFO 08-09 12:50:36 [logger.py:43] Received request cmpl-9febf406de1c4db39a01023f026e527d-0: prompt: 'What is the function of layer normalization in deep networks?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 278, 740, 310, 7546, 4226, 2133, 297, 6483, 14379, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:56430 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 12:50:36 [engine.py:331] Added request cmpl-9febf406de1c4db39a01023f026e527d-0.
INFO 08-09 12:50:38 [metrics.py:417] Avg prompt throughput: 4.6 tokens/s, Avg generation throughput: 86.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 12:50:43 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 91.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 12:50:47 [logger.py:43] Received request cmpl-8af69996a8c54291ac595c8e6976443b-0: prompt: 'Explain causal language modeling with examples.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 12027, 7420, 3269, 284, 4086, 1904, 292, 411, 6455, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:60920 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 12:50:47 [engine.py:331] Added request cmpl-8af69996a8c54291ac595c8e6976443b-0.
INFO 08-09 12:50:48 [metrics.py:417] Avg prompt throughput: 2.2 tokens/s, Avg generation throughput: 84.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 12:50:53 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 89.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 12:50:58 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 92.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 12:50:58 [logger.py:43] Received request cmpl-438cfe6b45c8417197920f6a9dd6cc66-0: prompt: 'What is quantization and how does it improve inference performance?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 4323, 2133, 322, 920, 947, 372, 11157, 27262, 4180, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:42874 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 12:50:58 [engine.py:331] Added request cmpl-438cfe6b45c8417197920f6a9dd6cc66-0.
INFO 08-09 12:51:03 [metrics.py:417] Avg prompt throughput: 2.6 tokens/s, Avg generation throughput: 84.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 12:51:08 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 88.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 12:51:10 [logger.py:43] Received request cmpl-2750a7a11f21416ea9995323b9a41d0f-0: prompt: 'Describe how fine-tuning works for large language models.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 20355, 915, 920, 2691, 29899, 29873, 27964, 1736, 363, 2919, 4086, 4733, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:52810 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 12:51:10 [engine.py:331] Added request cmpl-2750a7a11f21416ea9995323b9a41d0f-0.
INFO 08-09 12:51:13 [metrics.py:417] Avg prompt throughput: 2.8 tokens/s, Avg generation throughput: 88.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 12:51:18 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 91.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 12:51:21 [logger.py:43] Received request cmpl-5facee23415744bf96a50cdfa3d53b6a-0: prompt: 'What are LoRA adapters and how do they help in parameter-efficient training?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 526, 4309, 4717, 594, 481, 2153, 322, 920, 437, 896, 1371, 297, 3443, 29899, 8462, 6694, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:60094 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 12:51:21 [engine.py:331] Added request cmpl-5facee23415744bf96a50cdfa3d53b6a-0.
INFO 08-09 12:51:23 [metrics.py:417] Avg prompt throughput: 3.8 tokens/s, Avg generation throughput: 85.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 12:51:28 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 89.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 12:51:32 [logger.py:43] Received request cmpl-60276d2f5ad2411fbf7276e201812cee-0: prompt: 'Explain the purpose of rotary positional embeddings.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 12027, 7420, 278, 6437, 310, 5731, 653, 2602, 284, 8297, 29881, 886, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:39428 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 12:51:32 [engine.py:331] Added request cmpl-60276d2f5ad2411fbf7276e201812cee-0.
INFO 08-09 12:51:33 [metrics.py:417] Avg prompt throughput: 2.8 tokens/s, Avg generation throughput: 86.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 12:51:38 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 88.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 12:51:43 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 88.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 12:51:44 [logger.py:43] Received request cmpl-27a5294d0b7b402a8429e36745a15537-0: prompt: 'How does DeepSpeed improve model training efficiency?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 947, 21784, 26539, 11157, 1904, 6694, 19201, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:38046 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 12:51:44 [engine.py:331] Added request cmpl-27a5294d0b7b402a8429e36745a15537-0.
INFO 08-09 12:51:48 [metrics.py:417] Avg prompt throughput: 2.0 tokens/s, Avg generation throughput: 85.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 12:51:53 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 86.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 12:51:55 [logger.py:43] Received request cmpl-2a816ebc3dde4603b7d3e0e6abe971fd-0: prompt: 'Compare data parallelism and pipeline parallelism.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 3831, 598, 848, 8943, 1608, 322, 16439, 8943, 1608, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:58114 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 12:51:55 [engine.py:331] Added request cmpl-2a816ebc3dde4603b7d3e0e6abe971fd-0.
INFO 08-09 12:51:58 [metrics.py:417] Avg prompt throughput: 2.2 tokens/s, Avg generation throughput: 88.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 12:52:03 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 89.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 12:52:06 [logger.py:43] Received request cmpl-dc50f2981b6c48ad9f8a260617356bce-0: prompt: 'What is ALiBi positional encoding?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 14445, 29875, 20517, 2602, 284, 8025, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:34170 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 12:52:06 [engine.py:331] Added request cmpl-dc50f2981b6c48ad9f8a260617356bce-0.
INFO 08-09 12:52:08 [metrics.py:417] Avg prompt throughput: 2.0 tokens/s, Avg generation throughput: 89.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 12:52:13 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 83.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 12:52:18 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 82.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 12:52:18 [logger.py:43] Received request cmpl-e4ff18fd73ff4c72b2e2aed187e3b46f-0: prompt: 'Describe the purpose of HF Transformers library.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 20355, 915, 278, 6437, 310, 379, 29943, 4103, 689, 414, 3489, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:50486 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 12:52:18 [engine.py:331] Added request cmpl-e4ff18fd73ff4c72b2e2aed187e3b46f-0.
INFO 08-09 12:52:23 [metrics.py:417] Avg prompt throughput: 2.6 tokens/s, Avg generation throughput: 87.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 12:52:28 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 87.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 12:52:30 [logger.py:43] Received request cmpl-5a52c177ef3d44959c36184f921b903d-0: prompt: 'How do attention heads capture contextual meaning?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 437, 8570, 15883, 10446, 3030, 950, 6593, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:51394 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 12:52:30 [engine.py:331] Added request cmpl-5a52c177ef3d44959c36184f921b903d-0.
INFO 08-09 12:52:33 [metrics.py:417] Avg prompt throughput: 2.0 tokens/s, Avg generation throughput: 91.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 12:52:38 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 90.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 12:52:41 [logger.py:43] Received request cmpl-99402dee76f34d149cf920015e3b6dda-0: prompt: 'What is dynamic batching in inference servers?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 7343, 9853, 292, 297, 27262, 12424, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:36652 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 12:52:41 [engine.py:331] Added request cmpl-99402dee76f34d149cf920015e3b6dda-0.
INFO 08-09 12:52:43 [metrics.py:417] Avg prompt throughput: 2.0 tokens/s, Avg generation throughput: 89.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 12:52:48 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 88.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 12:52:52 [logger.py:43] Received request cmpl-9a475d69d15f4eb1895aadb12881f882-0: prompt: 'Explain greedy decoding vs beam search vs sampling.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 12027, 7420, 1395, 7584, 1602, 3689, 7186, 22913, 2740, 7186, 23460, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:47102 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 12:52:52 [engine.py:331] Added request cmpl-9a475d69d15f4eb1895aadb12881f882-0.
INFO 08-09 12:52:53 [metrics.py:417] Avg prompt throughput: 2.6 tokens/s, Avg generation throughput: 87.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 12:52:58 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 87.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 12:53:03 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 88.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 12:53:03 [logger.py:43] Received request cmpl-771d1ccc93a748b6b7736855db1f55b6-0: prompt: 'What is perplexity in language models and why is it important?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 639, 10709, 537, 297, 4086, 4733, 322, 2020, 338, 372, 4100, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:44432 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 12:53:03 [engine.py:331] Added request cmpl-771d1ccc93a748b6b7736855db1f55b6-0.
INFO 08-09 12:53:08 [metrics.py:417] Avg prompt throughput: 3.0 tokens/s, Avg generation throughput: 87.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 12:53:13 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 86.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 12:53:15 [logger.py:43] Received request cmpl-faa4795ba4734208b181ba29e9955ccc-0: prompt: 'How does mixed precision training benefit model training?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 947, 12849, 16716, 6694, 14169, 1904, 6694, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:51890 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 12:53:15 [engine.py:331] Added request cmpl-faa4795ba4734208b181ba29e9955ccc-0.
INFO 08-09 12:53:18 [metrics.py:417] Avg prompt throughput: 2.0 tokens/s, Avg generation throughput: 85.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 12:53:23 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 86.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 12:53:27 [logger.py:43] Received request cmpl-80c08a56ea5f4211a727823023486bf2-0: prompt: 'What is the function of the softmax layer in transformers?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 278, 740, 310, 278, 4964, 3317, 7546, 297, 4327, 414, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:43834 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 12:53:27 [engine.py:331] Added request cmpl-80c08a56ea5f4211a727823023486bf2-0.
INFO 08-09 12:53:28 [metrics.py:417] Avg prompt throughput: 2.8 tokens/s, Avg generation throughput: 82.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 12:53:32 [logger.py:43] Received request cmpl-9ed0d131276d4a79af0ff2529627f07d-0: prompt: "Explain embeddings and how they're used in NLP models.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 12027, 7420, 8297, 29881, 886, 322, 920, 896, 29915, 276, 1304, 297, 405, 13208, 4733, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:43846 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 12:53:32 [engine.py:331] Added request cmpl-9ed0d131276d4a79af0ff2529627f07d-0.
INFO 08-09 12:53:33 [metrics.py:417] Avg prompt throughput: 3.4 tokens/s, Avg generation throughput: 85.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 12:53:36 [logger.py:43] Received request cmpl-092b92eab9e6499dad13e640d3ddb513-0: prompt: "Describe the function of the tokenizer's vocabulary.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 20355, 915, 278, 740, 310, 278, 5993, 3950, 29915, 29879, 7931, 370, 352, 653, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:43860 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 12:53:36 [engine.py:331] Added request cmpl-092b92eab9e6499dad13e640d3ddb513-0.
INFO 08-09 12:53:38 [metrics.py:417] Avg prompt throughput: 3.2 tokens/s, Avg generation throughput: 86.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 12:53:43 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 86.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 12:53:47 [logger.py:43] Received request cmpl-7450880557704520aa2ce4bc472c0192-0: prompt: 'What is causal masking in transformer decoders?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 3269, 284, 11105, 292, 297, 4327, 261, 1602, 397, 414, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:37648 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 12:53:47 [engine.py:331] Added request cmpl-7450880557704520aa2ce4bc472c0192-0.
INFO 08-09 12:53:48 [metrics.py:417] Avg prompt throughput: 2.8 tokens/s, Avg generation throughput: 88.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 12:53:53 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 92.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 12:53:53 [logger.py:43] Received request cmpl-f648d5b2f8214002bcb612ed77441140-0: prompt: 'How does gradient checkpointing reduce memory usage?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 947, 16030, 1423, 3149, 292, 10032, 3370, 8744, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:37652 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 12:53:53 [engine.py:331] Added request cmpl-f648d5b2f8214002bcb612ed77441140-0.
INFO 08-09 12:53:58 [metrics.py:417] Avg prompt throughput: 2.2 tokens/s, Avg generation throughput: 91.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 12:54:03 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 85.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 12:54:05 [logger.py:43] Received request cmpl-013d135a25ec490faaaab0dfc42f94c3-0: prompt: 'Explain the difference between encoder-decoder and decoder-only models.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 12027, 7420, 278, 4328, 1546, 2094, 6119, 29899, 7099, 6119, 322, 1602, 6119, 29899, 6194, 4733, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:57938 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 12:54:05 [engine.py:331] Added request cmpl-013d135a25ec490faaaab0dfc42f94c3-0.
INFO 08-09 12:54:08 [metrics.py:417] Avg prompt throughput: 3.6 tokens/s, Avg generation throughput: 86.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 12:54:13 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 85.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 12:54:16 [logger.py:43] Received request cmpl-77bcd1ab6e284b4ca04f6208e1e9a4a5-0: prompt: 'What is zero-shot vs few-shot vs fine-tuned inference?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 5225, 29899, 8962, 7186, 2846, 29899, 8962, 7186, 2691, 29899, 29873, 348, 287, 27262, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:47694 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 12:54:16 [engine.py:331] Added request cmpl-77bcd1ab6e284b4ca04f6208e1e9a4a5-0.
INFO 08-09 12:54:18 [metrics.py:417] Avg prompt throughput: 3.6 tokens/s, Avg generation throughput: 84.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 12:54:23 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 86.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 12:54:28 [logger.py:43] Received request cmpl-3cad28fd7518412caebd19a854af879f-0: prompt: 'What are prompt templates and why do they matter?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 526, 9508, 17475, 322, 2020, 437, 896, 4383, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:53970 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 12:54:28 [engine.py:331] Added request cmpl-3cad28fd7518412caebd19a854af879f-0.
INFO 08-09 12:54:28 [metrics.py:417] Avg prompt throughput: 2.2 tokens/s, Avg generation throughput: 87.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 12:54:33 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 94.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 12:54:38 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 91.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 12:54:39 [logger.py:43] Received request cmpl-4adbf9ff3caf475fb8c94dd11e642ea8-0: prompt: 'Explain the role of BOS and EOS tokens in transformers.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 12027, 7420, 278, 6297, 310, 350, 3267, 322, 382, 3267, 18897, 297, 4327, 414, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:41034 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 12:54:39 [engine.py:331] Added request cmpl-4adbf9ff3caf475fb8c94dd11e642ea8-0.
INFO 08-09 12:54:43 [metrics.py:417] Avg prompt throughput: 3.2 tokens/s, Avg generation throughput: 89.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 12:54:48 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 90.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 12:54:50 [logger.py:43] Received request cmpl-3a3ce5672df14ed6b05c36101a325bed-0: prompt: 'What is a floating-point tensor and how is it represented?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 263, 16526, 29899, 3149, 12489, 322, 920, 338, 372, 9875, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:40750 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 12:54:50 [engine.py:331] Added request cmpl-3a3ce5672df14ed6b05c36101a325bed-0.
INFO 08-09 12:54:53 [metrics.py:417] Avg prompt throughput: 2.8 tokens/s, Avg generation throughput: 89.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 12:54:58 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 86.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 12:55:01 [logger.py:43] Received request cmpl-f77ff116915e4c8cb7fc9fa5c041a9ef-0: prompt: 'What are hypernetworks in neural architectures?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 526, 11266, 11618, 29879, 297, 19677, 6956, 1973, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:49172 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 12:55:01 [engine.py:331] Added request cmpl-f77ff116915e4c8cb7fc9fa5c041a9ef-0.
INFO 08-09 12:55:03 [metrics.py:417] Avg prompt throughput: 2.2 tokens/s, Avg generation throughput: 87.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 12:55:08 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 88.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 12:55:12 [logger.py:43] Received request cmpl-5067f842e64a41ad9162ed6dd9f9a376-0: prompt: 'Explain MoE (Mixture of Experts) architecture.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 12027, 7420, 4546, 29923, 313, 29924, 29875, 15546, 310, 28224, 1372, 29897, 11258, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:36788 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 12:55:12 [engine.py:331] Added request cmpl-5067f842e64a41ad9162ed6dd9f9a376-0.
INFO 08-09 12:55:13 [metrics.py:417] Avg prompt throughput: 3.0 tokens/s, Avg generation throughput: 89.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 12:55:18 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 88.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 12:55:23 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 86.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 12:55:24 [logger.py:43] Received request cmpl-89585e2fe484459c84e6e1c1bc5d3ca7-0: prompt: 'What is distillation in model compression?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 1320, 453, 362, 297, 1904, 24221, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:39874 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 12:55:24 [engine.py:331] Added request cmpl-89585e2fe484459c84e6e1c1bc5d3ca7-0.
INFO 08-09 12:55:28 [metrics.py:417] Avg prompt throughput: 2.0 tokens/s, Avg generation throughput: 86.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 12:55:33 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 84.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 12:55:36 [logger.py:43] Received request cmpl-86bf853e9ea4448aa6418681d4befb9a-0: prompt: 'How do you implement top-k and top-p sampling?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 437, 366, 2334, 2246, 29899, 29895, 322, 2246, 29899, 29886, 23460, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:42504 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 12:55:36 [engine.py:331] Added request cmpl-86bf853e9ea4448aa6418681d4befb9a-0.
INFO 08-09 12:55:38 [metrics.py:417] Avg prompt throughput: 2.8 tokens/s, Avg generation throughput: 86.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 12:55:43 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 87.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 12:55:47 [logger.py:43] Received request cmpl-c8c5288b2fdd4bf8a9e4ada7ca35b991-0: prompt: 'What is a vector database and how is it used with LLMs?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 263, 4608, 2566, 322, 920, 338, 372, 1304, 411, 365, 26369, 29879, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:57238 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 12:55:47 [engine.py:331] Added request cmpl-c8c5288b2fdd4bf8a9e4ada7ca35b991-0.
INFO 08-09 12:55:48 [metrics.py:417] Avg prompt throughput: 3.2 tokens/s, Avg generation throughput: 85.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 12:55:53 [logger.py:43] Received request cmpl-f3a483796028425395a5e7eaa260b9e4-0: prompt: 'Describe how you would use RAG (Retrieval Augmented Generation).', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 20355, 915, 920, 366, 723, 671, 390, 10051, 313, 8015, 2546, 791, 22333, 358, 287, 28203, 467], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:57242 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 12:55:53 [engine.py:331] Added request cmpl-f3a483796028425395a5e7eaa260b9e4-0.
INFO 08-09 12:55:53 [metrics.py:417] Avg prompt throughput: 3.6 tokens/s, Avg generation throughput: 85.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 12:55:58 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 87.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 12:56:03 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 87.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 12:56:05 [logger.py:43] Received request cmpl-921b4b8b490047a0a3453002c0a74c63-0: prompt: 'What is streaming inference and how is it used?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 24820, 27262, 322, 920, 338, 372, 1304, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:41152 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 12:56:05 [engine.py:331] Added request cmpl-921b4b8b490047a0a3453002c0a74c63-0.
INFO 08-09 12:56:08 [metrics.py:417] Avg prompt throughput: 2.2 tokens/s, Avg generation throughput: 87.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 12:56:13 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 90.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 12:56:16 [logger.py:43] Received request cmpl-515047e28db243f0aba317017026dd56-0: prompt: 'How does TTFT relate to user-perceived latency?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 947, 323, 29911, 7818, 29279, 304, 1404, 29899, 546, 346, 2347, 23316, 1270, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:51460 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 12:56:16 [engine.py:331] Added request cmpl-515047e28db243f0aba317017026dd56-0.
INFO 08-09 12:56:18 [metrics.py:417] Avg prompt throughput: 3.2 tokens/s, Avg generation throughput: 88.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 12:56:23 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 85.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 12:56:25 [logger.py:43] Received request cmpl-53af94e1e79c41b781303349ce0825c9-0: prompt: 'Explain transformer decoder blocks layer-by-layer.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 12027, 7420, 4327, 261, 1602, 6119, 10930, 7546, 29899, 1609, 29899, 13148, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:47844 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 12:56:25 [engine.py:331] Added request cmpl-53af94e1e79c41b781303349ce0825c9-0.
INFO 08-09 12:56:28 [metrics.py:417] Avg prompt throughput: 2.8 tokens/s, Avg generation throughput: 83.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 12:56:33 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 86.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 12:56:37 [logger.py:43] Received request cmpl-c755d3ad114b408cb3cf2a9db2ed09ca-0: prompt: 'What is a KV cache in transformer inference?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 263, 476, 29963, 7090, 297, 4327, 261, 27262, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:35396 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 12:56:37 [engine.py:331] Added request cmpl-c755d3ad114b408cb3cf2a9db2ed09ca-0.
INFO 08-09 12:56:38 [metrics.py:417] Avg prompt throughput: 2.4 tokens/s, Avg generation throughput: 86.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 12:56:43 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 81.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 12:56:48 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 85.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 12:56:49 [logger.py:43] Received request cmpl-ee6d40a95cf64624b1d2f1c9982cf003-0: prompt: 'Describe token-by-token generation and its challenges.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 20355, 915, 5993, 29899, 1609, 29899, 6979, 12623, 322, 967, 18066, 267, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:45364 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 12:56:49 [engine.py:331] Added request cmpl-ee6d40a95cf64624b1d2f1c9982cf003-0.
INFO 08-09 12:56:53 [metrics.py:417] Avg prompt throughput: 2.8 tokens/s, Avg generation throughput: 85.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 12:56:58 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 87.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 12:57:00 [logger.py:43] Received request cmpl-a9dbf59abfa048dfb3ea2927075abffd-0: prompt: 'How does LLaMA 2 compare to GPT-3.5?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 947, 365, 5661, 1529, 29871, 29906, 7252, 304, 402, 7982, 29899, 29941, 29889, 29945, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:34980 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 12:57:00 [engine.py:331] Added request cmpl-a9dbf59abfa048dfb3ea2927075abffd-0.
INFO 08-09 12:57:03 [metrics.py:417] Avg prompt throughput: 3.4 tokens/s, Avg generation throughput: 89.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 12:57:08 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 88.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 12:57:11 [logger.py:43] Received request cmpl-26f213724abd40a692f09d5edd623304-0: prompt: 'Explain the training data pipeline for a large model.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 12027, 7420, 278, 6694, 848, 16439, 363, 263, 2919, 1904, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:55556 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 12:57:11 [engine.py:331] Added request cmpl-26f213724abd40a692f09d5edd623304-0.
INFO 08-09 12:57:13 [metrics.py:417] Avg prompt throughput: 2.4 tokens/s, Avg generation throughput: 88.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 12:57:18 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 91.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 12:57:22 [logger.py:43] Received request cmpl-92091cdcceb94a0dbc41ce9dc32d21c5-0: prompt: 'What are the ethical concerns with generative models?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 526, 278, 11314, 936, 21838, 411, 1176, 1230, 4733, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:49772 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 12:57:22 [engine.py:331] Added request cmpl-92091cdcceb94a0dbc41ce9dc32d21c5-0.
INFO 08-09 12:57:23 [metrics.py:417] Avg prompt throughput: 2.4 tokens/s, Avg generation throughput: 90.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 12:57:28 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 93.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 12:57:33 [logger.py:43] Received request cmpl-5b812d2137e146cba87634c68437aa86-0: prompt: 'How do transformers process multilingual text?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 437, 4327, 414, 1889, 1773, 6504, 950, 1426, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:47390 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 12:57:33 [engine.py:331] Added request cmpl-5b812d2137e146cba87634c68437aa86-0.
INFO 08-09 12:57:33 [metrics.py:417] Avg prompt throughput: 2.2 tokens/s, Avg generation throughput: 91.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 12:57:38 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 93.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 12:57:43 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 91.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 12:57:44 [logger.py:43] Received request cmpl-aa1c6783078b4ea79ed3fad39bda9964-0: prompt: 'What are flash attention mechanisms?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 526, 11013, 8570, 7208, 12903, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:48696 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 12:57:44 [engine.py:331] Added request cmpl-aa1c6783078b4ea79ed3fad39bda9964-0.
INFO 08-09 12:57:48 [metrics.py:417] Avg prompt throughput: 1.6 tokens/s, Avg generation throughput: 89.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 12:57:53 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 89.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 12:57:55 [logger.py:43] Received request cmpl-125022a27a5247e496a40890b30297c0-0: prompt: 'Describe how memory-efficient attention works.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 20355, 915, 920, 3370, 29899, 8462, 8570, 1736, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:34672 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 12:57:55 [engine.py:331] Added request cmpl-125022a27a5247e496a40890b30297c0-0.
INFO 08-09 12:57:58 [metrics.py:417] Avg prompt throughput: 2.0 tokens/s, Avg generation throughput: 87.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 12:58:03 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 83.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 12:58:07 [logger.py:43] Received request cmpl-c6292714a8d94eeebfbcac621d3ea647-0: prompt: 'What is inference throughput and how is it calculated?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 27262, 1549, 649, 322, 920, 338, 372, 12833, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:48192 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 12:58:07 [engine.py:331] Added request cmpl-c6292714a8d94eeebfbcac621d3ea647-0.
INFO 08-09 12:58:08 [metrics.py:417] Avg prompt throughput: 2.4 tokens/s, Avg generation throughput: 89.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 12:58:13 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 88.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 12:58:18 [logger.py:43] Received request cmpl-776336119200433f8ccf9845b6583cce-0: prompt: 'Explain how masked self-attention works in BERT.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 12027, 7420, 920, 11105, 287, 1583, 29899, 1131, 2509, 1736, 297, 350, 20161, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:51352 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 12:58:18 [engine.py:331] Added request cmpl-776336119200433f8ccf9845b6583cce-0.
INFO 08-09 12:58:18 [metrics.py:417] Avg prompt throughput: 3.0 tokens/s, Avg generation throughput: 85.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 12:58:23 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 83.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 12:58:29 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 87.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 12:58:30 [logger.py:43] Received request cmpl-8dc7f579b66c45f184238e8eb514d7a1-0: prompt: 'How do instruction-tuned models differ from base models?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 437, 15278, 29899, 29873, 348, 287, 4733, 1163, 515, 2967, 4733, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:55490 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 12:58:30 [engine.py:331] Added request cmpl-8dc7f579b66c45f184238e8eb514d7a1-0.
INFO 08-09 12:58:34 [metrics.py:417] Avg prompt throughput: 2.8 tokens/s, Avg generation throughput: 84.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 12:58:39 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 85.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 12:58:42 [logger.py:43] Received request cmpl-37e66ffa0bbc4a04bf0b82ce442a5d8c-0: prompt: 'Describe an RLHF (Reinforcement Learning from Human Feedback) process.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 20355, 915, 385, 390, 29931, 29950, 29943, 313, 1123, 262, 1454, 13561, 29257, 515, 12968, 5169, 287, 1627, 29897, 1889, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:51858 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 12:58:42 [engine.py:331] Added request cmpl-37e66ffa0bbc4a04bf0b82ce442a5d8c-0.
INFO 08-09 12:58:44 [metrics.py:417] Avg prompt throughput: 4.4 tokens/s, Avg generation throughput: 86.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 12:58:49 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 92.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 12:58:53 [logger.py:43] Received request cmpl-9603c4d38fc1422fa6fe30f67d6414e9-0: prompt: 'What is SFT (Supervised Fine-Tuning) in LLM training?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 317, 7818, 313, 19111, 11292, 28896, 29899, 29911, 27964, 29897, 297, 365, 26369, 6694, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:60094 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 12:58:53 [engine.py:331] Added request cmpl-9603c4d38fc1422fa6fe30f67d6414e9-0.
INFO 08-09 12:58:54 [metrics.py:417] Avg prompt throughput: 3.6 tokens/s, Avg generation throughput: 90.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 12:58:59 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 95.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 12:59:03 [logger.py:43] Received request cmpl-a7dafe6edfb946f09249c7abaffac86f-0: prompt: 'Explain the role of HPU in Gaudi chips.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 12027, 7420, 278, 6297, 310, 379, 7056, 297, 10415, 4749, 521, 4512, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:49078 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 12:59:03 [engine.py:331] Added request cmpl-a7dafe6edfb946f09249c7abaffac86f-0.
INFO 08-09 12:59:04 [metrics.py:417] Avg prompt throughput: 2.8 tokens/s, Avg generation throughput: 90.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 12:59:09 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 88.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 12:59:14 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 88.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 12:59:15 [logger.py:43] Received request cmpl-3ec4d81e23ce429e9fcdae4a3a1e0164-0: prompt: 'What is a causal attention mask?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 263, 3269, 284, 8570, 11105, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:59842 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 12:59:15 [engine.py:331] Added request cmpl-3ec4d81e23ce429e9fcdae4a3a1e0164-0.
INFO 08-09 12:59:19 [metrics.py:417] Avg prompt throughput: 1.8 tokens/s, Avg generation throughput: 92.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 12:59:24 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 88.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 12:59:26 [logger.py:43] Received request cmpl-dcf0103246f34ebd9d49bad8b44b4648-0: prompt: 'Describe the HF `generate()` function in Transformers.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 20355, 915, 278, 379, 29943, 421, 17158, 2555, 740, 297, 4103, 689, 414, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:40258 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 12:59:26 [engine.py:331] Added request cmpl-dcf0103246f34ebd9d49bad8b44b4648-0.
INFO 08-09 12:59:29 [metrics.py:417] Avg prompt throughput: 3.0 tokens/s, Avg generation throughput: 91.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 12:59:34 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 91.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 12:59:37 [logger.py:43] Received request cmpl-7538bffbc4a8477e8c7f65732dc65f85-0: prompt: 'Explain differences between T5, BART, and GPT models.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 12027, 7420, 12651, 1546, 323, 29945, 29892, 350, 8322, 29892, 322, 402, 7982, 4733, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:46740 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 12:59:37 [engine.py:331] Added request cmpl-7538bffbc4a8477e8c7f65732dc65f85-0.
INFO 08-09 12:59:39 [metrics.py:417] Avg prompt throughput: 3.2 tokens/s, Avg generation throughput: 88.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 12:59:44 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 86.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 12:59:48 [logger.py:43] Received request cmpl-2b39e8c017eb4cc2a8283db556ce3694-0: prompt: 'What is model checkpointing and resumption?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 1904, 1423, 3149, 292, 322, 620, 28069, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:49560 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 12:59:48 [engine.py:331] Added request cmpl-2b39e8c017eb4cc2a8283db556ce3694-0.
INFO 08-09 12:59:49 [metrics.py:417] Avg prompt throughput: 2.2 tokens/s, Avg generation throughput: 83.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 12:59:54 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 86.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 12:59:59 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 86.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:00:00 [logger.py:43] Received request cmpl-5fa7a7146aec4f1a86f30f81f3ac1b6e-0: prompt: 'Describe how checkpoint sharding works in distributed training.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 20355, 915, 920, 1423, 3149, 528, 20272, 1736, 297, 13235, 6694, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:45118 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:00:00 [engine.py:331] Added request cmpl-5fa7a7146aec4f1a86f30f81f3ac1b6e-0.
INFO 08-09 13:00:04 [metrics.py:417] Avg prompt throughput: 2.6 tokens/s, Avg generation throughput: 86.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 13:00:09 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 88.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:00:11 [logger.py:43] Received request cmpl-c586f923848d4cda94129dd396586821-0: prompt: 'How are model weights stored in HuggingFace format?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 526, 1904, 18177, 6087, 297, 379, 688, 3460, 23360, 3402, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:51350 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:00:11 [engine.py:331] Added request cmpl-c586f923848d4cda94129dd396586821-0.
INFO 08-09 13:00:14 [metrics.py:417] Avg prompt throughput: 2.6 tokens/s, Avg generation throughput: 88.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 13:00:15 [logger.py:43] Received request cmpl-86caf8139bcc41bbb68691931f8d2215-0: prompt: 'Explain the impact of batch size on training and inference.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 12027, 7420, 278, 10879, 310, 9853, 2159, 373, 6694, 322, 27262, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:51366 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:00:15 [engine.py:331] Added request cmpl-86caf8139bcc41bbb68691931f8d2215-0.
INFO 08-09 13:00:19 [metrics.py:417] Avg prompt throughput: 2.6 tokens/s, Avg generation throughput: 85.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 13:00:24 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 87.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:00:26 [logger.py:43] Received request cmpl-d9ee07b85da842748bfd070039ef71e9-0: prompt: 'What are fused kernels and why are they fast?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 526, 285, 3880, 413, 824, 1379, 322, 2020, 526, 896, 5172, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:52062 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:00:26 [engine.py:331] Added request cmpl-d9ee07b85da842748bfd070039ef71e9-0.
INFO 08-09 13:00:29 [metrics.py:417] Avg prompt throughput: 2.8 tokens/s, Avg generation throughput: 86.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 13:00:34 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 86.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:00:38 [logger.py:43] Received request cmpl-08f436eb3e60438c9b027551db80580e-0: prompt: 'How does the `AutoModelForCausalLM` class work?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 947, 278, 421, 12300, 3195, 2831, 29907, 1485, 284, 26369, 29952, 770, 664, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:34572 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:00:38 [engine.py:331] Added request cmpl-08f436eb3e60438c9b027551db80580e-0.
INFO 08-09 13:00:39 [metrics.py:417] Avg prompt throughput: 3.2 tokens/s, Avg generation throughput: 85.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 13:00:44 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 82.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:00:49 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 84.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:00:50 [logger.py:43] Received request cmpl-7d6d57bcb49943019589473c73a164d8-0: prompt: 'What are tensor parallel and tensor slicing?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 526, 12489, 8943, 322, 12489, 269, 506, 292, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:46832 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:00:50 [engine.py:331] Added request cmpl-7d6d57bcb49943019589473c73a164d8-0.
INFO 08-09 13:00:54 [metrics.py:417] Avg prompt throughput: 2.2 tokens/s, Avg generation throughput: 86.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 13:00:59 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 93.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:01:01 [logger.py:43] Received request cmpl-4c26a995d5d7427cbd8b1ab9081fcab7-0: prompt: 'Describe the key steps of model serving using vLLM.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 20355, 915, 278, 1820, 6576, 310, 1904, 16330, 773, 325, 2208, 29924, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:38336 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:01:01 [engine.py:331] Added request cmpl-4c26a995d5d7427cbd8b1ab9081fcab7-0.
INFO 08-09 13:01:04 [metrics.py:417] Avg prompt throughput: 2.8 tokens/s, Avg generation throughput: 89.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 13:01:09 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 90.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:01:12 [logger.py:43] Received request cmpl-b85f7fe0003d4c7787ff672160cddbb2-0: prompt: 'What are inference graphs in deep learning?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 526, 27262, 18445, 297, 6483, 6509, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:59480 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:01:12 [engine.py:331] Added request cmpl-b85f7fe0003d4c7787ff672160cddbb2-0.
INFO 08-09 13:01:14 [metrics.py:417] Avg prompt throughput: 1.8 tokens/s, Avg generation throughput: 91.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 13:01:19 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 92.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:01:23 [logger.py:43] Received request cmpl-46afba0e28e44346a324fe778f2b8977-0: prompt: 'What is a streaming response in OpenAI-style APIs?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 263, 24820, 2933, 297, 4673, 23869, 29899, 3293, 23649, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:34840 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:01:23 [engine.py:331] Added request cmpl-46afba0e28e44346a324fe778f2b8977-0.
INFO 08-09 13:01:24 [metrics.py:417] Avg prompt throughput: 2.6 tokens/s, Avg generation throughput: 91.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 13:01:29 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 90.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:01:34 [logger.py:43] Received request cmpl-4340df84432142fbbfff122ee931a76b-0: prompt: 'How does speculative decoding reduce latency?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 947, 1580, 28524, 1602, 3689, 10032, 23316, 1270, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:43780 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:01:34 [engine.py:331] Added request cmpl-4340df84432142fbbfff122ee931a76b-0.
INFO 08-09 13:01:34 [metrics.py:417] Avg prompt throughput: 2.2 tokens/s, Avg generation throughput: 92.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 13:01:39 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 95.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:01:44 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 94.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:01:44 [logger.py:43] Received request cmpl-c8e04ebbff1242278c6d27e670b21ea0-0: prompt: 'What is JAX and how is it different from PyTorch?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 435, 6604, 322, 920, 338, 372, 1422, 515, 10772, 29911, 25350, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:54834 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:01:44 [engine.py:331] Added request cmpl-c8e04ebbff1242278c6d27e670b21ea0-0.
INFO 08-09 13:01:49 [metrics.py:417] Avg prompt throughput: 3.0 tokens/s, Avg generation throughput: 88.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:01:54 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 87.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:01:56 [logger.py:43] Received request cmpl-2ab4164d03e8403fb07c1a41544670f6-0: prompt: 'Describe lazy loading of weights in vLLM.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 20355, 915, 17366, 8363, 310, 18177, 297, 325, 2208, 29924, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:49718 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:01:56 [engine.py:331] Added request cmpl-2ab4164d03e8403fb07c1a41544670f6-0.
INFO 08-09 13:01:59 [metrics.py:417] Avg prompt throughput: 2.4 tokens/s, Avg generation throughput: 84.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 13:02:04 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 79.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:02:08 [logger.py:43] Received request cmpl-b1d8bd2c235d48a298b2aaec4291a497-0: prompt: 'Explain the role of SynapseAI in Gaudi software stack.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 12027, 7420, 278, 6297, 310, 10829, 481, 344, 23869, 297, 10415, 4749, 7047, 5096, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:55766 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:02:08 [engine.py:331] Added request cmpl-b1d8bd2c235d48a298b2aaec4291a497-0.
INFO 08-09 13:02:09 [metrics.py:417] Avg prompt throughput: 3.2 tokens/s, Avg generation throughput: 84.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 13:02:14 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 86.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:02:19 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 90.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:02:19 [logger.py:43] Received request cmpl-de92f673f0fc4900bef2a6ed68dfa881-0: prompt: 'What are the differences between HPU and GPU tensors?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 526, 278, 12651, 1546, 379, 7056, 322, 22796, 25187, 943, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:40056 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:02:19 [engine.py:331] Added request cmpl-de92f673f0fc4900bef2a6ed68dfa881-0.
INFO 08-09 13:02:24 [metrics.py:417] Avg prompt throughput: 2.6 tokens/s, Avg generation throughput: 89.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:02:29 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 87.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:02:30 [logger.py:43] Received request cmpl-bc7a8f59aad24b02bfebfa2b852777c7-0: prompt: 'What are the limitations of LLaMA 2?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 526, 278, 27028, 310, 365, 5661, 1529, 29871, 29906, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:42944 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:02:30 [engine.py:331] Added request cmpl-bc7a8f59aad24b02bfebfa2b852777c7-0.
INFO 08-09 13:02:34 [metrics.py:417] Avg prompt throughput: 2.4 tokens/s, Avg generation throughput: 87.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 13:02:39 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 88.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:02:42 [logger.py:43] Received request cmpl-31ceb3b5d51a4e50aa2a10861753e053-0: prompt: 'How is multilingual performance evaluated in LLMs?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 338, 1773, 6504, 950, 4180, 19030, 297, 365, 26369, 29879, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:60472 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:02:42 [engine.py:331] Added request cmpl-31ceb3b5d51a4e50aa2a10861753e053-0.
INFO 08-09 13:02:44 [metrics.py:417] Avg prompt throughput: 2.6 tokens/s, Avg generation throughput: 89.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 13:02:49 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 84.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:02:53 [logger.py:43] Received request cmpl-5620c01d5add4b3cb5bf22c9489a6990-0: prompt: 'Describe an example use of prompt tuning.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 20355, 915, 385, 1342, 671, 310, 9508, 18515, 292, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:44684 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:02:54 [engine.py:331] Added request cmpl-5620c01d5add4b3cb5bf22c9489a6990-0.
INFO 08-09 13:02:54 [metrics.py:417] Avg prompt throughput: 2.2 tokens/s, Avg generation throughput: 86.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 13:02:59 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 85.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:03:04 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 89.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:03:05 [logger.py:43] Received request cmpl-8c3fd8ff1e284bd6a14c48a552c0505f-0: prompt: 'What is the attention bottleneck and how to address it?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 278, 8570, 18046, 29880, 1600, 384, 322, 920, 304, 3211, 372, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:45464 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:03:05 [engine.py:331] Added request cmpl-8c3fd8ff1e284bd6a14c48a552c0505f-0.
INFO 08-09 13:03:09 [metrics.py:417] Avg prompt throughput: 3.0 tokens/s, Avg generation throughput: 84.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 13:03:14 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 90.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:03:16 [logger.py:43] Received request cmpl-901506e695b44975bea42e19adb00b89-0: prompt: 'How does token alignment affect downstream tasks?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 947, 5993, 22239, 6602, 1623, 5461, 9595, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:38446 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:03:16 [engine.py:331] Added request cmpl-901506e695b44975bea42e19adb00b89-0.
INFO 08-09 13:03:19 [metrics.py:417] Avg prompt throughput: 2.0 tokens/s, Avg generation throughput: 90.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 13:03:24 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 87.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:03:28 [logger.py:43] Received request cmpl-5252ff51240c4d88a48c512a6f99ab53-0: prompt: 'What are the challenges in inference on edge devices?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 526, 278, 18066, 267, 297, 27262, 373, 7636, 9224, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:55156 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:03:28 [engine.py:331] Added request cmpl-5252ff51240c4d88a48c512a6f99ab53-0.
INFO 08-09 13:03:29 [metrics.py:417] Avg prompt throughput: 2.4 tokens/s, Avg generation throughput: 85.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 13:03:34 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 87.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:03:39 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 84.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:03:40 [logger.py:43] Received request cmpl-2edeb891f5944ad788cb85751f3c46d3-0: prompt: 'Explain the principle behind rotary embeddings.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 12027, 7420, 278, 12502, 5742, 5731, 653, 8297, 29881, 886, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:56580 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:03:40 [engine.py:331] Added request cmpl-2edeb891f5944ad788cb85751f3c46d3-0.
INFO 08-09 13:03:43 [logger.py:43] Received request cmpl-ccbe3f5b14504771a008ab275f5594c0-0: prompt: 'How does temperature affect language model output?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 947, 10430, 6602, 4086, 1904, 1962, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:56582 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:03:43 [engine.py:331] Added request cmpl-ccbe3f5b14504771a008ab275f5594c0-0.
INFO 08-09 13:03:44 [metrics.py:417] Avg prompt throughput: 4.2 tokens/s, Avg generation throughput: 81.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 13:03:49 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 85.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:03:54 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 83.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:03:55 [logger.py:43] Received request cmpl-0c8247c617a34ea59fedc12dc69ec4b2-0: prompt: 'What are stop sequences in generation APIs?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 526, 5040, 15602, 297, 12623, 23649, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:47114 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:03:55 [engine.py:331] Added request cmpl-0c8247c617a34ea59fedc12dc69ec4b2-0.
INFO 08-09 13:03:59 [metrics.py:417] Avg prompt throughput: 1.8 tokens/s, Avg generation throughput: 91.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 13:04:04 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 90.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:04:06 [logger.py:43] Received request cmpl-10c6114dcf4644dc8a25ca546bf69f98-0: prompt: 'Describe token streaming latency optimization.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 20355, 915, 5993, 24820, 23316, 1270, 13883, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:57236 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:04:06 [engine.py:331] Added request cmpl-10c6114dcf4644dc8a25ca546bf69f98-0.
INFO 08-09 13:04:09 [metrics.py:417] Avg prompt throughput: 1.8 tokens/s, Avg generation throughput: 91.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 13:04:14 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 90.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:04:17 [logger.py:43] Received request cmpl-2579c73cdb9042eb9f26e8b1c112bdda-0: prompt: 'How to evaluate hallucinations in language models?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 304, 14707, 12713, 1682, 262, 800, 297, 4086, 4733, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:34060 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:04:17 [engine.py:331] Added request cmpl-2579c73cdb9042eb9f26e8b1c112bdda-0.
INFO 08-09 13:04:19 [metrics.py:417] Avg prompt throughput: 2.4 tokens/s, Avg generation throughput: 90.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 13:04:24 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 89.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:04:28 [logger.py:43] Received request cmpl-b909223c446d47038df3c05f86cddf23-0: prompt: 'What is the purpose of `bos_token_id`?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 278, 6437, 310, 421, 27737, 29918, 6979, 29918, 333, 6522], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:33782 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:04:28 [engine.py:331] Added request cmpl-b909223c446d47038df3c05f86cddf23-0.
INFO 08-09 13:04:29 [metrics.py:417] Avg prompt throughput: 2.6 tokens/s, Avg generation throughput: 89.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 13:04:31 [logger.py:43] Received request cmpl-7e6e15cc03e74b4f88317f63ca15be56-0: prompt: 'How are fused attention layers implemented?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 526, 285, 3880, 8570, 15359, 8762, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:33788 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:04:31 [engine.py:331] Added request cmpl-7e6e15cc03e74b4f88317f63ca15be56-0.
INFO 08-09 13:04:34 [metrics.py:417] Avg prompt throughput: 1.8 tokens/s, Avg generation throughput: 83.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 13:04:39 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 91.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:04:40 [logger.py:43] Received request cmpl-b9803f3ca8e64aa88ebbca663d0c56d7-0: prompt: 'Describe a pipeline for deploying LLaMA 2 in production.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 20355, 915, 263, 16439, 363, 7246, 292, 365, 5661, 1529, 29871, 29906, 297, 5802, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:54806 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:04:40 [engine.py:331] Added request cmpl-b9803f3ca8e64aa88ebbca663d0c56d7-0.
INFO 08-09 13:04:44 [metrics.py:417] Avg prompt throughput: 3.2 tokens/s, Avg generation throughput: 87.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 13:04:49 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 89.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:04:51 [logger.py:43] Received request cmpl-7fbb7ee9437c4929a34ed78d275efa6d-0: prompt: 'How does instruction following improve model usability?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 947, 15278, 1494, 11157, 1904, 502, 3097, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:46548 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:04:51 [engine.py:331] Added request cmpl-7fbb7ee9437c4929a34ed78d275efa6d-0.
INFO 08-09 13:04:54 [metrics.py:417] Avg prompt throughput: 2.0 tokens/s, Avg generation throughput: 88.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 13:04:59 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 89.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:05:03 [logger.py:43] Received request cmpl-120954bf4ac2461a8ffe03d60f9a1976-0: prompt: 'What is a sequence-to-sequence model?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 263, 5665, 29899, 517, 29899, 16506, 1904, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:42472 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:05:03 [engine.py:331] Added request cmpl-120954bf4ac2461a8ffe03d60f9a1976-0.
INFO 08-09 13:05:04 [metrics.py:417] Avg prompt throughput: 2.2 tokens/s, Avg generation throughput: 92.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 13:05:09 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 92.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:05:14 [logger.py:43] Received request cmpl-e8edff24052744959c141144d9e4e2ea-0: prompt: 'What is the purpose of `pad_token_id` in generation?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 278, 6437, 310, 421, 8305, 29918, 6979, 29918, 333, 29952, 297, 12623, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:41316 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:05:14 [engine.py:331] Added request cmpl-e8edff24052744959c141144d9e4e2ea-0.
INFO 08-09 13:05:14 [metrics.py:417] Avg prompt throughput: 3.2 tokens/s, Avg generation throughput: 89.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 13:05:18 [logger.py:43] Received request cmpl-a717d977e8ed46cf919fb465b249794b-0: prompt: 'Explain the concept of auto-regressive generation.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 12027, 7420, 278, 6964, 310, 4469, 29899, 276, 3663, 573, 12623, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:54138 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:05:18 [engine.py:331] Added request cmpl-a717d977e8ed46cf919fb465b249794b-0.
INFO 08-09 13:05:19 [metrics.py:417] Avg prompt throughput: 2.6 tokens/s, Avg generation throughput: 89.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 13:05:24 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 89.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:05:29 [logger.py:43] Received request cmpl-92f24675df684e9487eb6f89e5dde192-0: prompt: 'How is top-p sampling different from nucleus sampling?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 338, 2246, 29899, 29886, 23460, 1422, 515, 22699, 375, 23460, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:51384 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:05:29 [engine.py:331] Added request cmpl-92f24675df684e9487eb6f89e5dde192-0.
INFO 08-09 13:05:29 [metrics.py:417] Avg prompt throughput: 2.6 tokens/s, Avg generation throughput: 86.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 13:05:34 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 86.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:05:39 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 87.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:05:41 [logger.py:43] Received request cmpl-45dfd4621244479e8abaed3f9babcbda-0: prompt: 'What are decoding strategies for safe responses?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 526, 1602, 3689, 16650, 583, 363, 9109, 20890, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:33860 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:05:41 [engine.py:331] Added request cmpl-45dfd4621244479e8abaed3f9babcbda-0.
INFO 08-09 13:05:44 [metrics.py:417] Avg prompt throughput: 2.2 tokens/s, Avg generation throughput: 87.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 13:05:49 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 86.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:05:52 [logger.py:43] Received request cmpl-e3dc5bd8cb2f47f988b7e4d2ad5ab7e4-0: prompt: 'How are model prompts structured in ChatML?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 526, 1904, 9508, 29879, 2281, 2955, 297, 678, 271, 1988, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:53098 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:05:52 [engine.py:331] Added request cmpl-e3dc5bd8cb2f47f988b7e4d2ad5ab7e4-0.
INFO 08-09 13:05:54 [metrics.py:417] Avg prompt throughput: 2.6 tokens/s, Avg generation throughput: 92.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 13:05:59 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 88.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:06:03 [logger.py:43] Received request cmpl-2d43da058e3542e7b094fdcce55b5fb2-0: prompt: 'What is a system prompt in chat inference?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 263, 1788, 9508, 297, 13563, 27262, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:46748 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:06:03 [engine.py:331] Added request cmpl-2d43da058e3542e7b094fdcce55b5fb2-0.
INFO 08-09 13:06:04 [metrics.py:417] Avg prompt throughput: 2.0 tokens/s, Avg generation throughput: 88.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 13:06:09 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 89.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:06:14 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 86.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:06:14 [logger.py:43] Received request cmpl-f673473637404c128713f0bf437c39bd-0: prompt: 'How do you batch requests for high throughput?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 437, 366, 9853, 7274, 363, 1880, 1549, 649, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:48414 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:06:14 [engine.py:331] Added request cmpl-f673473637404c128713f0bf437c39bd-0.
INFO 08-09 13:06:19 [metrics.py:417] Avg prompt throughput: 2.2 tokens/s, Avg generation throughput: 86.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:06:24 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 92.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:06:25 [logger.py:43] Received request cmpl-badedcf4acca495e81e1a0e99270a6b4-0: prompt: 'Explain model serving with `transformers + vLLM`.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 12027, 7420, 1904, 16330, 411, 421, 9067, 414, 718, 325, 2208, 29924, 1412], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:34610 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:06:25 [engine.py:331] Added request cmpl-badedcf4acca495e81e1a0e99270a6b4-0.
INFO 08-09 13:06:29 [metrics.py:417] Avg prompt throughput: 2.8 tokens/s, Avg generation throughput: 91.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 13:06:34 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 89.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:06:37 [logger.py:43] Received request cmpl-af0ea98d066d4c1a98a921e0c66238ff-0: prompt: 'What is Triton Inference Server?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 323, 768, 265, 512, 1659, 5656, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:34302 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:06:37 [engine.py:331] Added request cmpl-af0ea98d066d4c1a98a921e0c66238ff-0.
INFO 08-09 13:06:39 [metrics.py:417] Avg prompt throughput: 2.0 tokens/s, Avg generation throughput: 88.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 13:06:44 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 85.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:06:48 [logger.py:43] Received request cmpl-67036561ca9f46c4aac02cd30fa06a1d-0: prompt: 'Describe the significance of large context windows.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 20355, 915, 278, 26002, 310, 2919, 3030, 5417, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:58500 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:06:48 [engine.py:331] Added request cmpl-67036561ca9f46c4aac02cd30fa06a1d-0.
INFO 08-09 13:06:49 [metrics.py:417] Avg prompt throughput: 2.0 tokens/s, Avg generation throughput: 82.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 13:06:50 [logger.py:43] Received request cmpl-7b9ba2bde6694b268129e53174d83585-0: prompt: 'What is dynamic quantization in transformers?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 7343, 4323, 2133, 297, 4327, 414, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:58516 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:06:50 [engine.py:331] Added request cmpl-7b9ba2bde6694b268129e53174d83585-0.
INFO 08-09 13:06:54 [metrics.py:417] Avg prompt throughput: 2.0 tokens/s, Avg generation throughput: 84.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:06:59 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 90.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:07:01 [logger.py:43] Received request cmpl-21b26a6d56864790a19c88e5e32a4fe3-0: prompt: 'How to prevent prompt injection in LLMs?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 304, 5557, 9508, 20859, 297, 365, 26369, 29879, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:39954 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:07:01 [engine.py:331] Added request cmpl-21b26a6d56864790a19c88e5e32a4fe3-0.
INFO 08-09 13:07:04 [metrics.py:417] Avg prompt throughput: 2.2 tokens/s, Avg generation throughput: 90.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 13:07:09 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 86.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:07:12 [logger.py:43] Received request cmpl-59528cb1795544ea8d9aac126e4d67ec-0: prompt: 'What are attention scores and how are they computed?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 526, 8570, 19435, 322, 920, 526, 896, 15712, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:50282 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:07:12 [engine.py:331] Added request cmpl-59528cb1795544ea8d9aac126e4d67ec-0.
INFO 08-09 13:07:14 [metrics.py:417] Avg prompt throughput: 2.2 tokens/s, Avg generation throughput: 85.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 13:07:19 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 89.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:07:24 [logger.py:43] Received request cmpl-fc0a82320efe46568f4f253c6c2e637b-0: prompt: 'What are residual connections in transformers?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 526, 10995, 950, 12368, 297, 4327, 414, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:58796 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:07:24 [engine.py:331] Added request cmpl-fc0a82320efe46568f4f253c6c2e637b-0.
INFO 08-09 13:07:24 [metrics.py:417] Avg prompt throughput: 2.0 tokens/s, Avg generation throughput: 88.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 13:07:29 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 89.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:07:34 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 85.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:07:35 [logger.py:43] Received request cmpl-6d046d0bf843425d836457fc2dc7d816-0: prompt: 'How to handle long sequences in generation?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 304, 4386, 1472, 15602, 297, 12623, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:45034 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:07:35 [engine.py:331] Added request cmpl-6d046d0bf843425d836457fc2dc7d816-0.
INFO 08-09 13:07:39 [logger.py:43] Received request cmpl-3ca89a281b174213941539f12c71005b-0: prompt: 'Explain the use of flash attention v2.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 12027, 7420, 278, 671, 310, 11013, 8570, 325, 29906, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:37142 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:07:39 [engine.py:331] Added request cmpl-3ca89a281b174213941539f12c71005b-0.
INFO 08-09 13:07:39 [metrics.py:417] Avg prompt throughput: 4.0 tokens/s, Avg generation throughput: 88.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 13:07:44 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 93.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:07:49 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 93.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:07:49 [logger.py:43] Received request cmpl-0c0098578e474429894133fd08f89412-0: prompt: 'What is a greedy search in inference?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 263, 1395, 7584, 2740, 297, 27262, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:49808 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:07:49 [engine.py:331] Added request cmpl-0c0098578e474429894133fd08f89412-0.
INFO 08-09 13:07:54 [metrics.py:417] Avg prompt throughput: 2.0 tokens/s, Avg generation throughput: 84.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:07:59 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 84.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:08:01 [logger.py:43] Received request cmpl-413ad32e15bc4d479ae394baad2f16dc-0: prompt: 'How do checkpoints improve fault tolerance?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 437, 1423, 9748, 11157, 12570, 20341, 749, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:35574 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:08:01 [engine.py:331] Added request cmpl-413ad32e15bc4d479ae394baad2f16dc-0.
INFO 08-09 13:08:04 [metrics.py:417] Avg prompt throughput: 2.0 tokens/s, Avg generation throughput: 81.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 13:08:09 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 86.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:08:13 [logger.py:43] Received request cmpl-74b9df89a669478c86d21d7da0268764-0: prompt: 'Describe how text-to-text generation works.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 20355, 915, 920, 1426, 29899, 517, 29899, 726, 12623, 1736, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:39576 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:08:13 [engine.py:331] Added request cmpl-74b9df89a669478c86d21d7da0268764-0.
INFO 08-09 13:08:14 [metrics.py:417] Avg prompt throughput: 2.4 tokens/s, Avg generation throughput: 85.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 13:08:19 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 90.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:08:24 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 88.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:08:24 [logger.py:43] Received request cmpl-6c54ab51c68948bf8aa879e117aa6876-0: prompt: 'Explain cosine similarity in embeddings.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 12027, 7420, 6776, 457, 29501, 297, 8297, 29881, 886, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:33384 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:08:24 [engine.py:331] Added request cmpl-6c54ab51c68948bf8aa879e117aa6876-0.
INFO 08-09 13:08:29 [metrics.py:417] Avg prompt throughput: 2.2 tokens/s, Avg generation throughput: 87.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:08:34 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 90.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:08:36 [logger.py:43] Received request cmpl-e391afc00b2b4c50afb95d324488b519-0: prompt: 'What is token-wise parallelism?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 5993, 29899, 3538, 8943, 1608, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:41914 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:08:36 [engine.py:331] Added request cmpl-e391afc00b2b4c50afb95d324488b519-0.
INFO 08-09 13:08:39 [metrics.py:417] Avg prompt throughput: 1.8 tokens/s, Avg generation throughput: 87.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 13:08:44 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 90.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:08:47 [logger.py:43] Received request cmpl-5cfec7ec3f0f4c1e8c870e7972afd71f-0: prompt: 'How do shared embeddings help in decoder-only models?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 437, 7258, 8297, 29881, 886, 1371, 297, 1602, 6119, 29899, 6194, 4733, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:52128 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:08:47 [engine.py:331] Added request cmpl-5cfec7ec3f0f4c1e8c870e7972afd71f-0.
INFO 08-09 13:08:49 [metrics.py:417] Avg prompt throughput: 3.0 tokens/s, Avg generation throughput: 94.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 13:08:54 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 90.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:08:57 [logger.py:43] Received request cmpl-6bc877660f9240f18e9e044cb659a066-0: prompt: 'Explain how to measure latency in model output.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 12027, 7420, 920, 304, 5645, 23316, 1270, 297, 1904, 1962, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:41518 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:08:57 [engine.py:331] Added request cmpl-6bc877660f9240f18e9e044cb659a066-0.
INFO 08-09 13:08:59 [metrics.py:417] Avg prompt throughput: 2.4 tokens/s, Avg generation throughput: 89.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 13:09:04 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 92.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:09:08 [logger.py:43] Received request cmpl-c98337853c564899a996f57e3d7967d1-0: prompt: 'What is the purpose of dropout during training?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 278, 6437, 310, 5768, 449, 2645, 6694, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:53304 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:09:08 [engine.py:331] Added request cmpl-c98337853c564899a996f57e3d7967d1-0.
INFO 08-09 13:09:09 [metrics.py:417] Avg prompt throughput: 2.2 tokens/s, Avg generation throughput: 93.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 13:09:14 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 95.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:09:19 [logger.py:43] Received request cmpl-2c800529d7a74cb9955e511b15396c22-0: prompt: 'What is the role of GELU activation in transformers?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 278, 6297, 310, 402, 6670, 29965, 26229, 297, 4327, 414, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:45830 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:09:19 [engine.py:331] Added request cmpl-2c800529d7a74cb9955e511b15396c22-0.
INFO 08-09 13:09:19 [metrics.py:417] Avg prompt throughput: 2.8 tokens/s, Avg generation throughput: 91.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 13:09:24 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 90.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:09:24 [logger.py:43] Received request cmpl-986b6921f8e348ee874abad523099fa3-0: prompt: 'Explain forward and backward pass in transformers.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 12027, 7420, 6375, 322, 1250, 1328, 1209, 297, 4327, 414, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:45834 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:09:24 [engine.py:331] Added request cmpl-986b6921f8e348ee874abad523099fa3-0.
INFO 08-09 13:09:29 [metrics.py:417] Avg prompt throughput: 2.4 tokens/s, Avg generation throughput: 90.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:09:34 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 91.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:09:35 [logger.py:43] Received request cmpl-f5792b4ceb9c405fbd69f770f0a8fe38-0: prompt: 'What is speculative decoding and how does it help?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 1580, 28524, 1602, 3689, 322, 920, 947, 372, 1371, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:54552 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:09:35 [engine.py:331] Added request cmpl-f5792b4ceb9c405fbd69f770f0a8fe38-0.
INFO 08-09 13:09:39 [metrics.py:417] Avg prompt throughput: 2.6 tokens/s, Avg generation throughput: 90.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 13:09:44 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 88.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:09:47 [logger.py:43] Received request cmpl-a584ea02cdcf40b4a73b27b9c9f43e91-0: prompt: 'How to optimize HPU memory usage during inference?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 304, 24656, 379, 7056, 3370, 8744, 2645, 27262, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:42590 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:09:47 [engine.py:331] Added request cmpl-a584ea02cdcf40b4a73b27b9c9f43e91-0.
INFO 08-09 13:09:49 [metrics.py:417] Avg prompt throughput: 2.2 tokens/s, Avg generation throughput: 90.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 13:09:54 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 89.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:09:58 [logger.py:43] Received request cmpl-7878628c793d48c5909e08877f1ffb9f-0: prompt: 'Explain profiling for LLM inference performance.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 12027, 7420, 20077, 292, 363, 365, 26369, 27262, 4180, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:56518 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:09:58 [engine.py:331] Added request cmpl-7878628c793d48c5909e08877f1ffb9f-0.
INFO 08-09 13:09:59 [metrics.py:417] Avg prompt throughput: 2.2 tokens/s, Avg generation throughput: 91.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 13:10:04 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 85.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:10:09 [logger.py:43] Received request cmpl-a5f16bbf277947a2b95d0c19362e7b3c-0: prompt: 'How to compute inter-token latency per request?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 304, 10272, 1006, 29899, 6979, 23316, 1270, 639, 2009, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:47056 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:10:09 [engine.py:331] Added request cmpl-a5f16bbf277947a2b95d0c19362e7b3c-0.
INFO 08-09 13:10:09 [metrics.py:417] Avg prompt throughput: 2.4 tokens/s, Avg generation throughput: 87.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 13:10:14 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 89.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:10:19 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 90.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:10:20 [logger.py:43] Received request cmpl-41dfbdd05d084bd98149be3f66744d3d-0: prompt: 'What is the difference between BFloat16 and Float16?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 278, 4328, 1546, 350, 11031, 29896, 29953, 322, 27842, 29896, 29953, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:33650 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:10:20 [engine.py:331] Added request cmpl-41dfbdd05d084bd98149be3f66744d3d-0.
INFO 08-09 13:10:24 [metrics.py:417] Avg prompt throughput: 3.0 tokens/s, Avg generation throughput: 87.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 13:10:29 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 93.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:10:31 [logger.py:43] Received request cmpl-c078e6c281864a809a8771c21fa88e85-0: prompt: 'What are logits and how are they interpreted?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 526, 1480, 1169, 322, 920, 526, 896, 21551, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:34234 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:10:31 [engine.py:331] Added request cmpl-c078e6c281864a809a8771c21fa88e85-0.
INFO 08-09 13:10:34 [metrics.py:417] Avg prompt throughput: 2.2 tokens/s, Avg generation throughput: 82.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 13:10:39 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 88.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:10:43 [logger.py:43] Received request cmpl-4c7ddbd401394bbd9fb7f6de9ab374b5-0: prompt: 'What is log probability in token prediction?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 1480, 6976, 297, 5993, 18988, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:58534 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:10:43 [engine.py:331] Added request cmpl-4c7ddbd401394bbd9fb7f6de9ab374b5-0.
INFO 08-09 13:10:44 [metrics.py:417] Avg prompt throughput: 1.8 tokens/s, Avg generation throughput: 85.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 13:10:49 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 89.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:10:54 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 89.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 13:10:54 [logger.py:43] Received request cmpl-f3f7f5ab93bc45f1909c52022c6e020b-0: prompt: 'How to use `TextStreamer` for streaming output?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 304, 671, 421, 1626, 3835, 261, 29952, 363, 24820, 1962, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:43060 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:10:54 [engine.py:331] Added request cmpl-f3f7f5ab93bc45f1909c52022c6e020b-0.
INFO 08-09 13:10:59 [logger.py:43] Received request cmpl-d18b4058cd384dee848eb8d4c28b4730-0: prompt: 'How to tokenize and detokenize a prompt manually?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 304, 5993, 675, 322, 1439, 4476, 675, 263, 9508, 7522, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:59260 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:10:59 [engine.py:331] Added request cmpl-d18b4058cd384dee848eb8d4c28b4730-0.
INFO 08-09 13:10:59 [metrics.py:417] Avg prompt throughput: 5.2 tokens/s, Avg generation throughput: 86.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 13:11:04 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 92.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:11:09 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 89.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:11:10 [logger.py:43] Received request cmpl-2df9f65131434ebabb01c99641e89189-0: prompt: 'Describe how a prompt flows through the transformer layers.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 20355, 915, 920, 263, 9508, 24536, 1549, 278, 4327, 261, 15359, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:33896 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:11:10 [engine.py:331] Added request cmpl-2df9f65131434ebabb01c99641e89189-0.
INFO 08-09 13:11:14 [metrics.py:417] Avg prompt throughput: 2.6 tokens/s, Avg generation throughput: 89.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:11:19 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 91.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:11:21 [logger.py:43] Received request cmpl-a5a79215c0ba495786856f7f496c4fc5-0: prompt: 'How to handle EOS in streaming responses?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 304, 4386, 382, 3267, 297, 24820, 20890, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:44460 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:11:21 [engine.py:331] Added request cmpl-a5a79215c0ba495786856f7f496c4fc5-0.
INFO 08-09 13:11:24 [metrics.py:417] Avg prompt throughput: 2.0 tokens/s, Avg generation throughput: 89.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 13:11:29 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 89.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:11:32 [logger.py:43] Received request cmpl-a6ff1a11bf594a7d86f2a6488377985e-0: prompt: 'What is a model config JSON in HF models?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 263, 1904, 2295, 4663, 297, 379, 29943, 4733, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:53414 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:11:32 [engine.py:331] Added request cmpl-a6ff1a11bf594a7d86f2a6488377985e-0.
INFO 08-09 13:11:34 [metrics.py:417] Avg prompt throughput: 2.4 tokens/s, Avg generation throughput: 88.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 13:11:39 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 88.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:11:43 [logger.py:43] Received request cmpl-e59127adb0504b12945cc12bac042bfa-0: prompt: 'What is kv_cache reuse in decoder models?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 10908, 29918, 8173, 24270, 297, 1602, 6119, 4733, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:33972 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:11:43 [engine.py:331] Added request cmpl-e59127adb0504b12945cc12bac042bfa-0.
INFO 08-09 13:11:44 [metrics.py:417] Avg prompt throughput: 2.4 tokens/s, Avg generation throughput: 87.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 13:11:49 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 89.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:11:54 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 89.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:11:55 [logger.py:43] Received request cmpl-69237b117e8f49cb8089d6ed8caeea3d-0: prompt: 'Describe a scenario of real-time chatbot streaming.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 20355, 915, 263, 10483, 310, 1855, 29899, 2230, 13563, 7451, 24820, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:43124 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:11:55 [engine.py:331] Added request cmpl-69237b117e8f49cb8089d6ed8caeea3d-0.
INFO 08-09 13:11:59 [metrics.py:417] Avg prompt throughput: 2.6 tokens/s, Avg generation throughput: 85.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:12:04 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 88.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:12:06 [logger.py:43] Received request cmpl-c8d349b6e8d74fd183bf248a9e10f5ac-0: prompt: 'How to compute throughput with variable prompts?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 304, 10272, 1549, 649, 411, 2286, 9508, 29879, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:46122 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:12:06 [engine.py:331] Added request cmpl-c8d349b6e8d74fd183bf248a9e10f5ac-0.
INFO 08-09 13:12:09 [metrics.py:417] Avg prompt throughput: 2.2 tokens/s, Avg generation throughput: 90.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 13:12:14 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 89.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:12:17 [logger.py:43] Received request cmpl-e099f9b5f474412db8e3a127ac0b0973-0: prompt: 'How does `max_new_tokens` differ from `max_length`?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 947, 421, 3317, 29918, 1482, 29918, 517, 12360, 29952, 1163, 515, 421, 3317, 29918, 2848, 6522], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:57426 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:12:17 [engine.py:331] Added request cmpl-e099f9b5f474412db8e3a127ac0b0973-0.
INFO 08-09 13:12:19 [metrics.py:417] Avg prompt throughput: 3.6 tokens/s, Avg generation throughput: 92.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 13:12:21 [logger.py:43] Received request cmpl-f4b9542de47347f5b4705a73c9519ac8-0: prompt: 'Explain the difference between loss and perplexity.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 12027, 7420, 278, 4328, 1546, 6410, 322, 639, 10709, 537, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:57442 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:12:21 [engine.py:331] Added request cmpl-f4b9542de47347f5b4705a73c9519ac8-0.
INFO 08-09 13:12:24 [metrics.py:417] Avg prompt throughput: 2.4 tokens/s, Avg generation throughput: 90.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 13:12:29 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 92.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:12:32 [logger.py:43] Received request cmpl-b2b903282a61455b8ee835a65e1db37f-0: prompt: 'How to log benchmark results in a structured CSV?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 304, 1480, 23513, 2582, 297, 263, 2281, 2955, 16874, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:47834 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:12:32 [engine.py:331] Added request cmpl-b2b903282a61455b8ee835a65e1db37f-0.
INFO 08-09 13:12:34 [metrics.py:417] Avg prompt throughput: 2.4 tokens/s, Avg generation throughput: 87.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 13:12:39 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 91.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:12:43 [logger.py:43] Received request cmpl-36a8e4f4375f40f09e6738a3ed3a2093-0: prompt: 'What are attention heads and how do they specialize?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 526, 8570, 15883, 322, 920, 437, 896, 4266, 675, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:59678 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:12:43 [engine.py:331] Added request cmpl-36a8e4f4375f40f09e6738a3ed3a2093-0.
INFO 08-09 13:12:45 [metrics.py:417] Avg prompt throughput: 2.4 tokens/s, Avg generation throughput: 93.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 13:12:50 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 92.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:12:54 [logger.py:43] Received request cmpl-adb6675783f145a7821fc1a768c40284-0: prompt: 'How to ensure reproducible benchmarking?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 304, 9801, 9483, 15520, 23513, 292, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:50892 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:12:54 [engine.py:331] Added request cmpl-adb6675783f145a7821fc1a768c40284-0.
INFO 08-09 13:12:55 [metrics.py:417] Avg prompt throughput: 1.8 tokens/s, Avg generation throughput: 92.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 13:13:00 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 94.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:13:05 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 93.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:13:05 [logger.py:43] Received request cmpl-c2a63bc7c8da4ff6852c7b84703bfa9f-0: prompt: 'What are model shards and when are they used?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 526, 1904, 528, 3163, 322, 746, 526, 896, 1304, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:41556 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:13:05 [engine.py:331] Added request cmpl-c2a63bc7c8da4ff6852c7b84703bfa9f-0.
INFO 08-09 13:13:10 [metrics.py:417] Avg prompt throughput: 2.4 tokens/s, Avg generation throughput: 94.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:13:12 [logger.py:43] Received request cmpl-e2f3565c7f4148b68a157526719dd135-0: prompt: 'Explain the relationship between prompt length and latency.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 12027, 7420, 278, 9443, 1546, 9508, 3309, 322, 23316, 1270, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:55542 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:13:12 [engine.py:331] Added request cmpl-e2f3565c7f4148b68a157526719dd135-0.
INFO 08-09 13:13:12 [logger.py:43] Received request cmpl-ab5930b9b1e049a9ba3729cf1ab566e3-0: prompt: 'What is batch inference and why is it faster?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 9853, 27262, 322, 2020, 338, 372, 8473, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:55554 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:13:12 [engine.py:331] Added request cmpl-ab5930b9b1e049a9ba3729cf1ab566e3-0.
INFO 08-09 13:13:15 [metrics.py:417] Avg prompt throughput: 4.6 tokens/s, Avg generation throughput: 88.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 13:13:20 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 92.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:13:23 [logger.py:43] Received request cmpl-9368afeea29a42cbb5cf2bdc82688786-0: prompt: 'What are rotary sin-cos embeddings?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 526, 5731, 653, 4457, 29899, 3944, 8297, 29881, 886, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:58198 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:13:23 [engine.py:331] Added request cmpl-9368afeea29a42cbb5cf2bdc82688786-0.
INFO 08-09 13:13:25 [metrics.py:417] Avg prompt throughput: 2.4 tokens/s, Avg generation throughput: 89.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 13:13:30 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 90.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:13:35 [logger.py:43] Received request cmpl-0112c679e2504a959360bbd4ea1a0957-0: prompt: "Describe HuggingFace's `AutoTokenizer` features.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 20355, 915, 379, 688, 3460, 23360, 29915, 29879, 421, 12300, 6066, 3950, 29952, 5680, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:36950 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:13:35 [engine.py:331] Added request cmpl-0112c679e2504a959360bbd4ea1a0957-0.
INFO 08-09 13:13:35 [metrics.py:417] Avg prompt throughput: 3.2 tokens/s, Avg generation throughput: 89.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 13:13:40 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 83.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:13:45 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 93.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:13:46 [logger.py:43] Received request cmpl-5dca82eef0dc4ddb8019368f854eb550-0: prompt: 'How does positional encoding interact with attention?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 947, 2602, 284, 8025, 16254, 411, 8570, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:56978 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:13:46 [engine.py:331] Added request cmpl-5dca82eef0dc4ddb8019368f854eb550-0.
INFO 08-09 13:13:50 [metrics.py:417] Avg prompt throughput: 2.0 tokens/s, Avg generation throughput: 88.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 13:13:55 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 93.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:13:57 [logger.py:43] Received request cmpl-d557966b67ff45b687990cc529f1c0a2-0: prompt: 'What is a streaming callback in Transformers?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 263, 24820, 6939, 297, 4103, 689, 414, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:47450 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:13:57 [engine.py:331] Added request cmpl-d557966b67ff45b687990cc529f1c0a2-0.
INFO 08-09 13:14:00 [metrics.py:417] Avg prompt throughput: 2.2 tokens/s, Avg generation throughput: 87.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 13:14:05 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 86.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:14:09 [logger.py:43] Received request cmpl-46ac8d426412400eaf84d7e3cab83bef-0: prompt: 'How to benchmark models using curl and Python?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 304, 23513, 4733, 773, 11051, 322, 5132, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:60940 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:14:09 [engine.py:331] Added request cmpl-46ac8d426412400eaf84d7e3cab83bef-0.
INFO 08-09 13:14:10 [metrics.py:417] Avg prompt throughput: 2.0 tokens/s, Avg generation throughput: 79.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 13:14:15 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 78.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:14:20 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 80.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:14:21 [logger.py:43] Received request cmpl-4efc042df72347959329e183a7dbb4de-0: prompt: 'How to parse streaming responses from vLLM?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 304, 6088, 24820, 20890, 515, 325, 2208, 29924, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:34844 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:14:21 [engine.py:331] Added request cmpl-4efc042df72347959329e183a7dbb4de-0.
INFO 08-09 13:14:25 [metrics.py:417] Avg prompt throughput: 2.2 tokens/s, Avg generation throughput: 86.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 13:14:30 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 82.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:14:33 [logger.py:43] Received request cmpl-586207779d0644ca8a83643ee729ac0e-0: prompt: 'How does deep caching work in inference?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 947, 6483, 22488, 664, 297, 27262, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:54272 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:14:33 [engine.py:331] Added request cmpl-586207779d0644ca8a83643ee729ac0e-0.
INFO 08-09 13:14:35 [metrics.py:417] Avg prompt throughput: 1.8 tokens/s, Avg generation throughput: 89.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 13:14:40 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 87.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:14:44 [logger.py:43] Received request cmpl-b7b308486a7346fba5efe8fa3500f5c0-0: prompt: 'What is a prompt-truncation error?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 263, 9508, 29899, 509, 4661, 362, 1059, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:39856 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:14:44 [engine.py:331] Added request cmpl-b7b308486a7346fba5efe8fa3500f5c0-0.
INFO 08-09 13:14:45 [metrics.py:417] Avg prompt throughput: 2.2 tokens/s, Avg generation throughput: 84.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 13:14:50 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 85.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:14:55 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 88.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:14:56 [logger.py:43] Received request cmpl-3c7802f9183f448fb157987f55980cf7-0: prompt: 'Explain the role of tokenizer config files.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 12027, 7420, 278, 6297, 310, 5993, 3950, 2295, 2066, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:58648 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:14:56 [engine.py:331] Added request cmpl-3c7802f9183f448fb157987f55980cf7-0.
INFO 08-09 13:15:00 [metrics.py:417] Avg prompt throughput: 2.2 tokens/s, Avg generation throughput: 90.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 13:15:05 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 90.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:15:07 [logger.py:43] Received request cmpl-854a0bc0765844d5a1051c722ced4637-0: prompt: 'How to batch multiple prompts into one request?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 304, 9853, 2999, 9508, 29879, 964, 697, 2009, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:33422 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:15:07 [engine.py:331] Added request cmpl-854a0bc0765844d5a1051c722ced4637-0.
INFO 08-09 13:15:10 [metrics.py:417] Avg prompt throughput: 2.2 tokens/s, Avg generation throughput: 87.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 13:15:15 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 88.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:15:18 [logger.py:43] Received request cmpl-f74f0c8938c34cdd91d1dd88206a2df6-0: prompt: 'What is a chat template in HuggingFace?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 263, 13563, 4472, 297, 379, 688, 3460, 23360, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:53630 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:15:18 [engine.py:331] Added request cmpl-f74f0c8938c34cdd91d1dd88206a2df6-0.
INFO 08-09 13:15:20 [metrics.py:417] Avg prompt throughput: 2.4 tokens/s, Avg generation throughput: 88.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 13:15:25 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 90.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:15:29 [logger.py:43] Received request cmpl-716db08f552b4a2da8bf175674ab373f-0: prompt: 'What is inference latency and why is it important?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 27262, 23316, 1270, 322, 2020, 338, 372, 4100, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:48504 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:15:29 [engine.py:331] Added request cmpl-716db08f552b4a2da8bf175674ab373f-0.
INFO 08-09 13:15:30 [metrics.py:417] Avg prompt throughput: 2.4 tokens/s, Avg generation throughput: 92.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 13:15:35 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 89.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:15:40 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 92.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:15:40 [logger.py:43] Received request cmpl-ed62ad2182004ad19787a227b18d0f1b-0: prompt: 'How to reduce memory footprint of LLaMA models?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 304, 10032, 3370, 3661, 2158, 310, 365, 5661, 1529, 4733, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:51506 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:15:40 [engine.py:331] Added request cmpl-ed62ad2182004ad19787a227b18d0f1b-0.
INFO 08-09 13:15:45 [metrics.py:417] Avg prompt throughput: 2.6 tokens/s, Avg generation throughput: 91.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:15:50 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 86.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:15:52 [logger.py:43] Received request cmpl-3393457202b146a698d592f86351d261-0: prompt: 'What is the difference between `generation_config.json` and `config.json`?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 278, 4328, 1546, 421, 4738, 362, 29918, 2917, 29889, 3126, 29952, 322, 421, 2917, 29889, 3126, 6522], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:51112 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:15:52 [engine.py:331] Added request cmpl-3393457202b146a698d592f86351d261-0.
INFO 08-09 13:15:55 [metrics.py:417] Avg prompt throughput: 4.0 tokens/s, Avg generation throughput: 88.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 13:15:56 [logger.py:43] Received request cmpl-a9b9553fae9a446d85aa13843cf1264e-0: prompt: 'What is a quantized model and how is it served?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 263, 4323, 1891, 1904, 322, 920, 338, 372, 6766, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:51116 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:15:56 [engine.py:331] Added request cmpl-a9b9553fae9a446d85aa13843cf1264e-0.
INFO 08-09 13:16:00 [metrics.py:417] Avg prompt throughput: 2.6 tokens/s, Avg generation throughput: 79.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 13:16:00 [logger.py:43] Received request cmpl-222e14a426464f678c127e504e5bde8a-0: prompt: 'How to handle prompt overflow in generation?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 304, 4386, 9508, 11969, 297, 12623, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:38420 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:16:00 [engine.py:331] Added request cmpl-222e14a426464f678c127e504e5bde8a-0.
INFO 08-09 13:16:05 [metrics.py:417] Avg prompt throughput: 1.8 tokens/s, Avg generation throughput: 80.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:16:10 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 82.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:16:12 [logger.py:43] Received request cmpl-a02ab4077ce74913abe063ba110f14dc-0: prompt: 'What is warmup time in inference pipelines?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 14294, 786, 931, 297, 27262, 8450, 24210, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:49682 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:16:12 [engine.py:331] Added request cmpl-a02ab4077ce74913abe063ba110f14dc-0.
INFO 08-09 13:16:15 [metrics.py:417] Avg prompt throughput: 2.2 tokens/s, Avg generation throughput: 85.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 13:16:20 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 89.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:16:23 [logger.py:43] Received request cmpl-a19100df030a4ebd82c1e168e4dc78ef-0: prompt: 'How to stream JSON tokens using curl?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 304, 4840, 4663, 18897, 773, 11051, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:51926 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:16:23 [engine.py:331] Added request cmpl-a19100df030a4ebd82c1e168e4dc78ef-0.
INFO 08-09 13:16:25 [metrics.py:417] Avg prompt throughput: 1.8 tokens/s, Avg generation throughput: 90.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 13:16:30 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 91.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:16:34 [logger.py:43] Received request cmpl-300c88a2d2854c39b708367f49b7a24e-0: prompt: 'What is a `generate_with_streaming()` wrapper?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 263, 421, 17158, 29918, 2541, 29918, 5461, 292, 2555, 14476, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:59992 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:16:34 [engine.py:331] Added request cmpl-300c88a2d2854c39b708367f49b7a24e-0.
INFO 08-09 13:16:35 [metrics.py:417] Avg prompt throughput: 2.8 tokens/s, Avg generation throughput: 89.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 13:16:39 [logger.py:43] Received request cmpl-cd8faa999d8445adb43d7ed76ae1ad40-0: prompt: 'How to evaluate TTFT and ITL for multiple prompts?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 304, 14707, 323, 29911, 7818, 322, 13315, 29931, 363, 2999, 9508, 29879, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:51474 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:16:39 [engine.py:331] Added request cmpl-cd8faa999d8445adb43d7ed76ae1ad40-0.
INFO 08-09 13:16:40 [metrics.py:417] Avg prompt throughput: 3.0 tokens/s, Avg generation throughput: 89.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 13:16:45 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 89.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:16:50 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 83.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:16:50 [logger.py:43] Received request cmpl-d91338eb61a143e5b8e02524aebb6a6f-0: prompt: 'What are server cold starts in model serving?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 526, 1923, 11220, 8665, 297, 1904, 16330, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:51694 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:16:50 [engine.py:331] Added request cmpl-d91338eb61a143e5b8e02524aebb6a6f-0.
INFO 08-09 13:16:55 [metrics.py:417] Avg prompt throughput: 2.0 tokens/s, Avg generation throughput: 86.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 13:17:00 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 91.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:17:02 [logger.py:43] Received request cmpl-129f1b5443a54de1a393af2e096b8f8a-0: prompt: 'Explain how to avoid padding inefficiencies.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 12027, 7420, 920, 304, 4772, 7164, 297, 29872, 2416, 819, 2478, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:59846 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:17:02 [engine.py:331] Added request cmpl-129f1b5443a54de1a393af2e096b8f8a-0.
INFO 08-09 13:17:04 [logger.py:43] Received request cmpl-266413a780a845d4833a7ff795e40e5d-0: prompt: 'How does tokenizer pre-processing affect TTFT?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 947, 5993, 3950, 758, 29899, 19170, 6602, 323, 29911, 7818, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:59862 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:17:04 [engine.py:331] Added request cmpl-266413a780a845d4833a7ff795e40e5d-0.
INFO 08-09 13:17:05 [metrics.py:417] Avg prompt throughput: 5.2 tokens/s, Avg generation throughput: 88.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 13:17:10 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 90.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:17:15 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 85.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:17:16 [logger.py:43] Received request cmpl-b1f128d3849e475a8db337609221adc6-0: prompt: 'How to track per-prompt latency in logs?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 304, 5702, 639, 29899, 14032, 415, 23316, 1270, 297, 10748, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:52612 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:17:16 [engine.py:331] Added request cmpl-b1f128d3849e475a8db337609221adc6-0.
INFO 08-09 13:17:20 [logger.py:43] Received request cmpl-16e5c7c70d2f447a81ac0c3499acf9fd-0: prompt: 'Explain streaming benchmarks with high concurrency.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 12027, 7420, 24820, 23513, 29879, 411, 1880, 3022, 10880, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:53092 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:17:20 [engine.py:331] Added request cmpl-16e5c7c70d2f447a81ac0c3499acf9fd-0.
INFO 08-09 13:17:20 [metrics.py:417] Avg prompt throughput: 4.8 tokens/s, Avg generation throughput: 86.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 13:17:25 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 87.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:17:30 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 88.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:17:31 [logger.py:43] Received request cmpl-b28701e88ad64cf0a524682f7867574d-0: prompt: 'How to calculate power usage per prompt?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 304, 8147, 3081, 8744, 639, 9508, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:39066 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:17:31 [engine.py:331] Added request cmpl-b28701e88ad64cf0a524682f7867574d-0.
INFO 08-09 13:17:35 [metrics.py:417] Avg prompt throughput: 1.8 tokens/s, Avg generation throughput: 90.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 13:17:40 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 89.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:17:42 [logger.py:43] Received request cmpl-6431ae8ae3f04c50b605a2dee2a8b3c5-0: prompt: 'How does the tokenizer handle emojis and symbols?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 947, 278, 5993, 3950, 4386, 953, 3848, 275, 322, 15072, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:45066 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:17:42 [engine.py:331] Added request cmpl-6431ae8ae3f04c50b605a2dee2a8b3c5-0.
INFO 08-09 13:17:45 [metrics.py:417] Avg prompt throughput: 2.6 tokens/s, Avg generation throughput: 91.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 13:17:50 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 90.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:17:53 [logger.py:43] Received request cmpl-fb9a773f4d2646db95bf2b3ca3895820-0: prompt: 'How does model size affect inference latency?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 947, 1904, 2159, 6602, 27262, 23316, 1270, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:60158 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:17:53 [engine.py:331] Added request cmpl-fb9a773f4d2646db95bf2b3ca3895820-0.
INFO 08-09 13:17:55 [metrics.py:417] Avg prompt throughput: 2.0 tokens/s, Avg generation throughput: 90.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 13:18:00 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 87.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:18:04 [logger.py:43] Received request cmpl-c91962e1bde342a78a7b147166967557-0: prompt: 'How to debug slow streaming responses?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 304, 4744, 5232, 24820, 20890, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:53258 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:18:04 [engine.py:331] Added request cmpl-c91962e1bde342a78a7b147166967557-0.
INFO 08-09 13:18:05 [metrics.py:417] Avg prompt throughput: 1.6 tokens/s, Avg generation throughput: 89.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 13:18:10 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 82.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:18:15 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 92.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:18:16 [logger.py:43] Received request cmpl-daa5def4789f4b1cb3ccf7a0f919c349-0: prompt: 'What causes generation to stall midway?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 9946, 12623, 304, 380, 497, 7145, 1582, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:34054 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:18:16 [engine.py:331] Added request cmpl-daa5def4789f4b1cb3ccf7a0f919c349-0.
INFO 08-09 13:18:20 [metrics.py:417] Avg prompt throughput: 2.0 tokens/s, Avg generation throughput: 83.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 13:18:25 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 88.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:18:27 [logger.py:43] Received request cmpl-79865956bcab43c3b1a480dd16d9630f-0: prompt: 'How to load a model on HPU using HF Transformers?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 304, 2254, 263, 1904, 373, 379, 7056, 773, 379, 29943, 4103, 689, 414, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:58710 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:18:27 [engine.py:331] Added request cmpl-79865956bcab43c3b1a480dd16d9630f-0.
INFO 08-09 13:18:30 [metrics.py:417] Avg prompt throughput: 3.2 tokens/s, Avg generation throughput: 82.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 13:18:35 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 87.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:18:39 [logger.py:43] Received request cmpl-4eb131e67aa64338bc0907c6b7253680-0: prompt: 'How to pass prompt + history into a chat model?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 304, 1209, 9508, 718, 4955, 964, 263, 13563, 1904, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:35018 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:18:39 [engine.py:331] Added request cmpl-4eb131e67aa64338bc0907c6b7253680-0.
INFO 08-09 13:18:40 [metrics.py:417] Avg prompt throughput: 2.4 tokens/s, Avg generation throughput: 89.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 13:18:45 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 83.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:18:50 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 83.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:18:51 [logger.py:43] Received request cmpl-eadec38b1b144d189ec16103397a9dd6-0: prompt: 'What is an OpenAI-compatible inference API?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 385, 4673, 23869, 29899, 23712, 27262, 3450, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:34822 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:18:51 [engine.py:331] Added request cmpl-eadec38b1b144d189ec16103397a9dd6-0.
INFO 08-09 13:18:55 [metrics.py:417] Avg prompt throughput: 2.2 tokens/s, Avg generation throughput: 89.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:19:00 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 92.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:19:02 [logger.py:43] Received request cmpl-ea90a2ce4738482ea8aa821f8d80e5a6-0: prompt: 'How to handle decoding errors in stream?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 304, 4386, 1602, 3689, 4436, 297, 4840, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:32974 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:19:02 [engine.py:331] Added request cmpl-ea90a2ce4738482ea8aa821f8d80e5a6-0.
INFO 08-09 13:19:05 [metrics.py:417] Avg prompt throughput: 2.0 tokens/s, Avg generation throughput: 89.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 13:19:10 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 90.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:19:13 [logger.py:43] Received request cmpl-cad40d06db10460e9c7ce5b134847e67-0: prompt: 'What are temperature vs randomness in sampling?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 526, 10430, 7186, 4036, 2264, 297, 23460, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:35358 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:19:13 [engine.py:331] Added request cmpl-cad40d06db10460e9c7ce5b134847e67-0.
INFO 08-09 13:19:15 [metrics.py:417] Avg prompt throughput: 2.0 tokens/s, Avg generation throughput: 90.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 13:19:20 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 83.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:19:25 [logger.py:43] Received request cmpl-6872592f50ec4aaa92bfcd9aa98d096c-0: prompt: 'Explain token logprobs in output.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 12027, 7420, 5993, 1480, 771, 5824, 297, 1962, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:43440 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:19:25 [engine.py:331] Added request cmpl-6872592f50ec4aaa92bfcd9aa98d096c-0.
INFO 08-09 13:19:25 [metrics.py:417] Avg prompt throughput: 2.0 tokens/s, Avg generation throughput: 83.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 13:19:30 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 84.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:19:35 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 84.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:19:36 [logger.py:43] Received request cmpl-e020c94e9c2e43c3987ae32d116ea336-0: prompt: 'How to measure and reduce TTFT on Gaudi?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 304, 5645, 322, 10032, 323, 29911, 7818, 373, 10415, 4749, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:51662 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:19:36 [engine.py:331] Added request cmpl-e020c94e9c2e43c3987ae32d116ea336-0.
INFO 08-09 13:19:40 [metrics.py:417] Avg prompt throughput: 2.6 tokens/s, Avg generation throughput: 88.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 13:19:45 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 90.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:19:48 [logger.py:43] Received request cmpl-7a8d1e6229034218a19069203b6564a0-0: prompt: 'Explain the internals of HF `generate()` loop.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 12027, 7420, 278, 2836, 1338, 310, 379, 29943, 421, 17158, 2555, 2425, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:36482 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:19:48 [engine.py:331] Added request cmpl-7a8d1e6229034218a19069203b6564a0-0.
INFO 08-09 13:19:50 [metrics.py:417] Avg prompt throughput: 2.8 tokens/s, Avg generation throughput: 89.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 13:19:55 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 88.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:19:59 [logger.py:43] Received request cmpl-5df9c80f2eb04e05a7d084d521f5aefc-0: prompt: 'What is speculative token generation?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 1580, 28524, 5993, 12623, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:33564 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:19:59 [engine.py:331] Added request cmpl-5df9c80f2eb04e05a7d084d521f5aefc-0.
INFO 08-09 13:20:00 [metrics.py:417] Avg prompt throughput: 1.6 tokens/s, Avg generation throughput: 83.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 13:20:05 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 88.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:20:10 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 91.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:20:10 [logger.py:43] Received request cmpl-20609bd67c274af29a65a57405a231bc-0: prompt: 'What are possible failure points in vLLM serving?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 526, 1950, 10672, 3291, 297, 325, 2208, 29924, 16330, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:46570 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:20:10 [engine.py:331] Added request cmpl-20609bd67c274af29a65a57405a231bc-0.
INFO 08-09 13:20:15 [metrics.py:417] Avg prompt throughput: 2.4 tokens/s, Avg generation throughput: 90.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:20:20 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 92.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:20:21 [logger.py:43] Received request cmpl-555e42f3a7ba41c39214a4d78f0399c3-0: prompt: 'How to catch stream decoding errors gracefully?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 304, 4380, 4840, 1602, 3689, 4436, 17659, 3730, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:56968 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:20:21 [engine.py:331] Added request cmpl-555e42f3a7ba41c39214a4d78f0399c3-0.
INFO 08-09 13:20:25 [metrics.py:417] Avg prompt throughput: 2.2 tokens/s, Avg generation throughput: 88.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 13:20:27 [logger.py:43] Received request cmpl-95d654b0609245fba9078fcfedf08058-0: prompt: 'How to enable/disable kv caching in transformers?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 304, 9025, 29914, 20472, 10908, 22488, 297, 4327, 414, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:57758 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:20:27 [engine.py:331] Added request cmpl-95d654b0609245fba9078fcfedf08058-0.
INFO 08-09 13:20:30 [metrics.py:417] Avg prompt throughput: 2.4 tokens/s, Avg generation throughput: 88.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 13:20:34 [logger.py:43] Received request cmpl-3931c2eb40214810a9e4833365f59323-0: prompt: 'What are challenges of multilingual streaming inference?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 526, 18066, 267, 310, 1773, 6504, 950, 24820, 27262, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:57768 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:20:34 [engine.py:331] Added request cmpl-3931c2eb40214810a9e4833365f59323-0.
INFO 08-09 13:20:35 [metrics.py:417] Avg prompt throughput: 2.4 tokens/s, Avg generation throughput: 88.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 13:20:40 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 90.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:20:45 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 91.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:20:45 [logger.py:43] Received request cmpl-2a9f03fe681347919dff59c71b0362bb-0: prompt: 'How to benchmark with different model precisions?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 304, 23513, 411, 1422, 1904, 12132, 1080, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:42024 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:20:45 [engine.py:331] Added request cmpl-2a9f03fe681347919dff59c71b0362bb-0.
INFO 08-09 13:20:50 [metrics.py:417] Avg prompt throughput: 2.0 tokens/s, Avg generation throughput: 89.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:20:55 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 93.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:20:56 [logger.py:43] Received request cmpl-618a299fc92d4452a1ade4bae9b3185d-0: prompt: 'What is the tradeoff between token latency and batch size?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 278, 11302, 2696, 1546, 5993, 23316, 1270, 322, 9853, 2159, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:51824 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:20:56 [engine.py:331] Added request cmpl-618a299fc92d4452a1ade4bae9b3185d-0.
INFO 08-09 13:21:00 [metrics.py:417] Avg prompt throughput: 2.8 tokens/s, Avg generation throughput: 89.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 13:21:05 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 86.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:21:08 [logger.py:43] Received request cmpl-d0215b80adfe4c869f7c843e818a7f13-0: prompt: 'Describe the advantages and limitations of transformer-based neural networks.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 20355, 915, 278, 25486, 322, 27028, 310, 4327, 261, 29899, 6707, 19677, 14379, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:48186 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:21:08 [engine.py:331] Added request cmpl-d0215b80adfe4c869f7c843e818a7f13-0.
INFO 08-09 13:21:10 [metrics.py:417] Avg prompt throughput: 3.0 tokens/s, Avg generation throughput: 85.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 13:21:15 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 85.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:21:20 [logger.py:43] Received request cmpl-e913ccb50c5448629d61f9ac0d66f4e3-0: prompt: 'Explain the concept of overfitting in machine learning and how to prevent it.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 12027, 7420, 278, 6964, 310, 975, 29888, 5367, 297, 4933, 6509, 322, 920, 304, 5557, 372, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:54134 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:21:20 [engine.py:331] Added request cmpl-e913ccb50c5448629d61f9ac0d66f4e3-0.
INFO 08-09 13:21:20 [metrics.py:417] Avg prompt throughput: 3.6 tokens/s, Avg generation throughput: 84.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 13:21:25 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 83.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:21:25 [logger.py:43] Received request cmpl-5a2f1aa605794812b096e80c8130c25f-0: prompt: 'How does the attention mechanism work in transformer models?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 947, 278, 8570, 13336, 664, 297, 4327, 261, 4733, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:54146 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:21:25 [engine.py:331] Added request cmpl-5a2f1aa605794812b096e80c8130c25f-0.
INFO 08-09 13:21:30 [metrics.py:417] Avg prompt throughput: 2.4 tokens/s, Avg generation throughput: 85.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:21:35 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 89.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:21:37 [logger.py:43] Received request cmpl-4c26e75ec98b4371a1f646f1e06868ba-0: prompt: 'Summarize the main differences between supervised and unsupervised learning.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 6991, 3034, 675, 278, 1667, 12651, 1546, 2428, 11292, 322, 443, 9136, 11292, 6509, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:54228 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:21:37 [engine.py:331] Added request cmpl-4c26e75ec98b4371a1f646f1e06868ba-0.
INFO 08-09 13:21:40 [metrics.py:417] Avg prompt throughput: 3.2 tokens/s, Avg generation throughput: 88.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 13:21:45 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 88.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:21:48 [logger.py:43] Received request cmpl-143eee5eed0a44848c9c3dfba2880896-0: prompt: 'What are the ethical concerns associated with deploying large language models?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 526, 278, 11314, 936, 21838, 6942, 411, 7246, 292, 2919, 4086, 4733, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:40664 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:21:48 [engine.py:331] Added request cmpl-143eee5eed0a44848c9c3dfba2880896-0.
INFO 08-09 13:21:50 [metrics.py:417] Avg prompt throughput: 3.0 tokens/s, Avg generation throughput: 85.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 13:21:55 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 84.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:22:00 [logger.py:43] Received request cmpl-2520c2bd9e7e4e15a4098df74fa60915-0: prompt: 'Explain how gradient descent works in training deep neural networks.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 12027, 7420, 920, 16030, 26815, 1736, 297, 6694, 6483, 19677, 14379, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:53376 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:22:00 [engine.py:331] Added request cmpl-2520c2bd9e7e4e15a4098df74fa60915-0.
INFO 08-09 13:22:00 [metrics.py:417] Avg prompt throughput: 2.6 tokens/s, Avg generation throughput: 85.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 13:22:05 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 87.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:22:10 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 85.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:22:11 [logger.py:43] Received request cmpl-6c9c88fb59f648899f361efe77e0266a-0: prompt: 'Describe the role of activation functions in deep learning.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 20355, 915, 278, 6297, 310, 26229, 3168, 297, 6483, 6509, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:60292 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:22:11 [engine.py:331] Added request cmpl-6c9c88fb59f648899f361efe77e0266a-0.
INFO 08-09 13:22:15 [metrics.py:417] Avg prompt throughput: 2.4 tokens/s, Avg generation throughput: 90.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 13:22:20 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 91.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:22:22 [logger.py:43] Received request cmpl-6b871384a83c47eabd854aba7cd6a12d-0: prompt: 'How can transfer learning improve model performance on small datasets?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 508, 6782, 6509, 11157, 1904, 4180, 373, 2319, 20035, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:54528 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:22:22 [engine.py:331] Added request cmpl-6b871384a83c47eabd854aba7cd6a12d-0.
INFO 08-09 13:22:25 [metrics.py:417] Avg prompt throughput: 2.4 tokens/s, Avg generation throughput: 87.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 13:22:30 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 90.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:22:33 [logger.py:43] Received request cmpl-d6dcd66b5abc427482f6abfac49309d5-0: prompt: 'Explain the concept of reinforcement learning with an example.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 12027, 7420, 278, 6964, 310, 15561, 1454, 13561, 6509, 411, 385, 1342, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:35802 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:22:33 [engine.py:331] Added request cmpl-d6dcd66b5abc427482f6abfac49309d5-0.
INFO 08-09 13:22:35 [metrics.py:417] Avg prompt throughput: 2.8 tokens/s, Avg generation throughput: 90.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 13:22:40 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 92.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:22:45 [logger.py:43] Received request cmpl-1ec31effefff492b9d0f00631bfed1f1-0: prompt: 'What are the main components of a convolutional neural network (CNN)?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 526, 278, 1667, 7117, 310, 263, 26851, 284, 19677, 3564, 313, 29907, 10262, 6877], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:49008 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:22:45 [engine.py:331] Added request cmpl-1ec31effefff492b9d0f00631bfed1f1-0.
INFO 08-09 13:22:45 [metrics.py:417] Avg prompt throughput: 3.2 tokens/s, Avg generation throughput: 88.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 13:22:50 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 90.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:22:55 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 90.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:22:56 [logger.py:43] Received request cmpl-f31dbf324d42470d8d0b5c809eac49d0-0: prompt: 'Describe how large language models like GPT or LLaMA are trained and fine-tuned.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 20355, 915, 920, 2919, 4086, 4733, 763, 402, 7982, 470, 365, 5661, 1529, 526, 16370, 322, 2691, 29899, 29873, 348, 287, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:44946 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:22:56 [engine.py:331] Added request cmpl-f31dbf324d42470d8d0b5c809eac49d0-0.
INFO 08-09 13:23:00 [metrics.py:417] Avg prompt throughput: 4.6 tokens/s, Avg generation throughput: 92.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:23:05 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 88.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:23:07 [logger.py:43] Received request cmpl-879ba437aab548018c16ea9bb03d5a03-0: prompt: 'Compare and contrast BERT and GPT in terms of architecture and use cases.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 3831, 598, 322, 12814, 350, 20161, 322, 402, 7982, 297, 4958, 310, 11258, 322, 671, 4251, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:45772 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:23:07 [engine.py:331] Added request cmpl-879ba437aab548018c16ea9bb03d5a03-0.
INFO 08-09 13:23:10 [metrics.py:417] Avg prompt throughput: 3.6 tokens/s, Avg generation throughput: 90.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 13:23:15 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 91.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:23:28 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 18.1 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 13:23:38 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
Received Interrupt
Received Interrupt
Received Interrupt
Received Interrupt
Received Interrupt
Received Interrupt
Received Interrupt
Received Interrupt
INFO 08-09 13:25:40 [launcher.py:80] Shutting down FastAPI HTTP server.
INFO:     Shutting down
INFO:     Waiting for application shutdown.
INFO:     Application shutdown complete.
/usr/lib/python3.10/multiprocessing/resource_tracker.py:224: UserWarning: resource_tracker: There appear to be 32 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
/usr/lib/python3.10/multiprocessing/resource_tracker.py:224: UserWarning: resource_tracker: There appear to be 1 leaked shared_memory objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
