Calling add_step_closure function does not have any effect. It's lazy mode only functionality. (warning logged once)
Calling mark_step function does not have any effect. It's lazy mode only functionality. (warning logged once)
Calling iter_mark_step function does not have any effect. It's lazy mode only functionality. (warning logged once)
INFO 08-09 12:32:10 [__init__.py:254] Automatically detected platform hpu.
INFO 08-09 12:32:13 [config.py:1975] Disabled the custom all-reduce kernel because it is not supported on current platform.
INFO 08-09 12:32:13 [config.py:1975] Disabled the custom all-reduce kernel because it is not supported on current platform.
INFO 08-09 12:32:14 [config.py:1975] Disabled the custom all-reduce kernel because it is not supported on current platform.
INFO 08-09 12:32:14 [config.py:1975] Disabled the custom all-reduce kernel because it is not supported on current platform.
INFO 08-09 12:32:14 [api_server.py:1289] vLLM API server version 0.6.3.dev5453+ge9c83fca1
INFO 08-09 12:32:15 [config.py:1975] Disabled the custom all-reduce kernel because it is not supported on current platform.
INFO 08-09 12:32:15 [cli_args.py:309] non-default args: {'model': '/software/data/llama_inference/Llama-2-7b-hf/', 'tensor_parallel_size': 4}
INFO 08-09 12:32:22 [config.py:822] This model supports multiple tasks: {'reward', 'score', 'generate', 'embed', 'classify'}. Defaulting to 'generate'.
WARNING 08-09 12:32:22 [config.py:3236] Your device 'hpu' doesn't support torch.float16. Falling back to torch.bfloat16 for compatibility.
WARNING 08-09 12:32:22 [config.py:3287] Casting torch.float16 to torch.bfloat16.
INFO 08-09 12:32:22 [arg_utils.py:1706] hpu is experimental on VLLM_USE_V1=1. Falling back to V0 Engine.
INFO 08-09 12:32:22 [config.py:1941] Defaulting to use mp for distributed inference
INFO 08-09 12:32:22 [config.py:1975] Disabled the custom all-reduce kernel because it is not supported on current platform.
INFO 08-09 12:32:22 [api_server.py:267] Started engine process with PID 19780
Calling add_step_closure function does not have any effect. It's lazy mode only functionality. (warning logged once)
Calling mark_step function does not have any effect. It's lazy mode only functionality. (warning logged once)
Calling iter_mark_step function does not have any effect. It's lazy mode only functionality. (warning logged once)
WARNING 08-09 12:32:25 [env_override.py:17] NCCL_CUMEM_ENABLE is set to 0, skipping override. This may increase memory overhead with cudagraph+allreduce: https://github.com/NVIDIA/nccl/issues/1234
INFO 08-09 12:32:26 [__init__.py:254] Automatically detected platform hpu.
INFO 08-09 12:32:29 [llm_engine.py:239] Initializing a V0 LLM engine (v0.6.3.dev5453+ge9c83fca1) with config: model='/software/data/llama_inference/Llama-2-7b-hf/', speculative_config=None, tokenizer='/software/data/llama_inference/Llama-2-7b-hf/', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=4, pipeline_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=hpu, decoding_config=DecodingConfig(backend='xgrammar', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=None, served_model_name=/software/data/llama_inference/Llama-2-7b-hf/, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, pooler_config=None, compilation_config={"level":0,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":[],"splitting_ops":[],"use_inductor":true,"compile_sizes":[],"inductor_compile_config":{"enable_auto_functionalized_v2":false},"inductor_passes":{},"use_cudagraph":false,"cudagraph_num_of_warmups":0,"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"max_capture_size":256,"local_cache_dir":null}, use_cached_outputs=True, 
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
WARNING 08-09 12:32:29 [multiproc_worker_utils.py:325] Reducing Torch parallelism from 72 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
WARNING 08-09 12:32:29 [rocm.py:28] Failed to import from amdsmi with ModuleNotFoundError("No module named 'amdsmi'")
WARNING 08-09 12:32:29 [rocm.py:33] Failed to import from vllm._C with ModuleNotFoundError("No module named 'vllm._C'")
WARNING 08-09 12:32:29 [rocm.py:39] Failed to import from vllm._rocm_C with ModuleNotFoundError("No module named 'vllm._rocm_C'")
WARNING 08-09 12:32:29 [utils.py:2850] Methods add_prompt_adapter,cache_config,compilation_config,current_platform,list_prompt_adapters,load_config,pin_prompt_adapter,remove_prompt_adapter,scheduler_config not implemented in <vllm.worker.hpu_worker.HPUWorker object at 0x7ac92777cfa0>
INFO 08-09 12:32:30 [runtime.py:26] Environment:
INFO 08-09 12:32:30 [runtime.py:30]     hw: gaudi2
INFO 08-09 12:32:30 [runtime.py:30]     build: 1.22.0.425
INFO 08-09 12:32:30 [runtime.py:30]     engine_version: v0
INFO 08-09 12:32:30 [runtime.py:30]     bridge_mode: eager
INFO 08-09 12:32:30 [runtime.py:30]     model_type: llama
INFO 08-09 12:32:30 [runtime.py:26] Features:
INFO 08-09 12:32:30 [runtime.py:30]     fp32_alibi_biases: True
INFO 08-09 12:32:30 [runtime.py:30]     fp32_softmax: False
INFO 08-09 12:32:30 [runtime.py:30]     fused_block_softmax_adjustment: False
INFO 08-09 12:32:30 [runtime.py:30]     fused_block_softmax: False
INFO 08-09 12:32:30 [runtime.py:30]     prompt_attn_impl: fsdpa_impl
INFO 08-09 12:32:30 [runtime.py:30]     skip_warmup: False
INFO 08-09 12:32:30 [runtime.py:30]     merged_prefill: False
INFO 08-09 12:32:30 [runtime.py:30]     use_contiguous_pa: True
INFO 08-09 12:32:30 [runtime.py:30]     use_delayed_sampling: True
INFO 08-09 12:32:30 [runtime.py:30]     use_bucketing: True
INFO 08-09 12:32:30 [runtime.py:30]     bucketing_strategy: exponential_bucketing
INFO 08-09 12:32:30 [runtime.py:26] User flags:
INFO 08-09 12:32:30 [runtime.py:30]     VLLM_USE_V1: False
WARNING 08-09 12:32:30 [hpu.py:135] Pin memory is not supported on HPU.
INFO 08-09 12:32:30 [hpu.py:60] Using HPUAttention backend.
Calling add_step_closure function does not have any effect. It's lazy mode only functionality. (warning logged once)
Calling mark_step function does not have any effect. It's lazy mode only functionality. (warning logged once)
Calling iter_mark_step function does not have any effect. It's lazy mode only functionality. (warning logged once)
Calling add_step_closure function does not have any effect. It's lazy mode only functionality. (warning logged once)
Calling mark_step function does not have any effect. It's lazy mode only functionality. (warning logged once)
Calling iter_mark_step function does not have any effect. It's lazy mode only functionality. (warning logged once)
Calling add_step_closure function does not have any effect. It's lazy mode only functionality. (warning logged once)
Calling mark_step function does not have any effect. It's lazy mode only functionality. (warning logged once)
Calling iter_mark_step function does not have any effect. It's lazy mode only functionality. (warning logged once)
WARNING 08-09 12:32:32 [env_override.py:17] NCCL_CUMEM_ENABLE is set to 0, skipping override. This may increase memory overhead with cudagraph+allreduce: https://github.com/NVIDIA/nccl/issues/1234
WARNING 08-09 12:32:32 [env_override.py:17] NCCL_CUMEM_ENABLE is set to 0, skipping override. This may increase memory overhead with cudagraph+allreduce: https://github.com/NVIDIA/nccl/issues/1234
WARNING 08-09 12:32:32 [env_override.py:17] NCCL_CUMEM_ENABLE is set to 0, skipping override. This may increase memory overhead with cudagraph+allreduce: https://github.com/NVIDIA/nccl/issues/1234
INFO 08-09 12:32:33 [__init__.py:254] Automatically detected platform hpu.
INFO 08-09 12:32:33 [__init__.py:254] Automatically detected platform hpu.
INFO 08-09 12:32:33 [__init__.py:254] Automatically detected platform hpu.
[1;36m(VllmWorkerProcess pid=19916)[0;0m INFO 08-09 12:32:35 [multiproc_worker_utils.py:228] Worker ready; awaiting tasks
[1;36m(VllmWorkerProcess pid=19917)[0;0m INFO 08-09 12:32:35 [multiproc_worker_utils.py:228] Worker ready; awaiting tasks
[1;36m(VllmWorkerProcess pid=19918)[0;0m INFO 08-09 12:32:35 [multiproc_worker_utils.py:228] Worker ready; awaiting tasks
[1;36m(VllmWorkerProcess pid=19916)[0;0m WARNING 08-09 12:32:36 [rocm.py:28] Failed to import from amdsmi with ModuleNotFoundError("No module named 'amdsmi'")
[1;36m(VllmWorkerProcess pid=19916)[0;0m WARNING 08-09 12:32:36 [rocm.py:33] Failed to import from vllm._C with ModuleNotFoundError("No module named 'vllm._C'")
[1;36m(VllmWorkerProcess pid=19916)[0;0m WARNING 08-09 12:32:36 [rocm.py:39] Failed to import from vllm._rocm_C with ModuleNotFoundError("No module named 'vllm._rocm_C'")
[1;36m(VllmWorkerProcess pid=19917)[0;0m WARNING 08-09 12:32:36 [rocm.py:28] Failed to import from amdsmi with ModuleNotFoundError("No module named 'amdsmi'")
[1;36m(VllmWorkerProcess pid=19917)[0;0m WARNING 08-09 12:32:36 [rocm.py:33] Failed to import from vllm._C with ModuleNotFoundError("No module named 'vllm._C'")
[1;36m(VllmWorkerProcess pid=19917)[0;0m WARNING 08-09 12:32:36 [rocm.py:39] Failed to import from vllm._rocm_C with ModuleNotFoundError("No module named 'vllm._rocm_C'")
[1;36m(VllmWorkerProcess pid=19918)[0;0m WARNING 08-09 12:32:36 [rocm.py:28] Failed to import from amdsmi with ModuleNotFoundError("No module named 'amdsmi'")
[1;36m(VllmWorkerProcess pid=19918)[0;0m WARNING 08-09 12:32:36 [rocm.py:33] Failed to import from vllm._C with ModuleNotFoundError("No module named 'vllm._C'")
[1;36m(VllmWorkerProcess pid=19918)[0;0m WARNING 08-09 12:32:36 [rocm.py:39] Failed to import from vllm._rocm_C with ModuleNotFoundError("No module named 'vllm._rocm_C'")
[1;36m(VllmWorkerProcess pid=19916)[0;0m WARNING 08-09 12:32:36 [utils.py:2850] Methods add_prompt_adapter,cache_config,compilation_config,current_platform,list_prompt_adapters,load_config,pin_prompt_adapter,remove_prompt_adapter,scheduler_config not implemented in <vllm.worker.hpu_worker.HPUWorker object at 0x743f13351cc0>
[1;36m(VllmWorkerProcess pid=19917)[0;0m WARNING 08-09 12:32:36 [utils.py:2850] Methods add_prompt_adapter,cache_config,compilation_config,current_platform,list_prompt_adapters,load_config,pin_prompt_adapter,remove_prompt_adapter,scheduler_config not implemented in <vllm.worker.hpu_worker.HPUWorker object at 0x7096ba341de0>
[1;36m(VllmWorkerProcess pid=19918)[0;0m WARNING 08-09 12:32:36 [utils.py:2850] Methods add_prompt_adapter,cache_config,compilation_config,current_platform,list_prompt_adapters,load_config,pin_prompt_adapter,remove_prompt_adapter,scheduler_config not implemented in <vllm.worker.hpu_worker.HPUWorker object at 0x704788ef5d20>
[1;36m(VllmWorkerProcess pid=19916)[0;0m INFO 08-09 12:32:37 [runtime.py:26] Environment:
[1;36m(VllmWorkerProcess pid=19916)[0;0m INFO 08-09 12:32:37 [runtime.py:30]     hw: gaudi2
[1;36m(VllmWorkerProcess pid=19916)[0;0m INFO 08-09 12:32:37 [runtime.py:30]     build: 1.22.0.425
[1;36m(VllmWorkerProcess pid=19916)[0;0m INFO 08-09 12:32:37 [runtime.py:30]     engine_version: v0
[1;36m(VllmWorkerProcess pid=19916)[0;0m INFO 08-09 12:32:37 [runtime.py:30]     bridge_mode: eager
[1;36m(VllmWorkerProcess pid=19916)[0;0m INFO 08-09 12:32:37 [runtime.py:30]     model_type: llama
[1;36m(VllmWorkerProcess pid=19916)[0;0m INFO 08-09 12:32:37 [runtime.py:26] Features:
[1;36m(VllmWorkerProcess pid=19916)[0;0m INFO 08-09 12:32:37 [runtime.py:30]     fp32_alibi_biases: True
[1;36m(VllmWorkerProcess pid=19916)[0;0m INFO 08-09 12:32:37 [runtime.py:30]     fp32_softmax: False
[1;36m(VllmWorkerProcess pid=19916)[0;0m INFO 08-09 12:32:37 [runtime.py:30]     fused_block_softmax_adjustment: False
[1;36m(VllmWorkerProcess pid=19916)[0;0m INFO 08-09 12:32:37 [runtime.py:30]     fused_block_softmax: False
[1;36m(VllmWorkerProcess pid=19916)[0;0m INFO 08-09 12:32:37 [runtime.py:30]     prompt_attn_impl: fsdpa_impl
[1;36m(VllmWorkerProcess pid=19916)[0;0m INFO 08-09 12:32:37 [runtime.py:30]     skip_warmup: False
[1;36m(VllmWorkerProcess pid=19916)[0;0m INFO 08-09 12:32:37 [runtime.py:30]     merged_prefill: False
[1;36m(VllmWorkerProcess pid=19916)[0;0m INFO 08-09 12:32:37 [runtime.py:30]     use_contiguous_pa: True
[1;36m(VllmWorkerProcess pid=19916)[0;0m INFO 08-09 12:32:37 [runtime.py:30]     use_delayed_sampling: True
[1;36m(VllmWorkerProcess pid=19916)[0;0m INFO 08-09 12:32:37 [runtime.py:30]     use_bucketing: True
[1;36m(VllmWorkerProcess pid=19916)[0;0m INFO 08-09 12:32:37 [runtime.py:30]     bucketing_strategy: exponential_bucketing
[1;36m(VllmWorkerProcess pid=19916)[0;0m INFO 08-09 12:32:37 [runtime.py:26] User flags:
[1;36m(VllmWorkerProcess pid=19916)[0;0m INFO 08-09 12:32:37 [runtime.py:30]     VLLM_USE_V1: False
[1;36m(VllmWorkerProcess pid=19916)[0;0m WARNING 08-09 12:32:37 [hpu.py:135] Pin memory is not supported on HPU.
[1;36m(VllmWorkerProcess pid=19916)[0;0m INFO 08-09 12:32:37 [hpu.py:60] Using HPUAttention backend.
[1;36m(VllmWorkerProcess pid=19918)[0;0m INFO 08-09 12:32:37 [runtime.py:26] Environment:
[1;36m(VllmWorkerProcess pid=19918)[0;0m INFO 08-09 12:32:37 [runtime.py:30]     hw: gaudi2
[1;36m(VllmWorkerProcess pid=19918)[0;0m INFO 08-09 12:32:37 [runtime.py:30]     build: 1.22.0.425
[1;36m(VllmWorkerProcess pid=19918)[0;0m INFO 08-09 12:32:37 [runtime.py:30]     engine_version: v0
[1;36m(VllmWorkerProcess pid=19918)[0;0m INFO 08-09 12:32:37 [runtime.py:30]     bridge_mode: eager
[1;36m(VllmWorkerProcess pid=19918)[0;0m INFO 08-09 12:32:37 [runtime.py:30]     model_type: llama
[1;36m(VllmWorkerProcess pid=19918)[0;0m INFO 08-09 12:32:37 [runtime.py:26] Features:
[1;36m(VllmWorkerProcess pid=19918)[0;0m INFO 08-09 12:32:37 [runtime.py:30]     fp32_alibi_biases: True
[1;36m(VllmWorkerProcess pid=19918)[0;0m INFO 08-09 12:32:37 [runtime.py:30]     fp32_softmax: False
[1;36m(VllmWorkerProcess pid=19918)[0;0m INFO 08-09 12:32:37 [runtime.py:30]     fused_block_softmax_adjustment: False
[1;36m(VllmWorkerProcess pid=19918)[0;0m INFO 08-09 12:32:37 [runtime.py:30]     fused_block_softmax: False
[1;36m(VllmWorkerProcess pid=19918)[0;0m INFO 08-09 12:32:37 [runtime.py:30]     prompt_attn_impl: fsdpa_impl
[1;36m(VllmWorkerProcess pid=19918)[0;0m INFO 08-09 12:32:37 [runtime.py:30]     skip_warmup: False
[1;36m(VllmWorkerProcess pid=19918)[0;0m INFO 08-09 12:32:37 [runtime.py:30]     merged_prefill: False
[1;36m(VllmWorkerProcess pid=19918)[0;0m INFO 08-09 12:32:37 [runtime.py:30]     use_contiguous_pa: True
[1;36m(VllmWorkerProcess pid=19918)[0;0m INFO 08-09 12:32:37 [runtime.py:30]     use_delayed_sampling: True
[1;36m(VllmWorkerProcess pid=19918)[0;0m INFO 08-09 12:32:37 [runtime.py:30]     use_bucketing: True
[1;36m(VllmWorkerProcess pid=19918)[0;0m INFO 08-09 12:32:37 [runtime.py:30]     bucketing_strategy: exponential_bucketing
[1;36m(VllmWorkerProcess pid=19918)[0;0m INFO 08-09 12:32:37 [runtime.py:26] User flags:
[1;36m(VllmWorkerProcess pid=19918)[0;0m INFO 08-09 12:32:37 [runtime.py:30]     VLLM_USE_V1: False
[1;36m(VllmWorkerProcess pid=19918)[0;0m WARNING 08-09 12:32:37 [hpu.py:135] Pin memory is not supported on HPU.
[1;36m(VllmWorkerProcess pid=19918)[0;0m INFO 08-09 12:32:37 [hpu.py:60] Using HPUAttention backend.
[1;36m(VllmWorkerProcess pid=19917)[0;0m INFO 08-09 12:32:37 [runtime.py:26] Environment:
[1;36m(VllmWorkerProcess pid=19917)[0;0m INFO 08-09 12:32:37 [runtime.py:30]     hw: gaudi2
[1;36m(VllmWorkerProcess pid=19917)[0;0m INFO 08-09 12:32:37 [runtime.py:30]     build: 1.22.0.425
[1;36m(VllmWorkerProcess pid=19917)[0;0m INFO 08-09 12:32:37 [runtime.py:30]     engine_version: v0
[1;36m(VllmWorkerProcess pid=19917)[0;0m INFO 08-09 12:32:37 [runtime.py:30]     bridge_mode: eager
[1;36m(VllmWorkerProcess pid=19917)[0;0m INFO 08-09 12:32:37 [runtime.py:30]     model_type: llama
[1;36m(VllmWorkerProcess pid=19917)[0;0m INFO 08-09 12:32:37 [runtime.py:26] Features:
[1;36m(VllmWorkerProcess pid=19917)[0;0m INFO 08-09 12:32:37 [runtime.py:30]     fp32_alibi_biases: True
[1;36m(VllmWorkerProcess pid=19917)[0;0m INFO 08-09 12:32:37 [runtime.py:30]     fp32_softmax: False
[1;36m(VllmWorkerProcess pid=19917)[0;0m INFO 08-09 12:32:37 [runtime.py:30]     fused_block_softmax_adjustment: False
[1;36m(VllmWorkerProcess pid=19917)[0;0m INFO 08-09 12:32:37 [runtime.py:30]     fused_block_softmax: False
[1;36m(VllmWorkerProcess pid=19917)[0;0m INFO 08-09 12:32:37 [runtime.py:30]     prompt_attn_impl: fsdpa_impl
[1;36m(VllmWorkerProcess pid=19917)[0;0m INFO 08-09 12:32:37 [runtime.py:30]     skip_warmup: False
[1;36m(VllmWorkerProcess pid=19917)[0;0m INFO 08-09 12:32:37 [runtime.py:30]     merged_prefill: False
[1;36m(VllmWorkerProcess pid=19917)[0;0m INFO 08-09 12:32:37 [runtime.py:30]     use_contiguous_pa: True
[1;36m(VllmWorkerProcess pid=19917)[0;0m INFO 08-09 12:32:37 [runtime.py:30]     use_delayed_sampling: True
[1;36m(VllmWorkerProcess pid=19917)[0;0m INFO 08-09 12:32:37 [runtime.py:30]     use_bucketing: True
[1;36m(VllmWorkerProcess pid=19917)[0;0m INFO 08-09 12:32:37 [runtime.py:30]     bucketing_strategy: exponential_bucketing
[1;36m(VllmWorkerProcess pid=19917)[0;0m INFO 08-09 12:32:37 [runtime.py:26] User flags:
[1;36m(VllmWorkerProcess pid=19917)[0;0m INFO 08-09 12:32:37 [runtime.py:30]     VLLM_USE_V1: False
[1;36m(VllmWorkerProcess pid=19917)[0;0m WARNING 08-09 12:32:37 [hpu.py:135] Pin memory is not supported on HPU.
[1;36m(VllmWorkerProcess pid=19917)[0;0m INFO 08-09 12:32:37 [hpu.py:60] Using HPUAttention backend.
============================= HPU SW VERSION ====================================== 
 HB_BUILD_VER = 1.22.0-425
============================= HPU PT BRIDGE CONFIGURATION ON RANK = 0 ============= 
 PT_HPU_LAZY_MODE = 0
 PT_HPU_RECIPE_CACHE_CONFIG = ,false,1024,false
 PT_HPU_MAX_COMPOUND_OP_SIZE = 9223372036854775807
 PT_HPU_LAZY_ACC_PAR_MODE = 1
 PT_HPU_ENABLE_REFINE_DYNAMIC_SHAPES = 0
 PT_HPU_EAGER_PIPELINE_ENABLE = 1
 PT_HPU_EAGER_COLLECTIVE_PIPELINE_ENABLE = 1
 PT_HPU_ENABLE_LAZY_COLLECTIVES = 1
---------------------------: System Configuration :---------------------------
Num CPU Cores : 160
CPU RAM       : 1007 GB
------------------------------------------------------------------------------
============================= HPU SW VERSION ====================================== 
 HB_BUILD_VER = 1.22.0-425
============================= HPU PT BRIDGE CONFIGURATION ON RANK = 0 ============= 
 PT_HPU_LAZY_MODE = 0
 PT_HPU_RECIPE_CACHE_CONFIG = ,false,1024,false
 PT_HPU_MAX_COMPOUND_OP_SIZE = 9223372036854775807
 PT_HPU_LAZY_ACC_PAR_MODE = 1
 PT_HPU_ENABLE_REFINE_DYNAMIC_SHAPES = 0
 PT_HPU_EAGER_PIPELINE_ENABLE = 1
 PT_HPU_EAGER_COLLECTIVE_PIPELINE_ENABLE = 1
 PT_HPU_ENABLE_LAZY_COLLECTIVES = 1
---------------------------: System Configuration :---------------------------
Num CPU Cores : 160
CPU RAM       : 1007 GB
------------------------------------------------------------------------------
============================= HPU SW VERSION ====================================== 
 HB_BUILD_VER = 1.22.0-425
============================= HPU PT BRIDGE CONFIGURATION ON RANK = 0 ============= 
 PT_HPU_LAZY_MODE = 0
 PT_HPU_RECIPE_CACHE_CONFIG = ,false,1024,false
 PT_HPU_MAX_COMPOUND_OP_SIZE = 9223372036854775807
 PT_HPU_LAZY_ACC_PAR_MODE = 1
 PT_HPU_ENABLE_REFINE_DYNAMIC_SHAPES = 0
 PT_HPU_EAGER_PIPELINE_ENABLE = 1
 PT_HPU_EAGER_COLLECTIVE_PIPELINE_ENABLE = 1
 PT_HPU_ENABLE_LAZY_COLLECTIVES = 1
---------------------------: System Configuration :---------------------------
Num CPU Cores : 160
CPU RAM       : 1007 GB
------------------------------------------------------------------------------
============================= HPU SW VERSION ====================================== 
 HB_BUILD_VER = 1.22.0-425
============================= HPU PT BRIDGE CONFIGURATION ON RANK = 0 ============= 
 PT_HPU_LAZY_MODE = 0
 PT_HPU_RECIPE_CACHE_CONFIG = ,false,1024,false
 PT_HPU_MAX_COMPOUND_OP_SIZE = 9223372036854775807
 PT_HPU_LAZY_ACC_PAR_MODE = 1
 PT_HPU_ENABLE_REFINE_DYNAMIC_SHAPES = 0
 PT_HPU_EAGER_PIPELINE_ENABLE = 1
 PT_HPU_EAGER_COLLECTIVE_PIPELINE_ENABLE = 1
 PT_HPU_ENABLE_LAZY_COLLECTIVES = 1
---------------------------: System Configuration :---------------------------
Num CPU Cores : 160
CPU RAM       : 1007 GB
------------------------------------------------------------------------------
INFO 08-09 12:32:41 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[1, 2, 3], buffer_handle=(3, 4194304, 6, 'psm_f03c8919'), local_subscribe_addr='ipc:///tmp/04e5a324-abee-4f01-991b-bade56a41814', remote_subscribe_addr=None, remote_addr_ipv6=False)
INFO 08-09 12:32:41 [parallel_state.py:1065] rank 0 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(VllmWorkerProcess pid=19916)[0;0m INFO 08-09 12:32:41 [parallel_state.py:1065] rank 1 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 1, EP rank 1
[1;36m(VllmWorkerProcess pid=19917)[0;0m INFO 08-09 12:32:41 [parallel_state.py:1065] rank 2 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 2, EP rank 2
[1;36m(VllmWorkerProcess pid=19918)[0;0m INFO 08-09 12:32:41 [parallel_state.py:1065] rank 3 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 3, EP rank 3
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  3.71it/s]
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.79it/s]
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.94it/s]

INFO 08-09 12:32:43 [default_loader.py:272] Loading weights took 1.11 seconds
INFO 08-09 12:32:43 [hpu_model_runner.py:1184] Pre-loading model weights on hpu:0 took 3.139 GiB of device memory (3.145 GiB/94.62 GiB used) and 168.8 MiB of host memory (418.6 GiB/1007 GiB used)
[1;36m(VllmWorkerProcess pid=19916)[0;0m INFO 08-09 12:32:43 [default_loader.py:272] Loading weights took 1.09 seconds
[1;36m(VllmWorkerProcess pid=19917)[0;0m INFO 08-09 12:32:43 [default_loader.py:272] Loading weights took 1.12 seconds
[1;36m(VllmWorkerProcess pid=19918)[0;0m INFO 08-09 12:32:43 [default_loader.py:272] Loading weights took 1.10 seconds
[1;36m(VllmWorkerProcess pid=19916)[0;0m INFO 08-09 12:32:43 [hpu_model_runner.py:1184] Pre-loading model weights on hpu:0 took 3.139 GiB of device memory (3.145 GiB/94.62 GiB used) and 132 MiB of host memory (418.6 GiB/1007 GiB used)
[1;36m(VllmWorkerProcess pid=19917)[0;0m INFO 08-09 12:32:43 [hpu_model_runner.py:1184] Pre-loading model weights on hpu:0 took 3.139 GiB of device memory (3.145 GiB/94.62 GiB used) and 137.2 MiB of host memory (418.6 GiB/1007 GiB used)
[1;36m(VllmWorkerProcess pid=19918)[0;0m INFO 08-09 12:32:43 [hpu_model_runner.py:1184] Pre-loading model weights on hpu:0 took 3.139 GiB of device memory (3.145 GiB/94.62 GiB used) and 125 MiB of host memory (418.6 GiB/1007 GiB used)
INFO 08-09 12:32:44 [hpu_model_runner.py:1266] Wrapping in HPU Graph took 0 B of device memory (3.145 GiB/94.62 GiB used) and -512 KiB of host memory (418.6 GiB/1007 GiB used)
[1;36m(VllmWorkerProcess pid=19916)[0;0m INFO 08-09 12:32:44 [hpu_model_runner.py:1266] Wrapping in HPU Graph took 0 B of device memory (3.145 GiB/94.62 GiB used) and -5.656 MiB of host memory (418.6 GiB/1007 GiB used)
[1;36m(VllmWorkerProcess pid=19917)[0;0m INFO 08-09 12:32:44 [hpu_model_runner.py:1266] Wrapping in HPU Graph took 0 B of device memory (3.145 GiB/94.62 GiB used) and -6.148 MiB of host memory (418.6 GiB/1007 GiB used)
[1;36m(VllmWorkerProcess pid=19918)[0;0m INFO 08-09 12:32:44 [hpu_model_runner.py:1266] Wrapping in HPU Graph took 0 B of device memory (3.145 GiB/94.62 GiB used) and -6.148 MiB of host memory (418.6 GiB/1007 GiB used)
INFO 08-09 12:32:44 [hpu_model_runner.py:1270] Compiling took 0 B of device memory (3.145 GiB/94.62 GiB used) and -4 KiB of host memory (418.6 GiB/1007 GiB used)
INFO 08-09 12:32:44 [hpu_model_runner.py:1274] Loading model weights took in total 3.139 GiB of device memory (3.145 GiB/94.62 GiB used) and 160.4 MiB of host memory (418.6 GiB/1007 GiB used)
[1;36m(VllmWorkerProcess pid=19916)[0;0m INFO 08-09 12:32:44 [hpu_model_runner.py:1270] Compiling took 0 B of device memory (3.145 GiB/94.62 GiB used) and -252 KiB of host memory (418.6 GiB/1007 GiB used)
[1;36m(VllmWorkerProcess pid=19917)[0;0m INFO 08-09 12:32:44 [hpu_model_runner.py:1270] Compiling took 0 B of device memory (3.145 GiB/94.62 GiB used) and -252 KiB of host memory (418.6 GiB/1007 GiB used)
[1;36m(VllmWorkerProcess pid=19918)[0;0m INFO 08-09 12:32:44 [hpu_model_runner.py:1270] Compiling took 0 B of device memory (3.145 GiB/94.62 GiB used) and -252 KiB of host memory (418.6 GiB/1007 GiB used)
[1;36m(VllmWorkerProcess pid=19916)[0;0m INFO 08-09 12:32:44 [hpu_model_runner.py:1274] Loading model weights took in total 3.139 GiB of device memory (3.145 GiB/94.62 GiB used) and 162.4 MiB of host memory (418.6 GiB/1007 GiB used)
[1;36m(VllmWorkerProcess pid=19917)[0;0m INFO 08-09 12:32:44 [hpu_model_runner.py:1274] Loading model weights took in total 3.139 GiB of device memory (3.145 GiB/94.62 GiB used) and 164.2 MiB of host memory (418.6 GiB/1007 GiB used)
[1;36m(VllmWorkerProcess pid=19918)[0;0m INFO 08-09 12:32:44 [hpu_model_runner.py:1274] Loading model weights took in total 3.139 GiB of device memory (3.145 GiB/94.62 GiB used) and 161.9 MiB of host memory (418.6 GiB/1007 GiB used)
WARNING 08-09 12:32:45 [common.py:139] Prompt bucket for (1, 4096, 0) was not prepared. Adding new bucket: (1, 4096, 0)
[1;36m(VllmWorkerProcess pid=19918)[0;0m WARNING 08-09 12:32:45 [common.py:139] Prompt bucket for (1, 4096, 0) was not prepared. Adding new bucket: (1, 4096, 0)
[1;36m(VllmWorkerProcess pid=19917)[0;0m WARNING 08-09 12:32:45 [common.py:139] Prompt bucket for (1, 4096, 0) was not prepared. Adding new bucket: (1, 4096, 0)
[1;36m(VllmWorkerProcess pid=19916)[0;0m WARNING 08-09 12:32:45 [common.py:139] Prompt bucket for (1, 4096, 0) was not prepared. Adding new bucket: (1, 4096, 0)
[1;36m(VllmWorkerProcess pid=19917)[0;0m INFO 08-09 12:32:47 [hpu_worker.py:289] Model profiling run took 172 MiB of device memory (3.313 GiB/94.62 GiB used) and 321 MiB of host memory (418.9 GiB/1007 GiB used)
[1;36m(VllmWorkerProcess pid=19917)[0;0m INFO 08-09 12:32:47 [hpu_worker.py:313] Free device memory: 91.31 GiB, 82.18 GiB usable (gpu_memory_utilization=0.9), 8.218 GiB reserved for HPUGraphs (VLLM_GRAPH_RESERVED_MEM=0.1), 73.96 GiB reserved for KV cache
[1;36m(VllmWorkerProcess pid=19918)[0;0m INFO 08-09 12:32:47 [hpu_worker.py:289] Model profiling run took 172 MiB of device memory (3.313 GiB/94.62 GiB used) and 321 MiB of host memory (418.9 GiB/1007 GiB used)
[1;36m(VllmWorkerProcess pid=19918)[0;0m INFO 08-09 12:32:47 [hpu_worker.py:313] Free device memory: 91.31 GiB, 82.18 GiB usable (gpu_memory_utilization=0.9), 8.218 GiB reserved for HPUGraphs (VLLM_GRAPH_RESERVED_MEM=0.1), 73.96 GiB reserved for KV cache
[1;36m(VllmWorkerProcess pid=19916)[0;0m INFO 08-09 12:32:47 [hpu_worker.py:289] Model profiling run took 172 MiB of device memory (3.313 GiB/94.62 GiB used) and 320.3 MiB of host memory (418.9 GiB/1007 GiB used)
[1;36m(VllmWorkerProcess pid=19916)[0;0m INFO 08-09 12:32:47 [hpu_worker.py:313] Free device memory: 91.31 GiB, 82.18 GiB usable (gpu_memory_utilization=0.9), 8.218 GiB reserved for HPUGraphs (VLLM_GRAPH_RESERVED_MEM=0.1), 73.96 GiB reserved for KV cache
INFO 08-09 12:32:47 [hpu_worker.py:289] Model profiling run took 172 MiB of device memory (3.313 GiB/94.62 GiB used) and 336.9 MiB of host memory (418.9 GiB/1007 GiB used)
INFO 08-09 12:32:47 [hpu_worker.py:313] Free device memory: 91.31 GiB, 82.18 GiB usable (gpu_memory_utilization=0.9), 8.218 GiB reserved for HPUGraphs (VLLM_GRAPH_RESERVED_MEM=0.1), 73.96 GiB reserved for KV cache
INFO 08-09 12:32:47 [executor_base.py:113] # hpu blocks: 4733, # CPU blocks: 256
INFO 08-09 12:32:47 [executor_base.py:118] Maximum concurrency for 4096 tokens per request: 147.91x
INFO 08-09 12:32:47 [exponential.py:49] Prompt bucket config (min, step, max_warmup, limit) bs:[1, 2, 256, 9], seq:[128, 128, 4096, 13]
[1;36m(VllmWorkerProcess pid=19917)[0;0m INFO 08-09 12:32:47 [exponential.py:49] Prompt bucket config (min, step, max_warmup, limit) bs:[1, 2, 256, 9], seq:[128, 128, 4096, 13]
[1;36m(VllmWorkerProcess pid=19918)[0;0m INFO 08-09 12:32:47 [exponential.py:49] Prompt bucket config (min, step, max_warmup, limit) bs:[1, 2, 256, 9], seq:[128, 128, 4096, 13]
INFO 08-09 12:32:47 [common.py:117] Generated 38 prompt buckets [bs, query, num_blocks]: [(1, 128, 0), (1, 256, 0), (1, 384, 0), (1, 512, 0), (1, 640, 0), (1, 768, 0), (1, 896, 0), (1, 1024, 0), (1, 1408, 0), (1, 1792, 0), (1, 2304, 0), (1, 3072, 0), (1, 4096, 0), (2, 128, 0), (2, 256, 0), (2, 384, 0), (2, 512, 0), (2, 640, 0), (2, 768, 0), (2, 896, 0), (2, 1024, 0), (2, 1408, 0), (2, 1792, 0), (4, 128, 0), (4, 256, 0), (4, 384, 0), (4, 512, 0), (4, 640, 0), (4, 768, 0), (4, 896, 0), (4, 1024, 0), (8, 128, 0), (8, 256, 0), (8, 384, 0), (8, 512, 0), (16, 128, 0), (16, 256, 0), (32, 128, 0)]
[1;36m(VllmWorkerProcess pid=19917)[0;0m INFO 08-09 12:32:47 [common.py:117] Generated 38 prompt buckets [bs, query, num_blocks]: [(1, 128, 0), (1, 256, 0), (1, 384, 0), (1, 512, 0), (1, 640, 0), (1, 768, 0), (1, 896, 0), (1, 1024, 0), (1, 1408, 0), (1, 1792, 0), (1, 2304, 0), (1, 3072, 0), (1, 4096, 0), (2, 128, 0), (2, 256, 0), (2, 384, 0), (2, 512, 0), (2, 640, 0), (2, 768, 0), (2, 896, 0), (2, 1024, 0), (2, 1408, 0), (2, 1792, 0), (4, 128, 0), (4, 256, 0), (4, 384, 0), (4, 512, 0), (4, 640, 0), (4, 768, 0), (4, 896, 0), (4, 1024, 0), (8, 128, 0), (8, 256, 0), (8, 384, 0), (8, 512, 0), (16, 128, 0), (16, 256, 0), (32, 128, 0)]
INFO 08-09 12:32:47 [exponential.py:79] Decode bucket config (min, step, max_warmup, limit) bs:[1, 2, 256, 9], block:[128, 128, 4733, 14]
[1;36m(VllmWorkerProcess pid=19918)[0;0m INFO 08-09 12:32:47 [common.py:117] Generated 38 prompt buckets [bs, query, num_blocks]: [(1, 128, 0), (1, 256, 0), (1, 384, 0), (1, 512, 0), (1, 640, 0), (1, 768, 0), (1, 896, 0), (1, 1024, 0), (1, 1408, 0), (1, 1792, 0), (1, 2304, 0), (1, 3072, 0), (1, 4096, 0), (2, 128, 0), (2, 256, 0), (2, 384, 0), (2, 512, 0), (2, 640, 0), (2, 768, 0), (2, 896, 0), (2, 1024, 0), (2, 1408, 0), (2, 1792, 0), (4, 128, 0), (4, 256, 0), (4, 384, 0), (4, 512, 0), (4, 640, 0), (4, 768, 0), (4, 896, 0), (4, 1024, 0), (8, 128, 0), (8, 256, 0), (8, 384, 0), (8, 512, 0), (16, 128, 0), (16, 256, 0), (32, 128, 0)]
[1;36m(VllmWorkerProcess pid=19917)[0;0m INFO 08-09 12:32:47 [exponential.py:79] Decode bucket config (min, step, max_warmup, limit) bs:[1, 2, 256, 9], block:[128, 128, 4733, 14]
[1;36m(VllmWorkerProcess pid=19918)[0;0m INFO 08-09 12:32:47 [exponential.py:79] Decode bucket config (min, step, max_warmup, limit) bs:[1, 2, 256, 9], block:[128, 128, 4733, 14]
[1;36m(VllmWorkerProcess pid=19916)[0;0m INFO 08-09 12:32:47 [exponential.py:49] Prompt bucket config (min, step, max_warmup, limit) bs:[1, 2, 256, 9], seq:[128, 128, 4096, 13]
INFO 08-09 12:32:47 [common.py:117] Generated 126 decode buckets [bs, query, num_blocks]: [(1, 1, 128), (1, 1, 256), (1, 1, 384), (1, 1, 512), (1, 1, 640), (1, 1, 768), (1, 1, 896), (1, 1, 1024), (1, 1, 1280), (1, 1, 1664), (1, 1, 2176), (1, 1, 2816), (1, 1, 3712), (1, 1, 4733), (2, 1, 128), (2, 1, 256), (2, 1, 384), (2, 1, 512), (2, 1, 640), (2, 1, 768), (2, 1, 896), (2, 1, 1024), (2, 1, 1280), (2, 1, 1664), (2, 1, 2176), (2, 1, 2816), (2, 1, 3712), (2, 1, 4733), (4, 1, 128), (4, 1, 256), (4, 1, 384), (4, 1, 512), (4, 1, 640), (4, 1, 768), (4, 1, 896), (4, 1, 1024), (4, 1, 1280), (4, 1, 1664), (4, 1, 2176), (4, 1, 2816), (4, 1, 3712), (4, 1, 4733), (8, 1, 128), (8, 1, 256), (8, 1, 384), (8, 1, 512), (8, 1, 640), (8, 1, 768), (8, 1, 896), (8, 1, 1024), (8, 1, 1280), (8, 1, 1664), (8, 1, 2176), (8, 1, 2816), (8, 1, 3712), (8, 1, 4733), (16, 1, 128), (16, 1, 256), (16, 1, 384), (16, 1, 512), (16, 1, 640), (16, 1, 768), (16, 1, 896), (16, 1, 1024), (16, 1, 1280), (16, 1, 1664), (16, 1, 2176), (16, 1, 2816), (16, 1, 3712), (16, 1, 4733), (32, 1, 128), (32, 1, 256), (32, 1, 384), (32, 1, 512), (32, 1, 640), (32, 1, 768), (32, 1, 896), (32, 1, 1024), (32, 1, 1280), (32, 1, 1664), (32, 1, 2176), (32, 1, 2816), (32, 1, 3712), (32, 1, 4733), (64, 1, 128), (64, 1, 256), (64, 1, 384), (64, 1, 512), (64, 1, 640), (64, 1, 768), (64, 1, 896), (64, 1, 1024), (64, 1, 1280), (64, 1, 1664), (64, 1, 2176), (64, 1, 2816), (64, 1, 3712), (64, 1, 4733), (128, 1, 128), (128, 1, 256), (128, 1, 384), (128, 1, 512), (128, 1, 640), (128, 1, 768), (128, 1, 896), (128, 1, 1024), (128, 1, 1280), (128, 1, 1664), (128, 1, 2176), (128, 1, 2816), (128, 1, 3712), (128, 1, 4733), (256, 1, 128), (256, 1, 256), (256, 1, 384), (256, 1, 512), (256, 1, 640), (256, 1, 768), (256, 1, 896), (256, 1, 1024), (256, 1, 1280), (256, 1, 1664), (256, 1, 2176), (256, 1, 2816), (256, 1, 3712), (256, 1, 4733)]
[1;36m(VllmWorkerProcess pid=19917)[0;0m INFO 08-09 12:32:47 [common.py:117] Generated 126 decode buckets [bs, query, num_blocks]: [(1, 1, 128), (1, 1, 256), (1, 1, 384), (1, 1, 512), (1, 1, 640), (1, 1, 768), (1, 1, 896), (1, 1, 1024), (1, 1, 1280), (1, 1, 1664), (1, 1, 2176), (1, 1, 2816), (1, 1, 3712), (1, 1, 4733), (2, 1, 128), (2, 1, 256), (2, 1, 384), (2, 1, 512), (2, 1, 640), (2, 1, 768), (2, 1, 896), (2, 1, 1024), (2, 1, 1280), (2, 1, 1664), (2, 1, 2176), (2, 1, 2816), (2, 1, 3712), (2, 1, 4733), (4, 1, 128), (4, 1, 256), (4, 1, 384), (4, 1, 512), (4, 1, 640), (4, 1, 768), (4, 1, 896), (4, 1, 1024), (4, 1, 1280), (4, 1, 1664), (4, 1, 2176), (4, 1, 2816), (4, 1, 3712), (4, 1, 4733), (8, 1, 128), (8, 1, 256), (8, 1, 384), (8, 1, 512), (8, 1, 640), (8, 1, 768), (8, 1, 896), (8, 1, 1024), (8, 1, 1280), (8, 1, 1664), (8, 1, 2176), (8, 1, 2816), (8, 1, 3712), (8, 1, 4733), (16, 1, 128), (16, 1, 256), (16, 1, 384), (16, 1, 512), (16, 1, 640), (16, 1, 768), (16, 1, 896), (16, 1, 1024), (16, 1, 1280), (16, 1, 1664), (16, 1, 2176), (16, 1, 2816), (16, 1, 3712), (16, 1, 4733), (32, 1, 128), (32, 1, 256), (32, 1, 384), (32, 1, 512), (32, 1, 640), (32, 1, 768), (32, 1, 896), (32, 1, 1024), (32, 1, 1280), (32, 1, 1664), (32, 1, 2176), (32, 1, 2816), (32, 1, 3712), (32, 1, 4733), (64, 1, 128), (64, 1, 256), (64, 1, 384), (64, 1, 512), (64, 1, 640), (64, 1, 768), (64, 1, 896), (64, 1, 1024), (64, 1, 1280), (64, 1, 1664), (64, 1, 2176), (64, 1, 2816), (64, 1, 3712), (64, 1, 4733), (128, 1, 128), (128, 1, 256), (128, 1, 384), (128, 1, 512), (128, 1, 640), (128, 1, 768), (128, 1, 896), (128, 1, 1024), (128, 1, 1280), (128, 1, 1664), (128, 1, 2176), (128, 1, 2816), (128, 1, 3712), (128, 1, 4733), (256, 1, 128), (256, 1, 256), (256, 1, 384), (256, 1, 512), (256, 1, 640), (256, 1, 768), (256, 1, 896), (256, 1, 1024), (256, 1, 1280), (256, 1, 1664), (256, 1, 2176), (256, 1, 2816), (256, 1, 3712), (256, 1, 4733)]
[1;36m(VllmWorkerProcess pid=19918)[0;0m INFO 08-09 12:32:47 [common.py:117] Generated 126 decode buckets [bs, query, num_blocks]: [(1, 1, 128), (1, 1, 256), (1, 1, 384), (1, 1, 512), (1, 1, 640), (1, 1, 768), (1, 1, 896), (1, 1, 1024), (1, 1, 1280), (1, 1, 1664), (1, 1, 2176), (1, 1, 2816), (1, 1, 3712), (1, 1, 4733), (2, 1, 128), (2, 1, 256), (2, 1, 384), (2, 1, 512), (2, 1, 640), (2, 1, 768), (2, 1, 896), (2, 1, 1024), (2, 1, 1280), (2, 1, 1664), (2, 1, 2176), (2, 1, 2816), (2, 1, 3712), (2, 1, 4733), (4, 1, 128), (4, 1, 256), (4, 1, 384), (4, 1, 512), (4, 1, 640), (4, 1, 768), (4, 1, 896), (4, 1, 1024), (4, 1, 1280), (4, 1, 1664), (4, 1, 2176), (4, 1, 2816), (4, 1, 3712), (4, 1, 4733), (8, 1, 128), (8, 1, 256), (8, 1, 384), (8, 1, 512), (8, 1, 640), (8, 1, 768), (8, 1, 896), (8, 1, 1024), (8, 1, 1280), (8, 1, 1664), (8, 1, 2176), (8, 1, 2816), (8, 1, 3712), (8, 1, 4733), (16, 1, 128), (16, 1, 256), (16, 1, 384), (16, 1, 512), (16, 1, 640), (16, 1, 768), (16, 1, 896), (16, 1, 1024), (16, 1, 1280), (16, 1, 1664), (16, 1, 2176), (16, 1, 2816), (16, 1, 3712), (16, 1, 4733), (32, 1, 128), (32, 1, 256), (32, 1, 384), (32, 1, 512), (32, 1, 640), (32, 1, 768), (32, 1, 896), (32, 1, 1024), (32, 1, 1280), (32, 1, 1664), (32, 1, 2176), (32, 1, 2816), (32, 1, 3712), (32, 1, 4733), (64, 1, 128), (64, 1, 256), (64, 1, 384), (64, 1, 512), (64, 1, 640), (64, 1, 768), (64, 1, 896), (64, 1, 1024), (64, 1, 1280), (64, 1, 1664), (64, 1, 2176), (64, 1, 2816), (64, 1, 3712), (64, 1, 4733), (128, 1, 128), (128, 1, 256), (128, 1, 384), (128, 1, 512), (128, 1, 640), (128, 1, 768), (128, 1, 896), (128, 1, 1024), (128, 1, 1280), (128, 1, 1664), (128, 1, 2176), (128, 1, 2816), (128, 1, 3712), (128, 1, 4733), (256, 1, 128), (256, 1, 256), (256, 1, 384), (256, 1, 512), (256, 1, 640), (256, 1, 768), (256, 1, 896), (256, 1, 1024), (256, 1, 1280), (256, 1, 1664), (256, 1, 2176), (256, 1, 2816), (256, 1, 3712), (256, 1, 4733)]
[1;36m(VllmWorkerProcess pid=19916)[0;0m INFO 08-09 12:32:47 [common.py:117] Generated 38 prompt buckets [bs, query, num_blocks]: [(1, 128, 0), (1, 256, 0), (1, 384, 0), (1, 512, 0), (1, 640, 0), (1, 768, 0), (1, 896, 0), (1, 1024, 0), (1, 1408, 0), (1, 1792, 0), (1, 2304, 0), (1, 3072, 0), (1, 4096, 0), (2, 128, 0), (2, 256, 0), (2, 384, 0), (2, 512, 0), (2, 640, 0), (2, 768, 0), (2, 896, 0), (2, 1024, 0), (2, 1408, 0), (2, 1792, 0), (4, 128, 0), (4, 256, 0), (4, 384, 0), (4, 512, 0), (4, 640, 0), (4, 768, 0), (4, 896, 0), (4, 1024, 0), (8, 128, 0), (8, 256, 0), (8, 384, 0), (8, 512, 0), (16, 128, 0), (16, 256, 0), (32, 128, 0)]
[1;36m(VllmWorkerProcess pid=19916)[0;0m INFO 08-09 12:32:47 [exponential.py:79] Decode bucket config (min, step, max_warmup, limit) bs:[1, 2, 256, 9], block:[128, 128, 4733, 14]
[1;36m(VllmWorkerProcess pid=19916)[0;0m INFO 08-09 12:32:47 [common.py:117] Generated 126 decode buckets [bs, query, num_blocks]: [(1, 1, 128), (1, 1, 256), (1, 1, 384), (1, 1, 512), (1, 1, 640), (1, 1, 768), (1, 1, 896), (1, 1, 1024), (1, 1, 1280), (1, 1, 1664), (1, 1, 2176), (1, 1, 2816), (1, 1, 3712), (1, 1, 4733), (2, 1, 128), (2, 1, 256), (2, 1, 384), (2, 1, 512), (2, 1, 640), (2, 1, 768), (2, 1, 896), (2, 1, 1024), (2, 1, 1280), (2, 1, 1664), (2, 1, 2176), (2, 1, 2816), (2, 1, 3712), (2, 1, 4733), (4, 1, 128), (4, 1, 256), (4, 1, 384), (4, 1, 512), (4, 1, 640), (4, 1, 768), (4, 1, 896), (4, 1, 1024), (4, 1, 1280), (4, 1, 1664), (4, 1, 2176), (4, 1, 2816), (4, 1, 3712), (4, 1, 4733), (8, 1, 128), (8, 1, 256), (8, 1, 384), (8, 1, 512), (8, 1, 640), (8, 1, 768), (8, 1, 896), (8, 1, 1024), (8, 1, 1280), (8, 1, 1664), (8, 1, 2176), (8, 1, 2816), (8, 1, 3712), (8, 1, 4733), (16, 1, 128), (16, 1, 256), (16, 1, 384), (16, 1, 512), (16, 1, 640), (16, 1, 768), (16, 1, 896), (16, 1, 1024), (16, 1, 1280), (16, 1, 1664), (16, 1, 2176), (16, 1, 2816), (16, 1, 3712), (16, 1, 4733), (32, 1, 128), (32, 1, 256), (32, 1, 384), (32, 1, 512), (32, 1, 640), (32, 1, 768), (32, 1, 896), (32, 1, 1024), (32, 1, 1280), (32, 1, 1664), (32, 1, 2176), (32, 1, 2816), (32, 1, 3712), (32, 1, 4733), (64, 1, 128), (64, 1, 256), (64, 1, 384), (64, 1, 512), (64, 1, 640), (64, 1, 768), (64, 1, 896), (64, 1, 1024), (64, 1, 1280), (64, 1, 1664), (64, 1, 2176), (64, 1, 2816), (64, 1, 3712), (64, 1, 4733), (128, 1, 128), (128, 1, 256), (128, 1, 384), (128, 1, 512), (128, 1, 640), (128, 1, 768), (128, 1, 896), (128, 1, 1024), (128, 1, 1280), (128, 1, 1664), (128, 1, 2176), (128, 1, 2816), (128, 1, 3712), (128, 1, 4733), (256, 1, 128), (256, 1, 256), (256, 1, 384), (256, 1, 512), (256, 1, 640), (256, 1, 768), (256, 1, 896), (256, 1, 1024), (256, 1, 1280), (256, 1, 1664), (256, 1, 2176), (256, 1, 2816), (256, 1, 3712), (256, 1, 4733)]
[1;36m(VllmWorkerProcess pid=19917)[0;0m INFO 08-09 12:32:49 [hpu_worker.py:350] Initializing cache engine took 73.95 GiB of device memory (77.27 GiB/94.62 GiB used) and 16.02 GiB of host memory (434.9 GiB/1007 GiB used)
[1;36m(VllmWorkerProcess pid=19916)[0;0m INFO 08-09 12:32:49 [hpu_worker.py:350] Initializing cache engine took 73.95 GiB of device memory (77.27 GiB/94.62 GiB used) and 16.02 GiB of host memory (434.9 GiB/1007 GiB used)
[1;36m(VllmWorkerProcess pid=19918)[0;0m INFO 08-09 12:32:49 [hpu_worker.py:350] Initializing cache engine took 73.95 GiB of device memory (77.27 GiB/94.62 GiB used) and 16.02 GiB of host memory (434.9 GiB/1007 GiB used)
INFO 08-09 12:32:49 [hpu_worker.py:350] Initializing cache engine took 73.95 GiB of device memory (77.27 GiB/94.62 GiB used) and 16.01 GiB of host memory (434.9 GiB/1007 GiB used)
INFO 08-09 12:32:49 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][1/38] batch_size:32 query_len:128 num_blocks:0 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19918)[0;0m INFO 08-09 12:32:49 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][1/38] batch_size:32 query_len:128 num_blocks:0 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19917)[0;0m INFO 08-09 12:32:49 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][1/38] batch_size:32 query_len:128 num_blocks:0 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19916)[0;0m INFO 08-09 12:32:49 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][1/38] batch_size:32 query_len:128 num_blocks:0 free_mem:17.36 GiB
INFO 08-09 12:32:51 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][2/38] batch_size:16 query_len:256 num_blocks:0 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19918)[0;0m INFO 08-09 12:32:51 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][2/38] batch_size:16 query_len:256 num_blocks:0 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19917)[0;0m INFO 08-09 12:32:51 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][2/38] batch_size:16 query_len:256 num_blocks:0 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19916)[0;0m INFO 08-09 12:32:51 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][2/38] batch_size:16 query_len:256 num_blocks:0 free_mem:17.36 GiB
INFO 08-09 12:32:53 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][3/38] batch_size:16 query_len:128 num_blocks:0 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19918)[0;0m INFO 08-09 12:32:53 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][3/38] batch_size:16 query_len:128 num_blocks:0 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19916)[0;0m INFO 08-09 12:32:53 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][3/38] batch_size:16 query_len:128 num_blocks:0 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19917)[0;0m INFO 08-09 12:32:53 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][3/38] batch_size:16 query_len:128 num_blocks:0 free_mem:17.36 GiB
INFO 08-09 12:32:55 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][4/38] batch_size:8 query_len:512 num_blocks:0 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19916)[0;0m INFO 08-09 12:32:55 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][4/38] batch_size:8 query_len:512 num_blocks:0 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19918)[0;0m INFO 08-09 12:32:55 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][4/38] batch_size:8 query_len:512 num_blocks:0 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19917)[0;0m INFO 08-09 12:32:55 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][4/38] batch_size:8 query_len:512 num_blocks:0 free_mem:17.36 GiB
INFO 08-09 12:32:57 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][5/38] batch_size:8 query_len:384 num_blocks:0 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19917)[0;0m INFO 08-09 12:32:57 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][5/38] batch_size:8 query_len:384 num_blocks:0 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19916)[0;0m INFO 08-09 12:32:57 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][5/38] batch_size:8 query_len:384 num_blocks:0 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19918)[0;0m INFO 08-09 12:32:57 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][5/38] batch_size:8 query_len:384 num_blocks:0 free_mem:17.36 GiB
INFO 08-09 12:33:00 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][6/38] batch_size:8 query_len:256 num_blocks:0 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19918)[0;0m INFO 08-09 12:33:00 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][6/38] batch_size:8 query_len:256 num_blocks:0 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19916)[0;0m INFO 08-09 12:33:00 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][6/38] batch_size:8 query_len:256 num_blocks:0 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19917)[0;0m INFO 08-09 12:33:00 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][6/38] batch_size:8 query_len:256 num_blocks:0 free_mem:17.36 GiB
INFO 08-09 12:33:02 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][7/38] batch_size:8 query_len:128 num_blocks:0 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19918)[0;0m INFO 08-09 12:33:02 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][7/38] batch_size:8 query_len:128 num_blocks:0 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19916)[0;0m INFO 08-09 12:33:02 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][7/38] batch_size:8 query_len:128 num_blocks:0 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19917)[0;0m INFO 08-09 12:33:02 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][7/38] batch_size:8 query_len:128 num_blocks:0 free_mem:17.36 GiB
INFO 08-09 12:33:04 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][8/38] batch_size:4 query_len:1024 num_blocks:0 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19918)[0;0m INFO 08-09 12:33:04 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][8/38] batch_size:4 query_len:1024 num_blocks:0 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19916)[0;0m INFO 08-09 12:33:04 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][8/38] batch_size:4 query_len:1024 num_blocks:0 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19917)[0;0m INFO 08-09 12:33:04 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][8/38] batch_size:4 query_len:1024 num_blocks:0 free_mem:17.36 GiB
INFO 08-09 12:33:06 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][9/38] batch_size:4 query_len:896 num_blocks:0 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19916)[0;0m INFO 08-09 12:33:06 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][9/38] batch_size:4 query_len:896 num_blocks:0 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19918)[0;0m INFO 08-09 12:33:06 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][9/38] batch_size:4 query_len:896 num_blocks:0 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19917)[0;0m INFO 08-09 12:33:06 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][9/38] batch_size:4 query_len:896 num_blocks:0 free_mem:17.36 GiB
INFO 08-09 12:33:08 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][10/38] batch_size:4 query_len:768 num_blocks:0 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19918)[0;0m INFO 08-09 12:33:08 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][10/38] batch_size:4 query_len:768 num_blocks:0 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19917)[0;0m INFO 08-09 12:33:08 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][10/38] batch_size:4 query_len:768 num_blocks:0 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19916)[0;0m INFO 08-09 12:33:08 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][10/38] batch_size:4 query_len:768 num_blocks:0 free_mem:17.36 GiB
INFO 08-09 12:33:11 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][11/38] batch_size:4 query_len:640 num_blocks:0 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19916)[0;0m INFO 08-09 12:33:11 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][11/38] batch_size:4 query_len:640 num_blocks:0 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19917)[0;0m INFO 08-09 12:33:11 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][11/38] batch_size:4 query_len:640 num_blocks:0 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19918)[0;0m INFO 08-09 12:33:11 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][11/38] batch_size:4 query_len:640 num_blocks:0 free_mem:17.36 GiB
INFO 08-09 12:33:13 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][12/38] batch_size:4 query_len:512 num_blocks:0 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19918)[0;0m INFO 08-09 12:33:13 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][12/38] batch_size:4 query_len:512 num_blocks:0 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19916)[0;0m INFO 08-09 12:33:13 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][12/38] batch_size:4 query_len:512 num_blocks:0 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19917)[0;0m INFO 08-09 12:33:13 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][12/38] batch_size:4 query_len:512 num_blocks:0 free_mem:17.36 GiB
INFO 08-09 12:33:15 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][13/38] batch_size:4 query_len:384 num_blocks:0 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19917)[0;0m INFO 08-09 12:33:15 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][13/38] batch_size:4 query_len:384 num_blocks:0 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19916)[0;0m INFO 08-09 12:33:15 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][13/38] batch_size:4 query_len:384 num_blocks:0 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19918)[0;0m INFO 08-09 12:33:15 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][13/38] batch_size:4 query_len:384 num_blocks:0 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19918)[0;0m INFO 08-09 12:33:18 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][14/38] batch_size:4 query_len:256 num_blocks:0 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19916)[0;0m INFO 08-09 12:33:18 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][14/38] batch_size:4 query_len:256 num_blocks:0 free_mem:17.36 GiB
INFO 08-09 12:33:18 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][14/38] batch_size:4 query_len:256 num_blocks:0 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19917)[0;0m INFO 08-09 12:33:18 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][14/38] batch_size:4 query_len:256 num_blocks:0 free_mem:17.36 GiB
INFO 08-09 12:33:20 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][15/38] batch_size:4 query_len:128 num_blocks:0 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19918)[0;0m INFO 08-09 12:33:20 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][15/38] batch_size:4 query_len:128 num_blocks:0 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19916)[0;0m INFO 08-09 12:33:20 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][15/38] batch_size:4 query_len:128 num_blocks:0 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19917)[0;0m INFO 08-09 12:33:20 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][15/38] batch_size:4 query_len:128 num_blocks:0 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19916)[0;0m INFO 08-09 12:33:22 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][16/38] batch_size:2 query_len:1792 num_blocks:0 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19917)[0;0m INFO 08-09 12:33:22 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][16/38] batch_size:2 query_len:1792 num_blocks:0 free_mem:17.36 GiB
INFO 08-09 12:33:22 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][16/38] batch_size:2 query_len:1792 num_blocks:0 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19918)[0;0m INFO 08-09 12:33:22 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][16/38] batch_size:2 query_len:1792 num_blocks:0 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19916)[0;0m INFO 08-09 12:33:25 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][17/38] batch_size:2 query_len:1408 num_blocks:0 free_mem:17.36 GiB
INFO 08-09 12:33:25 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][17/38] batch_size:2 query_len:1408 num_blocks:0 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19917)[0;0m INFO 08-09 12:33:25 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][17/38] batch_size:2 query_len:1408 num_blocks:0 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19918)[0;0m INFO 08-09 12:33:25 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][17/38] batch_size:2 query_len:1408 num_blocks:0 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19918)[0;0m INFO 08-09 12:33:27 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][18/38] batch_size:2 query_len:1024 num_blocks:0 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19916)[0;0m INFO 08-09 12:33:27 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][18/38] batch_size:2 query_len:1024 num_blocks:0 free_mem:17.36 GiB
INFO 08-09 12:33:27 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][18/38] batch_size:2 query_len:1024 num_blocks:0 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19917)[0;0m INFO 08-09 12:33:27 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][18/38] batch_size:2 query_len:1024 num_blocks:0 free_mem:17.36 GiB
INFO 08-09 12:33:30 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][19/38] batch_size:2 query_len:896 num_blocks:0 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19918)[0;0m INFO 08-09 12:33:30 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][19/38] batch_size:2 query_len:896 num_blocks:0 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19916)[0;0m INFO 08-09 12:33:30 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][19/38] batch_size:2 query_len:896 num_blocks:0 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19917)[0;0m INFO 08-09 12:33:30 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][19/38] batch_size:2 query_len:896 num_blocks:0 free_mem:17.36 GiB
INFO 08-09 12:33:32 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][20/38] batch_size:2 query_len:768 num_blocks:0 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19916)[0;0m INFO 08-09 12:33:32 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][20/38] batch_size:2 query_len:768 num_blocks:0 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19918)[0;0m INFO 08-09 12:33:32 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][20/38] batch_size:2 query_len:768 num_blocks:0 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19917)[0;0m INFO 08-09 12:33:32 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][20/38] batch_size:2 query_len:768 num_blocks:0 free_mem:17.36 GiB
INFO 08-09 12:33:35 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][21/38] batch_size:2 query_len:640 num_blocks:0 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19917)[0;0m INFO 08-09 12:33:35 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][21/38] batch_size:2 query_len:640 num_blocks:0 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19918)[0;0m INFO 08-09 12:33:35 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][21/38] batch_size:2 query_len:640 num_blocks:0 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19916)[0;0m INFO 08-09 12:33:35 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][21/38] batch_size:2 query_len:640 num_blocks:0 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19918)[0;0m INFO 08-09 12:33:37 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][22/38] batch_size:2 query_len:512 num_blocks:0 free_mem:17.36 GiB
INFO 08-09 12:33:37 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][22/38] batch_size:2 query_len:512 num_blocks:0 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19916)[0;0m INFO 08-09 12:33:37 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][22/38] batch_size:2 query_len:512 num_blocks:0 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19917)[0;0m INFO 08-09 12:33:37 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][22/38] batch_size:2 query_len:512 num_blocks:0 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19917)[0;0m INFO 08-09 12:33:40 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][23/38] batch_size:2 query_len:384 num_blocks:0 free_mem:17.36 GiB
INFO 08-09 12:33:40 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][23/38] batch_size:2 query_len:384 num_blocks:0 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19918)[0;0m INFO 08-09 12:33:40 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][23/38] batch_size:2 query_len:384 num_blocks:0 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19916)[0;0m INFO 08-09 12:33:40 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][23/38] batch_size:2 query_len:384 num_blocks:0 free_mem:17.36 GiB
INFO 08-09 12:33:42 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][24/38] batch_size:2 query_len:256 num_blocks:0 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19917)[0;0m INFO 08-09 12:33:42 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][24/38] batch_size:2 query_len:256 num_blocks:0 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19916)[0;0m INFO 08-09 12:33:42 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][24/38] batch_size:2 query_len:256 num_blocks:0 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19918)[0;0m INFO 08-09 12:33:42 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][24/38] batch_size:2 query_len:256 num_blocks:0 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19916)[0;0m INFO 08-09 12:33:45 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][25/38] batch_size:2 query_len:128 num_blocks:0 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19917)[0;0m INFO 08-09 12:33:45 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][25/38] batch_size:2 query_len:128 num_blocks:0 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19918)[0;0m INFO 08-09 12:33:45 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][25/38] batch_size:2 query_len:128 num_blocks:0 free_mem:17.36 GiB
INFO 08-09 12:33:45 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][25/38] batch_size:2 query_len:128 num_blocks:0 free_mem:17.36 GiB
INFO 08-09 12:33:47 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][26/38] batch_size:1 query_len:4096 num_blocks:0 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19917)[0;0m INFO 08-09 12:33:47 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][26/38] batch_size:1 query_len:4096 num_blocks:0 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19918)[0;0m INFO 08-09 12:33:47 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][26/38] batch_size:1 query_len:4096 num_blocks:0 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19916)[0;0m INFO 08-09 12:33:47 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][26/38] batch_size:1 query_len:4096 num_blocks:0 free_mem:17.36 GiB
INFO 08-09 12:33:49 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][27/38] batch_size:1 query_len:3072 num_blocks:0 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19918)[0;0m INFO 08-09 12:33:49 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][27/38] batch_size:1 query_len:3072 num_blocks:0 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19917)[0;0m INFO 08-09 12:33:49 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][27/38] batch_size:1 query_len:3072 num_blocks:0 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19916)[0;0m INFO 08-09 12:33:49 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][27/38] batch_size:1 query_len:3072 num_blocks:0 free_mem:17.36 GiB
INFO 08-09 12:33:52 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][28/38] batch_size:1 query_len:2304 num_blocks:0 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19917)[0;0m INFO 08-09 12:33:52 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][28/38] batch_size:1 query_len:2304 num_blocks:0 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19918)[0;0m INFO 08-09 12:33:52 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][28/38] batch_size:1 query_len:2304 num_blocks:0 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19916)[0;0m INFO 08-09 12:33:52 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][28/38] batch_size:1 query_len:2304 num_blocks:0 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19916)[0;0m INFO 08-09 12:33:55 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][29/38] batch_size:1 query_len:1792 num_blocks:0 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19918)[0;0m INFO 08-09 12:33:55 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][29/38] batch_size:1 query_len:1792 num_blocks:0 free_mem:17.36 GiB
INFO 08-09 12:33:55 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][29/38] batch_size:1 query_len:1792 num_blocks:0 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19917)[0;0m INFO 08-09 12:33:55 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][29/38] batch_size:1 query_len:1792 num_blocks:0 free_mem:17.36 GiB
INFO 08-09 12:33:57 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][30/38] batch_size:1 query_len:1408 num_blocks:0 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19917)[0;0m INFO 08-09 12:33:57 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][30/38] batch_size:1 query_len:1408 num_blocks:0 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19916)[0;0m INFO 08-09 12:33:57 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][30/38] batch_size:1 query_len:1408 num_blocks:0 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19918)[0;0m INFO 08-09 12:33:57 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][30/38] batch_size:1 query_len:1408 num_blocks:0 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19918)[0;0m INFO 08-09 12:34:00 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][31/38] batch_size:1 query_len:1024 num_blocks:0 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19917)[0;0m INFO 08-09 12:34:00 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][31/38] batch_size:1 query_len:1024 num_blocks:0 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19916)[0;0m INFO 08-09 12:34:00 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][31/38] batch_size:1 query_len:1024 num_blocks:0 free_mem:17.36 GiB
INFO 08-09 12:34:00 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][31/38] batch_size:1 query_len:1024 num_blocks:0 free_mem:17.36 GiB
INFO 08-09 12:34:03 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][32/38] batch_size:1 query_len:896 num_blocks:0 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19918)[0;0m INFO 08-09 12:34:03 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][32/38] batch_size:1 query_len:896 num_blocks:0 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19916)[0;0m INFO 08-09 12:34:03 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][32/38] batch_size:1 query_len:896 num_blocks:0 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19917)[0;0m INFO 08-09 12:34:03 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][32/38] batch_size:1 query_len:896 num_blocks:0 free_mem:17.36 GiB
INFO 08-09 12:34:05 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][33/38] batch_size:1 query_len:768 num_blocks:0 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19918)[0;0m INFO 08-09 12:34:05 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][33/38] batch_size:1 query_len:768 num_blocks:0 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19917)[0;0m INFO 08-09 12:34:05 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][33/38] batch_size:1 query_len:768 num_blocks:0 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19916)[0;0m INFO 08-09 12:34:05 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][33/38] batch_size:1 query_len:768 num_blocks:0 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19918)[0;0m INFO 08-09 12:34:08 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][34/38] batch_size:1 query_len:640 num_blocks:0 free_mem:17.36 GiB
INFO 08-09 12:34:08 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][34/38] batch_size:1 query_len:640 num_blocks:0 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19916)[0;0m INFO 08-09 12:34:08 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][34/38] batch_size:1 query_len:640 num_blocks:0 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19917)[0;0m INFO 08-09 12:34:08 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][34/38] batch_size:1 query_len:640 num_blocks:0 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19918)[0;0m INFO 08-09 12:34:11 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][35/38] batch_size:1 query_len:512 num_blocks:0 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19916)[0;0m INFO 08-09 12:34:11 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][35/38] batch_size:1 query_len:512 num_blocks:0 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19917)[0;0m INFO 08-09 12:34:11 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][35/38] batch_size:1 query_len:512 num_blocks:0 free_mem:17.36 GiB
INFO 08-09 12:34:11 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][35/38] batch_size:1 query_len:512 num_blocks:0 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19918)[0;0m INFO 08-09 12:34:13 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][36/38] batch_size:1 query_len:384 num_blocks:0 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19916)[0;0m INFO 08-09 12:34:13 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][36/38] batch_size:1 query_len:384 num_blocks:0 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19917)[0;0m INFO 08-09 12:34:13 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][36/38] batch_size:1 query_len:384 num_blocks:0 free_mem:17.36 GiB
INFO 08-09 12:34:13 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][36/38] batch_size:1 query_len:384 num_blocks:0 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19917)[0;0m INFO 08-09 12:34:16 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][37/38] batch_size:1 query_len:256 num_blocks:0 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19916)[0;0m INFO 08-09 12:34:16 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][37/38] batch_size:1 query_len:256 num_blocks:0 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19918)[0;0m INFO 08-09 12:34:16 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][37/38] batch_size:1 query_len:256 num_blocks:0 free_mem:17.36 GiB
INFO 08-09 12:34:16 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][37/38] batch_size:1 query_len:256 num_blocks:0 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19918)[0;0m INFO 08-09 12:34:19 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][38/38] batch_size:1 query_len:128 num_blocks:0 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19917)[0;0m INFO 08-09 12:34:19 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][38/38] batch_size:1 query_len:128 num_blocks:0 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19916)[0;0m INFO 08-09 12:34:19 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][38/38] batch_size:1 query_len:128 num_blocks:0 free_mem:17.36 GiB
INFO 08-09 12:34:19 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][38/38] batch_size:1 query_len:128 num_blocks:0 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19917)[0;0m INFO 08-09 12:34:21 [hpu_model_runner.py:3029] [Warmup][Graph/decode][1/126] batch_size:256 query_len:1 num_blocks:4733 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19918)[0;0m INFO 08-09 12:34:21 [hpu_model_runner.py:3029] [Warmup][Graph/decode][1/126] batch_size:256 query_len:1 num_blocks:4733 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19916)[0;0m INFO 08-09 12:34:21 [hpu_model_runner.py:3029] [Warmup][Graph/decode][1/126] batch_size:256 query_len:1 num_blocks:4733 free_mem:17.36 GiB
INFO 08-09 12:34:21 [hpu_model_runner.py:3029] [Warmup][Graph/decode][1/126] batch_size:256 query_len:1 num_blocks:4733 free_mem:17.36 GiB
INFO 08-09 12:34:25 [hpu_model_runner.py:3029] [Warmup][Graph/decode][2/126] batch_size:256 query_len:1 num_blocks:3712 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19918)[0;0m INFO 08-09 12:34:25 [hpu_model_runner.py:3029] [Warmup][Graph/decode][2/126] batch_size:256 query_len:1 num_blocks:3712 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19917)[0;0m INFO 08-09 12:34:25 [hpu_model_runner.py:3029] [Warmup][Graph/decode][2/126] batch_size:256 query_len:1 num_blocks:3712 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19916)[0;0m INFO 08-09 12:34:25 [hpu_model_runner.py:3029] [Warmup][Graph/decode][2/126] batch_size:256 query_len:1 num_blocks:3712 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19916)[0;0m INFO 08-09 12:34:28 [hpu_model_runner.py:3029] [Warmup][Graph/decode][3/126] batch_size:256 query_len:1 num_blocks:2816 free_mem:17.36 GiB
INFO 08-09 12:34:28 [hpu_model_runner.py:3029] [Warmup][Graph/decode][3/126] batch_size:256 query_len:1 num_blocks:2816 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19917)[0;0m INFO 08-09 12:34:28 [hpu_model_runner.py:3029] [Warmup][Graph/decode][3/126] batch_size:256 query_len:1 num_blocks:2816 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19918)[0;0m INFO 08-09 12:34:28 [hpu_model_runner.py:3029] [Warmup][Graph/decode][3/126] batch_size:256 query_len:1 num_blocks:2816 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19917)[0;0m INFO 08-09 12:34:30 [hpu_model_runner.py:3029] [Warmup][Graph/decode][4/126] batch_size:256 query_len:1 num_blocks:2176 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19916)[0;0m INFO 08-09 12:34:30 [hpu_model_runner.py:3029] [Warmup][Graph/decode][4/126] batch_size:256 query_len:1 num_blocks:2176 free_mem:17.36 GiB
INFO 08-09 12:34:30 [hpu_model_runner.py:3029] [Warmup][Graph/decode][4/126] batch_size:256 query_len:1 num_blocks:2176 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19918)[0;0m INFO 08-09 12:34:30 [hpu_model_runner.py:3029] [Warmup][Graph/decode][4/126] batch_size:256 query_len:1 num_blocks:2176 free_mem:17.36 GiB
INFO 08-09 12:34:33 [hpu_model_runner.py:3029] [Warmup][Graph/decode][5/126] batch_size:256 query_len:1 num_blocks:1664 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19916)[0;0m INFO 08-09 12:34:33 [hpu_model_runner.py:3029] [Warmup][Graph/decode][5/126] batch_size:256 query_len:1 num_blocks:1664 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19917)[0;0m INFO 08-09 12:34:33 [hpu_model_runner.py:3029] [Warmup][Graph/decode][5/126] batch_size:256 query_len:1 num_blocks:1664 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19918)[0;0m INFO 08-09 12:34:33 [hpu_model_runner.py:3029] [Warmup][Graph/decode][5/126] batch_size:256 query_len:1 num_blocks:1664 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19917)[0;0m INFO 08-09 12:34:36 [hpu_model_runner.py:3029] [Warmup][Graph/decode][6/126] batch_size:256 query_len:1 num_blocks:1280 free_mem:17.36 GiB
INFO 08-09 12:34:36 [hpu_model_runner.py:3029] [Warmup][Graph/decode][6/126] batch_size:256 query_len:1 num_blocks:1280 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19916)[0;0m INFO 08-09 12:34:36 [hpu_model_runner.py:3029] [Warmup][Graph/decode][6/126] batch_size:256 query_len:1 num_blocks:1280 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19918)[0;0m INFO 08-09 12:34:36 [hpu_model_runner.py:3029] [Warmup][Graph/decode][6/126] batch_size:256 query_len:1 num_blocks:1280 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19918)[0;0m INFO 08-09 12:34:39 [hpu_model_runner.py:3029] [Warmup][Graph/decode][7/126] batch_size:256 query_len:1 num_blocks:1024 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19916)[0;0m INFO 08-09 12:34:39 [hpu_model_runner.py:3029] [Warmup][Graph/decode][7/126] batch_size:256 query_len:1 num_blocks:1024 free_mem:17.36 GiB
INFO 08-09 12:34:39 [hpu_model_runner.py:3029] [Warmup][Graph/decode][7/126] batch_size:256 query_len:1 num_blocks:1024 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19917)[0;0m INFO 08-09 12:34:39 [hpu_model_runner.py:3029] [Warmup][Graph/decode][7/126] batch_size:256 query_len:1 num_blocks:1024 free_mem:17.36 GiB
INFO 08-09 12:34:42 [hpu_model_runner.py:3029] [Warmup][Graph/decode][8/126] batch_size:256 query_len:1 num_blocks:896 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19917)[0;0m INFO 08-09 12:34:42 [hpu_model_runner.py:3029] [Warmup][Graph/decode][8/126] batch_size:256 query_len:1 num_blocks:896 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19918)[0;0m INFO 08-09 12:34:42 [hpu_model_runner.py:3029] [Warmup][Graph/decode][8/126] batch_size:256 query_len:1 num_blocks:896 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19916)[0;0m INFO 08-09 12:34:42 [hpu_model_runner.py:3029] [Warmup][Graph/decode][8/126] batch_size:256 query_len:1 num_blocks:896 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19918)[0;0m INFO 08-09 12:34:45 [hpu_model_runner.py:3029] [Warmup][Graph/decode][9/126] batch_size:256 query_len:1 num_blocks:768 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19917)[0;0m INFO 08-09 12:34:45 [hpu_model_runner.py:3029] [Warmup][Graph/decode][9/126] batch_size:256 query_len:1 num_blocks:768 free_mem:17.36 GiB
INFO 08-09 12:34:45 [hpu_model_runner.py:3029] [Warmup][Graph/decode][9/126] batch_size:256 query_len:1 num_blocks:768 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19916)[0;0m INFO 08-09 12:34:45 [hpu_model_runner.py:3029] [Warmup][Graph/decode][9/126] batch_size:256 query_len:1 num_blocks:768 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19917)[0;0m INFO 08-09 12:34:48 [hpu_model_runner.py:3029] [Warmup][Graph/decode][10/126] batch_size:256 query_len:1 num_blocks:640 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19916)[0;0m INFO 08-09 12:34:48 [hpu_model_runner.py:3029] [Warmup][Graph/decode][10/126] batch_size:256 query_len:1 num_blocks:640 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19918)[0;0m INFO 08-09 12:34:48 [hpu_model_runner.py:3029] [Warmup][Graph/decode][10/126] batch_size:256 query_len:1 num_blocks:640 free_mem:17.36 GiB
INFO 08-09 12:34:48 [hpu_model_runner.py:3029] [Warmup][Graph/decode][10/126] batch_size:256 query_len:1 num_blocks:640 free_mem:17.36 GiB
INFO 08-09 12:34:51 [hpu_model_runner.py:3029] [Warmup][Graph/decode][11/126] batch_size:256 query_len:1 num_blocks:512 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19918)[0;0m INFO 08-09 12:34:51 [hpu_model_runner.py:3029] [Warmup][Graph/decode][11/126] batch_size:256 query_len:1 num_blocks:512 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19917)[0;0m INFO 08-09 12:34:51 [hpu_model_runner.py:3029] [Warmup][Graph/decode][11/126] batch_size:256 query_len:1 num_blocks:512 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19916)[0;0m INFO 08-09 12:34:51 [hpu_model_runner.py:3029] [Warmup][Graph/decode][11/126] batch_size:256 query_len:1 num_blocks:512 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19918)[0;0m INFO 08-09 12:34:54 [hpu_model_runner.py:3029] [Warmup][Graph/decode][12/126] batch_size:256 query_len:1 num_blocks:384 free_mem:17.36 GiB
INFO 08-09 12:34:54 [hpu_model_runner.py:3029] [Warmup][Graph/decode][12/126] batch_size:256 query_len:1 num_blocks:384 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19916)[0;0m INFO 08-09 12:34:54 [hpu_model_runner.py:3029] [Warmup][Graph/decode][12/126] batch_size:256 query_len:1 num_blocks:384 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19917)[0;0m INFO 08-09 12:34:54 [hpu_model_runner.py:3029] [Warmup][Graph/decode][12/126] batch_size:256 query_len:1 num_blocks:384 free_mem:17.36 GiB
INFO 08-09 12:34:57 [hpu_model_runner.py:3029] [Warmup][Graph/decode][13/126] batch_size:256 query_len:1 num_blocks:256 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19916)[0;0m INFO 08-09 12:34:57 [hpu_model_runner.py:3029] [Warmup][Graph/decode][13/126] batch_size:256 query_len:1 num_blocks:256 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19917)[0;0m INFO 08-09 12:34:57 [hpu_model_runner.py:3029] [Warmup][Graph/decode][13/126] batch_size:256 query_len:1 num_blocks:256 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19918)[0;0m INFO 08-09 12:34:57 [hpu_model_runner.py:3029] [Warmup][Graph/decode][13/126] batch_size:256 query_len:1 num_blocks:256 free_mem:17.36 GiB
INFO 08-09 12:35:00 [hpu_model_runner.py:3029] [Warmup][Graph/decode][14/126] batch_size:256 query_len:1 num_blocks:128 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19918)[0;0m INFO 08-09 12:35:00 [hpu_model_runner.py:3029] [Warmup][Graph/decode][14/126] batch_size:256 query_len:1 num_blocks:128 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19917)[0;0m INFO 08-09 12:35:00 [hpu_model_runner.py:3029] [Warmup][Graph/decode][14/126] batch_size:256 query_len:1 num_blocks:128 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19916)[0;0m INFO 08-09 12:35:00 [hpu_model_runner.py:3029] [Warmup][Graph/decode][14/126] batch_size:256 query_len:1 num_blocks:128 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19918)[0;0m INFO 08-09 12:35:03 [hpu_model_runner.py:3029] [Warmup][Graph/decode][15/126] batch_size:128 query_len:1 num_blocks:4733 free_mem:17.36 GiB
INFO 08-09 12:35:03 [hpu_model_runner.py:3029] [Warmup][Graph/decode][15/126] batch_size:128 query_len:1 num_blocks:4733 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19917)[0;0m INFO 08-09 12:35:03 [hpu_model_runner.py:3029] [Warmup][Graph/decode][15/126] batch_size:128 query_len:1 num_blocks:4733 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19916)[0;0m INFO 08-09 12:35:03 [hpu_model_runner.py:3029] [Warmup][Graph/decode][15/126] batch_size:128 query_len:1 num_blocks:4733 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19918)[0;0m INFO 08-09 12:35:07 [hpu_model_runner.py:3029] [Warmup][Graph/decode][16/126] batch_size:128 query_len:1 num_blocks:3712 free_mem:17.36 GiB
INFO 08-09 12:35:07 [hpu_model_runner.py:3029] [Warmup][Graph/decode][16/126] batch_size:128 query_len:1 num_blocks:3712 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19917)[0;0m INFO 08-09 12:35:07 [hpu_model_runner.py:3029] [Warmup][Graph/decode][16/126] batch_size:128 query_len:1 num_blocks:3712 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19916)[0;0m INFO 08-09 12:35:07 [hpu_model_runner.py:3029] [Warmup][Graph/decode][16/126] batch_size:128 query_len:1 num_blocks:3712 free_mem:17.36 GiB
INFO 08-09 12:35:10 [hpu_model_runner.py:3029] [Warmup][Graph/decode][17/126] batch_size:128 query_len:1 num_blocks:2816 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19918)[0;0m INFO 08-09 12:35:10 [hpu_model_runner.py:3029] [Warmup][Graph/decode][17/126] batch_size:128 query_len:1 num_blocks:2816 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19916)[0;0m INFO 08-09 12:35:10 [hpu_model_runner.py:3029] [Warmup][Graph/decode][17/126] batch_size:128 query_len:1 num_blocks:2816 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19917)[0;0m INFO 08-09 12:35:10 [hpu_model_runner.py:3029] [Warmup][Graph/decode][17/126] batch_size:128 query_len:1 num_blocks:2816 free_mem:17.36 GiB
INFO 08-09 12:35:13 [hpu_model_runner.py:3029] [Warmup][Graph/decode][18/126] batch_size:128 query_len:1 num_blocks:2176 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19918)[0;0m INFO 08-09 12:35:13 [hpu_model_runner.py:3029] [Warmup][Graph/decode][18/126] batch_size:128 query_len:1 num_blocks:2176 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19916)[0;0m INFO 08-09 12:35:13 [hpu_model_runner.py:3029] [Warmup][Graph/decode][18/126] batch_size:128 query_len:1 num_blocks:2176 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19917)[0;0m INFO 08-09 12:35:13 [hpu_model_runner.py:3029] [Warmup][Graph/decode][18/126] batch_size:128 query_len:1 num_blocks:2176 free_mem:17.36 GiB
INFO 08-09 12:35:17 [hpu_model_runner.py:3029] [Warmup][Graph/decode][19/126] batch_size:128 query_len:1 num_blocks:1664 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19916)[0;0m INFO 08-09 12:35:17 [hpu_model_runner.py:3029] [Warmup][Graph/decode][19/126] batch_size:128 query_len:1 num_blocks:1664 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19917)[0;0m INFO 08-09 12:35:17 [hpu_model_runner.py:3029] [Warmup][Graph/decode][19/126] batch_size:128 query_len:1 num_blocks:1664 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19918)[0;0m INFO 08-09 12:35:17 [hpu_model_runner.py:3029] [Warmup][Graph/decode][19/126] batch_size:128 query_len:1 num_blocks:1664 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19917)[0;0m INFO 08-09 12:35:20 [hpu_model_runner.py:3029] [Warmup][Graph/decode][20/126] batch_size:128 query_len:1 num_blocks:1280 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19916)[0;0m INFO 08-09 12:35:20 [hpu_model_runner.py:3029] [Warmup][Graph/decode][20/126] batch_size:128 query_len:1 num_blocks:1280 free_mem:17.36 GiB
INFO 08-09 12:35:20 [hpu_model_runner.py:3029] [Warmup][Graph/decode][20/126] batch_size:128 query_len:1 num_blocks:1280 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19918)[0;0m INFO 08-09 12:35:20 [hpu_model_runner.py:3029] [Warmup][Graph/decode][20/126] batch_size:128 query_len:1 num_blocks:1280 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19916)[0;0m INFO 08-09 12:35:23 [hpu_model_runner.py:3029] [Warmup][Graph/decode][21/126] batch_size:128 query_len:1 num_blocks:1024 free_mem:17.36 GiB
INFO 08-09 12:35:23 [hpu_model_runner.py:3029] [Warmup][Graph/decode][21/126] batch_size:128 query_len:1 num_blocks:1024 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19918)[0;0m INFO 08-09 12:35:23 [hpu_model_runner.py:3029] [Warmup][Graph/decode][21/126] batch_size:128 query_len:1 num_blocks:1024 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19917)[0;0m INFO 08-09 12:35:23 [hpu_model_runner.py:3029] [Warmup][Graph/decode][21/126] batch_size:128 query_len:1 num_blocks:1024 free_mem:17.36 GiB
INFO 08-09 12:35:26 [hpu_model_runner.py:3029] [Warmup][Graph/decode][22/126] batch_size:128 query_len:1 num_blocks:896 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19917)[0;0m INFO 08-09 12:35:26 [hpu_model_runner.py:3029] [Warmup][Graph/decode][22/126] batch_size:128 query_len:1 num_blocks:896 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19916)[0;0m INFO 08-09 12:35:26 [hpu_model_runner.py:3029] [Warmup][Graph/decode][22/126] batch_size:128 query_len:1 num_blocks:896 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19918)[0;0m INFO 08-09 12:35:26 [hpu_model_runner.py:3029] [Warmup][Graph/decode][22/126] batch_size:128 query_len:1 num_blocks:896 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19917)[0;0m INFO 08-09 12:35:29 [hpu_model_runner.py:3029] [Warmup][Graph/decode][23/126] batch_size:128 query_len:1 num_blocks:768 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19916)[0;0m INFO 08-09 12:35:29 [hpu_model_runner.py:3029] [Warmup][Graph/decode][23/126] batch_size:128 query_len:1 num_blocks:768 free_mem:17.36 GiB
INFO 08-09 12:35:29 [hpu_model_runner.py:3029] [Warmup][Graph/decode][23/126] batch_size:128 query_len:1 num_blocks:768 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19918)[0;0m INFO 08-09 12:35:29 [hpu_model_runner.py:3029] [Warmup][Graph/decode][23/126] batch_size:128 query_len:1 num_blocks:768 free_mem:17.36 GiB
INFO 08-09 12:35:33 [hpu_model_runner.py:3029] [Warmup][Graph/decode][24/126] batch_size:128 query_len:1 num_blocks:640 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19918)[0;0m INFO 08-09 12:35:33 [hpu_model_runner.py:3029] [Warmup][Graph/decode][24/126] batch_size:128 query_len:1 num_blocks:640 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19917)[0;0m INFO 08-09 12:35:33 [hpu_model_runner.py:3029] [Warmup][Graph/decode][24/126] batch_size:128 query_len:1 num_blocks:640 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19916)[0;0m INFO 08-09 12:35:33 [hpu_model_runner.py:3029] [Warmup][Graph/decode][24/126] batch_size:128 query_len:1 num_blocks:640 free_mem:17.36 GiB
INFO 08-09 12:35:36 [hpu_model_runner.py:3029] [Warmup][Graph/decode][25/126] batch_size:128 query_len:1 num_blocks:512 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19918)[0;0m INFO 08-09 12:35:36 [hpu_model_runner.py:3029] [Warmup][Graph/decode][25/126] batch_size:128 query_len:1 num_blocks:512 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19917)[0;0m INFO 08-09 12:35:36 [hpu_model_runner.py:3029] [Warmup][Graph/decode][25/126] batch_size:128 query_len:1 num_blocks:512 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19916)[0;0m INFO 08-09 12:35:36 [hpu_model_runner.py:3029] [Warmup][Graph/decode][25/126] batch_size:128 query_len:1 num_blocks:512 free_mem:17.36 GiB
INFO 08-09 12:35:39 [hpu_model_runner.py:3029] [Warmup][Graph/decode][26/126] batch_size:128 query_len:1 num_blocks:384 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19916)[0;0m INFO 08-09 12:35:39 [hpu_model_runner.py:3029] [Warmup][Graph/decode][26/126] batch_size:128 query_len:1 num_blocks:384 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19917)[0;0m INFO 08-09 12:35:39 [hpu_model_runner.py:3029] [Warmup][Graph/decode][26/126] batch_size:128 query_len:1 num_blocks:384 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19918)[0;0m INFO 08-09 12:35:39 [hpu_model_runner.py:3029] [Warmup][Graph/decode][26/126] batch_size:128 query_len:1 num_blocks:384 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19917)[0;0m INFO 08-09 12:35:43 [hpu_model_runner.py:3029] [Warmup][Graph/decode][27/126] batch_size:128 query_len:1 num_blocks:256 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19918)[0;0m INFO 08-09 12:35:43 [hpu_model_runner.py:3029] [Warmup][Graph/decode][27/126] batch_size:128 query_len:1 num_blocks:256 free_mem:17.36 GiB
INFO 08-09 12:35:43 [hpu_model_runner.py:3029] [Warmup][Graph/decode][27/126] batch_size:128 query_len:1 num_blocks:256 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19916)[0;0m INFO 08-09 12:35:43 [hpu_model_runner.py:3029] [Warmup][Graph/decode][27/126] batch_size:128 query_len:1 num_blocks:256 free_mem:17.36 GiB
INFO 08-09 12:35:46 [hpu_model_runner.py:3029] [Warmup][Graph/decode][28/126] batch_size:128 query_len:1 num_blocks:128 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19918)[0;0m INFO 08-09 12:35:46 [hpu_model_runner.py:3029] [Warmup][Graph/decode][28/126] batch_size:128 query_len:1 num_blocks:128 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19917)[0;0m INFO 08-09 12:35:46 [hpu_model_runner.py:3029] [Warmup][Graph/decode][28/126] batch_size:128 query_len:1 num_blocks:128 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19916)[0;0m INFO 08-09 12:35:46 [hpu_model_runner.py:3029] [Warmup][Graph/decode][28/126] batch_size:128 query_len:1 num_blocks:128 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19916)[0;0m INFO 08-09 12:35:49 [hpu_model_runner.py:3029] [Warmup][Graph/decode][29/126] batch_size:64 query_len:1 num_blocks:4733 free_mem:17.36 GiB
INFO 08-09 12:35:49 [hpu_model_runner.py:3029] [Warmup][Graph/decode][29/126] batch_size:64 query_len:1 num_blocks:4733 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19918)[0;0m INFO 08-09 12:35:49 [hpu_model_runner.py:3029] [Warmup][Graph/decode][29/126] batch_size:64 query_len:1 num_blocks:4733 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19917)[0;0m INFO 08-09 12:35:49 [hpu_model_runner.py:3029] [Warmup][Graph/decode][29/126] batch_size:64 query_len:1 num_blocks:4733 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19918)[0;0m INFO 08-09 12:35:53 [hpu_model_runner.py:3029] [Warmup][Graph/decode][30/126] batch_size:64 query_len:1 num_blocks:3712 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19917)[0;0m INFO 08-09 12:35:53 [hpu_model_runner.py:3029] [Warmup][Graph/decode][30/126] batch_size:64 query_len:1 num_blocks:3712 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19916)[0;0m INFO 08-09 12:35:53 [hpu_model_runner.py:3029] [Warmup][Graph/decode][30/126] batch_size:64 query_len:1 num_blocks:3712 free_mem:17.36 GiB
INFO 08-09 12:35:53 [hpu_model_runner.py:3029] [Warmup][Graph/decode][30/126] batch_size:64 query_len:1 num_blocks:3712 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19917)[0;0m INFO 08-09 12:35:56 [hpu_model_runner.py:3029] [Warmup][Graph/decode][31/126] batch_size:64 query_len:1 num_blocks:2816 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19916)[0;0m INFO 08-09 12:35:56 [hpu_model_runner.py:3029] [Warmup][Graph/decode][31/126] batch_size:64 query_len:1 num_blocks:2816 free_mem:17.36 GiB
INFO 08-09 12:35:56 [hpu_model_runner.py:3029] [Warmup][Graph/decode][31/126] batch_size:64 query_len:1 num_blocks:2816 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19918)[0;0m INFO 08-09 12:35:56 [hpu_model_runner.py:3029] [Warmup][Graph/decode][31/126] batch_size:64 query_len:1 num_blocks:2816 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19918)[0;0m INFO 08-09 12:36:00 [hpu_model_runner.py:3029] [Warmup][Graph/decode][32/126] batch_size:64 query_len:1 num_blocks:2176 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19916)[0;0m INFO 08-09 12:36:00 [hpu_model_runner.py:3029] [Warmup][Graph/decode][32/126] batch_size:64 query_len:1 num_blocks:2176 free_mem:17.36 GiB
INFO 08-09 12:36:00 [hpu_model_runner.py:3029] [Warmup][Graph/decode][32/126] batch_size:64 query_len:1 num_blocks:2176 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19917)[0;0m INFO 08-09 12:36:00 [hpu_model_runner.py:3029] [Warmup][Graph/decode][32/126] batch_size:64 query_len:1 num_blocks:2176 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19917)[0;0m INFO 08-09 12:36:03 [hpu_model_runner.py:3029] [Warmup][Graph/decode][33/126] batch_size:64 query_len:1 num_blocks:1664 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19916)[0;0m INFO 08-09 12:36:03 [hpu_model_runner.py:3029] [Warmup][Graph/decode][33/126] batch_size:64 query_len:1 num_blocks:1664 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19918)[0;0m INFO 08-09 12:36:03 [hpu_model_runner.py:3029] [Warmup][Graph/decode][33/126] batch_size:64 query_len:1 num_blocks:1664 free_mem:17.36 GiB
INFO 08-09 12:36:03 [hpu_model_runner.py:3029] [Warmup][Graph/decode][33/126] batch_size:64 query_len:1 num_blocks:1664 free_mem:17.36 GiB
INFO 08-09 12:36:07 [hpu_model_runner.py:3029] [Warmup][Graph/decode][34/126] batch_size:64 query_len:1 num_blocks:1280 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19918)[0;0m INFO 08-09 12:36:07 [hpu_model_runner.py:3029] [Warmup][Graph/decode][34/126] batch_size:64 query_len:1 num_blocks:1280 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19916)[0;0m INFO 08-09 12:36:07 [hpu_model_runner.py:3029] [Warmup][Graph/decode][34/126] batch_size:64 query_len:1 num_blocks:1280 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19917)[0;0m INFO 08-09 12:36:07 [hpu_model_runner.py:3029] [Warmup][Graph/decode][34/126] batch_size:64 query_len:1 num_blocks:1280 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19917)[0;0m INFO 08-09 12:36:10 [hpu_model_runner.py:3029] [Warmup][Graph/decode][35/126] batch_size:64 query_len:1 num_blocks:1024 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19916)[0;0m INFO 08-09 12:36:10 [hpu_model_runner.py:3029] [Warmup][Graph/decode][35/126] batch_size:64 query_len:1 num_blocks:1024 free_mem:17.36 GiB
INFO 08-09 12:36:10 [hpu_model_runner.py:3029] [Warmup][Graph/decode][35/126] batch_size:64 query_len:1 num_blocks:1024 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19918)[0;0m INFO 08-09 12:36:10 [hpu_model_runner.py:3029] [Warmup][Graph/decode][35/126] batch_size:64 query_len:1 num_blocks:1024 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19918)[0;0m INFO 08-09 12:36:14 [hpu_model_runner.py:3029] [Warmup][Graph/decode][36/126] batch_size:64 query_len:1 num_blocks:896 free_mem:17.36 GiB
INFO 08-09 12:36:14 [hpu_model_runner.py:3029] [Warmup][Graph/decode][36/126] batch_size:64 query_len:1 num_blocks:896 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19917)[0;0m INFO 08-09 12:36:14 [hpu_model_runner.py:3029] [Warmup][Graph/decode][36/126] batch_size:64 query_len:1 num_blocks:896 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19916)[0;0m INFO 08-09 12:36:14 [hpu_model_runner.py:3029] [Warmup][Graph/decode][36/126] batch_size:64 query_len:1 num_blocks:896 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19917)[0;0m INFO 08-09 12:36:17 [hpu_model_runner.py:3029] [Warmup][Graph/decode][37/126] batch_size:64 query_len:1 num_blocks:768 free_mem:17.36 GiB
INFO 08-09 12:36:17 [hpu_model_runner.py:3029] [Warmup][Graph/decode][37/126] batch_size:64 query_len:1 num_blocks:768 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19918)[0;0m INFO 08-09 12:36:17 [hpu_model_runner.py:3029] [Warmup][Graph/decode][37/126] batch_size:64 query_len:1 num_blocks:768 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19916)[0;0m INFO 08-09 12:36:17 [hpu_model_runner.py:3029] [Warmup][Graph/decode][37/126] batch_size:64 query_len:1 num_blocks:768 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19917)[0;0m INFO 08-09 12:36:21 [hpu_model_runner.py:3029] [Warmup][Graph/decode][38/126] batch_size:64 query_len:1 num_blocks:640 free_mem:17.36 GiB
INFO 08-09 12:36:21 [hpu_model_runner.py:3029] [Warmup][Graph/decode][38/126] batch_size:64 query_len:1 num_blocks:640 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19918)[0;0m INFO 08-09 12:36:21 [hpu_model_runner.py:3029] [Warmup][Graph/decode][38/126] batch_size:64 query_len:1 num_blocks:640 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19916)[0;0m INFO 08-09 12:36:21 [hpu_model_runner.py:3029] [Warmup][Graph/decode][38/126] batch_size:64 query_len:1 num_blocks:640 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19917)[0;0m INFO 08-09 12:36:24 [hpu_model_runner.py:3029] [Warmup][Graph/decode][39/126] batch_size:64 query_len:1 num_blocks:512 free_mem:17.36 GiB
INFO 08-09 12:36:24 [hpu_model_runner.py:3029] [Warmup][Graph/decode][39/126] batch_size:64 query_len:1 num_blocks:512 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19916)[0;0m INFO 08-09 12:36:24 [hpu_model_runner.py:3029] [Warmup][Graph/decode][39/126] batch_size:64 query_len:1 num_blocks:512 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19918)[0;0m INFO 08-09 12:36:24 [hpu_model_runner.py:3029] [Warmup][Graph/decode][39/126] batch_size:64 query_len:1 num_blocks:512 free_mem:17.36 GiB
INFO 08-09 12:36:28 [hpu_model_runner.py:3029] [Warmup][Graph/decode][40/126] batch_size:64 query_len:1 num_blocks:384 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19917)[0;0m INFO 08-09 12:36:28 [hpu_model_runner.py:3029] [Warmup][Graph/decode][40/126] batch_size:64 query_len:1 num_blocks:384 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19918)[0;0m INFO 08-09 12:36:28 [hpu_model_runner.py:3029] [Warmup][Graph/decode][40/126] batch_size:64 query_len:1 num_blocks:384 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19916)[0;0m INFO 08-09 12:36:28 [hpu_model_runner.py:3029] [Warmup][Graph/decode][40/126] batch_size:64 query_len:1 num_blocks:384 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19918)[0;0m INFO 08-09 12:36:31 [hpu_model_runner.py:3029] [Warmup][Graph/decode][41/126] batch_size:64 query_len:1 num_blocks:256 free_mem:17.36 GiB
INFO 08-09 12:36:31 [hpu_model_runner.py:3029] [Warmup][Graph/decode][41/126] batch_size:64 query_len:1 num_blocks:256 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19916)[0;0m INFO 08-09 12:36:31 [hpu_model_runner.py:3029] [Warmup][Graph/decode][41/126] batch_size:64 query_len:1 num_blocks:256 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19917)[0;0m INFO 08-09 12:36:31 [hpu_model_runner.py:3029] [Warmup][Graph/decode][41/126] batch_size:64 query_len:1 num_blocks:256 free_mem:17.36 GiB
INFO 08-09 12:36:35 [hpu_model_runner.py:3029] [Warmup][Graph/decode][42/126] batch_size:64 query_len:1 num_blocks:128 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19918)[0;0m INFO 08-09 12:36:35 [hpu_model_runner.py:3029] [Warmup][Graph/decode][42/126] batch_size:64 query_len:1 num_blocks:128 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19916)[0;0m INFO 08-09 12:36:35 [hpu_model_runner.py:3029] [Warmup][Graph/decode][42/126] batch_size:64 query_len:1 num_blocks:128 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19917)[0;0m INFO 08-09 12:36:35 [hpu_model_runner.py:3029] [Warmup][Graph/decode][42/126] batch_size:64 query_len:1 num_blocks:128 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19918)[0;0m INFO 08-09 12:36:39 [hpu_model_runner.py:3029] [Warmup][Graph/decode][43/126] batch_size:32 query_len:1 num_blocks:4733 free_mem:17.36 GiB
INFO 08-09 12:36:39 [hpu_model_runner.py:3029] [Warmup][Graph/decode][43/126] batch_size:32 query_len:1 num_blocks:4733 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19917)[0;0m INFO 08-09 12:36:39 [hpu_model_runner.py:3029] [Warmup][Graph/decode][43/126] batch_size:32 query_len:1 num_blocks:4733 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19916)[0;0m INFO 08-09 12:36:39 [hpu_model_runner.py:3029] [Warmup][Graph/decode][43/126] batch_size:32 query_len:1 num_blocks:4733 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19918)[0;0m INFO 08-09 12:36:43 [hpu_model_runner.py:3029] [Warmup][Graph/decode][44/126] batch_size:32 query_len:1 num_blocks:3712 free_mem:17.36 GiB
INFO 08-09 12:36:43 [hpu_model_runner.py:3029] [Warmup][Graph/decode][44/126] batch_size:32 query_len:1 num_blocks:3712 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19917)[0;0m INFO 08-09 12:36:43 [hpu_model_runner.py:3029] [Warmup][Graph/decode][44/126] batch_size:32 query_len:1 num_blocks:3712 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19916)[0;0m INFO 08-09 12:36:43 [hpu_model_runner.py:3029] [Warmup][Graph/decode][44/126] batch_size:32 query_len:1 num_blocks:3712 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19918)[0;0m INFO 08-09 12:36:47 [hpu_model_runner.py:3029] [Warmup][Graph/decode][45/126] batch_size:32 query_len:1 num_blocks:2816 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19917)[0;0m INFO 08-09 12:36:47 [hpu_model_runner.py:3029] [Warmup][Graph/decode][45/126] batch_size:32 query_len:1 num_blocks:2816 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19916)[0;0m INFO 08-09 12:36:47 [hpu_model_runner.py:3029] [Warmup][Graph/decode][45/126] batch_size:32 query_len:1 num_blocks:2816 free_mem:17.36 GiB
INFO 08-09 12:36:47 [hpu_model_runner.py:3029] [Warmup][Graph/decode][45/126] batch_size:32 query_len:1 num_blocks:2816 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19918)[0;0m INFO 08-09 12:36:50 [hpu_model_runner.py:3029] [Warmup][Graph/decode][46/126] batch_size:32 query_len:1 num_blocks:2176 free_mem:17.36 GiB
INFO 08-09 12:36:50 [hpu_model_runner.py:3029] [Warmup][Graph/decode][46/126] batch_size:32 query_len:1 num_blocks:2176 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19917)[0;0m INFO 08-09 12:36:50 [hpu_model_runner.py:3029] [Warmup][Graph/decode][46/126] batch_size:32 query_len:1 num_blocks:2176 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19916)[0;0m INFO 08-09 12:36:50 [hpu_model_runner.py:3029] [Warmup][Graph/decode][46/126] batch_size:32 query_len:1 num_blocks:2176 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19918)[0;0m INFO 08-09 12:36:54 [hpu_model_runner.py:3029] [Warmup][Graph/decode][47/126] batch_size:32 query_len:1 num_blocks:1664 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19917)[0;0m INFO 08-09 12:36:54 [hpu_model_runner.py:3029] [Warmup][Graph/decode][47/126] batch_size:32 query_len:1 num_blocks:1664 free_mem:17.36 GiB
INFO 08-09 12:36:54 [hpu_model_runner.py:3029] [Warmup][Graph/decode][47/126] batch_size:32 query_len:1 num_blocks:1664 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19916)[0;0m INFO 08-09 12:36:54 [hpu_model_runner.py:3029] [Warmup][Graph/decode][47/126] batch_size:32 query_len:1 num_blocks:1664 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19918)[0;0m INFO 08-09 12:36:58 [hpu_model_runner.py:3029] [Warmup][Graph/decode][48/126] batch_size:32 query_len:1 num_blocks:1280 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19917)[0;0m INFO 08-09 12:36:58 [hpu_model_runner.py:3029] [Warmup][Graph/decode][48/126] batch_size:32 query_len:1 num_blocks:1280 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19916)[0;0m INFO 08-09 12:36:58 [hpu_model_runner.py:3029] [Warmup][Graph/decode][48/126] batch_size:32 query_len:1 num_blocks:1280 free_mem:17.36 GiB
INFO 08-09 12:36:58 [hpu_model_runner.py:3029] [Warmup][Graph/decode][48/126] batch_size:32 query_len:1 num_blocks:1280 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19918)[0;0m INFO 08-09 12:37:02 [hpu_model_runner.py:3029] [Warmup][Graph/decode][49/126] batch_size:32 query_len:1 num_blocks:1024 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19917)[0;0m INFO 08-09 12:37:02 [hpu_model_runner.py:3029] [Warmup][Graph/decode][49/126] batch_size:32 query_len:1 num_blocks:1024 free_mem:17.36 GiB
INFO 08-09 12:37:02 [hpu_model_runner.py:3029] [Warmup][Graph/decode][49/126] batch_size:32 query_len:1 num_blocks:1024 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19916)[0;0m INFO 08-09 12:37:02 [hpu_model_runner.py:3029] [Warmup][Graph/decode][49/126] batch_size:32 query_len:1 num_blocks:1024 free_mem:17.36 GiB
INFO 08-09 12:37:05 [hpu_model_runner.py:3029] [Warmup][Graph/decode][50/126] batch_size:32 query_len:1 num_blocks:896 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19918)[0;0m INFO 08-09 12:37:05 [hpu_model_runner.py:3029] [Warmup][Graph/decode][50/126] batch_size:32 query_len:1 num_blocks:896 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19917)[0;0m INFO 08-09 12:37:05 [hpu_model_runner.py:3029] [Warmup][Graph/decode][50/126] batch_size:32 query_len:1 num_blocks:896 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19916)[0;0m INFO 08-09 12:37:05 [hpu_model_runner.py:3029] [Warmup][Graph/decode][50/126] batch_size:32 query_len:1 num_blocks:896 free_mem:17.36 GiB
INFO 08-09 12:37:09 [hpu_model_runner.py:3029] [Warmup][Graph/decode][51/126] batch_size:32 query_len:1 num_blocks:768 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19916)[0;0m INFO 08-09 12:37:09 [hpu_model_runner.py:3029] [Warmup][Graph/decode][51/126] batch_size:32 query_len:1 num_blocks:768 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19917)[0;0m INFO 08-09 12:37:09 [hpu_model_runner.py:3029] [Warmup][Graph/decode][51/126] batch_size:32 query_len:1 num_blocks:768 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19918)[0;0m INFO 08-09 12:37:09 [hpu_model_runner.py:3029] [Warmup][Graph/decode][51/126] batch_size:32 query_len:1 num_blocks:768 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19916)[0;0m INFO 08-09 12:37:13 [hpu_model_runner.py:3029] [Warmup][Graph/decode][52/126] batch_size:32 query_len:1 num_blocks:640 free_mem:17.36 GiB
INFO 08-09 12:37:13 [hpu_model_runner.py:3029] [Warmup][Graph/decode][52/126] batch_size:32 query_len:1 num_blocks:640 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19918)[0;0m INFO 08-09 12:37:13 [hpu_model_runner.py:3029] [Warmup][Graph/decode][52/126] batch_size:32 query_len:1 num_blocks:640 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19917)[0;0m INFO 08-09 12:37:13 [hpu_model_runner.py:3029] [Warmup][Graph/decode][52/126] batch_size:32 query_len:1 num_blocks:640 free_mem:17.36 GiB
INFO 08-09 12:37:17 [hpu_model_runner.py:3029] [Warmup][Graph/decode][53/126] batch_size:32 query_len:1 num_blocks:512 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19917)[0;0m INFO 08-09 12:37:17 [hpu_model_runner.py:3029] [Warmup][Graph/decode][53/126] batch_size:32 query_len:1 num_blocks:512 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19918)[0;0m INFO 08-09 12:37:17 [hpu_model_runner.py:3029] [Warmup][Graph/decode][53/126] batch_size:32 query_len:1 num_blocks:512 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19916)[0;0m INFO 08-09 12:37:17 [hpu_model_runner.py:3029] [Warmup][Graph/decode][53/126] batch_size:32 query_len:1 num_blocks:512 free_mem:17.36 GiB
INFO 08-09 12:37:21 [hpu_model_runner.py:3029] [Warmup][Graph/decode][54/126] batch_size:32 query_len:1 num_blocks:384 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19918)[0;0m INFO 08-09 12:37:21 [hpu_model_runner.py:3029] [Warmup][Graph/decode][54/126] batch_size:32 query_len:1 num_blocks:384 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19917)[0;0m INFO 08-09 12:37:21 [hpu_model_runner.py:3029] [Warmup][Graph/decode][54/126] batch_size:32 query_len:1 num_blocks:384 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19916)[0;0m INFO 08-09 12:37:21 [hpu_model_runner.py:3029] [Warmup][Graph/decode][54/126] batch_size:32 query_len:1 num_blocks:384 free_mem:17.36 GiB
INFO 08-09 12:37:25 [hpu_model_runner.py:3029] [Warmup][Graph/decode][55/126] batch_size:32 query_len:1 num_blocks:256 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19918)[0;0m INFO 08-09 12:37:25 [hpu_model_runner.py:3029] [Warmup][Graph/decode][55/126] batch_size:32 query_len:1 num_blocks:256 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19916)[0;0m INFO 08-09 12:37:25 [hpu_model_runner.py:3029] [Warmup][Graph/decode][55/126] batch_size:32 query_len:1 num_blocks:256 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19917)[0;0m INFO 08-09 12:37:25 [hpu_model_runner.py:3029] [Warmup][Graph/decode][55/126] batch_size:32 query_len:1 num_blocks:256 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19916)[0;0m INFO 08-09 12:37:28 [hpu_model_runner.py:3029] [Warmup][Graph/decode][56/126] batch_size:32 query_len:1 num_blocks:128 free_mem:17.36 GiB
INFO 08-09 12:37:28 [hpu_model_runner.py:3029] [Warmup][Graph/decode][56/126] batch_size:32 query_len:1 num_blocks:128 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19917)[0;0m INFO 08-09 12:37:28 [hpu_model_runner.py:3029] [Warmup][Graph/decode][56/126] batch_size:32 query_len:1 num_blocks:128 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19918)[0;0m INFO 08-09 12:37:28 [hpu_model_runner.py:3029] [Warmup][Graph/decode][56/126] batch_size:32 query_len:1 num_blocks:128 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19916)[0;0m INFO 08-09 12:37:32 [hpu_model_runner.py:3029] [Warmup][Graph/decode][57/126] batch_size:16 query_len:1 num_blocks:4733 free_mem:17.36 GiB
INFO 08-09 12:37:32 [hpu_model_runner.py:3029] [Warmup][Graph/decode][57/126] batch_size:16 query_len:1 num_blocks:4733 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19917)[0;0m INFO 08-09 12:37:32 [hpu_model_runner.py:3029] [Warmup][Graph/decode][57/126] batch_size:16 query_len:1 num_blocks:4733 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19918)[0;0m INFO 08-09 12:37:32 [hpu_model_runner.py:3029] [Warmup][Graph/decode][57/126] batch_size:16 query_len:1 num_blocks:4733 free_mem:17.36 GiB
INFO 08-09 12:37:37 [hpu_model_runner.py:3029] [Warmup][Graph/decode][58/126] batch_size:16 query_len:1 num_blocks:3712 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19918)[0;0m INFO 08-09 12:37:37 [hpu_model_runner.py:3029] [Warmup][Graph/decode][58/126] batch_size:16 query_len:1 num_blocks:3712 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19917)[0;0m INFO 08-09 12:37:37 [hpu_model_runner.py:3029] [Warmup][Graph/decode][58/126] batch_size:16 query_len:1 num_blocks:3712 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19916)[0;0m INFO 08-09 12:37:37 [hpu_model_runner.py:3029] [Warmup][Graph/decode][58/126] batch_size:16 query_len:1 num_blocks:3712 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19918)[0;0m INFO 08-09 12:37:41 [hpu_model_runner.py:3029] [Warmup][Graph/decode][59/126] batch_size:16 query_len:1 num_blocks:2816 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19916)[0;0m INFO 08-09 12:37:41 [hpu_model_runner.py:3029] [Warmup][Graph/decode][59/126] batch_size:16 query_len:1 num_blocks:2816 free_mem:17.36 GiB
INFO 08-09 12:37:41 [hpu_model_runner.py:3029] [Warmup][Graph/decode][59/126] batch_size:16 query_len:1 num_blocks:2816 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19917)[0;0m INFO 08-09 12:37:41 [hpu_model_runner.py:3029] [Warmup][Graph/decode][59/126] batch_size:16 query_len:1 num_blocks:2816 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19916)[0;0m INFO 08-09 12:37:45 [hpu_model_runner.py:3029] [Warmup][Graph/decode][60/126] batch_size:16 query_len:1 num_blocks:2176 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19918)[0;0m INFO 08-09 12:37:45 [hpu_model_runner.py:3029] [Warmup][Graph/decode][60/126] batch_size:16 query_len:1 num_blocks:2176 free_mem:17.36 GiB
INFO 08-09 12:37:45 [hpu_model_runner.py:3029] [Warmup][Graph/decode][60/126] batch_size:16 query_len:1 num_blocks:2176 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19917)[0;0m INFO 08-09 12:37:45 [hpu_model_runner.py:3029] [Warmup][Graph/decode][60/126] batch_size:16 query_len:1 num_blocks:2176 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19918)[0;0m INFO 08-09 12:37:49 [hpu_model_runner.py:3029] [Warmup][Graph/decode][61/126] batch_size:16 query_len:1 num_blocks:1664 free_mem:17.36 GiB
INFO 08-09 12:37:49 [hpu_model_runner.py:3029] [Warmup][Graph/decode][61/126] batch_size:16 query_len:1 num_blocks:1664 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19917)[0;0m INFO 08-09 12:37:49 [hpu_model_runner.py:3029] [Warmup][Graph/decode][61/126] batch_size:16 query_len:1 num_blocks:1664 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19916)[0;0m INFO 08-09 12:37:49 [hpu_model_runner.py:3029] [Warmup][Graph/decode][61/126] batch_size:16 query_len:1 num_blocks:1664 free_mem:17.36 GiB
INFO 08-09 12:37:53 [hpu_model_runner.py:3029] [Warmup][Graph/decode][62/126] batch_size:16 query_len:1 num_blocks:1280 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19917)[0;0m INFO 08-09 12:37:53 [hpu_model_runner.py:3029] [Warmup][Graph/decode][62/126] batch_size:16 query_len:1 num_blocks:1280 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19916)[0;0m INFO 08-09 12:37:53 [hpu_model_runner.py:3029] [Warmup][Graph/decode][62/126] batch_size:16 query_len:1 num_blocks:1280 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19918)[0;0m INFO 08-09 12:37:53 [hpu_model_runner.py:3029] [Warmup][Graph/decode][62/126] batch_size:16 query_len:1 num_blocks:1280 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19917)[0;0m INFO 08-09 12:37:57 [hpu_model_runner.py:3029] [Warmup][Graph/decode][63/126] batch_size:16 query_len:1 num_blocks:1024 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19916)[0;0m INFO 08-09 12:37:57 [hpu_model_runner.py:3029] [Warmup][Graph/decode][63/126] batch_size:16 query_len:1 num_blocks:1024 free_mem:17.36 GiB
INFO 08-09 12:37:57 [hpu_model_runner.py:3029] [Warmup][Graph/decode][63/126] batch_size:16 query_len:1 num_blocks:1024 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19918)[0;0m INFO 08-09 12:37:57 [hpu_model_runner.py:3029] [Warmup][Graph/decode][63/126] batch_size:16 query_len:1 num_blocks:1024 free_mem:17.36 GiB
INFO 08-09 12:38:01 [hpu_model_runner.py:3029] [Warmup][Graph/decode][64/126] batch_size:16 query_len:1 num_blocks:896 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19918)[0;0m INFO 08-09 12:38:01 [hpu_model_runner.py:3029] [Warmup][Graph/decode][64/126] batch_size:16 query_len:1 num_blocks:896 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19917)[0;0m INFO 08-09 12:38:01 [hpu_model_runner.py:3029] [Warmup][Graph/decode][64/126] batch_size:16 query_len:1 num_blocks:896 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19916)[0;0m INFO 08-09 12:38:01 [hpu_model_runner.py:3029] [Warmup][Graph/decode][64/126] batch_size:16 query_len:1 num_blocks:896 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19918)[0;0m INFO 08-09 12:38:05 [hpu_model_runner.py:3029] [Warmup][Graph/decode][65/126] batch_size:16 query_len:1 num_blocks:768 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19916)[0;0m INFO 08-09 12:38:05 [hpu_model_runner.py:3029] [Warmup][Graph/decode][65/126] batch_size:16 query_len:1 num_blocks:768 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19917)[0;0m INFO 08-09 12:38:05 [hpu_model_runner.py:3029] [Warmup][Graph/decode][65/126] batch_size:16 query_len:1 num_blocks:768 free_mem:17.36 GiB
INFO 08-09 12:38:05 [hpu_model_runner.py:3029] [Warmup][Graph/decode][65/126] batch_size:16 query_len:1 num_blocks:768 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19918)[0;0m INFO 08-09 12:38:09 [hpu_model_runner.py:3029] [Warmup][Graph/decode][66/126] batch_size:16 query_len:1 num_blocks:640 free_mem:17.36 GiB
INFO 08-09 12:38:09 [hpu_model_runner.py:3029] [Warmup][Graph/decode][66/126] batch_size:16 query_len:1 num_blocks:640 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19917)[0;0m INFO 08-09 12:38:09 [hpu_model_runner.py:3029] [Warmup][Graph/decode][66/126] batch_size:16 query_len:1 num_blocks:640 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19916)[0;0m INFO 08-09 12:38:09 [hpu_model_runner.py:3029] [Warmup][Graph/decode][66/126] batch_size:16 query_len:1 num_blocks:640 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19918)[0;0m INFO 08-09 12:38:13 [hpu_model_runner.py:3029] [Warmup][Graph/decode][67/126] batch_size:16 query_len:1 num_blocks:512 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19917)[0;0m INFO 08-09 12:38:13 [hpu_model_runner.py:3029] [Warmup][Graph/decode][67/126] batch_size:16 query_len:1 num_blocks:512 free_mem:17.36 GiB
INFO 08-09 12:38:13 [hpu_model_runner.py:3029] [Warmup][Graph/decode][67/126] batch_size:16 query_len:1 num_blocks:512 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19916)[0;0m INFO 08-09 12:38:13 [hpu_model_runner.py:3029] [Warmup][Graph/decode][67/126] batch_size:16 query_len:1 num_blocks:512 free_mem:17.36 GiB
INFO 08-09 12:38:17 [hpu_model_runner.py:3029] [Warmup][Graph/decode][68/126] batch_size:16 query_len:1 num_blocks:384 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19916)[0;0m INFO 08-09 12:38:17 [hpu_model_runner.py:3029] [Warmup][Graph/decode][68/126] batch_size:16 query_len:1 num_blocks:384 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19918)[0;0m INFO 08-09 12:38:17 [hpu_model_runner.py:3029] [Warmup][Graph/decode][68/126] batch_size:16 query_len:1 num_blocks:384 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19917)[0;0m INFO 08-09 12:38:17 [hpu_model_runner.py:3029] [Warmup][Graph/decode][68/126] batch_size:16 query_len:1 num_blocks:384 free_mem:17.36 GiB
INFO 08-09 12:38:21 [hpu_model_runner.py:3029] [Warmup][Graph/decode][69/126] batch_size:16 query_len:1 num_blocks:256 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19918)[0;0m INFO 08-09 12:38:21 [hpu_model_runner.py:3029] [Warmup][Graph/decode][69/126] batch_size:16 query_len:1 num_blocks:256 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19917)[0;0m INFO 08-09 12:38:21 [hpu_model_runner.py:3029] [Warmup][Graph/decode][69/126] batch_size:16 query_len:1 num_blocks:256 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19916)[0;0m INFO 08-09 12:38:21 [hpu_model_runner.py:3029] [Warmup][Graph/decode][69/126] batch_size:16 query_len:1 num_blocks:256 free_mem:17.36 GiB
INFO 08-09 12:38:26 [hpu_model_runner.py:3029] [Warmup][Graph/decode][70/126] batch_size:16 query_len:1 num_blocks:128 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19916)[0;0m INFO 08-09 12:38:26 [hpu_model_runner.py:3029] [Warmup][Graph/decode][70/126] batch_size:16 query_len:1 num_blocks:128 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19918)[0;0m INFO 08-09 12:38:26 [hpu_model_runner.py:3029] [Warmup][Graph/decode][70/126] batch_size:16 query_len:1 num_blocks:128 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19917)[0;0m INFO 08-09 12:38:26 [hpu_model_runner.py:3029] [Warmup][Graph/decode][70/126] batch_size:16 query_len:1 num_blocks:128 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19918)[0;0m INFO 08-09 12:38:30 [hpu_model_runner.py:3029] [Warmup][Graph/decode][71/126] batch_size:8 query_len:1 num_blocks:4733 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19917)[0;0m INFO 08-09 12:38:30 [hpu_model_runner.py:3029] [Warmup][Graph/decode][71/126] batch_size:8 query_len:1 num_blocks:4733 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19916)[0;0m INFO 08-09 12:38:30 [hpu_model_runner.py:3029] [Warmup][Graph/decode][71/126] batch_size:8 query_len:1 num_blocks:4733 free_mem:17.36 GiB
INFO 08-09 12:38:30 [hpu_model_runner.py:3029] [Warmup][Graph/decode][71/126] batch_size:8 query_len:1 num_blocks:4733 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19917)[0;0m INFO 08-09 12:38:34 [hpu_model_runner.py:3029] [Warmup][Graph/decode][72/126] batch_size:8 query_len:1 num_blocks:3712 free_mem:17.36 GiB
INFO 08-09 12:38:34 [hpu_model_runner.py:3029] [Warmup][Graph/decode][72/126] batch_size:8 query_len:1 num_blocks:3712 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19918)[0;0m INFO 08-09 12:38:34 [hpu_model_runner.py:3029] [Warmup][Graph/decode][72/126] batch_size:8 query_len:1 num_blocks:3712 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19916)[0;0m INFO 08-09 12:38:34 [hpu_model_runner.py:3029] [Warmup][Graph/decode][72/126] batch_size:8 query_len:1 num_blocks:3712 free_mem:17.36 GiB
INFO 08-09 12:38:39 [hpu_model_runner.py:3029] [Warmup][Graph/decode][73/126] batch_size:8 query_len:1 num_blocks:2816 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19918)[0;0m INFO 08-09 12:38:39 [hpu_model_runner.py:3029] [Warmup][Graph/decode][73/126] batch_size:8 query_len:1 num_blocks:2816 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19917)[0;0m INFO 08-09 12:38:39 [hpu_model_runner.py:3029] [Warmup][Graph/decode][73/126] batch_size:8 query_len:1 num_blocks:2816 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19916)[0;0m INFO 08-09 12:38:39 [hpu_model_runner.py:3029] [Warmup][Graph/decode][73/126] batch_size:8 query_len:1 num_blocks:2816 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19918)[0;0m INFO 08-09 12:38:43 [hpu_model_runner.py:3029] [Warmup][Graph/decode][74/126] batch_size:8 query_len:1 num_blocks:2176 free_mem:17.36 GiB
INFO 08-09 12:38:43 [hpu_model_runner.py:3029] [Warmup][Graph/decode][74/126] batch_size:8 query_len:1 num_blocks:2176 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19917)[0;0m INFO 08-09 12:38:43 [hpu_model_runner.py:3029] [Warmup][Graph/decode][74/126] batch_size:8 query_len:1 num_blocks:2176 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19916)[0;0m INFO 08-09 12:38:43 [hpu_model_runner.py:3029] [Warmup][Graph/decode][74/126] batch_size:8 query_len:1 num_blocks:2176 free_mem:17.36 GiB
INFO 08-09 12:38:47 [hpu_model_runner.py:3029] [Warmup][Graph/decode][75/126] batch_size:8 query_len:1 num_blocks:1664 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19917)[0;0m INFO 08-09 12:38:47 [hpu_model_runner.py:3029] [Warmup][Graph/decode][75/126] batch_size:8 query_len:1 num_blocks:1664 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19918)[0;0m INFO 08-09 12:38:47 [hpu_model_runner.py:3029] [Warmup][Graph/decode][75/126] batch_size:8 query_len:1 num_blocks:1664 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19916)[0;0m INFO 08-09 12:38:47 [hpu_model_runner.py:3029] [Warmup][Graph/decode][75/126] batch_size:8 query_len:1 num_blocks:1664 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19917)[0;0m INFO 08-09 12:38:52 [hpu_model_runner.py:3029] [Warmup][Graph/decode][76/126] batch_size:8 query_len:1 num_blocks:1280 free_mem:17.36 GiB
INFO 08-09 12:38:52 [hpu_model_runner.py:3029] [Warmup][Graph/decode][76/126] batch_size:8 query_len:1 num_blocks:1280 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19916)[0;0m INFO 08-09 12:38:52 [hpu_model_runner.py:3029] [Warmup][Graph/decode][76/126] batch_size:8 query_len:1 num_blocks:1280 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19918)[0;0m INFO 08-09 12:38:52 [hpu_model_runner.py:3029] [Warmup][Graph/decode][76/126] batch_size:8 query_len:1 num_blocks:1280 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19917)[0;0m INFO 08-09 12:38:56 [hpu_model_runner.py:3029] [Warmup][Graph/decode][77/126] batch_size:8 query_len:1 num_blocks:1024 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19916)[0;0m INFO 08-09 12:38:56 [hpu_model_runner.py:3029] [Warmup][Graph/decode][77/126] batch_size:8 query_len:1 num_blocks:1024 free_mem:17.36 GiB
INFO 08-09 12:38:56 [hpu_model_runner.py:3029] [Warmup][Graph/decode][77/126] batch_size:8 query_len:1 num_blocks:1024 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19918)[0;0m INFO 08-09 12:38:56 [hpu_model_runner.py:3029] [Warmup][Graph/decode][77/126] batch_size:8 query_len:1 num_blocks:1024 free_mem:17.36 GiB
INFO 08-09 12:39:01 [hpu_model_runner.py:3029] [Warmup][Graph/decode][78/126] batch_size:8 query_len:1 num_blocks:896 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19918)[0;0m INFO 08-09 12:39:01 [hpu_model_runner.py:3029] [Warmup][Graph/decode][78/126] batch_size:8 query_len:1 num_blocks:896 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19917)[0;0m INFO 08-09 12:39:01 [hpu_model_runner.py:3029] [Warmup][Graph/decode][78/126] batch_size:8 query_len:1 num_blocks:896 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19916)[0;0m INFO 08-09 12:39:01 [hpu_model_runner.py:3029] [Warmup][Graph/decode][78/126] batch_size:8 query_len:1 num_blocks:896 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19917)[0;0m INFO 08-09 12:39:05 [hpu_model_runner.py:3029] [Warmup][Graph/decode][79/126] batch_size:8 query_len:1 num_blocks:768 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19916)[0;0m INFO 08-09 12:39:05 [hpu_model_runner.py:3029] [Warmup][Graph/decode][79/126] batch_size:8 query_len:1 num_blocks:768 free_mem:17.36 GiB
INFO 08-09 12:39:05 [hpu_model_runner.py:3029] [Warmup][Graph/decode][79/126] batch_size:8 query_len:1 num_blocks:768 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19918)[0;0m INFO 08-09 12:39:05 [hpu_model_runner.py:3029] [Warmup][Graph/decode][79/126] batch_size:8 query_len:1 num_blocks:768 free_mem:17.36 GiB
INFO 08-09 12:39:09 [hpu_model_runner.py:3029] [Warmup][Graph/decode][80/126] batch_size:8 query_len:1 num_blocks:640 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19917)[0;0m INFO 08-09 12:39:09 [hpu_model_runner.py:3029] [Warmup][Graph/decode][80/126] batch_size:8 query_len:1 num_blocks:640 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19916)[0;0m INFO 08-09 12:39:09 [hpu_model_runner.py:3029] [Warmup][Graph/decode][80/126] batch_size:8 query_len:1 num_blocks:640 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19918)[0;0m INFO 08-09 12:39:09 [hpu_model_runner.py:3029] [Warmup][Graph/decode][80/126] batch_size:8 query_len:1 num_blocks:640 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19917)[0;0m INFO 08-09 12:39:14 [hpu_model_runner.py:3029] [Warmup][Graph/decode][81/126] batch_size:8 query_len:1 num_blocks:512 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19916)[0;0m INFO 08-09 12:39:14 [hpu_model_runner.py:3029] [Warmup][Graph/decode][81/126] batch_size:8 query_len:1 num_blocks:512 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19918)[0;0m INFO 08-09 12:39:14 [hpu_model_runner.py:3029] [Warmup][Graph/decode][81/126] batch_size:8 query_len:1 num_blocks:512 free_mem:17.36 GiB
INFO 08-09 12:39:14 [hpu_model_runner.py:3029] [Warmup][Graph/decode][81/126] batch_size:8 query_len:1 num_blocks:512 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19918)[0;0m INFO 08-09 12:39:18 [hpu_model_runner.py:3029] [Warmup][Graph/decode][82/126] batch_size:8 query_len:1 num_blocks:384 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19917)[0;0m INFO 08-09 12:39:18 [hpu_model_runner.py:3029] [Warmup][Graph/decode][82/126] batch_size:8 query_len:1 num_blocks:384 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19916)[0;0m INFO 08-09 12:39:18 [hpu_model_runner.py:3029] [Warmup][Graph/decode][82/126] batch_size:8 query_len:1 num_blocks:384 free_mem:17.36 GiB
INFO 08-09 12:39:18 [hpu_model_runner.py:3029] [Warmup][Graph/decode][82/126] batch_size:8 query_len:1 num_blocks:384 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19918)[0;0m INFO 08-09 12:39:23 [hpu_model_runner.py:3029] [Warmup][Graph/decode][83/126] batch_size:8 query_len:1 num_blocks:256 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19916)[0;0m INFO 08-09 12:39:23 [hpu_model_runner.py:3029] [Warmup][Graph/decode][83/126] batch_size:8 query_len:1 num_blocks:256 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19917)[0;0m INFO 08-09 12:39:23 [hpu_model_runner.py:3029] [Warmup][Graph/decode][83/126] batch_size:8 query_len:1 num_blocks:256 free_mem:17.36 GiB
INFO 08-09 12:39:23 [hpu_model_runner.py:3029] [Warmup][Graph/decode][83/126] batch_size:8 query_len:1 num_blocks:256 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19918)[0;0m INFO 08-09 12:39:27 [hpu_model_runner.py:3029] [Warmup][Graph/decode][84/126] batch_size:8 query_len:1 num_blocks:128 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19916)[0;0m INFO 08-09 12:39:27 [hpu_model_runner.py:3029] [Warmup][Graph/decode][84/126] batch_size:8 query_len:1 num_blocks:128 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19917)[0;0m INFO 08-09 12:39:27 [hpu_model_runner.py:3029] [Warmup][Graph/decode][84/126] batch_size:8 query_len:1 num_blocks:128 free_mem:17.36 GiB
INFO 08-09 12:39:27 [hpu_model_runner.py:3029] [Warmup][Graph/decode][84/126] batch_size:8 query_len:1 num_blocks:128 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19918)[0;0m INFO 08-09 12:39:32 [hpu_model_runner.py:3029] [Warmup][Graph/decode][85/126] batch_size:4 query_len:1 num_blocks:4733 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19916)[0;0m INFO 08-09 12:39:32 [hpu_model_runner.py:3029] [Warmup][Graph/decode][85/126] batch_size:4 query_len:1 num_blocks:4733 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19917)[0;0m INFO 08-09 12:39:32 [hpu_model_runner.py:3029] [Warmup][Graph/decode][85/126] batch_size:4 query_len:1 num_blocks:4733 free_mem:17.36 GiB
INFO 08-09 12:39:32 [hpu_model_runner.py:3029] [Warmup][Graph/decode][85/126] batch_size:4 query_len:1 num_blocks:4733 free_mem:17.36 GiB
INFO 08-09 12:39:36 [hpu_model_runner.py:3029] [Warmup][Graph/decode][86/126] batch_size:4 query_len:1 num_blocks:3712 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19918)[0;0m INFO 08-09 12:39:36 [hpu_model_runner.py:3029] [Warmup][Graph/decode][86/126] batch_size:4 query_len:1 num_blocks:3712 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19917)[0;0m INFO 08-09 12:39:36 [hpu_model_runner.py:3029] [Warmup][Graph/decode][86/126] batch_size:4 query_len:1 num_blocks:3712 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19916)[0;0m INFO 08-09 12:39:36 [hpu_model_runner.py:3029] [Warmup][Graph/decode][86/126] batch_size:4 query_len:1 num_blocks:3712 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19918)[0;0m INFO 08-09 12:39:41 [hpu_model_runner.py:3029] [Warmup][Graph/decode][87/126] batch_size:4 query_len:1 num_blocks:2816 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19917)[0;0m INFO 08-09 12:39:41 [hpu_model_runner.py:3029] [Warmup][Graph/decode][87/126] batch_size:4 query_len:1 num_blocks:2816 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19916)[0;0m INFO 08-09 12:39:41 [hpu_model_runner.py:3029] [Warmup][Graph/decode][87/126] batch_size:4 query_len:1 num_blocks:2816 free_mem:17.36 GiB
INFO 08-09 12:39:41 [hpu_model_runner.py:3029] [Warmup][Graph/decode][87/126] batch_size:4 query_len:1 num_blocks:2816 free_mem:17.36 GiB
INFO 08-09 12:39:46 [hpu_model_runner.py:3029] [Warmup][Graph/decode][88/126] batch_size:4 query_len:1 num_blocks:2176 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19918)[0;0m INFO 08-09 12:39:46 [hpu_model_runner.py:3029] [Warmup][Graph/decode][88/126] batch_size:4 query_len:1 num_blocks:2176 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19916)[0;0m INFO 08-09 12:39:46 [hpu_model_runner.py:3029] [Warmup][Graph/decode][88/126] batch_size:4 query_len:1 num_blocks:2176 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19917)[0;0m INFO 08-09 12:39:46 [hpu_model_runner.py:3029] [Warmup][Graph/decode][88/126] batch_size:4 query_len:1 num_blocks:2176 free_mem:17.36 GiB
INFO 08-09 12:39:50 [hpu_model_runner.py:3029] [Warmup][Graph/decode][89/126] batch_size:4 query_len:1 num_blocks:1664 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19918)[0;0m INFO 08-09 12:39:50 [hpu_model_runner.py:3029] [Warmup][Graph/decode][89/126] batch_size:4 query_len:1 num_blocks:1664 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19917)[0;0m INFO 08-09 12:39:50 [hpu_model_runner.py:3029] [Warmup][Graph/decode][89/126] batch_size:4 query_len:1 num_blocks:1664 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19916)[0;0m INFO 08-09 12:39:50 [hpu_model_runner.py:3029] [Warmup][Graph/decode][89/126] batch_size:4 query_len:1 num_blocks:1664 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19917)[0;0m INFO 08-09 12:39:55 [hpu_model_runner.py:3029] [Warmup][Graph/decode][90/126] batch_size:4 query_len:1 num_blocks:1280 free_mem:17.36 GiB
INFO 08-09 12:39:55 [hpu_model_runner.py:3029] [Warmup][Graph/decode][90/126] batch_size:4 query_len:1 num_blocks:1280 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19918)[0;0m INFO 08-09 12:39:55 [hpu_model_runner.py:3029] [Warmup][Graph/decode][90/126] batch_size:4 query_len:1 num_blocks:1280 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19916)[0;0m INFO 08-09 12:39:55 [hpu_model_runner.py:3029] [Warmup][Graph/decode][90/126] batch_size:4 query_len:1 num_blocks:1280 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19918)[0;0m INFO 08-09 12:40:00 [hpu_model_runner.py:3029] [Warmup][Graph/decode][91/126] batch_size:4 query_len:1 num_blocks:1024 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19916)[0;0m INFO 08-09 12:40:00 [hpu_model_runner.py:3029] [Warmup][Graph/decode][91/126] batch_size:4 query_len:1 num_blocks:1024 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19917)[0;0m INFO 08-09 12:40:00 [hpu_model_runner.py:3029] [Warmup][Graph/decode][91/126] batch_size:4 query_len:1 num_blocks:1024 free_mem:17.36 GiB
INFO 08-09 12:40:00 [hpu_model_runner.py:3029] [Warmup][Graph/decode][91/126] batch_size:4 query_len:1 num_blocks:1024 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19918)[0;0m INFO 08-09 12:40:04 [hpu_model_runner.py:3029] [Warmup][Graph/decode][92/126] batch_size:4 query_len:1 num_blocks:896 free_mem:17.36 GiB
INFO 08-09 12:40:04 [hpu_model_runner.py:3029] [Warmup][Graph/decode][92/126] batch_size:4 query_len:1 num_blocks:896 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19916)[0;0m INFO 08-09 12:40:04 [hpu_model_runner.py:3029] [Warmup][Graph/decode][92/126] batch_size:4 query_len:1 num_blocks:896 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19917)[0;0m INFO 08-09 12:40:04 [hpu_model_runner.py:3029] [Warmup][Graph/decode][92/126] batch_size:4 query_len:1 num_blocks:896 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19917)[0;0m INFO 08-09 12:40:09 [hpu_model_runner.py:3029] [Warmup][Graph/decode][93/126] batch_size:4 query_len:1 num_blocks:768 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19916)[0;0m INFO 08-09 12:40:09 [hpu_model_runner.py:3029] [Warmup][Graph/decode][93/126] batch_size:4 query_len:1 num_blocks:768 free_mem:17.36 GiB
INFO 08-09 12:40:09 [hpu_model_runner.py:3029] [Warmup][Graph/decode][93/126] batch_size:4 query_len:1 num_blocks:768 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19918)[0;0m INFO 08-09 12:40:09 [hpu_model_runner.py:3029] [Warmup][Graph/decode][93/126] batch_size:4 query_len:1 num_blocks:768 free_mem:17.36 GiB
INFO 08-09 12:40:14 [hpu_model_runner.py:3029] [Warmup][Graph/decode][94/126] batch_size:4 query_len:1 num_blocks:640 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19918)[0;0m INFO 08-09 12:40:14 [hpu_model_runner.py:3029] [Warmup][Graph/decode][94/126] batch_size:4 query_len:1 num_blocks:640 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19917)[0;0m INFO 08-09 12:40:14 [hpu_model_runner.py:3029] [Warmup][Graph/decode][94/126] batch_size:4 query_len:1 num_blocks:640 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19916)[0;0m INFO 08-09 12:40:14 [hpu_model_runner.py:3029] [Warmup][Graph/decode][94/126] batch_size:4 query_len:1 num_blocks:640 free_mem:17.36 GiB
INFO 08-09 12:40:18 [hpu_model_runner.py:3029] [Warmup][Graph/decode][95/126] batch_size:4 query_len:1 num_blocks:512 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19916)[0;0m INFO 08-09 12:40:18 [hpu_model_runner.py:3029] [Warmup][Graph/decode][95/126] batch_size:4 query_len:1 num_blocks:512 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19917)[0;0m INFO 08-09 12:40:18 [hpu_model_runner.py:3029] [Warmup][Graph/decode][95/126] batch_size:4 query_len:1 num_blocks:512 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19918)[0;0m INFO 08-09 12:40:18 [hpu_model_runner.py:3029] [Warmup][Graph/decode][95/126] batch_size:4 query_len:1 num_blocks:512 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19916)[0;0m INFO 08-09 12:40:23 [hpu_model_runner.py:3029] [Warmup][Graph/decode][96/126] batch_size:4 query_len:1 num_blocks:384 free_mem:17.36 GiB
INFO 08-09 12:40:23 [hpu_model_runner.py:3029] [Warmup][Graph/decode][96/126] batch_size:4 query_len:1 num_blocks:384 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19918)[0;0m INFO 08-09 12:40:23 [hpu_model_runner.py:3029] [Warmup][Graph/decode][96/126] batch_size:4 query_len:1 num_blocks:384 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19917)[0;0m INFO 08-09 12:40:23 [hpu_model_runner.py:3029] [Warmup][Graph/decode][96/126] batch_size:4 query_len:1 num_blocks:384 free_mem:17.36 GiB
INFO 08-09 12:40:28 [hpu_model_runner.py:3029] [Warmup][Graph/decode][97/126] batch_size:4 query_len:1 num_blocks:256 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19916)[0;0m INFO 08-09 12:40:28 [hpu_model_runner.py:3029] [Warmup][Graph/decode][97/126] batch_size:4 query_len:1 num_blocks:256 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19917)[0;0m INFO 08-09 12:40:28 [hpu_model_runner.py:3029] [Warmup][Graph/decode][97/126] batch_size:4 query_len:1 num_blocks:256 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19918)[0;0m INFO 08-09 12:40:28 [hpu_model_runner.py:3029] [Warmup][Graph/decode][97/126] batch_size:4 query_len:1 num_blocks:256 free_mem:17.36 GiB
INFO 08-09 12:40:33 [hpu_model_runner.py:3029] [Warmup][Graph/decode][98/126] batch_size:4 query_len:1 num_blocks:128 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19918)[0;0m INFO 08-09 12:40:33 [hpu_model_runner.py:3029] [Warmup][Graph/decode][98/126] batch_size:4 query_len:1 num_blocks:128 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19916)[0;0m INFO 08-09 12:40:33 [hpu_model_runner.py:3029] [Warmup][Graph/decode][98/126] batch_size:4 query_len:1 num_blocks:128 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19917)[0;0m INFO 08-09 12:40:33 [hpu_model_runner.py:3029] [Warmup][Graph/decode][98/126] batch_size:4 query_len:1 num_blocks:128 free_mem:17.36 GiB
INFO 08-09 12:40:37 [hpu_model_runner.py:3029] [Warmup][Graph/decode][99/126] batch_size:2 query_len:1 num_blocks:4733 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19918)[0;0m INFO 08-09 12:40:37 [hpu_model_runner.py:3029] [Warmup][Graph/decode][99/126] batch_size:2 query_len:1 num_blocks:4733 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19917)[0;0m INFO 08-09 12:40:37 [hpu_model_runner.py:3029] [Warmup][Graph/decode][99/126] batch_size:2 query_len:1 num_blocks:4733 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19916)[0;0m INFO 08-09 12:40:37 [hpu_model_runner.py:3029] [Warmup][Graph/decode][99/126] batch_size:2 query_len:1 num_blocks:4733 free_mem:17.36 GiB
INFO 08-09 12:40:43 [hpu_model_runner.py:3029] [Warmup][Graph/decode][100/126] batch_size:2 query_len:1 num_blocks:3712 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19918)[0;0m INFO 08-09 12:40:43 [hpu_model_runner.py:3029] [Warmup][Graph/decode][100/126] batch_size:2 query_len:1 num_blocks:3712 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19917)[0;0m INFO 08-09 12:40:43 [hpu_model_runner.py:3029] [Warmup][Graph/decode][100/126] batch_size:2 query_len:1 num_blocks:3712 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19916)[0;0m INFO 08-09 12:40:43 [hpu_model_runner.py:3029] [Warmup][Graph/decode][100/126] batch_size:2 query_len:1 num_blocks:3712 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19917)[0;0m INFO 08-09 12:40:48 [hpu_model_runner.py:3029] [Warmup][Graph/decode][101/126] batch_size:2 query_len:1 num_blocks:2816 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19916)[0;0m INFO 08-09 12:40:48 [hpu_model_runner.py:3029] [Warmup][Graph/decode][101/126] batch_size:2 query_len:1 num_blocks:2816 free_mem:17.36 GiB
INFO 08-09 12:40:48 [hpu_model_runner.py:3029] [Warmup][Graph/decode][101/126] batch_size:2 query_len:1 num_blocks:2816 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19918)[0;0m INFO 08-09 12:40:48 [hpu_model_runner.py:3029] [Warmup][Graph/decode][101/126] batch_size:2 query_len:1 num_blocks:2816 free_mem:17.36 GiB
INFO 08-09 12:40:52 [hpu_model_runner.py:3029] [Warmup][Graph/decode][102/126] batch_size:2 query_len:1 num_blocks:2176 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19918)[0;0m INFO 08-09 12:40:52 [hpu_model_runner.py:3029] [Warmup][Graph/decode][102/126] batch_size:2 query_len:1 num_blocks:2176 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19916)[0;0m INFO 08-09 12:40:52 [hpu_model_runner.py:3029] [Warmup][Graph/decode][102/126] batch_size:2 query_len:1 num_blocks:2176 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19917)[0;0m INFO 08-09 12:40:52 [hpu_model_runner.py:3029] [Warmup][Graph/decode][102/126] batch_size:2 query_len:1 num_blocks:2176 free_mem:17.36 GiB
INFO 08-09 12:40:57 [hpu_model_runner.py:3029] [Warmup][Graph/decode][103/126] batch_size:2 query_len:1 num_blocks:1664 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19918)[0;0m INFO 08-09 12:40:57 [hpu_model_runner.py:3029] [Warmup][Graph/decode][103/126] batch_size:2 query_len:1 num_blocks:1664 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19917)[0;0m INFO 08-09 12:40:57 [hpu_model_runner.py:3029] [Warmup][Graph/decode][103/126] batch_size:2 query_len:1 num_blocks:1664 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19916)[0;0m INFO 08-09 12:40:57 [hpu_model_runner.py:3029] [Warmup][Graph/decode][103/126] batch_size:2 query_len:1 num_blocks:1664 free_mem:17.36 GiB
INFO 08-09 12:41:02 [hpu_model_runner.py:3029] [Warmup][Graph/decode][104/126] batch_size:2 query_len:1 num_blocks:1280 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19918)[0;0m INFO 08-09 12:41:02 [hpu_model_runner.py:3029] [Warmup][Graph/decode][104/126] batch_size:2 query_len:1 num_blocks:1280 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19917)[0;0m INFO 08-09 12:41:02 [hpu_model_runner.py:3029] [Warmup][Graph/decode][104/126] batch_size:2 query_len:1 num_blocks:1280 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19916)[0;0m INFO 08-09 12:41:02 [hpu_model_runner.py:3029] [Warmup][Graph/decode][104/126] batch_size:2 query_len:1 num_blocks:1280 free_mem:17.36 GiB
INFO 08-09 12:41:07 [hpu_model_runner.py:3029] [Warmup][Graph/decode][105/126] batch_size:2 query_len:1 num_blocks:1024 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19918)[0;0m INFO 08-09 12:41:07 [hpu_model_runner.py:3029] [Warmup][Graph/decode][105/126] batch_size:2 query_len:1 num_blocks:1024 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19917)[0;0m INFO 08-09 12:41:07 [hpu_model_runner.py:3029] [Warmup][Graph/decode][105/126] batch_size:2 query_len:1 num_blocks:1024 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19916)[0;0m INFO 08-09 12:41:07 [hpu_model_runner.py:3029] [Warmup][Graph/decode][105/126] batch_size:2 query_len:1 num_blocks:1024 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19916)[0;0m INFO 08-09 12:41:12 [hpu_model_runner.py:3029] [Warmup][Graph/decode][106/126] batch_size:2 query_len:1 num_blocks:896 free_mem:17.36 GiB
INFO 08-09 12:41:12 [hpu_model_runner.py:3029] [Warmup][Graph/decode][106/126] batch_size:2 query_len:1 num_blocks:896 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19917)[0;0m INFO 08-09 12:41:12 [hpu_model_runner.py:3029] [Warmup][Graph/decode][106/126] batch_size:2 query_len:1 num_blocks:896 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19918)[0;0m INFO 08-09 12:41:12 [hpu_model_runner.py:3029] [Warmup][Graph/decode][106/126] batch_size:2 query_len:1 num_blocks:896 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19918)[0;0m INFO 08-09 12:41:17 [hpu_model_runner.py:3029] [Warmup][Graph/decode][107/126] batch_size:2 query_len:1 num_blocks:768 free_mem:17.36 GiB
INFO 08-09 12:41:17 [hpu_model_runner.py:3029] [Warmup][Graph/decode][107/126] batch_size:2 query_len:1 num_blocks:768 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19917)[0;0m INFO 08-09 12:41:17 [hpu_model_runner.py:3029] [Warmup][Graph/decode][107/126] batch_size:2 query_len:1 num_blocks:768 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19916)[0;0m INFO 08-09 12:41:17 [hpu_model_runner.py:3029] [Warmup][Graph/decode][107/126] batch_size:2 query_len:1 num_blocks:768 free_mem:17.36 GiB
INFO 08-09 12:41:22 [hpu_model_runner.py:3029] [Warmup][Graph/decode][108/126] batch_size:2 query_len:1 num_blocks:640 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19918)[0;0m INFO 08-09 12:41:22 [hpu_model_runner.py:3029] [Warmup][Graph/decode][108/126] batch_size:2 query_len:1 num_blocks:640 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19916)[0;0m INFO 08-09 12:41:22 [hpu_model_runner.py:3029] [Warmup][Graph/decode][108/126] batch_size:2 query_len:1 num_blocks:640 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19917)[0;0m INFO 08-09 12:41:22 [hpu_model_runner.py:3029] [Warmup][Graph/decode][108/126] batch_size:2 query_len:1 num_blocks:640 free_mem:17.36 GiB
INFO 08-09 12:41:27 [hpu_model_runner.py:3029] [Warmup][Graph/decode][109/126] batch_size:2 query_len:1 num_blocks:512 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19916)[0;0m INFO 08-09 12:41:27 [hpu_model_runner.py:3029] [Warmup][Graph/decode][109/126] batch_size:2 query_len:1 num_blocks:512 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19918)[0;0m INFO 08-09 12:41:27 [hpu_model_runner.py:3029] [Warmup][Graph/decode][109/126] batch_size:2 query_len:1 num_blocks:512 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19917)[0;0m INFO 08-09 12:41:27 [hpu_model_runner.py:3029] [Warmup][Graph/decode][109/126] batch_size:2 query_len:1 num_blocks:512 free_mem:17.36 GiB
INFO 08-09 12:41:32 [hpu_model_runner.py:3029] [Warmup][Graph/decode][110/126] batch_size:2 query_len:1 num_blocks:384 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19917)[0;0m INFO 08-09 12:41:32 [hpu_model_runner.py:3029] [Warmup][Graph/decode][110/126] batch_size:2 query_len:1 num_blocks:384 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19916)[0;0m INFO 08-09 12:41:32 [hpu_model_runner.py:3029] [Warmup][Graph/decode][110/126] batch_size:2 query_len:1 num_blocks:384 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19918)[0;0m INFO 08-09 12:41:32 [hpu_model_runner.py:3029] [Warmup][Graph/decode][110/126] batch_size:2 query_len:1 num_blocks:384 free_mem:17.36 GiB
INFO 08-09 12:41:37 [hpu_model_runner.py:3029] [Warmup][Graph/decode][111/126] batch_size:2 query_len:1 num_blocks:256 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19917)[0;0m INFO 08-09 12:41:37 [hpu_model_runner.py:3029] [Warmup][Graph/decode][111/126] batch_size:2 query_len:1 num_blocks:256 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19918)[0;0m INFO 08-09 12:41:37 [hpu_model_runner.py:3029] [Warmup][Graph/decode][111/126] batch_size:2 query_len:1 num_blocks:256 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19916)[0;0m INFO 08-09 12:41:37 [hpu_model_runner.py:3029] [Warmup][Graph/decode][111/126] batch_size:2 query_len:1 num_blocks:256 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19918)[0;0m INFO 08-09 12:41:43 [hpu_model_runner.py:3029] [Warmup][Graph/decode][112/126] batch_size:2 query_len:1 num_blocks:128 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19916)[0;0m INFO 08-09 12:41:43 [hpu_model_runner.py:3029] [Warmup][Graph/decode][112/126] batch_size:2 query_len:1 num_blocks:128 free_mem:17.36 GiB
INFO 08-09 12:41:43 [hpu_model_runner.py:3029] [Warmup][Graph/decode][112/126] batch_size:2 query_len:1 num_blocks:128 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19917)[0;0m INFO 08-09 12:41:43 [hpu_model_runner.py:3029] [Warmup][Graph/decode][112/126] batch_size:2 query_len:1 num_blocks:128 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19917)[0;0m INFO 08-09 12:41:48 [hpu_model_runner.py:3029] [Warmup][Graph/decode][113/126] batch_size:1 query_len:1 num_blocks:4733 free_mem:17.36 GiB
INFO 08-09 12:41:48 [hpu_model_runner.py:3029] [Warmup][Graph/decode][113/126] batch_size:1 query_len:1 num_blocks:4733 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19918)[0;0m INFO 08-09 12:41:48 [hpu_model_runner.py:3029] [Warmup][Graph/decode][113/126] batch_size:1 query_len:1 num_blocks:4733 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19916)[0;0m INFO 08-09 12:41:48 [hpu_model_runner.py:3029] [Warmup][Graph/decode][113/126] batch_size:1 query_len:1 num_blocks:4733 free_mem:17.36 GiB
INFO 08-09 12:41:53 [hpu_model_runner.py:3029] [Warmup][Graph/decode][114/126] batch_size:1 query_len:1 num_blocks:3712 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19918)[0;0m INFO 08-09 12:41:53 [hpu_model_runner.py:3029] [Warmup][Graph/decode][114/126] batch_size:1 query_len:1 num_blocks:3712 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19917)[0;0m INFO 08-09 12:41:53 [hpu_model_runner.py:3029] [Warmup][Graph/decode][114/126] batch_size:1 query_len:1 num_blocks:3712 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19916)[0;0m INFO 08-09 12:41:53 [hpu_model_runner.py:3029] [Warmup][Graph/decode][114/126] batch_size:1 query_len:1 num_blocks:3712 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19918)[0;0m INFO 08-09 12:41:58 [hpu_model_runner.py:3029] [Warmup][Graph/decode][115/126] batch_size:1 query_len:1 num_blocks:2816 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19917)[0;0m INFO 08-09 12:41:58 [hpu_model_runner.py:3029] [Warmup][Graph/decode][115/126] batch_size:1 query_len:1 num_blocks:2816 free_mem:17.36 GiB
INFO 08-09 12:41:58 [hpu_model_runner.py:3029] [Warmup][Graph/decode][115/126] batch_size:1 query_len:1 num_blocks:2816 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19916)[0;0m INFO 08-09 12:41:58 [hpu_model_runner.py:3029] [Warmup][Graph/decode][115/126] batch_size:1 query_len:1 num_blocks:2816 free_mem:17.36 GiB
INFO 08-09 12:42:04 [hpu_model_runner.py:3029] [Warmup][Graph/decode][116/126] batch_size:1 query_len:1 num_blocks:2176 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19916)[0;0m INFO 08-09 12:42:04 [hpu_model_runner.py:3029] [Warmup][Graph/decode][116/126] batch_size:1 query_len:1 num_blocks:2176 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19918)[0;0m INFO 08-09 12:42:04 [hpu_model_runner.py:3029] [Warmup][Graph/decode][116/126] batch_size:1 query_len:1 num_blocks:2176 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19917)[0;0m INFO 08-09 12:42:04 [hpu_model_runner.py:3029] [Warmup][Graph/decode][116/126] batch_size:1 query_len:1 num_blocks:2176 free_mem:17.36 GiB
INFO 08-09 12:42:09 [hpu_model_runner.py:3029] [Warmup][Graph/decode][117/126] batch_size:1 query_len:1 num_blocks:1664 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19918)[0;0m INFO 08-09 12:42:09 [hpu_model_runner.py:3029] [Warmup][Graph/decode][117/126] batch_size:1 query_len:1 num_blocks:1664 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19917)[0;0m INFO 08-09 12:42:09 [hpu_model_runner.py:3029] [Warmup][Graph/decode][117/126] batch_size:1 query_len:1 num_blocks:1664 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19916)[0;0m INFO 08-09 12:42:09 [hpu_model_runner.py:3029] [Warmup][Graph/decode][117/126] batch_size:1 query_len:1 num_blocks:1664 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19916)[0;0m INFO 08-09 12:42:14 [hpu_model_runner.py:3029] [Warmup][Graph/decode][118/126] batch_size:1 query_len:1 num_blocks:1280 free_mem:17.36 GiB
INFO 08-09 12:42:14 [hpu_model_runner.py:3029] [Warmup][Graph/decode][118/126] batch_size:1 query_len:1 num_blocks:1280 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19917)[0;0m INFO 08-09 12:42:14 [hpu_model_runner.py:3029] [Warmup][Graph/decode][118/126] batch_size:1 query_len:1 num_blocks:1280 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19918)[0;0m INFO 08-09 12:42:14 [hpu_model_runner.py:3029] [Warmup][Graph/decode][118/126] batch_size:1 query_len:1 num_blocks:1280 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19918)[0;0m INFO 08-09 12:42:19 [hpu_model_runner.py:3029] [Warmup][Graph/decode][119/126] batch_size:1 query_len:1 num_blocks:1024 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19917)[0;0m INFO 08-09 12:42:19 [hpu_model_runner.py:3029] [Warmup][Graph/decode][119/126] batch_size:1 query_len:1 num_blocks:1024 free_mem:17.36 GiB
INFO 08-09 12:42:19 [hpu_model_runner.py:3029] [Warmup][Graph/decode][119/126] batch_size:1 query_len:1 num_blocks:1024 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19916)[0;0m INFO 08-09 12:42:19 [hpu_model_runner.py:3029] [Warmup][Graph/decode][119/126] batch_size:1 query_len:1 num_blocks:1024 free_mem:17.36 GiB
INFO 08-09 12:42:25 [hpu_model_runner.py:3029] [Warmup][Graph/decode][120/126] batch_size:1 query_len:1 num_blocks:896 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19916)[0;0m INFO 08-09 12:42:25 [hpu_model_runner.py:3029] [Warmup][Graph/decode][120/126] batch_size:1 query_len:1 num_blocks:896 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19918)[0;0m INFO 08-09 12:42:25 [hpu_model_runner.py:3029] [Warmup][Graph/decode][120/126] batch_size:1 query_len:1 num_blocks:896 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19917)[0;0m INFO 08-09 12:42:25 [hpu_model_runner.py:3029] [Warmup][Graph/decode][120/126] batch_size:1 query_len:1 num_blocks:896 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19918)[0;0m INFO 08-09 12:42:30 [hpu_model_runner.py:3029] [Warmup][Graph/decode][121/126] batch_size:1 query_len:1 num_blocks:768 free_mem:17.36 GiB
INFO 08-09 12:42:30 [hpu_model_runner.py:3029] [Warmup][Graph/decode][121/126] batch_size:1 query_len:1 num_blocks:768 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19916)[0;0m INFO 08-09 12:42:30 [hpu_model_runner.py:3029] [Warmup][Graph/decode][121/126] batch_size:1 query_len:1 num_blocks:768 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19917)[0;0m INFO 08-09 12:42:30 [hpu_model_runner.py:3029] [Warmup][Graph/decode][121/126] batch_size:1 query_len:1 num_blocks:768 free_mem:17.36 GiB
INFO 08-09 12:42:35 [hpu_model_runner.py:3029] [Warmup][Graph/decode][122/126] batch_size:1 query_len:1 num_blocks:640 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19918)[0;0m INFO 08-09 12:42:35 [hpu_model_runner.py:3029] [Warmup][Graph/decode][122/126] batch_size:1 query_len:1 num_blocks:640 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19916)[0;0m INFO 08-09 12:42:35 [hpu_model_runner.py:3029] [Warmup][Graph/decode][122/126] batch_size:1 query_len:1 num_blocks:640 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19917)[0;0m INFO 08-09 12:42:35 [hpu_model_runner.py:3029] [Warmup][Graph/decode][122/126] batch_size:1 query_len:1 num_blocks:640 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19918)[0;0m INFO 08-09 12:42:41 [hpu_model_runner.py:3029] [Warmup][Graph/decode][123/126] batch_size:1 query_len:1 num_blocks:512 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19917)[0;0m INFO 08-09 12:42:41 [hpu_model_runner.py:3029] [Warmup][Graph/decode][123/126] batch_size:1 query_len:1 num_blocks:512 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19916)[0;0m INFO 08-09 12:42:41 [hpu_model_runner.py:3029] [Warmup][Graph/decode][123/126] batch_size:1 query_len:1 num_blocks:512 free_mem:17.36 GiB
INFO 08-09 12:42:41 [hpu_model_runner.py:3029] [Warmup][Graph/decode][123/126] batch_size:1 query_len:1 num_blocks:512 free_mem:17.36 GiB
INFO 08-09 12:42:46 [hpu_model_runner.py:3029] [Warmup][Graph/decode][124/126] batch_size:1 query_len:1 num_blocks:384 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19918)[0;0m INFO 08-09 12:42:46 [hpu_model_runner.py:3029] [Warmup][Graph/decode][124/126] batch_size:1 query_len:1 num_blocks:384 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19916)[0;0m INFO 08-09 12:42:46 [hpu_model_runner.py:3029] [Warmup][Graph/decode][124/126] batch_size:1 query_len:1 num_blocks:384 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19917)[0;0m INFO 08-09 12:42:46 [hpu_model_runner.py:3029] [Warmup][Graph/decode][124/126] batch_size:1 query_len:1 num_blocks:384 free_mem:17.36 GiB
INFO 08-09 12:42:51 [hpu_model_runner.py:3029] [Warmup][Graph/decode][125/126] batch_size:1 query_len:1 num_blocks:256 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19918)[0;0m INFO 08-09 12:42:51 [hpu_model_runner.py:3029] [Warmup][Graph/decode][125/126] batch_size:1 query_len:1 num_blocks:256 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19916)[0;0m INFO 08-09 12:42:51 [hpu_model_runner.py:3029] [Warmup][Graph/decode][125/126] batch_size:1 query_len:1 num_blocks:256 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19917)[0;0m INFO 08-09 12:42:51 [hpu_model_runner.py:3029] [Warmup][Graph/decode][125/126] batch_size:1 query_len:1 num_blocks:256 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19917)[0;0m INFO 08-09 12:42:57 [hpu_model_runner.py:3029] [Warmup][Graph/decode][126/126] batch_size:1 query_len:1 num_blocks:128 free_mem:17.36 GiB
INFO 08-09 12:42:57 [hpu_model_runner.py:3029] [Warmup][Graph/decode][126/126] batch_size:1 query_len:1 num_blocks:128 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19918)[0;0m INFO 08-09 12:42:57 [hpu_model_runner.py:3029] [Warmup][Graph/decode][126/126] batch_size:1 query_len:1 num_blocks:128 free_mem:17.36 GiB
[1;36m(VllmWorkerProcess pid=19916)[0;0m INFO 08-09 12:42:57 [hpu_model_runner.py:3029] [Warmup][Graph/decode][126/126] batch_size:1 query_len:1 num_blocks:128 free_mem:17.36 GiB
INFO 08-09 12:43:02 [hpu_model_runner.py:3152] Prompt captured:38 (100.0%) used_mem:0 B
[1;36m(VllmWorkerProcess pid=19918)[0;0m INFO 08-09 12:43:02 [hpu_model_runner.py:3152] Prompt captured:38 (100.0%) used_mem:0 B
INFO 08-09 12:43:02 [hpu_model_runner.py:3152] Decode captured:126 (100.0%) used_mem:4 KiB
[1;36m(VllmWorkerProcess pid=19918)[0;0m INFO 08-09 12:43:02 [hpu_model_runner.py:3152] Decode captured:126 (100.0%) used_mem:4 KiB
INFO 08-09 12:43:02 [hpu_model_runner.py:3280] Warmup finished in 613 secs, allocated 0 B of device memory
[1;36m(VllmWorkerProcess pid=19918)[0;0m INFO 08-09 12:43:02 [hpu_model_runner.py:3280] Warmup finished in 613 secs, allocated 0 B of device memory
[1;36m(VllmWorkerProcess pid=19917)[0;0m INFO 08-09 12:43:02 [hpu_model_runner.py:3152] Prompt captured:38 (100.0%) used_mem:0 B
[1;36m(VllmWorkerProcess pid=19916)[0;0m INFO 08-09 12:43:02 [hpu_model_runner.py:3152] Prompt captured:38 (100.0%) used_mem:0 B
[1;36m(VllmWorkerProcess pid=19917)[0;0m INFO 08-09 12:43:02 [hpu_model_runner.py:3152] Decode captured:126 (100.0%) used_mem:4 KiB
[1;36m(VllmWorkerProcess pid=19916)[0;0m INFO 08-09 12:43:02 [hpu_model_runner.py:3152] Decode captured:126 (100.0%) used_mem:4 KiB
[1;36m(VllmWorkerProcess pid=19917)[0;0m INFO 08-09 12:43:02 [hpu_model_runner.py:3280] Warmup finished in 613 secs, allocated 0 B of device memory
[1;36m(VllmWorkerProcess pid=19916)[0;0m INFO 08-09 12:43:02 [hpu_model_runner.py:3280] Warmup finished in 613 secs, allocated 0 B of device memory
INFO 08-09 12:43:02 [llm_engine.py:431] init engine (profile, create kv cache, warmup model) took 617.57 seconds
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
WARNING 08-09 12:43:02 [config.py:1364] Default sampling parameters have been overridden by the model's Hugging Face generation config recommended from the model creator. If this is not intended, please relaunch vLLM instance with `--generation-config vllm`.
INFO 08-09 12:43:02 [serving_chat.py:118] Using default chat sampling params from model: {'temperature': 0.9, 'top_p': 0.6}
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
INFO 08-09 12:43:02 [serving_completion.py:66] Using default completion sampling params from model: {'temperature': 0.9, 'top_p': 0.6}
INFO 08-09 12:43:02 [api_server.py:1351] Starting vLLM API server 0 on http://0.0.0.0:8000
INFO 08-09 12:43:02 [launcher.py:29] Available routes are:
INFO 08-09 12:43:02 [launcher.py:37] Route: /openapi.json, Methods: HEAD, GET
INFO 08-09 12:43:02 [launcher.py:37] Route: /docs, Methods: HEAD, GET
INFO 08-09 12:43:02 [launcher.py:37] Route: /docs/oauth2-redirect, Methods: HEAD, GET
INFO 08-09 12:43:02 [launcher.py:37] Route: /redoc, Methods: HEAD, GET
INFO 08-09 12:43:02 [launcher.py:37] Route: /health, Methods: GET
INFO 08-09 12:43:02 [launcher.py:37] Route: /load, Methods: GET
INFO 08-09 12:43:02 [launcher.py:37] Route: /ping, Methods: POST
INFO 08-09 12:43:02 [launcher.py:37] Route: /ping, Methods: GET
INFO 08-09 12:43:02 [launcher.py:37] Route: /tokenize, Methods: POST
INFO 08-09 12:43:02 [launcher.py:37] Route: /detokenize, Methods: POST
INFO 08-09 12:43:02 [launcher.py:37] Route: /v1/models, Methods: GET
INFO 08-09 12:43:02 [launcher.py:37] Route: /version, Methods: GET
INFO 08-09 12:43:02 [launcher.py:37] Route: /v1/chat/completions, Methods: POST
INFO 08-09 12:43:02 [launcher.py:37] Route: /v1/completions, Methods: POST
INFO 08-09 12:43:02 [launcher.py:37] Route: /v1/embeddings, Methods: POST
INFO 08-09 12:43:02 [launcher.py:37] Route: /pooling, Methods: POST
INFO 08-09 12:43:02 [launcher.py:37] Route: /classify, Methods: POST
INFO 08-09 12:43:02 [launcher.py:37] Route: /score, Methods: POST
INFO 08-09 12:43:02 [launcher.py:37] Route: /v1/score, Methods: POST
INFO 08-09 12:43:02 [launcher.py:37] Route: /v1/audio/transcriptions, Methods: POST
INFO 08-09 12:43:02 [launcher.py:37] Route: /rerank, Methods: POST
INFO 08-09 12:43:02 [launcher.py:37] Route: /v1/rerank, Methods: POST
INFO 08-09 12:43:02 [launcher.py:37] Route: /v2/rerank, Methods: POST
INFO 08-09 12:43:02 [launcher.py:37] Route: /invocations, Methods: POST
INFO 08-09 12:43:02 [launcher.py:37] Route: /metrics, Methods: GET
INFO:     Started server process [19514]
INFO:     Waiting for application startup.
INFO:     Application startup complete.
INFO 08-09 12:48:29 [logger.py:43] Received request cmpl-a4b51e98eab74a118324384df5b1c74f-0: prompt: 'What is LLaMA 2 and how does it work?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 365, 5661, 1529, 29871, 29906, 322, 920, 947, 372, 664, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:42758 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 12:48:29 [engine.py:331] Added request cmpl-a4b51e98eab74a118324384df5b1c74f-0.
INFO 08-09 12:48:31 [metrics.py:417] Avg prompt throughput: 1.7 tokens/s, Avg generation throughput: 0.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 12:48:36 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 62.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 12:48:41 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 90.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-09 12:48:41 [logger.py:43] Received request cmpl-9296855896d64071a95c1370e8bc2ef7-0: prompt: 'Explain the architecture of a transformer model.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 12027, 7420, 278, 11258, 310, 263, 4327, 261, 1904, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:59080 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 12:48:41 [engine.py:331] Added request cmpl-9296855896d64071a95c1370e8bc2ef7-0.
INFO 08-09 12:48:46 [metrics.py:417] Avg prompt throughput: 2.2 tokens/s, Avg generation throughput: 96.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 12:48:51 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 97.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-09 12:48:51 [logger.py:43] Received request cmpl-2553b633265846569f33dac7c0756b72-0: prompt: 'What are the core differences between CNN and RNN?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 526, 278, 7136, 12651, 1546, 29696, 322, 390, 10262, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:52830 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 12:48:51 [engine.py:331] Added request cmpl-2553b633265846569f33dac7c0756b72-0.
INFO 08-09 12:48:56 [metrics.py:417] Avg prompt throughput: 2.4 tokens/s, Avg generation throughput: 96.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 12:49:01 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 92.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-09 12:49:02 [logger.py:43] Received request cmpl-cbc1eb6af0fa46e4a42d12d37f900910-0: prompt: 'Describe how attention mechanism improves translation models.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 20355, 915, 920, 8570, 13336, 4857, 1960, 13962, 4733, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:44304 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 12:49:02 [engine.py:331] Added request cmpl-cbc1eb6af0fa46e4a42d12d37f900910-0.
INFO 08-09 12:49:06 [metrics.py:417] Avg prompt throughput: 2.2 tokens/s, Avg generation throughput: 94.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 12:49:11 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 94.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-09 12:49:12 [logger.py:43] Received request cmpl-c464be247ba2479f94a6523dc714700d-0: prompt: 'What is the role of positional encoding in transformers?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 278, 6297, 310, 2602, 284, 8025, 297, 4327, 414, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:55028 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 12:49:12 [engine.py:331] Added request cmpl-c464be247ba2479f94a6523dc714700d-0.
INFO 08-09 12:49:16 [metrics.py:417] Avg prompt throughput: 2.6 tokens/s, Avg generation throughput: 95.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 12:49:21 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 92.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-09 12:49:23 [logger.py:43] Received request cmpl-774fa9669a2c4359ae5bb4e43e8c7bab-0: prompt: 'How does self-attention differ from cross-attention?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 947, 1583, 29899, 1131, 2509, 1163, 515, 4891, 29899, 1131, 2509, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:43164 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 12:49:23 [engine.py:331] Added request cmpl-774fa9669a2c4359ae5bb4e43e8c7bab-0.
INFO 08-09 12:49:26 [metrics.py:417] Avg prompt throughput: 2.8 tokens/s, Avg generation throughput: 95.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 12:49:31 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 95.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 12:49:34 [logger.py:43] Received request cmpl-6f53d82f248741f7be44ac26faf91311-0: prompt: 'Explain the concept of tokenization in NLP.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 12027, 7420, 278, 6964, 310, 5993, 2133, 297, 405, 13208, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:60056 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 12:49:34 [engine.py:331] Added request cmpl-6f53d82f248741f7be44ac26faf91311-0.
INFO 08-09 12:49:36 [metrics.py:417] Avg prompt throughput: 2.4 tokens/s, Avg generation throughput: 94.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 12:49:41 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 97.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 12:49:44 [logger.py:43] Received request cmpl-f082164aa4854573a404e92391c224f4-0: prompt: 'What is the vanishing gradient problem and how to mitigate it?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 278, 1109, 14424, 16030, 1108, 322, 920, 304, 1380, 335, 403, 372, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:59266 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 12:49:44 [engine.py:331] Added request cmpl-f082164aa4854573a404e92391c224f4-0.
INFO 08-09 12:49:46 [metrics.py:417] Avg prompt throughput: 3.2 tokens/s, Avg generation throughput: 97.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 12:49:51 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 100.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 12:49:54 [logger.py:43] Received request cmpl-6e11fa4a7a994412befce993ddf1d322-0: prompt: 'Describe how Gaudi architecture differs from NVIDIA GPUs.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 20355, 915, 920, 10415, 4749, 11258, 2923, 414, 515, 405, 13044, 10764, 22796, 29879, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:59960 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 12:49:54 [engine.py:331] Added request cmpl-6e11fa4a7a994412befce993ddf1d322-0.
INFO 08-09 12:49:56 [metrics.py:417] Avg prompt throughput: 3.2 tokens/s, Avg generation throughput: 99.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 12:50:01 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 95.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 12:50:04 [logger.py:43] Received request cmpl-be1b7bfe1cc8432b974a89f606bbb2e5-0: prompt: 'What is model parallelism and how does it help scale large models?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 1904, 8943, 1608, 322, 920, 947, 372, 1371, 6287, 2919, 4733, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:38818 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 12:50:04 [engine.py:331] Added request cmpl-be1b7bfe1cc8432b974a89f606bbb2e5-0.
INFO 08-09 12:50:06 [metrics.py:417] Avg prompt throughput: 3.0 tokens/s, Avg generation throughput: 95.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 12:50:11 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 96.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 12:50:15 [logger.py:43] Received request cmpl-c7c21fdca91245a982f9e3a67cef5fec-0: prompt: 'Explain the difference between GPT and BERT architectures.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 12027, 7420, 278, 4328, 1546, 402, 7982, 322, 350, 20161, 6956, 1973, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:45974 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 12:50:15 [engine.py:331] Added request cmpl-c7c21fdca91245a982f9e3a67cef5fec-0.
INFO 08-09 12:50:16 [metrics.py:417] Avg prompt throughput: 2.8 tokens/s, Avg generation throughput: 97.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 12:50:21 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 96.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 12:50:25 [logger.py:43] Received request cmpl-8195c05215a94f2c99a44ce1c97657d3-0: prompt: 'How does prompt engineering affect language model outputs?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 947, 9508, 21639, 6602, 4086, 1904, 14391, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:54716 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 12:50:25 [engine.py:331] Added request cmpl-8195c05215a94f2c99a44ce1c97657d3-0.
INFO 08-09 12:50:26 [metrics.py:417] Avg prompt throughput: 2.0 tokens/s, Avg generation throughput: 94.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 12:50:31 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 95.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 12:50:36 [logger.py:43] Received request cmpl-1cfdb57f6a8241eebed4d5305f40b129-0: prompt: 'What is the function of layer normalization in deep networks?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 278, 740, 310, 7546, 4226, 2133, 297, 6483, 14379, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:50552 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 12:50:36 [engine.py:331] Added request cmpl-1cfdb57f6a8241eebed4d5305f40b129-0.
INFO 08-09 12:50:36 [metrics.py:417] Avg prompt throughput: 2.6 tokens/s, Avg generation throughput: 94.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 12:50:41 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 97.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 12:50:46 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 88.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-09 12:50:47 [logger.py:43] Received request cmpl-8b1f173441254e73855fe521b8fa7e26-0: prompt: 'Explain causal language modeling with examples.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 12027, 7420, 3269, 284, 4086, 1904, 292, 411, 6455, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:33942 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 12:50:47 [engine.py:331] Added request cmpl-8b1f173441254e73855fe521b8fa7e26-0.
INFO 08-09 12:50:51 [metrics.py:417] Avg prompt throughput: 2.2 tokens/s, Avg generation throughput: 94.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 12:50:56 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 97.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-09 12:50:57 [logger.py:43] Received request cmpl-a9bc89dbe00642eba0e43b413bae8472-0: prompt: 'What is quantization and how does it improve inference performance?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 4323, 2133, 322, 920, 947, 372, 11157, 27262, 4180, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:35656 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 12:50:57 [engine.py:331] Added request cmpl-a9bc89dbe00642eba0e43b413bae8472-0.
INFO 08-09 12:51:00 [logger.py:43] Received request cmpl-f168547eebd5482b8b542e5414834788-0: prompt: 'Describe how fine-tuning works for large language models.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 20355, 915, 920, 2691, 29899, 29873, 27964, 1736, 363, 2919, 4086, 4733, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:35670 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 12:51:00 [engine.py:331] Added request cmpl-f168547eebd5482b8b542e5414834788-0.
INFO 08-09 12:51:01 [metrics.py:417] Avg prompt throughput: 5.4 tokens/s, Avg generation throughput: 94.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 12:51:06 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 99.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 12:51:11 [logger.py:43] Received request cmpl-a4eb6492b9ae4fd6b57c4866b2596554-0: prompt: 'What are LoRA adapters and how do they help in parameter-efficient training?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 526, 4309, 4717, 594, 481, 2153, 322, 920, 437, 896, 1371, 297, 3443, 29899, 8462, 6694, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:56694 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 12:51:11 [engine.py:331] Added request cmpl-a4eb6492b9ae4fd6b57c4866b2596554-0.
INFO 08-09 12:51:11 [metrics.py:417] Avg prompt throughput: 3.8 tokens/s, Avg generation throughput: 96.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 12:51:16 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 100.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 12:51:21 [logger.py:43] Received request cmpl-c4a41552e3fc4d84814d9a1c0ee5c587-0: prompt: 'Explain the purpose of rotary positional embeddings.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 12027, 7420, 278, 6437, 310, 5731, 653, 2602, 284, 8297, 29881, 886, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:34078 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 12:51:21 [engine.py:331] Added request cmpl-c4a41552e3fc4d84814d9a1c0ee5c587-0.
INFO 08-09 12:51:21 [metrics.py:417] Avg prompt throughput: 2.8 tokens/s, Avg generation throughput: 96.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 12:51:24 [logger.py:43] Received request cmpl-a221d183ae22458f8b343a62c290020b-0: prompt: 'How does DeepSpeed improve model training efficiency?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 947, 21784, 26539, 11157, 1904, 6694, 19201, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:46466 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 12:51:24 [engine.py:331] Added request cmpl-a221d183ae22458f8b343a62c290020b-0.
INFO 08-09 12:51:26 [metrics.py:417] Avg prompt throughput: 2.0 tokens/s, Avg generation throughput: 94.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 12:51:31 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 97.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 12:51:34 [logger.py:43] Received request cmpl-a74d5c2d702448bd80e994894aed9a38-0: prompt: 'Compare data parallelism and pipeline parallelism.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 3831, 598, 848, 8943, 1608, 322, 16439, 8943, 1608, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:38542 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 12:51:34 [engine.py:331] Added request cmpl-a74d5c2d702448bd80e994894aed9a38-0.
INFO 08-09 12:51:36 [metrics.py:417] Avg prompt throughput: 2.2 tokens/s, Avg generation throughput: 96.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 12:51:41 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 90.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 12:51:45 [logger.py:43] Received request cmpl-af6304cd64f445c1ab30e879426690a0-0: prompt: 'What is ALiBi positional encoding?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 14445, 29875, 20517, 2602, 284, 8025, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:45200 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 12:51:45 [engine.py:331] Added request cmpl-af6304cd64f445c1ab30e879426690a0-0.
INFO 08-09 12:51:46 [metrics.py:417] Avg prompt throughput: 2.0 tokens/s, Avg generation throughput: 84.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 12:51:51 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 93.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 12:51:56 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 91.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-09 12:51:56 [logger.py:43] Received request cmpl-8ccb14caebd14385b911d9e0388abf8a-0: prompt: 'Describe the purpose of HF Transformers library.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 20355, 915, 278, 6437, 310, 379, 29943, 4103, 689, 414, 3489, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:54346 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 12:51:56 [engine.py:331] Added request cmpl-8ccb14caebd14385b911d9e0388abf8a-0.
INFO 08-09 12:52:01 [metrics.py:417] Avg prompt throughput: 2.6 tokens/s, Avg generation throughput: 89.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 12:52:06 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 94.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-09 12:52:07 [logger.py:43] Received request cmpl-989f6b5124424af8b12b6cf29f36eac4-0: prompt: 'How do attention heads capture contextual meaning?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 437, 8570, 15883, 10446, 3030, 950, 6593, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:54280 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 12:52:07 [engine.py:331] Added request cmpl-989f6b5124424af8b12b6cf29f36eac4-0.
INFO 08-09 12:52:11 [metrics.py:417] Avg prompt throughput: 2.0 tokens/s, Avg generation throughput: 91.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 12:52:13 [logger.py:43] Received request cmpl-dd34c982834b4a2c84aec132948542cd-0: prompt: 'What is dynamic batching in inference servers?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 7343, 9853, 292, 297, 27262, 12424, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:45904 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 12:52:13 [engine.py:331] Added request cmpl-dd34c982834b4a2c84aec132948542cd-0.
INFO 08-09 12:52:16 [metrics.py:417] Avg prompt throughput: 2.0 tokens/s, Avg generation throughput: 91.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 12:52:21 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 92.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-09 12:52:23 [logger.py:43] Received request cmpl-aef6bd05b630408db41884ea79886deb-0: prompt: 'Explain greedy decoding vs beam search vs sampling.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 12027, 7420, 1395, 7584, 1602, 3689, 7186, 22913, 2740, 7186, 23460, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:56246 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 12:52:23 [engine.py:331] Added request cmpl-aef6bd05b630408db41884ea79886deb-0.
INFO 08-09 12:52:26 [metrics.py:417] Avg prompt throughput: 2.6 tokens/s, Avg generation throughput: 91.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 12:52:31 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 91.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 12:52:34 [logger.py:43] Received request cmpl-bc3bf08be4084d7bad1e3656e72c93f5-0: prompt: 'What is perplexity in language models and why is it important?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 639, 10709, 537, 297, 4086, 4733, 322, 2020, 338, 372, 4100, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:52070 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 12:52:34 [engine.py:331] Added request cmpl-bc3bf08be4084d7bad1e3656e72c93f5-0.
INFO 08-09 12:52:36 [metrics.py:417] Avg prompt throughput: 3.0 tokens/s, Avg generation throughput: 93.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 12:52:41 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 93.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 12:52:45 [logger.py:43] Received request cmpl-021824cf227149a1b6c8c96df01d66a3-0: prompt: 'How does mixed precision training benefit model training?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 947, 12849, 16716, 6694, 14169, 1904, 6694, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:52880 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 12:52:45 [engine.py:331] Added request cmpl-021824cf227149a1b6c8c96df01d66a3-0.
INFO 08-09 12:52:46 [metrics.py:417] Avg prompt throughput: 2.0 tokens/s, Avg generation throughput: 88.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 12:52:51 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 93.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 12:52:56 [logger.py:43] Received request cmpl-759d12ccf7ff49df86dfd8853f0a3867-0: prompt: 'What is the function of the softmax layer in transformers?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 278, 740, 310, 278, 4964, 3317, 7546, 297, 4327, 414, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:49794 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 12:52:56 [engine.py:331] Added request cmpl-759d12ccf7ff49df86dfd8853f0a3867-0.
INFO 08-09 12:52:56 [metrics.py:417] Avg prompt throughput: 2.8 tokens/s, Avg generation throughput: 99.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 12:53:01 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 89.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 12:53:06 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 92.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-09 12:53:07 [logger.py:43] Received request cmpl-f0554b56ff954e13887b9a1c4c656d41-0: prompt: "Explain embeddings and how they're used in NLP models.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 12027, 7420, 8297, 29881, 886, 322, 920, 896, 29915, 276, 1304, 297, 405, 13208, 4733, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:39772 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 12:53:07 [engine.py:331] Added request cmpl-f0554b56ff954e13887b9a1c4c656d41-0.
INFO 08-09 12:53:11 [metrics.py:417] Avg prompt throughput: 3.4 tokens/s, Avg generation throughput: 94.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 12:53:16 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 90.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-09 12:53:17 [logger.py:43] Received request cmpl-c9a65519ed1a4ddf88cf880215e2f703-0: prompt: "Describe the function of the tokenizer's vocabulary.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 20355, 915, 278, 740, 310, 278, 5993, 3950, 29915, 29879, 7931, 370, 352, 653, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:37416 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 12:53:17 [engine.py:331] Added request cmpl-c9a65519ed1a4ddf88cf880215e2f703-0.
INFO 08-09 12:53:21 [metrics.py:417] Avg prompt throughput: 3.2 tokens/s, Avg generation throughput: 93.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 12:53:26 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 90.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-09 12:53:28 [logger.py:43] Received request cmpl-c466dd0117814373bfd312231a43598f-0: prompt: 'What is causal masking in transformer decoders?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 3269, 284, 11105, 292, 297, 4327, 261, 1602, 397, 414, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:47038 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 12:53:28 [engine.py:331] Added request cmpl-c466dd0117814373bfd312231a43598f-0.
INFO 08-09 12:53:31 [metrics.py:417] Avg prompt throughput: 2.8 tokens/s, Avg generation throughput: 97.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 12:53:36 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 100.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-09 12:53:38 [logger.py:43] Received request cmpl-a5d015b2415b4b4ea73e176f521c7b68-0: prompt: 'How does gradient checkpointing reduce memory usage?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 947, 16030, 1423, 3149, 292, 10032, 3370, 8744, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:53280 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 12:53:38 [engine.py:331] Added request cmpl-a5d015b2415b4b4ea73e176f521c7b68-0.
INFO 08-09 12:53:41 [metrics.py:417] Avg prompt throughput: 2.2 tokens/s, Avg generation throughput: 97.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 12:53:46 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 96.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 12:53:49 [logger.py:43] Received request cmpl-e367f40e016d4b248c93f82eb25abc4a-0: prompt: 'Explain the difference between encoder-decoder and decoder-only models.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 12027, 7420, 278, 4328, 1546, 2094, 6119, 29899, 7099, 6119, 322, 1602, 6119, 29899, 6194, 4733, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:58004 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 12:53:49 [engine.py:331] Added request cmpl-e367f40e016d4b248c93f82eb25abc4a-0.
INFO 08-09 12:53:51 [metrics.py:417] Avg prompt throughput: 3.6 tokens/s, Avg generation throughput: 99.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 12:53:56 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 96.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 12:53:59 [logger.py:43] Received request cmpl-68f8145e5cc44b87827445b3b6ee6291-0: prompt: 'What is zero-shot vs few-shot vs fine-tuned inference?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 5225, 29899, 8962, 7186, 2846, 29899, 8962, 7186, 2691, 29899, 29873, 348, 287, 27262, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:49954 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 12:53:59 [engine.py:331] Added request cmpl-68f8145e5cc44b87827445b3b6ee6291-0.
INFO 08-09 12:54:01 [logger.py:43] Received request cmpl-51abaf148301466ab10f38d6831a7a9d-0: prompt: 'What are prompt templates and why do they matter?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 526, 9508, 17475, 322, 2020, 437, 896, 4383, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:49960 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 12:54:01 [engine.py:331] Added request cmpl-51abaf148301466ab10f38d6831a7a9d-0.
INFO 08-09 12:54:01 [metrics.py:417] Avg prompt throughput: 5.8 tokens/s, Avg generation throughput: 98.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 12:54:06 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 99.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 12:54:11 [logger.py:43] Received request cmpl-29bfe29d433845b9ab926b09e6dfa12b-0: prompt: 'Explain the role of BOS and EOS tokens in transformers.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 12027, 7420, 278, 6297, 310, 350, 3267, 322, 382, 3267, 18897, 297, 4327, 414, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:51528 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 12:54:11 [engine.py:331] Added request cmpl-29bfe29d433845b9ab926b09e6dfa12b-0.
INFO 08-09 12:54:11 [metrics.py:417] Avg prompt throughput: 3.2 tokens/s, Avg generation throughput: 97.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 12:54:16 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 97.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 12:54:21 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 96.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-09 12:54:21 [logger.py:43] Received request cmpl-b3c56a9b3f2842f59aee23780848b370-0: prompt: 'What is a floating-point tensor and how is it represented?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 263, 16526, 29899, 3149, 12489, 322, 920, 338, 372, 9875, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:51366 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 12:54:21 [engine.py:331] Added request cmpl-b3c56a9b3f2842f59aee23780848b370-0.
INFO 08-09 12:54:26 [metrics.py:417] Avg prompt throughput: 2.8 tokens/s, Avg generation throughput: 95.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 12:54:31 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 95.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-09 12:54:32 [logger.py:43] Received request cmpl-be9bc143949c4e228ed8899eeab18d31-0: prompt: 'What are hypernetworks in neural architectures?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 526, 11266, 11618, 29879, 297, 19677, 6956, 1973, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:53324 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 12:54:32 [engine.py:331] Added request cmpl-be9bc143949c4e228ed8899eeab18d31-0.
INFO 08-09 12:54:36 [metrics.py:417] Avg prompt throughput: 2.2 tokens/s, Avg generation throughput: 94.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 12:54:41 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 98.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-09 12:54:42 [logger.py:43] Received request cmpl-04682a3ae2a04200a953f6c8a8f1c4e6-0: prompt: 'Explain MoE (Mixture of Experts) architecture.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 12027, 7420, 4546, 29923, 313, 29924, 29875, 15546, 310, 28224, 1372, 29897, 11258, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:57108 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 12:54:42 [engine.py:331] Added request cmpl-04682a3ae2a04200a953f6c8a8f1c4e6-0.
INFO 08-09 12:54:46 [metrics.py:417] Avg prompt throughput: 3.0 tokens/s, Avg generation throughput: 98.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 12:54:51 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 98.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-09 12:54:52 [logger.py:43] Received request cmpl-2cc31f36982a464b920da725c886d98d-0: prompt: 'What is distillation in model compression?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 1320, 453, 362, 297, 1904, 24221, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:39726 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 12:54:52 [engine.py:331] Added request cmpl-2cc31f36982a464b920da725c886d98d-0.
INFO 08-09 12:54:56 [metrics.py:417] Avg prompt throughput: 2.0 tokens/s, Avg generation throughput: 97.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 12:55:01 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 94.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-09 12:55:03 [logger.py:43] Received request cmpl-ff26e41463454b46964d9500ea105f82-0: prompt: 'How do you implement top-k and top-p sampling?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 437, 366, 2334, 2246, 29899, 29895, 322, 2246, 29899, 29886, 23460, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:56480 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 12:55:03 [engine.py:331] Added request cmpl-ff26e41463454b46964d9500ea105f82-0.
INFO 08-09 12:55:06 [metrics.py:417] Avg prompt throughput: 2.8 tokens/s, Avg generation throughput: 91.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 12:55:11 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 96.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-09 12:55:14 [logger.py:43] Received request cmpl-46cf30c859aa4ef0933b7aac4be1ac03-0: prompt: 'What is a vector database and how is it used with LLMs?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 263, 4608, 2566, 322, 920, 338, 372, 1304, 411, 365, 26369, 29879, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:36316 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 12:55:14 [engine.py:331] Added request cmpl-46cf30c859aa4ef0933b7aac4be1ac03-0.
INFO 08-09 12:55:16 [metrics.py:417] Avg prompt throughput: 3.2 tokens/s, Avg generation throughput: 87.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 12:55:21 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 97.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 12:55:24 [logger.py:43] Received request cmpl-a2f8634448f64668b352e392891fb261-0: prompt: 'Describe how you would use RAG (Retrieval Augmented Generation).', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 20355, 915, 920, 366, 723, 671, 390, 10051, 313, 8015, 2546, 791, 22333, 358, 287, 28203, 467], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:55990 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 12:55:24 [engine.py:331] Added request cmpl-a2f8634448f64668b352e392891fb261-0.
INFO 08-09 12:55:26 [metrics.py:417] Avg prompt throughput: 3.6 tokens/s, Avg generation throughput: 100.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 12:55:31 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 100.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 12:55:34 [logger.py:43] Received request cmpl-1823667adb8641e59c0c8fabfecc2367-0: prompt: 'What is streaming inference and how is it used?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 24820, 27262, 322, 920, 338, 372, 1304, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:58426 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 12:55:34 [engine.py:331] Added request cmpl-1823667adb8641e59c0c8fabfecc2367-0.
INFO 08-09 12:55:36 [metrics.py:417] Avg prompt throughput: 2.2 tokens/s, Avg generation throughput: 98.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 12:55:41 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 98.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 12:55:44 [logger.py:43] Received request cmpl-8cfb2474a9694a7a8fd8de25b65b4925-0: prompt: 'How does TTFT relate to user-perceived latency?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 947, 323, 29911, 7818, 29279, 304, 1404, 29899, 546, 346, 2347, 23316, 1270, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:51492 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 12:55:44 [engine.py:331] Added request cmpl-8cfb2474a9694a7a8fd8de25b65b4925-0.
INFO 08-09 12:55:46 [metrics.py:417] Avg prompt throughput: 3.2 tokens/s, Avg generation throughput: 99.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 12:55:51 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 97.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 12:55:54 [logger.py:43] Received request cmpl-0c23d4f7559a4cef908e4e37a06c6cfa-0: prompt: 'Explain transformer decoder blocks layer-by-layer.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 12027, 7420, 4327, 261, 1602, 6119, 10930, 7546, 29899, 1609, 29899, 13148, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:57614 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 12:55:54 [engine.py:331] Added request cmpl-0c23d4f7559a4cef908e4e37a06c6cfa-0.
INFO 08-09 12:55:56 [metrics.py:417] Avg prompt throughput: 2.8 tokens/s, Avg generation throughput: 95.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 12:56:01 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 96.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 12:56:05 [logger.py:43] Received request cmpl-1a0d609b9f3641d5a034c4c292031e7a-0: prompt: 'What is a KV cache in transformer inference?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 263, 476, 29963, 7090, 297, 4327, 261, 27262, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:39598 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 12:56:05 [engine.py:331] Added request cmpl-1a0d609b9f3641d5a034c4c292031e7a-0.
INFO 08-09 12:56:06 [metrics.py:417] Avg prompt throughput: 2.4 tokens/s, Avg generation throughput: 95.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 12:56:11 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 96.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 12:56:15 [logger.py:43] Received request cmpl-b009852675b440edb536d61d6f211908-0: prompt: 'Describe token-by-token generation and its challenges.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 20355, 915, 5993, 29899, 1609, 29899, 6979, 12623, 322, 967, 18066, 267, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:57848 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 12:56:15 [engine.py:331] Added request cmpl-b009852675b440edb536d61d6f211908-0.
INFO 08-09 12:56:16 [metrics.py:417] Avg prompt throughput: 2.8 tokens/s, Avg generation throughput: 95.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 12:56:21 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 95.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 12:56:26 [logger.py:43] Received request cmpl-76cdfcd0cd644c3f82f243e217e3780c-0: prompt: 'How does LLaMA 2 compare to GPT-3.5?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 947, 365, 5661, 1529, 29871, 29906, 7252, 304, 402, 7982, 29899, 29941, 29889, 29945, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:57850 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 12:56:26 [engine.py:331] Added request cmpl-76cdfcd0cd644c3f82f243e217e3780c-0.
INFO 08-09 12:56:26 [metrics.py:417] Avg prompt throughput: 3.4 tokens/s, Avg generation throughput: 95.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 12:56:31 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 98.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 12:56:36 [logger.py:43] Received request cmpl-ea8cb7f659c848c4990118ca54bcc91c-0: prompt: 'Explain the training data pipeline for a large model.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 12027, 7420, 278, 6694, 848, 16439, 363, 263, 2919, 1904, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:33514 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 12:56:36 [engine.py:331] Added request cmpl-ea8cb7f659c848c4990118ca54bcc91c-0.
INFO 08-09 12:56:36 [metrics.py:417] Avg prompt throughput: 2.4 tokens/s, Avg generation throughput: 98.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 12:56:41 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 96.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 12:56:46 [logger.py:43] Received request cmpl-62c85a7fa4954cf9ac359a50ed36f489-0: prompt: 'What are the ethical concerns with generative models?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 526, 278, 11314, 936, 21838, 411, 1176, 1230, 4733, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:58078 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 12:56:46 [engine.py:331] Added request cmpl-62c85a7fa4954cf9ac359a50ed36f489-0.
INFO 08-09 12:56:46 [metrics.py:417] Avg prompt throughput: 2.4 tokens/s, Avg generation throughput: 98.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 12:56:51 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 96.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 12:56:56 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 92.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-09 12:56:57 [logger.py:43] Received request cmpl-27dc7b7471f34733a24c74467e20f637-0: prompt: 'How do transformers process multilingual text?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 437, 4327, 414, 1889, 1773, 6504, 950, 1426, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:33530 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 12:56:57 [engine.py:331] Added request cmpl-27dc7b7471f34733a24c74467e20f637-0.
INFO 08-09 12:57:01 [metrics.py:417] Avg prompt throughput: 2.2 tokens/s, Avg generation throughput: 90.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 12:57:06 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 91.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-09 12:57:08 [logger.py:43] Received request cmpl-3104a82624274230bb9c1c48f955eaec-0: prompt: 'What are flash attention mechanisms?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 526, 11013, 8570, 7208, 12903, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:38970 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 12:57:08 [engine.py:331] Added request cmpl-3104a82624274230bb9c1c48f955eaec-0.
INFO 08-09 12:57:11 [metrics.py:417] Avg prompt throughput: 1.6 tokens/s, Avg generation throughput: 94.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 12:57:16 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 94.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-09 12:57:18 [logger.py:43] Received request cmpl-557f6c0aff1042e4900bbb8ca20af151-0: prompt: 'Describe how memory-efficient attention works.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 20355, 915, 920, 3370, 29899, 8462, 8570, 1736, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:47452 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 12:57:18 [engine.py:331] Added request cmpl-557f6c0aff1042e4900bbb8ca20af151-0.
INFO 08-09 12:57:21 [metrics.py:417] Avg prompt throughput: 2.0 tokens/s, Avg generation throughput: 93.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 12:57:26 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 92.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 12:57:29 [logger.py:43] Received request cmpl-411632b16d2a419aa09904e086cd92ea-0: prompt: 'What is inference throughput and how is it calculated?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 27262, 1549, 649, 322, 920, 338, 372, 12833, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:58564 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 12:57:29 [engine.py:331] Added request cmpl-411632b16d2a419aa09904e086cd92ea-0.
INFO 08-09 12:57:31 [metrics.py:417] Avg prompt throughput: 2.4 tokens/s, Avg generation throughput: 97.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 12:57:36 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 94.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 12:57:39 [logger.py:43] Received request cmpl-dae33c673d7f4542bbe006fa6cc2716a-0: prompt: 'Explain how masked self-attention works in BERT.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 12027, 7420, 920, 11105, 287, 1583, 29899, 1131, 2509, 1736, 297, 350, 20161, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:48362 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 12:57:39 [engine.py:331] Added request cmpl-dae33c673d7f4542bbe006fa6cc2716a-0.
INFO 08-09 12:57:41 [metrics.py:417] Avg prompt throughput: 3.0 tokens/s, Avg generation throughput: 96.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 12:57:46 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 98.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 12:57:50 [logger.py:43] Received request cmpl-be3e01527b8e4518a842d2eca1e5bad1-0: prompt: 'How do instruction-tuned models differ from base models?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 437, 15278, 29899, 29873, 348, 287, 4733, 1163, 515, 2967, 4733, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:39376 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 12:57:50 [engine.py:331] Added request cmpl-be3e01527b8e4518a842d2eca1e5bad1-0.
INFO 08-09 12:57:51 [metrics.py:417] Avg prompt throughput: 2.8 tokens/s, Avg generation throughput: 89.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 12:57:56 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 96.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 12:58:01 [logger.py:43] Received request cmpl-15b840b9f4214e369cfdb070c97dad42-0: prompt: 'Describe an RLHF (Reinforcement Learning from Human Feedback) process.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 20355, 915, 385, 390, 29931, 29950, 29943, 313, 1123, 262, 1454, 13561, 29257, 515, 12968, 5169, 287, 1627, 29897, 1889, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:46656 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 12:58:01 [engine.py:331] Added request cmpl-15b840b9f4214e369cfdb070c97dad42-0.
INFO 08-09 12:58:01 [metrics.py:417] Avg prompt throughput: 4.4 tokens/s, Avg generation throughput: 91.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 12:58:06 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 92.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 12:58:11 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 93.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-09 12:58:11 [logger.py:43] Received request cmpl-ae627cc2348748908901537ba925774c-0: prompt: 'What is SFT (Supervised Fine-Tuning) in LLM training?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 317, 7818, 313, 19111, 11292, 28896, 29899, 29911, 27964, 29897, 297, 365, 26369, 6694, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:46670 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 12:58:11 [engine.py:331] Added request cmpl-ae627cc2348748908901537ba925774c-0.
INFO 08-09 12:58:16 [metrics.py:417] Avg prompt throughput: 3.6 tokens/s, Avg generation throughput: 93.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 12:58:20 [logger.py:43] Received request cmpl-54024c92fb4447af97d304f350bd4542-0: prompt: 'Explain the role of HPU in Gaudi chips.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 12027, 7420, 278, 6297, 310, 379, 7056, 297, 10415, 4749, 521, 4512, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:46678 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 12:58:20 [engine.py:331] Added request cmpl-54024c92fb4447af97d304f350bd4542-0.
INFO 08-09 12:58:21 [metrics.py:417] Avg prompt throughput: 2.8 tokens/s, Avg generation throughput: 97.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 12:58:26 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 97.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 12:58:31 [logger.py:43] Received request cmpl-aa5f22e351144484860642b56b531f5e-0: prompt: 'What is a causal attention mask?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 263, 3269, 284, 8570, 11105, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:49384 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 12:58:31 [engine.py:331] Added request cmpl-aa5f22e351144484860642b56b531f5e-0.
INFO 08-09 12:58:31 [metrics.py:417] Avg prompt throughput: 1.8 tokens/s, Avg generation throughput: 96.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 12:58:36 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 93.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 12:58:41 [logger.py:43] Received request cmpl-fde62bf56fef4479b2da7bd523a83f07-0: prompt: 'Describe the HF `generate()` function in Transformers.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 20355, 915, 278, 379, 29943, 421, 17158, 2555, 740, 297, 4103, 689, 414, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:59744 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 12:58:41 [engine.py:331] Added request cmpl-fde62bf56fef4479b2da7bd523a83f07-0.
INFO 08-09 12:58:41 [metrics.py:417] Avg prompt throughput: 3.0 tokens/s, Avg generation throughput: 91.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 12:58:46 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 94.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 12:58:51 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 91.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-09 12:58:52 [logger.py:43] Received request cmpl-51a1eaaac43d46c48622014de8f72d2c-0: prompt: 'Explain differences between T5, BART, and GPT models.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 12027, 7420, 12651, 1546, 323, 29945, 29892, 350, 8322, 29892, 322, 402, 7982, 4733, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:49086 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 12:58:52 [engine.py:331] Added request cmpl-51a1eaaac43d46c48622014de8f72d2c-0.
INFO 08-09 12:58:56 [metrics.py:417] Avg prompt throughput: 3.2 tokens/s, Avg generation throughput: 93.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 12:59:02 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 96.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-09 12:59:03 [logger.py:43] Received request cmpl-0ea742935d574518a732bf4883cd1050-0: prompt: 'What is model checkpointing and resumption?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 1904, 1423, 3149, 292, 322, 620, 28069, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:58270 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 12:59:03 [engine.py:331] Added request cmpl-0ea742935d574518a732bf4883cd1050-0.
INFO 08-09 12:59:07 [metrics.py:417] Avg prompt throughput: 2.2 tokens/s, Avg generation throughput: 96.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 12:59:12 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 96.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-09 12:59:13 [logger.py:43] Received request cmpl-86a586ac17bc4dbab2210a70353199ef-0: prompt: 'Describe how checkpoint sharding works in distributed training.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 20355, 915, 920, 1423, 3149, 528, 20272, 1736, 297, 13235, 6694, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:40032 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 12:59:13 [engine.py:331] Added request cmpl-86a586ac17bc4dbab2210a70353199ef-0.
INFO 08-09 12:59:17 [metrics.py:417] Avg prompt throughput: 2.6 tokens/s, Avg generation throughput: 97.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 12:59:22 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 92.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-09 12:59:24 [logger.py:43] Received request cmpl-ab7ab38ad61848978cfa9f770b24b129-0: prompt: 'How are model weights stored in HuggingFace format?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 526, 1904, 18177, 6087, 297, 379, 688, 3460, 23360, 3402, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:33902 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 12:59:24 [engine.py:331] Added request cmpl-ab7ab38ad61848978cfa9f770b24b129-0.
INFO 08-09 12:59:27 [metrics.py:417] Avg prompt throughput: 2.6 tokens/s, Avg generation throughput: 89.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 12:59:32 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 92.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 12:59:35 [logger.py:43] Received request cmpl-d25d8e8ddd494b9999d8d21381bc4a34-0: prompt: 'Explain the impact of batch size on training and inference.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 12027, 7420, 278, 10879, 310, 9853, 2159, 373, 6694, 322, 27262, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:45910 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 12:59:35 [engine.py:331] Added request cmpl-d25d8e8ddd494b9999d8d21381bc4a34-0.
INFO 08-09 12:59:37 [metrics.py:417] Avg prompt throughput: 2.6 tokens/s, Avg generation throughput: 90.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 12:59:42 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 90.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 12:59:46 [logger.py:43] Received request cmpl-2a6a976f89e94358a6ce711bb537e725-0: prompt: 'What are fused kernels and why are they fast?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 526, 285, 3880, 413, 824, 1379, 322, 2020, 526, 896, 5172, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:54180 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 12:59:46 [engine.py:331] Added request cmpl-2a6a976f89e94358a6ce711bb537e725-0.
INFO 08-09 12:59:47 [metrics.py:417] Avg prompt throughput: 2.8 tokens/s, Avg generation throughput: 90.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 12:59:52 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 92.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 12:59:56 [logger.py:43] Received request cmpl-bb804ce0e94e4c91bd13a36c81ac2ff1-0: prompt: 'How does the `AutoModelForCausalLM` class work?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 947, 278, 421, 12300, 3195, 2831, 29907, 1485, 284, 26369, 29952, 770, 664, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:54434 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 12:59:56 [engine.py:331] Added request cmpl-bb804ce0e94e4c91bd13a36c81ac2ff1-0.
INFO 08-09 12:59:57 [metrics.py:417] Avg prompt throughput: 3.2 tokens/s, Avg generation throughput: 90.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 13:00:01 [logger.py:43] Received request cmpl-6004b417015e41f6bf16e0f14d71723b-0: prompt: 'What are tensor parallel and tensor slicing?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 526, 12489, 8943, 322, 12489, 269, 506, 292, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:54894 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:00:01 [engine.py:331] Added request cmpl-6004b417015e41f6bf16e0f14d71723b-0.
INFO 08-09 13:00:02 [metrics.py:417] Avg prompt throughput: 2.2 tokens/s, Avg generation throughput: 95.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 13:00:07 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 93.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:00:12 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 92.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-09 13:00:12 [logger.py:43] Received request cmpl-b1c9359e2b60481381724bfe490759a1-0: prompt: 'Describe the key steps of model serving using vLLM.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 20355, 915, 278, 1820, 6576, 310, 1904, 16330, 773, 325, 2208, 29924, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:55838 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:00:12 [engine.py:331] Added request cmpl-b1c9359e2b60481381724bfe490759a1-0.
INFO 08-09 13:00:17 [metrics.py:417] Avg prompt throughput: 2.8 tokens/s, Avg generation throughput: 100.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:00:22 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 86.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-09 13:00:23 [logger.py:43] Received request cmpl-dfb7de4a8b6340c0bcba565d7bb25044-0: prompt: 'What are inference graphs in deep learning?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 526, 27262, 18445, 297, 6483, 6509, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:34766 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:00:23 [engine.py:331] Added request cmpl-dfb7de4a8b6340c0bcba565d7bb25044-0.
INFO 08-09 13:00:27 [metrics.py:417] Avg prompt throughput: 1.8 tokens/s, Avg generation throughput: 85.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:00:32 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 91.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-09 13:00:34 [logger.py:43] Received request cmpl-96c6af6f38ef4fbbb6695ce2ce060072-0: prompt: 'What is a streaming response in OpenAI-style APIs?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 263, 24820, 2933, 297, 4673, 23869, 29899, 3293, 23649, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:46160 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:00:34 [engine.py:331] Added request cmpl-96c6af6f38ef4fbbb6695ce2ce060072-0.
INFO 08-09 13:00:37 [metrics.py:417] Avg prompt throughput: 2.6 tokens/s, Avg generation throughput: 94.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:00:42 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 95.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:00:44 [logger.py:43] Received request cmpl-fbd4d5aa9d0244f39674cba9d121c373-0: prompt: 'How does speculative decoding reduce latency?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 947, 1580, 28524, 1602, 3689, 10032, 23316, 1270, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:54578 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:00:44 [engine.py:331] Added request cmpl-fbd4d5aa9d0244f39674cba9d121c373-0.
INFO 08-09 13:00:47 [metrics.py:417] Avg prompt throughput: 2.2 tokens/s, Avg generation throughput: 94.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:00:52 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 96.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:00:55 [logger.py:43] Received request cmpl-3907925134454ec5b3585d9d8974f7fe-0: prompt: 'What is JAX and how is it different from PyTorch?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 435, 6604, 322, 920, 338, 372, 1422, 515, 10772, 29911, 25350, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:56070 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:00:55 [engine.py:331] Added request cmpl-3907925134454ec5b3585d9d8974f7fe-0.
INFO 08-09 13:00:57 [metrics.py:417] Avg prompt throughput: 3.0 tokens/s, Avg generation throughput: 95.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:01:02 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 95.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:01:05 [logger.py:43] Received request cmpl-00ca0863bdae45df808b44189b9b98d6-0: prompt: 'Describe lazy loading of weights in vLLM.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 20355, 915, 17366, 8363, 310, 18177, 297, 325, 2208, 29924, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:54800 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:01:05 [engine.py:331] Added request cmpl-00ca0863bdae45df808b44189b9b98d6-0.
INFO 08-09 13:01:07 [metrics.py:417] Avg prompt throughput: 2.4 tokens/s, Avg generation throughput: 94.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 13:01:12 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 93.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:01:16 [logger.py:43] Received request cmpl-a7a38abf29724fefa6b73d58cf215794-0: prompt: 'Explain the role of SynapseAI in Gaudi software stack.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 12027, 7420, 278, 6297, 310, 10829, 481, 344, 23869, 297, 10415, 4749, 7047, 5096, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:51118 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:01:16 [engine.py:331] Added request cmpl-a7a38abf29724fefa6b73d58cf215794-0.
INFO 08-09 13:01:17 [metrics.py:417] Avg prompt throughput: 3.2 tokens/s, Avg generation throughput: 95.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 13:01:22 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 93.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:01:27 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 92.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-09 13:01:27 [logger.py:43] Received request cmpl-65ff8cee88c24d338b176e7dec96141a-0: prompt: 'What are the differences between HPU and GPU tensors?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 526, 278, 12651, 1546, 379, 7056, 322, 22796, 25187, 943, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:37284 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:01:27 [engine.py:331] Added request cmpl-65ff8cee88c24d338b176e7dec96141a-0.
INFO 08-09 13:01:32 [metrics.py:417] Avg prompt throughput: 2.6 tokens/s, Avg generation throughput: 97.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:01:37 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 98.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-09 13:01:37 [logger.py:43] Received request cmpl-69d3429b0a084636a5d6b139d5497d25-0: prompt: 'What are the limitations of LLaMA 2?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 526, 278, 27028, 310, 365, 5661, 1529, 29871, 29906, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:44010 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:01:37 [engine.py:331] Added request cmpl-69d3429b0a084636a5d6b139d5497d25-0.
INFO 08-09 13:01:42 [metrics.py:417] Avg prompt throughput: 2.4 tokens/s, Avg generation throughput: 89.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:01:47 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 91.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-09 13:01:48 [logger.py:43] Received request cmpl-c711f2fc725a4d8dbd46a4b7ecac98b6-0: prompt: 'How is multilingual performance evaluated in LLMs?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 338, 1773, 6504, 950, 4180, 19030, 297, 365, 26369, 29879, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:43392 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:01:48 [engine.py:331] Added request cmpl-c711f2fc725a4d8dbd46a4b7ecac98b6-0.
INFO 08-09 13:01:52 [metrics.py:417] Avg prompt throughput: 2.6 tokens/s, Avg generation throughput: 96.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:01:57 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 93.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-09 13:01:58 [logger.py:43] Received request cmpl-d40ef14ba34c42d6bcaf7dc0623a03a0-0: prompt: 'Describe an example use of prompt tuning.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 20355, 915, 385, 1342, 671, 310, 9508, 18515, 292, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:58192 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:01:58 [engine.py:331] Added request cmpl-d40ef14ba34c42d6bcaf7dc0623a03a0-0.
INFO 08-09 13:02:02 [metrics.py:417] Avg prompt throughput: 2.2 tokens/s, Avg generation throughput: 92.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:02:07 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 93.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-09 13:02:09 [logger.py:43] Received request cmpl-8851e737b0c447a7845e427f235c8545-0: prompt: 'What is the attention bottleneck and how to address it?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 278, 8570, 18046, 29880, 1600, 384, 322, 920, 304, 3211, 372, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:42852 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:02:09 [engine.py:331] Added request cmpl-8851e737b0c447a7845e427f235c8545-0.
INFO 08-09 13:02:12 [metrics.py:417] Avg prompt throughput: 3.0 tokens/s, Avg generation throughput: 92.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:02:17 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 93.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:02:20 [logger.py:43] Received request cmpl-c965c08be11b48d0911f2533b3c6da0f-0: prompt: 'How does token alignment affect downstream tasks?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 947, 5993, 22239, 6602, 1623, 5461, 9595, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:53474 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:02:20 [engine.py:331] Added request cmpl-c965c08be11b48d0911f2533b3c6da0f-0.
INFO 08-09 13:02:22 [metrics.py:417] Avg prompt throughput: 2.0 tokens/s, Avg generation throughput: 93.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:02:27 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 94.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:02:30 [logger.py:43] Received request cmpl-77e8909b0423473e96e4250749899b67-0: prompt: 'What are the challenges in inference on edge devices?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 526, 278, 18066, 267, 297, 27262, 373, 7636, 9224, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:53462 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:02:30 [engine.py:331] Added request cmpl-77e8909b0423473e96e4250749899b67-0.
INFO 08-09 13:02:32 [metrics.py:417] Avg prompt throughput: 2.4 tokens/s, Avg generation throughput: 96.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:02:33 [logger.py:43] Received request cmpl-63c19eec09ef4793b5333eee9774fcc4-0: prompt: 'Explain the principle behind rotary embeddings.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 12027, 7420, 278, 12502, 5742, 5731, 653, 8297, 29881, 886, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:43822 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:02:33 [engine.py:331] Added request cmpl-63c19eec09ef4793b5333eee9774fcc4-0.
INFO 08-09 13:02:37 [metrics.py:417] Avg prompt throughput: 2.4 tokens/s, Avg generation throughput: 85.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:02:42 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 93.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-09 13:02:44 [logger.py:43] Received request cmpl-7bebd0579fc14ec1abff3f9c0b851e6e-0: prompt: 'How does temperature affect language model output?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 947, 10430, 6602, 4086, 1904, 1962, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:41088 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:02:44 [engine.py:331] Added request cmpl-7bebd0579fc14ec1abff3f9c0b851e6e-0.
INFO 08-09 13:02:47 [metrics.py:417] Avg prompt throughput: 1.8 tokens/s, Avg generation throughput: 96.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:02:52 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 92.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:02:54 [logger.py:43] Received request cmpl-2b6b9dee8d2d4ac7a6f1a758246c1e2f-0: prompt: 'What are stop sequences in generation APIs?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 526, 5040, 15602, 297, 12623, 23649, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:34568 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:02:54 [engine.py:331] Added request cmpl-2b6b9dee8d2d4ac7a6f1a758246c1e2f-0.
INFO 08-09 13:02:57 [metrics.py:417] Avg prompt throughput: 1.8 tokens/s, Avg generation throughput: 99.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:03:02 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 95.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:03:05 [logger.py:43] Received request cmpl-ed64f31ccdb84c2b86d5fba8ba824840-0: prompt: 'Describe token streaming latency optimization.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 20355, 915, 5993, 24820, 23316, 1270, 13883, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:58954 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:03:05 [engine.py:331] Added request cmpl-ed64f31ccdb84c2b86d5fba8ba824840-0.
INFO 08-09 13:03:07 [metrics.py:417] Avg prompt throughput: 1.8 tokens/s, Avg generation throughput: 94.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:03:12 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 97.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:03:15 [logger.py:43] Received request cmpl-5a7ae8d78a444fe7b15b3e84c13cf29a-0: prompt: 'How to evaluate hallucinations in language models?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 304, 14707, 12713, 1682, 262, 800, 297, 4086, 4733, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:54156 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:03:15 [engine.py:331] Added request cmpl-5a7ae8d78a444fe7b15b3e84c13cf29a-0.
INFO 08-09 13:03:17 [metrics.py:417] Avg prompt throughput: 2.4 tokens/s, Avg generation throughput: 97.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:03:22 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 96.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:03:26 [logger.py:43] Received request cmpl-e9f5a28a515247dcad759ea074b78754-0: prompt: 'What is the purpose of `bos_token_id`?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 278, 6437, 310, 421, 27737, 29918, 6979, 29918, 333, 6522], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:59384 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:03:26 [engine.py:331] Added request cmpl-e9f5a28a515247dcad759ea074b78754-0.
INFO 08-09 13:03:27 [metrics.py:417] Avg prompt throughput: 2.6 tokens/s, Avg generation throughput: 94.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 13:03:30 [logger.py:43] Received request cmpl-bfa1228607e8453780c2c7f8979e8018-0: prompt: 'How are fused attention layers implemented?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 526, 285, 3880, 8570, 15359, 8762, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:59390 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:03:30 [engine.py:331] Added request cmpl-bfa1228607e8453780c2c7f8979e8018-0.
INFO 08-09 13:03:32 [metrics.py:417] Avg prompt throughput: 1.8 tokens/s, Avg generation throughput: 89.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:03:37 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 97.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:03:40 [logger.py:43] Received request cmpl-b9ad71af16ae4e1c98496670f97d9af6-0: prompt: 'Describe a pipeline for deploying LLaMA 2 in production.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 20355, 915, 263, 16439, 363, 7246, 292, 365, 5661, 1529, 29871, 29906, 297, 5802, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:60548 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:03:40 [engine.py:331] Added request cmpl-b9ad71af16ae4e1c98496670f97d9af6-0.
INFO 08-09 13:03:42 [metrics.py:417] Avg prompt throughput: 3.2 tokens/s, Avg generation throughput: 94.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:03:46 [logger.py:43] Received request cmpl-c16288464031409980377a6b770a0ff7-0: prompt: 'How does instruction following improve model usability?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 947, 15278, 1494, 11157, 1904, 502, 3097, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:57876 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:03:46 [engine.py:331] Added request cmpl-c16288464031409980377a6b770a0ff7-0.
INFO 08-09 13:03:47 [metrics.py:417] Avg prompt throughput: 2.0 tokens/s, Avg generation throughput: 94.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 13:03:52 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 94.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:03:56 [logger.py:43] Received request cmpl-18df7a9b17c74f479e69c90126a527f4-0: prompt: 'What is a sequence-to-sequence model?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 263, 5665, 29899, 517, 29899, 16506, 1904, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:54800 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:03:56 [engine.py:331] Added request cmpl-18df7a9b17c74f479e69c90126a527f4-0.
INFO 08-09 13:03:57 [metrics.py:417] Avg prompt throughput: 2.2 tokens/s, Avg generation throughput: 95.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 13:04:02 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 97.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:04:07 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 94.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-09 13:04:07 [logger.py:43] Received request cmpl-7b1db7bfc66f4eb6afca37acb0c19191-0: prompt: 'What is the purpose of `pad_token_id` in generation?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 278, 6437, 310, 421, 8305, 29918, 6979, 29918, 333, 29952, 297, 12623, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:52828 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:04:07 [engine.py:331] Added request cmpl-7b1db7bfc66f4eb6afca37acb0c19191-0.
INFO 08-09 13:04:11 [logger.py:43] Received request cmpl-99a5aacbd8ba463e8c4ca8a0ad0213e5-0: prompt: 'Explain the concept of auto-regressive generation.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 12027, 7420, 278, 6964, 310, 4469, 29899, 276, 3663, 573, 12623, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:52832 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:04:11 [engine.py:331] Added request cmpl-99a5aacbd8ba463e8c4ca8a0ad0213e5-0.
INFO 08-09 13:04:12 [metrics.py:417] Avg prompt throughput: 5.8 tokens/s, Avg generation throughput: 94.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 13:04:17 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 96.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:04:22 [logger.py:43] Received request cmpl-e4e22e2924fe4e038717e3abcf481a3d-0: prompt: 'How is top-p sampling different from nucleus sampling?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 338, 2246, 29899, 29886, 23460, 1422, 515, 22699, 375, 23460, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:48998 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:04:22 [engine.py:331] Added request cmpl-e4e22e2924fe4e038717e3abcf481a3d-0.
INFO 08-09 13:04:22 [metrics.py:417] Avg prompt throughput: 2.6 tokens/s, Avg generation throughput: 87.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 13:04:27 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 92.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:04:32 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 95.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-09 13:04:32 [logger.py:43] Received request cmpl-f41c1939a92b40d1b89b87840f448e4d-0: prompt: 'What are decoding strategies for safe responses?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 526, 1602, 3689, 16650, 583, 363, 9109, 20890, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:42746 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:04:32 [engine.py:331] Added request cmpl-f41c1939a92b40d1b89b87840f448e4d-0.
INFO 08-09 13:04:37 [metrics.py:417] Avg prompt throughput: 2.2 tokens/s, Avg generation throughput: 95.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:04:42 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 95.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-09 13:04:43 [logger.py:43] Received request cmpl-76a1ce0931334658b826b9a75b0bc408-0: prompt: 'How are model prompts structured in ChatML?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 526, 1904, 9508, 29879, 2281, 2955, 297, 678, 271, 1988, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:38456 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:04:43 [engine.py:331] Added request cmpl-76a1ce0931334658b826b9a75b0bc408-0.
INFO 08-09 13:04:47 [metrics.py:417] Avg prompt throughput: 2.6 tokens/s, Avg generation throughput: 93.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:04:52 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 94.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-09 13:04:53 [logger.py:43] Received request cmpl-b29fed9487f74b499020d81bad191d70-0: prompt: 'What is a system prompt in chat inference?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 263, 1788, 9508, 297, 13563, 27262, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:38216 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:04:53 [engine.py:331] Added request cmpl-b29fed9487f74b499020d81bad191d70-0.
INFO 08-09 13:04:57 [metrics.py:417] Avg prompt throughput: 2.0 tokens/s, Avg generation throughput: 94.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:05:02 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 94.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-09 13:05:04 [logger.py:43] Received request cmpl-0763448aa5b14bb1a1429309f96278a7-0: prompt: 'How do you batch requests for high throughput?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 437, 366, 9853, 7274, 363, 1880, 1549, 649, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:48256 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:05:04 [engine.py:331] Added request cmpl-0763448aa5b14bb1a1429309f96278a7-0.
INFO 08-09 13:05:07 [metrics.py:417] Avg prompt throughput: 2.2 tokens/s, Avg generation throughput: 92.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:05:12 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 97.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:05:15 [logger.py:43] Received request cmpl-82bb9fef5dd949b39d5046bb02b23e28-0: prompt: 'Explain model serving with `transformers + vLLM`.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 12027, 7420, 1904, 16330, 411, 421, 9067, 414, 718, 325, 2208, 29924, 1412], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:53476 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:05:15 [engine.py:331] Added request cmpl-82bb9fef5dd949b39d5046bb02b23e28-0.
INFO 08-09 13:05:17 [metrics.py:417] Avg prompt throughput: 2.8 tokens/s, Avg generation throughput: 92.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:05:22 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 95.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:05:25 [logger.py:43] Received request cmpl-905771be84574cdaac5f74602a4c1d28-0: prompt: 'What is Triton Inference Server?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 323, 768, 265, 512, 1659, 5656, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:53592 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:05:25 [engine.py:331] Added request cmpl-905771be84574cdaac5f74602a4c1d28-0.
INFO 08-09 13:05:27 [metrics.py:417] Avg prompt throughput: 2.0 tokens/s, Avg generation throughput: 95.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:05:32 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 92.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:05:36 [logger.py:43] Received request cmpl-38320c7953b44569ac6c6bb1ff7df206-0: prompt: 'Describe the significance of large context windows.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 20355, 915, 278, 26002, 310, 2919, 3030, 5417, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:42014 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:05:36 [engine.py:331] Added request cmpl-38320c7953b44569ac6c6bb1ff7df206-0.
INFO 08-09 13:05:37 [metrics.py:417] Avg prompt throughput: 2.0 tokens/s, Avg generation throughput: 93.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 13:05:42 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 92.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:05:47 [logger.py:43] Received request cmpl-cb16fbe450b84f4386cda129ff374e22-0: prompt: 'What is dynamic quantization in transformers?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 7343, 4323, 2133, 297, 4327, 414, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:34062 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:05:47 [engine.py:331] Added request cmpl-cb16fbe450b84f4386cda129ff374e22-0.
INFO 08-09 13:05:47 [metrics.py:417] Avg prompt throughput: 2.0 tokens/s, Avg generation throughput: 94.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 13:05:52 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 95.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:05:57 [logger.py:43] Received request cmpl-cb9419b382a9476193914fc031785372-0: prompt: 'How to prevent prompt injection in LLMs?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 304, 5557, 9508, 20859, 297, 365, 26369, 29879, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:48830 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:05:57 [engine.py:331] Added request cmpl-cb9419b382a9476193914fc031785372-0.
INFO 08-09 13:05:57 [metrics.py:417] Avg prompt throughput: 2.2 tokens/s, Avg generation throughput: 100.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 13:06:02 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 100.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:06:04 [logger.py:43] Received request cmpl-29887b9f99a842a1b45473f37b331b35-0: prompt: 'What are attention scores and how are they computed?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 526, 8570, 19435, 322, 920, 526, 896, 15712, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:51616 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:06:04 [engine.py:331] Added request cmpl-29887b9f99a842a1b45473f37b331b35-0.
INFO 08-09 13:06:07 [metrics.py:417] Avg prompt throughput: 2.2 tokens/s, Avg generation throughput: 99.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:06:12 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 96.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-09 13:06:14 [logger.py:43] Received request cmpl-ac7a0882f8874dbfb26626666652165b-0: prompt: 'What are residual connections in transformers?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 526, 10995, 950, 12368, 297, 4327, 414, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:43154 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:06:14 [engine.py:331] Added request cmpl-ac7a0882f8874dbfb26626666652165b-0.
INFO 08-09 13:06:17 [metrics.py:417] Avg prompt throughput: 2.0 tokens/s, Avg generation throughput: 95.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:06:22 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 95.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-09 13:06:24 [logger.py:43] Received request cmpl-1cf23a0e61374f4b93acdc5a3052f5c3-0: prompt: 'How to handle long sequences in generation?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 304, 4386, 1472, 15602, 297, 12623, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:41176 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:06:24 [engine.py:331] Added request cmpl-1cf23a0e61374f4b93acdc5a3052f5c3-0.
INFO 08-09 13:06:27 [metrics.py:417] Avg prompt throughput: 1.8 tokens/s, Avg generation throughput: 94.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:06:32 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 97.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:06:35 [logger.py:43] Received request cmpl-518495d406a5480e97e891aa06d8a5e6-0: prompt: 'Explain the use of flash attention v2.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 12027, 7420, 278, 671, 310, 11013, 8570, 325, 29906, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:46680 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:06:35 [engine.py:331] Added request cmpl-518495d406a5480e97e891aa06d8a5e6-0.
INFO 08-09 13:06:37 [metrics.py:417] Avg prompt throughput: 2.2 tokens/s, Avg generation throughput: 87.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:06:42 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 96.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:06:46 [logger.py:43] Received request cmpl-73c085f946c64c9e88bbe542c04664d0-0: prompt: 'What is a greedy search in inference?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 263, 1395, 7584, 2740, 297, 27262, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:36962 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:06:46 [engine.py:331] Added request cmpl-73c085f946c64c9e88bbe542c04664d0-0.
INFO 08-09 13:06:47 [metrics.py:417] Avg prompt throughput: 2.0 tokens/s, Avg generation throughput: 91.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 13:06:52 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 92.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:06:57 [logger.py:43] Received request cmpl-e9763220d9714e1b81f5d4f958a88f33-0: prompt: 'How do checkpoints improve fault tolerance?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 437, 1423, 9748, 11157, 12570, 20341, 749, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:33516 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:06:57 [engine.py:331] Added request cmpl-e9763220d9714e1b81f5d4f958a88f33-0.
INFO 08-09 13:06:57 [metrics.py:417] Avg prompt throughput: 2.0 tokens/s, Avg generation throughput: 93.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 13:07:02 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 94.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:07:07 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 96.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-09 13:07:07 [logger.py:43] Received request cmpl-51c0277f68dd422a900070b0d70c428c-0: prompt: 'Describe how text-to-text generation works.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 20355, 915, 920, 1426, 29899, 517, 29899, 726, 12623, 1736, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:36480 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:07:07 [engine.py:331] Added request cmpl-51c0277f68dd422a900070b0d70c428c-0.
INFO 08-09 13:07:12 [metrics.py:417] Avg prompt throughput: 2.4 tokens/s, Avg generation throughput: 94.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:07:17 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 98.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-09 13:07:17 [logger.py:43] Received request cmpl-864839033464469d9d3e9e0056c433fb-0: prompt: 'Explain cosine similarity in embeddings.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 12027, 7420, 6776, 457, 29501, 297, 8297, 29881, 886, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:41056 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:07:17 [engine.py:331] Added request cmpl-864839033464469d9d3e9e0056c433fb-0.
INFO 08-09 13:07:22 [metrics.py:417] Avg prompt throughput: 2.2 tokens/s, Avg generation throughput: 98.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:07:27 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 98.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-09 13:07:28 [logger.py:43] Received request cmpl-51c9f4fa089a48a289a3b07f6d3d1658-0: prompt: 'What is token-wise parallelism?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 5993, 29899, 3538, 8943, 1608, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:60890 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:07:28 [engine.py:331] Added request cmpl-51c9f4fa089a48a289a3b07f6d3d1658-0.
INFO 08-09 13:07:32 [metrics.py:417] Avg prompt throughput: 1.8 tokens/s, Avg generation throughput: 96.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:07:37 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 93.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-09 13:07:38 [logger.py:43] Received request cmpl-adbc80c1bcf441e8adf0810a5b33ff67-0: prompt: 'How do shared embeddings help in decoder-only models?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 437, 7258, 8297, 29881, 886, 1371, 297, 1602, 6119, 29899, 6194, 4733, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:52620 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:07:38 [engine.py:331] Added request cmpl-adbc80c1bcf441e8adf0810a5b33ff67-0.
INFO 08-09 13:07:41 [logger.py:43] Received request cmpl-03af76b5c4b44c9f972e025053937333-0: prompt: 'Explain how to measure latency in model output.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 12027, 7420, 920, 304, 5645, 23316, 1270, 297, 1904, 1962, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:43158 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:07:41 [engine.py:331] Added request cmpl-03af76b5c4b44c9f972e025053937333-0.
INFO 08-09 13:07:42 [metrics.py:417] Avg prompt throughput: 5.4 tokens/s, Avg generation throughput: 96.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 13:07:47 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 89.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:07:52 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 87.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-09 13:07:52 [logger.py:43] Received request cmpl-a07ed2024720495a9bc41b9d4fb0cc4e-0: prompt: 'What is the purpose of dropout during training?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 278, 6437, 310, 5768, 449, 2645, 6694, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:36332 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:07:52 [engine.py:331] Added request cmpl-a07ed2024720495a9bc41b9d4fb0cc4e-0.
INFO 08-09 13:07:57 [metrics.py:417] Avg prompt throughput: 2.2 tokens/s, Avg generation throughput: 84.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:08:02 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 85.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-09 13:08:04 [logger.py:43] Received request cmpl-b40796e1bb204dfcba574b765fe5d2cf-0: prompt: 'What is the role of GELU activation in transformers?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 278, 6297, 310, 402, 6670, 29965, 26229, 297, 4327, 414, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:54736 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:08:04 [engine.py:331] Added request cmpl-b40796e1bb204dfcba574b765fe5d2cf-0.
INFO 08-09 13:08:07 [metrics.py:417] Avg prompt throughput: 2.8 tokens/s, Avg generation throughput: 91.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:08:12 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 91.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-09 13:08:15 [logger.py:43] Received request cmpl-9cb144bf8ac3449e8a3e23f8942607ea-0: prompt: 'Explain forward and backward pass in transformers.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 12027, 7420, 6375, 322, 1250, 1328, 1209, 297, 4327, 414, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:33414 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:08:15 [engine.py:331] Added request cmpl-9cb144bf8ac3449e8a3e23f8942607ea-0.
INFO 08-09 13:08:17 [metrics.py:417] Avg prompt throughput: 2.4 tokens/s, Avg generation throughput: 95.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:08:22 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 87.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:08:26 [logger.py:43] Received request cmpl-5adf7f8c484f4edea62c6cdddab07966-0: prompt: 'What is speculative decoding and how does it help?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 1580, 28524, 1602, 3689, 322, 920, 947, 372, 1371, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:60628 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:08:26 [engine.py:331] Added request cmpl-5adf7f8c484f4edea62c6cdddab07966-0.
INFO 08-09 13:08:27 [metrics.py:417] Avg prompt throughput: 2.6 tokens/s, Avg generation throughput: 95.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:08:32 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 91.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:08:36 [logger.py:43] Received request cmpl-2372d43df8bd4659bf30168d1a2b8de0-0: prompt: 'How to optimize HPU memory usage during inference?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 304, 24656, 379, 7056, 3370, 8744, 2645, 27262, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:36688 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:08:36 [engine.py:331] Added request cmpl-2372d43df8bd4659bf30168d1a2b8de0-0.
INFO 08-09 13:08:37 [metrics.py:417] Avg prompt throughput: 2.2 tokens/s, Avg generation throughput: 94.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 13:08:42 [logger.py:43] Received request cmpl-df7e7aeb7edf4364a3531c1b38079a3a-0: prompt: 'Explain profiling for LLM inference performance.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 12027, 7420, 20077, 292, 363, 365, 26369, 27262, 4180, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:34602 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:08:42 [engine.py:331] Added request cmpl-df7e7aeb7edf4364a3531c1b38079a3a-0.
INFO 08-09 13:08:42 [metrics.py:417] Avg prompt throughput: 2.2 tokens/s, Avg generation throughput: 99.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 13:08:45 [logger.py:43] Received request cmpl-63c8b91ba8c348268f273113902ad9a9-0: prompt: 'How to compute inter-token latency per request?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 304, 10272, 1006, 29899, 6979, 23316, 1270, 639, 2009, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:34612 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:08:45 [engine.py:331] Added request cmpl-63c8b91ba8c348268f273113902ad9a9-0.
INFO 08-09 13:08:47 [metrics.py:417] Avg prompt throughput: 2.4 tokens/s, Avg generation throughput: 98.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:08:52 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 89.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:08:55 [logger.py:43] Received request cmpl-a19a3c7e5f34427bbd96d2f7af3dee7c-0: prompt: 'What is the difference between BFloat16 and Float16?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 278, 4328, 1546, 350, 11031, 29896, 29953, 322, 27842, 29896, 29953, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:34786 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:08:55 [engine.py:331] Added request cmpl-a19a3c7e5f34427bbd96d2f7af3dee7c-0.
INFO 08-09 13:08:57 [metrics.py:417] Avg prompt throughput: 3.0 tokens/s, Avg generation throughput: 92.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:09:02 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 96.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:09:06 [logger.py:43] Received request cmpl-d77ada77511f4e45b9f415e24a530448-0: prompt: 'What are logits and how are they interpreted?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 526, 1480, 1169, 322, 920, 526, 896, 21551, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:54568 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:09:06 [engine.py:331] Added request cmpl-d77ada77511f4e45b9f415e24a530448-0.
INFO 08-09 13:09:07 [metrics.py:417] Avg prompt throughput: 2.2 tokens/s, Avg generation throughput: 93.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 13:09:12 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 95.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:09:16 [logger.py:43] Received request cmpl-a67f1948cc304e90943d45ec20b6f758-0: prompt: 'What is log probability in token prediction?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 1480, 6976, 297, 5993, 18988, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:35500 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:09:16 [engine.py:331] Added request cmpl-a67f1948cc304e90943d45ec20b6f758-0.
INFO 08-09 13:09:17 [metrics.py:417] Avg prompt throughput: 1.8 tokens/s, Avg generation throughput: 93.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 13:09:22 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 94.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:09:27 [logger.py:43] Received request cmpl-6a3bb15994c044418d37a85c388cf9c0-0: prompt: 'How to use `TextStreamer` for streaming output?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 304, 671, 421, 1626, 3835, 261, 29952, 363, 24820, 1962, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:35580 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:09:27 [engine.py:331] Added request cmpl-6a3bb15994c044418d37a85c388cf9c0-0.
INFO 08-09 13:09:27 [metrics.py:417] Avg prompt throughput: 2.6 tokens/s, Avg generation throughput: 94.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 13:09:32 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 94.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:09:37 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 94.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-09 13:09:38 [logger.py:43] Received request cmpl-2bb4a5863f7b473692bf38b201020537-0: prompt: 'How to tokenize and detokenize a prompt manually?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 304, 5993, 675, 322, 1439, 4476, 675, 263, 9508, 7522, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:48870 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:09:38 [engine.py:331] Added request cmpl-2bb4a5863f7b473692bf38b201020537-0.
INFO 08-09 13:09:42 [metrics.py:417] Avg prompt throughput: 2.6 tokens/s, Avg generation throughput: 85.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:09:47 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 98.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-09 13:09:49 [logger.py:43] Received request cmpl-9f66a750c9c34e64976bb9d0ebc81cb7-0: prompt: 'Describe how a prompt flows through the transformer layers.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 20355, 915, 920, 263, 9508, 24536, 1549, 278, 4327, 261, 15359, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:49766 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:09:49 [engine.py:331] Added request cmpl-9f66a750c9c34e64976bb9d0ebc81cb7-0.
INFO 08-09 13:09:52 [metrics.py:417] Avg prompt throughput: 2.6 tokens/s, Avg generation throughput: 93.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:09:57 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 94.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-09 13:09:59 [logger.py:43] Received request cmpl-86abc73ce137455da4193467fbf4759e-0: prompt: 'How to handle EOS in streaming responses?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 304, 4386, 382, 3267, 297, 24820, 20890, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:49478 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:09:59 [engine.py:331] Added request cmpl-86abc73ce137455da4193467fbf4759e-0.
INFO 08-09 13:10:02 [metrics.py:417] Avg prompt throughput: 2.0 tokens/s, Avg generation throughput: 89.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:10:07 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 94.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:10:10 [logger.py:43] Received request cmpl-4a811a077a664656897d8877360307cb-0: prompt: 'What is a model config JSON in HF models?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 263, 1904, 2295, 4663, 297, 379, 29943, 4733, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:45738 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:10:10 [engine.py:331] Added request cmpl-4a811a077a664656897d8877360307cb-0.
INFO 08-09 13:10:12 [metrics.py:417] Avg prompt throughput: 2.4 tokens/s, Avg generation throughput: 96.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:10:17 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 92.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:10:21 [logger.py:43] Received request cmpl-ce73db6c98b24bc39a7ec02a6ffce641-0: prompt: 'What is kv_cache reuse in decoder models?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 10908, 29918, 8173, 24270, 297, 1602, 6119, 4733, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:52148 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:10:21 [engine.py:331] Added request cmpl-ce73db6c98b24bc39a7ec02a6ffce641-0.
INFO 08-09 13:10:22 [metrics.py:417] Avg prompt throughput: 2.4 tokens/s, Avg generation throughput: 86.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:10:27 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 87.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:10:32 [logger.py:43] Received request cmpl-40120cf91422448e995dc5c3346d830d-0: prompt: 'Describe a scenario of real-time chatbot streaming.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 20355, 915, 263, 10483, 310, 1855, 29899, 2230, 13563, 7451, 24820, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:39278 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:10:32 [engine.py:331] Added request cmpl-40120cf91422448e995dc5c3346d830d-0.
INFO 08-09 13:10:32 [metrics.py:417] Avg prompt throughput: 2.6 tokens/s, Avg generation throughput: 94.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 13:10:37 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 94.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:10:42 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 91.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-09 13:10:43 [logger.py:43] Received request cmpl-7fc23c65ea5848379984ef3694796b9f-0: prompt: 'How to compute throughput with variable prompts?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 304, 10272, 1549, 649, 411, 2286, 9508, 29879, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:46968 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:10:43 [engine.py:331] Added request cmpl-7fc23c65ea5848379984ef3694796b9f-0.
INFO 08-09 13:10:47 [metrics.py:417] Avg prompt throughput: 2.2 tokens/s, Avg generation throughput: 92.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:10:51 [logger.py:43] Received request cmpl-c2b2ed1ac6714da2ae96188a055d13f9-0: prompt: 'How does `max_new_tokens` differ from `max_length`?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 947, 421, 3317, 29918, 1482, 29918, 517, 12360, 29952, 1163, 515, 421, 3317, 29918, 2848, 6522], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:46972 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:10:51 [engine.py:331] Added request cmpl-c2b2ed1ac6714da2ae96188a055d13f9-0.
INFO 08-09 13:10:52 [metrics.py:417] Avg prompt throughput: 3.6 tokens/s, Avg generation throughput: 94.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:10:55 [logger.py:43] Received request cmpl-4922f6e6fb7c4dc38875a38ee919c6af-0: prompt: 'Explain the difference between loss and perplexity.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 12027, 7420, 278, 4328, 1546, 6410, 322, 639, 10709, 537, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:40174 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:10:55 [engine.py:331] Added request cmpl-4922f6e6fb7c4dc38875a38ee919c6af-0.
INFO 08-09 13:10:57 [metrics.py:417] Avg prompt throughput: 2.4 tokens/s, Avg generation throughput: 94.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:11:02 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 96.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:11:06 [logger.py:43] Received request cmpl-f4a980a57afc482b8195135e99d088a7-0: prompt: 'How to log benchmark results in a structured CSV?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 304, 1480, 23513, 2582, 297, 263, 2281, 2955, 16874, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:45724 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:11:06 [engine.py:331] Added request cmpl-f4a980a57afc482b8195135e99d088a7-0.
INFO 08-09 13:11:07 [metrics.py:417] Avg prompt throughput: 2.4 tokens/s, Avg generation throughput: 93.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:11:12 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 95.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:11:16 [logger.py:43] Received request cmpl-cdf5188d13f247ec9ff767e39fa68103-0: prompt: 'What are attention heads and how do they specialize?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 526, 8570, 15883, 322, 920, 437, 896, 4266, 675, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:33526 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:11:16 [engine.py:331] Added request cmpl-cdf5188d13f247ec9ff767e39fa68103-0.
INFO 08-09 13:11:17 [metrics.py:417] Avg prompt throughput: 2.4 tokens/s, Avg generation throughput: 94.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 13:11:19 [logger.py:43] Received request cmpl-85780cc94d8f45aaa8555300ce45727f-0: prompt: 'How to ensure reproducible benchmarking?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 304, 9801, 9483, 15520, 23513, 292, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:33534 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:11:19 [engine.py:331] Added request cmpl-85780cc94d8f45aaa8555300ce45727f-0.
INFO 08-09 13:11:22 [metrics.py:417] Avg prompt throughput: 1.8 tokens/s, Avg generation throughput: 95.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:11:27 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 95.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-09 13:11:29 [logger.py:43] Received request cmpl-40ecf3b80601432fa064eb681674f029-0: prompt: 'What are model shards and when are they used?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 526, 1904, 528, 3163, 322, 746, 526, 896, 1304, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:37388 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:11:29 [engine.py:331] Added request cmpl-40ecf3b80601432fa064eb681674f029-0.
INFO 08-09 13:11:32 [metrics.py:417] Avg prompt throughput: 2.4 tokens/s, Avg generation throughput: 85.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:11:35 [logger.py:43] Received request cmpl-61e1fb7405fb41b6820aeec9adec5e98-0: prompt: 'Explain the relationship between prompt length and latency.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 12027, 7420, 278, 9443, 1546, 9508, 3309, 322, 23316, 1270, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:42152 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:11:35 [engine.py:331] Added request cmpl-61e1fb7405fb41b6820aeec9adec5e98-0.
INFO 08-09 13:11:37 [metrics.py:417] Avg prompt throughput: 2.4 tokens/s, Avg generation throughput: 82.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:11:42 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 96.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:11:46 [logger.py:43] Received request cmpl-6703b31832784ba58ddf4791b0092a23-0: prompt: 'What is batch inference and why is it faster?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 9853, 27262, 322, 2020, 338, 372, 8473, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:34260 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:11:46 [engine.py:331] Added request cmpl-6703b31832784ba58ddf4791b0092a23-0.
INFO 08-09 13:11:47 [metrics.py:417] Avg prompt throughput: 2.2 tokens/s, Avg generation throughput: 91.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 13:11:52 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 93.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:11:57 [logger.py:43] Received request cmpl-77f99954a335463fa01288794d7d1207-0: prompt: 'What are rotary sin-cos embeddings?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 526, 5731, 653, 4457, 29899, 3944, 8297, 29881, 886, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:56770 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:11:57 [engine.py:331] Added request cmpl-77f99954a335463fa01288794d7d1207-0.
INFO 08-09 13:11:57 [metrics.py:417] Avg prompt throughput: 2.4 tokens/s, Avg generation throughput: 98.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 13:12:02 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 99.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:12:07 [logger.py:43] Received request cmpl-2817b7c7e800491782375e56b66fb585-0: prompt: "Describe HuggingFace's `AutoTokenizer` features.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 20355, 915, 379, 688, 3460, 23360, 29915, 29879, 421, 12300, 6066, 3950, 29952, 5680, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:55440 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:12:07 [engine.py:331] Added request cmpl-2817b7c7e800491782375e56b66fb585-0.
INFO 08-09 13:12:07 [metrics.py:417] Avg prompt throughput: 3.2 tokens/s, Avg generation throughput: 99.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 13:12:12 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 99.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:12:17 [logger.py:43] Received request cmpl-9953415d3f1d442aae993de01d5bdf8b-0: prompt: 'How does positional encoding interact with attention?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 947, 2602, 284, 8025, 16254, 411, 8570, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:60424 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:12:17 [engine.py:331] Added request cmpl-9953415d3f1d442aae993de01d5bdf8b-0.
INFO 08-09 13:12:17 [metrics.py:417] Avg prompt throughput: 2.0 tokens/s, Avg generation throughput: 98.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 13:12:22 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 100.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:12:27 [logger.py:43] Received request cmpl-f04bddd4a961426b87abf991d18bdd85-0: prompt: 'What is a streaming callback in Transformers?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 263, 24820, 6939, 297, 4103, 689, 414, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:48872 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:12:27 [engine.py:331] Added request cmpl-f04bddd4a961426b87abf991d18bdd85-0.
INFO 08-09 13:12:27 [metrics.py:417] Avg prompt throughput: 2.2 tokens/s, Avg generation throughput: 98.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 13:12:32 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 96.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:12:37 [logger.py:43] Received request cmpl-0db3a893f74c4620894ab7a7a7a03fd9-0: prompt: 'How to benchmark models using curl and Python?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 304, 23513, 4733, 773, 11051, 322, 5132, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:36840 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:12:37 [engine.py:331] Added request cmpl-0db3a893f74c4620894ab7a7a7a03fd9-0.
INFO 08-09 13:12:37 [metrics.py:417] Avg prompt throughput: 2.0 tokens/s, Avg generation throughput: 95.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 13:12:42 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 96.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:12:47 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 100.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-09 13:12:47 [logger.py:43] Received request cmpl-e00161509dcc488489c610a850221c6f-0: prompt: 'How to parse streaming responses from vLLM?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 304, 6088, 24820, 20890, 515, 325, 2208, 29924, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:41298 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:12:47 [engine.py:331] Added request cmpl-e00161509dcc488489c610a850221c6f-0.
INFO 08-09 13:12:52 [metrics.py:417] Avg prompt throughput: 2.2 tokens/s, Avg generation throughput: 99.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:12:57 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 94.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-09 13:12:58 [logger.py:43] Received request cmpl-a32167281bca45f7863eb1f32452ab94-0: prompt: 'How does deep caching work in inference?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 947, 6483, 22488, 664, 297, 27262, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:41618 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:12:58 [engine.py:331] Added request cmpl-a32167281bca45f7863eb1f32452ab94-0.
INFO 08-09 13:13:02 [metrics.py:417] Avg prompt throughput: 1.8 tokens/s, Avg generation throughput: 97.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:13:07 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 98.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-09 13:13:08 [logger.py:43] Received request cmpl-07dc1ca0048d4cd3bdb3ebb1c138a03e-0: prompt: 'What is a prompt-truncation error?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 263, 9508, 29899, 509, 4661, 362, 1059, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:43312 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:13:08 [engine.py:331] Added request cmpl-07dc1ca0048d4cd3bdb3ebb1c138a03e-0.
INFO 08-09 13:13:12 [metrics.py:417] Avg prompt throughput: 2.2 tokens/s, Avg generation throughput: 92.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:13:17 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 96.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-09 13:13:19 [logger.py:43] Received request cmpl-29a5583a2b674f2ab5d7ebf146a1508f-0: prompt: 'Explain the role of tokenizer config files.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 12027, 7420, 278, 6297, 310, 5993, 3950, 2295, 2066, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:45110 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:13:19 [engine.py:331] Added request cmpl-29a5583a2b674f2ab5d7ebf146a1508f-0.
INFO 08-09 13:13:22 [metrics.py:417] Avg prompt throughput: 2.2 tokens/s, Avg generation throughput: 96.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:13:27 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 94.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-09 13:13:29 [logger.py:43] Received request cmpl-6b570ae565fd4d79bd20ef77803a61e7-0: prompt: 'How to batch multiple prompts into one request?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 304, 9853, 2999, 9508, 29879, 964, 697, 2009, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:33438 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:13:29 [engine.py:331] Added request cmpl-6b570ae565fd4d79bd20ef77803a61e7-0.
INFO 08-09 13:13:32 [metrics.py:417] Avg prompt throughput: 2.2 tokens/s, Avg generation throughput: 94.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:13:37 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 94.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-09 13:13:40 [logger.py:43] Received request cmpl-54a0ad164d6740b8a0e87b8f6c5792fd-0: prompt: 'What is a chat template in HuggingFace?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 263, 13563, 4472, 297, 379, 688, 3460, 23360, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:35214 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:13:40 [engine.py:331] Added request cmpl-54a0ad164d6740b8a0e87b8f6c5792fd-0.
INFO 08-09 13:13:42 [metrics.py:417] Avg prompt throughput: 2.4 tokens/s, Avg generation throughput: 94.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:13:47 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 97.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:13:50 [logger.py:43] Received request cmpl-e948b56da6914350970706bd0483a2bf-0: prompt: 'What is inference latency and why is it important?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 27262, 23316, 1270, 322, 2020, 338, 372, 4100, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:60432 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:13:50 [engine.py:331] Added request cmpl-e948b56da6914350970706bd0483a2bf-0.
INFO 08-09 13:13:52 [metrics.py:417] Avg prompt throughput: 2.4 tokens/s, Avg generation throughput: 99.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:13:57 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 100.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:14:00 [logger.py:43] Received request cmpl-a65c264e4a6f424eb4b457575a20c993-0: prompt: 'How to reduce memory footprint of LLaMA models?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 304, 10032, 3370, 3661, 2158, 310, 365, 5661, 1529, 4733, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:52678 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:14:00 [engine.py:331] Added request cmpl-a65c264e4a6f424eb4b457575a20c993-0.
INFO 08-09 13:14:02 [metrics.py:417] Avg prompt throughput: 2.6 tokens/s, Avg generation throughput: 94.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:14:07 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 95.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:14:11 [logger.py:43] Received request cmpl-512a83cf24ae4a7f9bc9093d6c3ea789-0: prompt: 'What is the difference between `generation_config.json` and `config.json`?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 278, 4328, 1546, 421, 4738, 362, 29918, 2917, 29889, 3126, 29952, 322, 421, 2917, 29889, 3126, 6522], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:46638 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:14:11 [engine.py:331] Added request cmpl-512a83cf24ae4a7f9bc9093d6c3ea789-0.
INFO 08-09 13:14:12 [metrics.py:417] Avg prompt throughput: 4.0 tokens/s, Avg generation throughput: 94.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:14:17 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 93.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:14:21 [logger.py:43] Received request cmpl-a210c923d28d47d59b67e9564e080e23-0: prompt: 'What is a quantized model and how is it served?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 263, 4323, 1891, 1904, 322, 920, 338, 372, 6766, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:55508 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:14:21 [engine.py:331] Added request cmpl-a210c923d28d47d59b67e9564e080e23-0.
INFO 08-09 13:14:22 [metrics.py:417] Avg prompt throughput: 2.6 tokens/s, Avg generation throughput: 93.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 13:14:27 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 95.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:14:32 [logger.py:43] Received request cmpl-8a98b0f79bf7478e80b893926ccddb52-0: prompt: 'How to handle prompt overflow in generation?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 304, 4386, 9508, 11969, 297, 12623, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:40422 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:14:32 [engine.py:331] Added request cmpl-8a98b0f79bf7478e80b893926ccddb52-0.
INFO 08-09 13:14:32 [metrics.py:417] Avg prompt throughput: 1.8 tokens/s, Avg generation throughput: 95.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 13:14:37 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 95.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:14:43 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 88.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-09 13:14:43 [logger.py:43] Received request cmpl-28242fc28c1e49d3a0184da6b9719da8-0: prompt: 'What is warmup time in inference pipelines?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 14294, 786, 931, 297, 27262, 8450, 24210, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:41638 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:14:43 [engine.py:331] Added request cmpl-28242fc28c1e49d3a0184da6b9719da8-0.
INFO 08-09 13:14:48 [metrics.py:417] Avg prompt throughput: 2.2 tokens/s, Avg generation throughput: 93.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:14:53 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 93.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-09 13:14:53 [logger.py:43] Received request cmpl-f0aae6df97aa4a208cb38f99cd5f95da-0: prompt: 'How to stream JSON tokens using curl?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 304, 4840, 4663, 18897, 773, 11051, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:56588 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:14:53 [engine.py:331] Added request cmpl-f0aae6df97aa4a208cb38f99cd5f95da-0.
INFO 08-09 13:14:58 [metrics.py:417] Avg prompt throughput: 1.8 tokens/s, Avg generation throughput: 93.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:14:59 [logger.py:43] Received request cmpl-4a3898cc77cd45798bdccf92ca85e709-0: prompt: 'What is a `generate_with_streaming()` wrapper?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 263, 421, 17158, 29918, 2541, 29918, 5461, 292, 2555, 14476, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:56590 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:14:59 [engine.py:331] Added request cmpl-4a3898cc77cd45798bdccf92ca85e709-0.
INFO 08-09 13:15:03 [metrics.py:417] Avg prompt throughput: 2.8 tokens/s, Avg generation throughput: 93.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:15:08 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 92.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-09 13:15:09 [logger.py:43] Received request cmpl-0f98f3ad5634421595fe6017929da46f-0: prompt: 'How to evaluate TTFT and ITL for multiple prompts?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 304, 14707, 323, 29911, 7818, 322, 13315, 29931, 363, 2999, 9508, 29879, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:37674 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:15:09 [engine.py:331] Added request cmpl-0f98f3ad5634421595fe6017929da46f-0.
INFO 08-09 13:15:13 [metrics.py:417] Avg prompt throughput: 3.0 tokens/s, Avg generation throughput: 88.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:15:14 [logger.py:43] Received request cmpl-29f39b2b4aeb47b9a4030d3e9d29eba5-0: prompt: 'What are server cold starts in model serving?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 526, 1923, 11220, 8665, 297, 1904, 16330, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:39780 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:15:14 [engine.py:331] Added request cmpl-29f39b2b4aeb47b9a4030d3e9d29eba5-0.
INFO 08-09 13:15:18 [metrics.py:417] Avg prompt throughput: 2.0 tokens/s, Avg generation throughput: 85.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:15:23 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 84.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:15:26 [logger.py:43] Received request cmpl-0d12ffc062dc4241abb9792a49bc2610-0: prompt: 'Explain how to avoid padding inefficiencies.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 12027, 7420, 920, 304, 4772, 7164, 297, 29872, 2416, 819, 2478, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:36136 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:15:26 [engine.py:331] Added request cmpl-0d12ffc062dc4241abb9792a49bc2610-0.
INFO 08-09 13:15:28 [metrics.py:417] Avg prompt throughput: 2.6 tokens/s, Avg generation throughput: 92.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:15:33 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 94.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:15:36 [logger.py:43] Received request cmpl-a5d00d38b2224ef495bc7ec0d7261376-0: prompt: 'How does tokenizer pre-processing affect TTFT?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 947, 5993, 3950, 758, 29899, 19170, 6602, 323, 29911, 7818, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:41898 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:15:36 [engine.py:331] Added request cmpl-a5d00d38b2224ef495bc7ec0d7261376-0.
INFO 08-09 13:15:38 [metrics.py:417] Avg prompt throughput: 2.6 tokens/s, Avg generation throughput: 94.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 13:15:42 [logger.py:43] Received request cmpl-95ca2a16c7a944f1947d930be6ccfc3b-0: prompt: 'How to track per-prompt latency in logs?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 304, 5702, 639, 29899, 14032, 415, 23316, 1270, 297, 10748, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:43574 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:15:42 [engine.py:331] Added request cmpl-95ca2a16c7a944f1947d930be6ccfc3b-0.
INFO 08-09 13:15:43 [metrics.py:417] Avg prompt throughput: 2.6 tokens/s, Avg generation throughput: 97.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 13:15:48 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 94.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:15:53 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 96.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-09 13:15:53 [logger.py:43] Received request cmpl-06a47b9d2c5e445a8803e38d8b1a1d48-0: prompt: 'Explain streaming benchmarks with high concurrency.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 12027, 7420, 24820, 23513, 29879, 411, 1880, 3022, 10880, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:52880 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:15:53 [engine.py:331] Added request cmpl-06a47b9d2c5e445a8803e38d8b1a1d48-0.
INFO 08-09 13:15:58 [metrics.py:417] Avg prompt throughput: 2.2 tokens/s, Avg generation throughput: 94.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:16:03 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 100.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-09 13:16:03 [logger.py:43] Received request cmpl-e65949bfce234742aff80cf6325eaa9a-0: prompt: 'How to calculate power usage per prompt?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 304, 8147, 3081, 8744, 639, 9508, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:50004 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:16:03 [engine.py:331] Added request cmpl-e65949bfce234742aff80cf6325eaa9a-0.
INFO 08-09 13:16:08 [metrics.py:417] Avg prompt throughput: 1.8 tokens/s, Avg generation throughput: 95.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:16:13 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 95.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-09 13:16:14 [logger.py:43] Received request cmpl-5a499971e59d4bd9aeb90af87af840fb-0: prompt: 'How does the tokenizer handle emojis and symbols?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 947, 278, 5993, 3950, 4386, 953, 3848, 275, 322, 15072, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:45332 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:16:14 [engine.py:331] Added request cmpl-5a499971e59d4bd9aeb90af87af840fb-0.
INFO 08-09 13:16:18 [metrics.py:417] Avg prompt throughput: 2.6 tokens/s, Avg generation throughput: 95.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:16:23 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 94.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-09 13:16:24 [logger.py:43] Received request cmpl-5ff1c75ff08b4db7a18f38cf985b6001-0: prompt: 'How does model size affect inference latency?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 947, 1904, 2159, 6602, 27262, 23316, 1270, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:58674 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:16:24 [engine.py:331] Added request cmpl-5ff1c75ff08b4db7a18f38cf985b6001-0.
INFO 08-09 13:16:28 [metrics.py:417] Avg prompt throughput: 2.0 tokens/s, Avg generation throughput: 95.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:16:33 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 92.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-09 13:16:35 [logger.py:43] Received request cmpl-33d6008623994394b85d953fe1d21d53-0: prompt: 'How to debug slow streaming responses?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 304, 4744, 5232, 24820, 20890, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:47450 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:16:35 [engine.py:331] Added request cmpl-33d6008623994394b85d953fe1d21d53-0.
INFO 08-09 13:16:38 [metrics.py:417] Avg prompt throughput: 1.6 tokens/s, Avg generation throughput: 94.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:16:43 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 93.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:16:46 [logger.py:43] Received request cmpl-e9953df77eb04ebebf66f861a4fdd99d-0: prompt: 'What causes generation to stall midway?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 9946, 12623, 304, 380, 497, 7145, 1582, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:60940 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:16:46 [engine.py:331] Added request cmpl-e9953df77eb04ebebf66f861a4fdd99d-0.
INFO 08-09 13:16:48 [metrics.py:417] Avg prompt throughput: 2.0 tokens/s, Avg generation throughput: 93.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:16:53 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 94.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:16:56 [logger.py:43] Received request cmpl-d81fe67a3e9b42728882209e437b46bc-0: prompt: 'How to load a model on HPU using HF Transformers?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 304, 2254, 263, 1904, 373, 379, 7056, 773, 379, 29943, 4103, 689, 414, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:50122 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:16:56 [engine.py:331] Added request cmpl-d81fe67a3e9b42728882209e437b46bc-0.
INFO 08-09 13:16:58 [metrics.py:417] Avg prompt throughput: 3.2 tokens/s, Avg generation throughput: 94.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:17:00 [logger.py:43] Received request cmpl-41481b84d051434eab5e020f68ece634-0: prompt: 'How to pass prompt + history into a chat model?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 304, 1209, 9508, 718, 4955, 964, 263, 13563, 1904, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:50136 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:17:00 [engine.py:331] Added request cmpl-41481b84d051434eab5e020f68ece634-0.
INFO 08-09 13:17:03 [metrics.py:417] Avg prompt throughput: 2.4 tokens/s, Avg generation throughput: 97.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:17:08 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 96.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-09 13:17:10 [logger.py:43] Received request cmpl-2144c06df4794d7694379f0c3c5ad088-0: prompt: 'What is an OpenAI-compatible inference API?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 385, 4673, 23869, 29899, 23712, 27262, 3450, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:60108 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:17:10 [engine.py:331] Added request cmpl-2144c06df4794d7694379f0c3c5ad088-0.
INFO 08-09 13:17:13 [metrics.py:417] Avg prompt throughput: 2.2 tokens/s, Avg generation throughput: 93.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:17:18 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 95.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:17:21 [logger.py:43] Received request cmpl-6a0e40a87c114909b9a88e2436678f4e-0: prompt: 'How to handle decoding errors in stream?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 304, 4386, 1602, 3689, 4436, 297, 4840, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:42582 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:17:21 [engine.py:331] Added request cmpl-6a0e40a87c114909b9a88e2436678f4e-0.
INFO 08-09 13:17:23 [metrics.py:417] Avg prompt throughput: 2.0 tokens/s, Avg generation throughput: 97.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:17:28 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 98.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:17:31 [logger.py:43] Received request cmpl-5c8987cfb47a4c0b94c3953c83a5386d-0: prompt: 'What are temperature vs randomness in sampling?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 526, 10430, 7186, 4036, 2264, 297, 23460, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:44198 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:17:31 [engine.py:331] Added request cmpl-5c8987cfb47a4c0b94c3953c83a5386d-0.
INFO 08-09 13:17:33 [metrics.py:417] Avg prompt throughput: 2.0 tokens/s, Avg generation throughput: 94.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:17:38 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 95.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:17:41 [logger.py:43] Received request cmpl-2192f6d0c6074f0ea03f40c07f416710-0: prompt: 'Explain token logprobs in output.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 12027, 7420, 5993, 1480, 771, 5824, 297, 1962, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:49616 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:17:41 [engine.py:331] Added request cmpl-2192f6d0c6074f0ea03f40c07f416710-0.
INFO 08-09 13:17:43 [metrics.py:417] Avg prompt throughput: 2.0 tokens/s, Avg generation throughput: 98.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:17:48 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 100.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:17:51 [logger.py:43] Received request cmpl-77b8aa4f84174b6486a83e227a7bc4f5-0: prompt: 'How to measure and reduce TTFT on Gaudi?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 304, 5645, 322, 10032, 323, 29911, 7818, 373, 10415, 4749, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:56740 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:17:51 [engine.py:331] Added request cmpl-77b8aa4f84174b6486a83e227a7bc4f5-0.
INFO 08-09 13:17:53 [metrics.py:417] Avg prompt throughput: 2.6 tokens/s, Avg generation throughput: 97.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:17:58 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 95.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:18:02 [logger.py:43] Received request cmpl-c8f869ec453a47a79e5940e9ab4bfae2-0: prompt: 'Explain the internals of HF `generate()` loop.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 12027, 7420, 278, 2836, 1338, 310, 379, 29943, 421, 17158, 2555, 2425, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:37590 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:18:02 [engine.py:331] Added request cmpl-c8f869ec453a47a79e5940e9ab4bfae2-0.
INFO 08-09 13:18:03 [metrics.py:417] Avg prompt throughput: 2.8 tokens/s, Avg generation throughput: 95.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 13:18:08 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 95.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:18:12 [logger.py:43] Received request cmpl-f89bcc87814240aa96d1c7f5f9fdb316-0: prompt: 'What is speculative token generation?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 1580, 28524, 5993, 12623, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:46702 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:18:12 [engine.py:331] Added request cmpl-f89bcc87814240aa96d1c7f5f9fdb316-0.
INFO 08-09 13:18:13 [metrics.py:417] Avg prompt throughput: 1.6 tokens/s, Avg generation throughput: 94.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 13:18:18 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 95.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:18:23 [logger.py:43] Received request cmpl-c4c8ef489d0347db8f19a88202e67ec0-0: prompt: 'What are possible failure points in vLLM serving?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 526, 1950, 10672, 3291, 297, 325, 2208, 29924, 16330, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:42296 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:18:23 [engine.py:331] Added request cmpl-c4c8ef489d0347db8f19a88202e67ec0-0.
INFO 08-09 13:18:23 [metrics.py:417] Avg prompt throughput: 2.4 tokens/s, Avg generation throughput: 99.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 13:18:25 [logger.py:43] Received request cmpl-502974eecc734f9dad111ca7ff6ca7a2-0: prompt: 'How to catch stream decoding errors gracefully?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 304, 4380, 4840, 1602, 3689, 4436, 17659, 3730, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:42308 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:18:25 [engine.py:331] Added request cmpl-502974eecc734f9dad111ca7ff6ca7a2-0.
INFO 08-09 13:18:28 [metrics.py:417] Avg prompt throughput: 2.2 tokens/s, Avg generation throughput: 100.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:18:32 [logger.py:43] Received request cmpl-50bc26f1b33244eeb0c888e0b3a6a06f-0: prompt: 'How to enable/disable kv caching in transformers?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 304, 9025, 29914, 20472, 10908, 22488, 297, 4327, 414, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:52702 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:18:32 [engine.py:331] Added request cmpl-50bc26f1b33244eeb0c888e0b3a6a06f-0.
INFO 08-09 13:18:33 [metrics.py:417] Avg prompt throughput: 2.4 tokens/s, Avg generation throughput: 100.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 13:18:38 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 101.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:18:42 [logger.py:43] Received request cmpl-5b36e95703144131a9bb7e9371bcb72e-0: prompt: 'What are challenges of multilingual streaming inference?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 526, 18066, 267, 310, 1773, 6504, 950, 24820, 27262, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:47604 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:18:42 [engine.py:331] Added request cmpl-5b36e95703144131a9bb7e9371bcb72e-0.
INFO 08-09 13:18:43 [metrics.py:417] Avg prompt throughput: 2.4 tokens/s, Avg generation throughput: 99.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 13:18:48 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 96.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:18:53 [logger.py:43] Received request cmpl-b98dacba4a9a4c1aaadc75072c0196ad-0: prompt: 'How to benchmark with different model precisions?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 304, 23513, 411, 1422, 1904, 12132, 1080, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:40202 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:18:53 [engine.py:331] Added request cmpl-b98dacba4a9a4c1aaadc75072c0196ad-0.
INFO 08-09 13:18:53 [metrics.py:417] Avg prompt throughput: 2.0 tokens/s, Avg generation throughput: 95.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 13:18:58 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 95.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:19:03 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 94.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-09 13:19:03 [logger.py:43] Received request cmpl-66db2f0e4f4144ae95dc63453397ce0c-0: prompt: 'What is the tradeoff between token latency and batch size?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 278, 11302, 2696, 1546, 5993, 23316, 1270, 322, 9853, 2159, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:40250 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:19:03 [engine.py:331] Added request cmpl-66db2f0e4f4144ae95dc63453397ce0c-0.
INFO 08-09 13:19:08 [metrics.py:417] Avg prompt throughput: 2.8 tokens/s, Avg generation throughput: 94.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:19:13 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 93.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-09 13:19:14 [logger.py:43] Received request cmpl-d93dca9b44a04257ace08e9433d37952-0: prompt: 'Describe the advantages and limitations of transformer-based neural networks.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 20355, 915, 278, 25486, 322, 27028, 310, 4327, 261, 29899, 6707, 19677, 14379, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:59756 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:19:14 [engine.py:331] Added request cmpl-d93dca9b44a04257ace08e9433d37952-0.
INFO 08-09 13:19:18 [metrics.py:417] Avg prompt throughput: 3.0 tokens/s, Avg generation throughput: 93.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:19:23 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 93.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-09 13:19:25 [logger.py:43] Received request cmpl-0fcf300933d046bbab5cbb8538f399f6-0: prompt: 'Explain the concept of overfitting in machine learning and how to prevent it.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 12027, 7420, 278, 6964, 310, 975, 29888, 5367, 297, 4933, 6509, 322, 920, 304, 5557, 372, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:53698 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:19:25 [engine.py:331] Added request cmpl-0fcf300933d046bbab5cbb8538f399f6-0.
INFO 08-09 13:19:28 [metrics.py:417] Avg prompt throughput: 3.6 tokens/s, Avg generation throughput: 94.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:19:33 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 97.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-09 13:19:35 [logger.py:43] Received request cmpl-6fec4df079454c01892bb83596b47730-0: prompt: 'How does the attention mechanism work in transformer models?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 947, 278, 8570, 13336, 664, 297, 4327, 261, 4733, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:37172 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:19:35 [engine.py:331] Added request cmpl-6fec4df079454c01892bb83596b47730-0.
INFO 08-09 13:19:38 [metrics.py:417] Avg prompt throughput: 2.4 tokens/s, Avg generation throughput: 98.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:19:43 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 96.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-09 13:19:45 [logger.py:43] Received request cmpl-46a1a59d72474096aa65f37bbffe358f-0: prompt: 'Summarize the main differences between supervised and unsupervised learning.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 6991, 3034, 675, 278, 1667, 12651, 1546, 2428, 11292, 322, 443, 9136, 11292, 6509, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:43392 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:19:45 [engine.py:331] Added request cmpl-46a1a59d72474096aa65f37bbffe358f-0.
INFO 08-09 13:19:48 [metrics.py:417] Avg prompt throughput: 3.2 tokens/s, Avg generation throughput: 90.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:19:53 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 91.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:19:56 [logger.py:43] Received request cmpl-d8de03f355b14d28bc78d5eda8efb94d-0: prompt: 'What are the ethical concerns associated with deploying large language models?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 526, 278, 11314, 936, 21838, 6942, 411, 7246, 292, 2919, 4086, 4733, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:34548 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:19:56 [engine.py:331] Added request cmpl-d8de03f355b14d28bc78d5eda8efb94d-0.
INFO 08-09 13:19:58 [metrics.py:417] Avg prompt throughput: 3.0 tokens/s, Avg generation throughput: 90.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:20:03 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 91.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:20:07 [logger.py:43] Received request cmpl-a2c1ea03eebc4d56b334c4d8a3673086-0: prompt: 'Explain how gradient descent works in training deep neural networks.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 12027, 7420, 920, 16030, 26815, 1736, 297, 6694, 6483, 19677, 14379, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:60004 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:20:07 [engine.py:331] Added request cmpl-a2c1ea03eebc4d56b334c4d8a3673086-0.
INFO 08-09 13:20:08 [metrics.py:417] Avg prompt throughput: 2.6 tokens/s, Avg generation throughput: 90.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 13:20:13 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 93.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:20:18 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 92.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-09 13:20:18 [logger.py:43] Received request cmpl-5256edc90eda415bbcd591ff8f72363d-0: prompt: 'Describe the role of activation functions in deep learning.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 20355, 915, 278, 6297, 310, 26229, 3168, 297, 6483, 6509, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:44976 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:20:18 [engine.py:331] Added request cmpl-5256edc90eda415bbcd591ff8f72363d-0.
INFO 08-09 13:20:23 [metrics.py:417] Avg prompt throughput: 2.4 tokens/s, Avg generation throughput: 91.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:20:28 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 91.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-09 13:20:29 [logger.py:43] Received request cmpl-8a7657db306645d295d327c05cfc3763-0: prompt: 'How can transfer learning improve model performance on small datasets?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 508, 6782, 6509, 11157, 1904, 4180, 373, 2319, 20035, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:43314 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:20:29 [engine.py:331] Added request cmpl-8a7657db306645d295d327c05cfc3763-0.
INFO 08-09 13:20:33 [metrics.py:417] Avg prompt throughput: 2.4 tokens/s, Avg generation throughput: 87.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:20:38 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 91.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-09 13:20:40 [logger.py:43] Received request cmpl-6bbf32398ff44308b9ad1e14b7b757f8-0: prompt: 'Explain the concept of reinforcement learning with an example.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 12027, 7420, 278, 6964, 310, 15561, 1454, 13561, 6509, 411, 385, 1342, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:48352 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:20:40 [engine.py:331] Added request cmpl-6bbf32398ff44308b9ad1e14b7b757f8-0.
INFO 08-09 13:20:43 [metrics.py:417] Avg prompt throughput: 2.8 tokens/s, Avg generation throughput: 92.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:20:48 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 94.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:20:51 [logger.py:43] Received request cmpl-cf4ea96f970d45bdb24879014e32a815-0: prompt: 'What are the main components of a convolutional neural network (CNN)?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 526, 278, 1667, 7117, 310, 263, 26851, 284, 19677, 3564, 313, 29907, 10262, 6877], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:42792 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:20:51 [engine.py:331] Added request cmpl-cf4ea96f970d45bdb24879014e32a815-0.
INFO 08-09 13:20:53 [metrics.py:417] Avg prompt throughput: 3.2 tokens/s, Avg generation throughput: 93.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:20:58 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 90.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:21:02 [logger.py:43] Received request cmpl-fc7edfea25824b2ca96e66ad227a32f3-0: prompt: 'Describe how large language models like GPT or LLaMA are trained and fine-tuned.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 20355, 915, 920, 2919, 4086, 4733, 763, 402, 7982, 470, 365, 5661, 1529, 526, 16370, 322, 2691, 29899, 29873, 348, 287, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:54188 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:21:02 [engine.py:331] Added request cmpl-fc7edfea25824b2ca96e66ad227a32f3-0.
INFO 08-09 13:21:03 [metrics.py:417] Avg prompt throughput: 4.6 tokens/s, Avg generation throughput: 93.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:21:08 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 96.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:21:12 [logger.py:43] Received request cmpl-6de889dc54904c3395746737109f8ea3-0: prompt: 'Compare and contrast BERT and GPT in terms of architecture and use cases.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 3831, 598, 322, 12814, 350, 20161, 322, 402, 7982, 297, 4958, 310, 11258, 322, 671, 4251, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:37048 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 13:21:12 [engine.py:331] Added request cmpl-6de889dc54904c3395746737109f8ea3-0.
INFO 08-09 13:21:13 [metrics.py:417] Avg prompt throughput: 3.6 tokens/s, Avg generation throughput: 94.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 13:21:18 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 94.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 13:21:33 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 30.5 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 13:21:43 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
Received Interrupt
Received Interrupt
Received Interrupt
Received Interrupt
INFO 08-09 13:25:26 [launcher.py:80] Shutting down FastAPI HTTP server.
INFO:     Shutting down
INFO:     Waiting for application shutdown.
INFO:     Application shutdown complete.
/usr/lib/python3.10/multiprocessing/resource_tracker.py:224: UserWarning: resource_tracker: There appear to be 16 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
/usr/lib/python3.10/multiprocessing/resource_tracker.py:224: UserWarning: resource_tracker: There appear to be 1 leaked shared_memory objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
