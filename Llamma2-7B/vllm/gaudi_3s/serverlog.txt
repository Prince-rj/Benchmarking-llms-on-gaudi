Calling add_step_closure function does not have any effect. It's lazy mode only functionality. (warning logged once)
Calling mark_step function does not have any effect. It's lazy mode only functionality. (warning logged once)
Calling iter_mark_step function does not have any effect. It's lazy mode only functionality. (warning logged once)
INFO 08-09 13:50:58 [__init__.py:254] Automatically detected platform hpu.
INFO 08-09 13:51:00 [config.py:1975] Disabled the custom all-reduce kernel because it is not supported on current platform.
INFO 08-09 13:51:01 [config.py:1975] Disabled the custom all-reduce kernel because it is not supported on current platform.
INFO 08-09 13:51:01 [config.py:1975] Disabled the custom all-reduce kernel because it is not supported on current platform.
INFO 08-09 13:51:02 [config.py:1975] Disabled the custom all-reduce kernel because it is not supported on current platform.
INFO 08-09 13:51:02 [api_server.py:1289] vLLM API server version 0.6.3.dev5453+ge9c83fca1
INFO 08-09 13:51:02 [config.py:1975] Disabled the custom all-reduce kernel because it is not supported on current platform.
INFO 08-09 13:51:02 [cli_args.py:309] non-default args: {'model': '/software/data/llama_inference/Llama-2-7b-hf/', 'tensor_parallel_size': 2}
INFO 08-09 13:51:12 [config.py:822] This model supports multiple tasks: {'score', 'reward', 'embed', 'generate', 'classify'}. Defaulting to 'generate'.
WARNING 08-09 13:51:12 [config.py:3236] Your device 'hpu' doesn't support torch.float16. Falling back to torch.bfloat16 for compatibility.
WARNING 08-09 13:51:12 [config.py:3287] Casting torch.float16 to torch.bfloat16.
INFO 08-09 13:51:12 [arg_utils.py:1706] hpu is experimental on VLLM_USE_V1=1. Falling back to V0 Engine.
INFO 08-09 13:51:12 [config.py:1941] Defaulting to use mp for distributed inference
INFO 08-09 13:51:12 [config.py:1975] Disabled the custom all-reduce kernel because it is not supported on current platform.
INFO 08-09 13:51:12 [api_server.py:267] Started engine process with PID 8834
Calling add_step_closure function does not have any effect. It's lazy mode only functionality. (warning logged once)
Calling mark_step function does not have any effect. It's lazy mode only functionality. (warning logged once)
Calling iter_mark_step function does not have any effect. It's lazy mode only functionality. (warning logged once)
WARNING 08-09 13:51:14 [env_override.py:17] NCCL_CUMEM_ENABLE is set to 0, skipping override. This may increase memory overhead with cudagraph+allreduce: https://github.com/NVIDIA/nccl/issues/1234
INFO 08-09 13:51:15 [__init__.py:254] Automatically detected platform hpu.
INFO 08-09 13:51:17 [llm_engine.py:239] Initializing a V0 LLM engine (v0.6.3.dev5453+ge9c83fca1) with config: model='/software/data/llama_inference/Llama-2-7b-hf/', speculative_config=None, tokenizer='/software/data/llama_inference/Llama-2-7b-hf/', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, pipeline_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=hpu, decoding_config=DecodingConfig(backend='xgrammar', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=None, served_model_name=/software/data/llama_inference/Llama-2-7b-hf/, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, pooler_config=None, compilation_config={"level":0,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":[],"splitting_ops":[],"use_inductor":true,"compile_sizes":[],"inductor_compile_config":{"enable_auto_functionalized_v2":false},"inductor_passes":{},"use_cudagraph":false,"cudagraph_num_of_warmups":0,"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"max_capture_size":256,"local_cache_dir":null}, use_cached_outputs=True, 
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
WARNING 08-09 13:51:17 [multiproc_worker_utils.py:325] Reducing Torch parallelism from 52 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
WARNING 08-09 13:51:17 [rocm.py:28] Failed to import from amdsmi with ModuleNotFoundError("No module named 'amdsmi'")
WARNING 08-09 13:51:17 [rocm.py:33] Failed to import from vllm._C with ModuleNotFoundError("No module named 'vllm._C'")
WARNING 08-09 13:51:17 [rocm.py:39] Failed to import from vllm._rocm_C with ModuleNotFoundError("No module named 'vllm._rocm_C'")
WARNING 08-09 13:51:17 [utils.py:2850] Methods add_prompt_adapter,cache_config,compilation_config,current_platform,list_prompt_adapters,load_config,pin_prompt_adapter,remove_prompt_adapter,scheduler_config not implemented in <vllm.worker.hpu_worker.HPUWorker object at 0x79b692017e20>
INFO 08-09 13:51:18 [runtime.py:26] Environment:
INFO 08-09 13:51:18 [runtime.py:30]     hw: gaudi3
INFO 08-09 13:51:18 [runtime.py:30]     build: 1.22.0.425
INFO 08-09 13:51:18 [runtime.py:30]     engine_version: v0
INFO 08-09 13:51:18 [runtime.py:30]     bridge_mode: eager
INFO 08-09 13:51:18 [runtime.py:30]     model_type: llama
INFO 08-09 13:51:18 [runtime.py:26] Features:
INFO 08-09 13:51:18 [runtime.py:30]     fp32_alibi_biases: True
INFO 08-09 13:51:18 [runtime.py:30]     fp32_softmax: False
INFO 08-09 13:51:18 [runtime.py:30]     fused_block_softmax_adjustment: False
INFO 08-09 13:51:18 [runtime.py:30]     fused_block_softmax: False
INFO 08-09 13:51:18 [runtime.py:30]     prompt_attn_impl: fsdpa_impl
INFO 08-09 13:51:18 [runtime.py:30]     skip_warmup: False
INFO 08-09 13:51:18 [runtime.py:30]     merged_prefill: False
INFO 08-09 13:51:18 [runtime.py:30]     use_contiguous_pa: True
INFO 08-09 13:51:18 [runtime.py:30]     use_delayed_sampling: True
INFO 08-09 13:51:18 [runtime.py:30]     use_bucketing: True
INFO 08-09 13:51:18 [runtime.py:30]     bucketing_strategy: exponential_bucketing
INFO 08-09 13:51:18 [runtime.py:26] User flags:
INFO 08-09 13:51:18 [runtime.py:30]     VLLM_USE_V1: False
WARNING 08-09 13:51:18 [hpu.py:135] Pin memory is not supported on HPU.
INFO 08-09 13:51:18 [hpu.py:60] Using HPUAttention backend.
Calling add_step_closure function does not have any effect. It's lazy mode only functionality. (warning logged once)
Calling mark_step function does not have any effect. It's lazy mode only functionality. (warning logged once)
Calling iter_mark_step function does not have any effect. It's lazy mode only functionality. (warning logged once)
WARNING 08-09 13:51:19 [env_override.py:17] NCCL_CUMEM_ENABLE is set to 0, skipping override. This may increase memory overhead with cudagraph+allreduce: https://github.com/NVIDIA/nccl/issues/1234
INFO 08-09 13:51:20 [__init__.py:254] Automatically detected platform hpu.
[1;36m(VllmWorkerProcess pid=8946)[0;0m INFO 08-09 13:51:22 [multiproc_worker_utils.py:228] Worker ready; awaiting tasks
[1;36m(VllmWorkerProcess pid=8946)[0;0m WARNING 08-09 13:51:23 [rocm.py:28] Failed to import from amdsmi with ModuleNotFoundError("No module named 'amdsmi'")
[1;36m(VllmWorkerProcess pid=8946)[0;0m WARNING 08-09 13:51:23 [rocm.py:33] Failed to import from vllm._C with ModuleNotFoundError("No module named 'vllm._C'")
[1;36m(VllmWorkerProcess pid=8946)[0;0m WARNING 08-09 13:51:23 [rocm.py:39] Failed to import from vllm._rocm_C with ModuleNotFoundError("No module named 'vllm._rocm_C'")
[1;36m(VllmWorkerProcess pid=8946)[0;0m WARNING 08-09 13:51:23 [utils.py:2850] Methods add_prompt_adapter,cache_config,compilation_config,current_platform,list_prompt_adapters,load_config,pin_prompt_adapter,remove_prompt_adapter,scheduler_config not implemented in <vllm.worker.hpu_worker.HPUWorker object at 0x77ec21285360>
[1;36m(VllmWorkerProcess pid=8946)[0;0m INFO 08-09 13:51:23 [runtime.py:26] Environment:
[1;36m(VllmWorkerProcess pid=8946)[0;0m INFO 08-09 13:51:23 [runtime.py:30]     hw: gaudi3
[1;36m(VllmWorkerProcess pid=8946)[0;0m INFO 08-09 13:51:23 [runtime.py:30]     build: 1.22.0.425
[1;36m(VllmWorkerProcess pid=8946)[0;0m INFO 08-09 13:51:23 [runtime.py:30]     engine_version: v0
[1;36m(VllmWorkerProcess pid=8946)[0;0m INFO 08-09 13:51:23 [runtime.py:30]     bridge_mode: eager
[1;36m(VllmWorkerProcess pid=8946)[0;0m INFO 08-09 13:51:23 [runtime.py:30]     model_type: llama
[1;36m(VllmWorkerProcess pid=8946)[0;0m INFO 08-09 13:51:23 [runtime.py:26] Features:
[1;36m(VllmWorkerProcess pid=8946)[0;0m INFO 08-09 13:51:23 [runtime.py:30]     fp32_alibi_biases: True
[1;36m(VllmWorkerProcess pid=8946)[0;0m INFO 08-09 13:51:23 [runtime.py:30]     fp32_softmax: False
[1;36m(VllmWorkerProcess pid=8946)[0;0m INFO 08-09 13:51:23 [runtime.py:30]     fused_block_softmax_adjustment: False
[1;36m(VllmWorkerProcess pid=8946)[0;0m INFO 08-09 13:51:23 [runtime.py:30]     fused_block_softmax: False
[1;36m(VllmWorkerProcess pid=8946)[0;0m INFO 08-09 13:51:23 [runtime.py:30]     prompt_attn_impl: fsdpa_impl
[1;36m(VllmWorkerProcess pid=8946)[0;0m INFO 08-09 13:51:23 [runtime.py:30]     skip_warmup: False
[1;36m(VllmWorkerProcess pid=8946)[0;0m INFO 08-09 13:51:23 [runtime.py:30]     merged_prefill: False
[1;36m(VllmWorkerProcess pid=8946)[0;0m INFO 08-09 13:51:23 [runtime.py:30]     use_contiguous_pa: True
[1;36m(VllmWorkerProcess pid=8946)[0;0m INFO 08-09 13:51:23 [runtime.py:30]     use_delayed_sampling: True
[1;36m(VllmWorkerProcess pid=8946)[0;0m INFO 08-09 13:51:23 [runtime.py:30]     use_bucketing: True
[1;36m(VllmWorkerProcess pid=8946)[0;0m INFO 08-09 13:51:23 [runtime.py:30]     bucketing_strategy: exponential_bucketing
[1;36m(VllmWorkerProcess pid=8946)[0;0m INFO 08-09 13:51:23 [runtime.py:26] User flags:
[1;36m(VllmWorkerProcess pid=8946)[0;0m INFO 08-09 13:51:23 [runtime.py:30]     VLLM_USE_V1: False
[1;36m(VllmWorkerProcess pid=8946)[0;0m WARNING 08-09 13:51:23 [hpu.py:135] Pin memory is not supported on HPU.
[1;36m(VllmWorkerProcess pid=8946)[0;0m INFO 08-09 13:51:23 [hpu.py:60] Using HPUAttention backend.
============================= HPU SW VERSION ====================================== 
 HB_BUILD_VER = 1.22.0-425
============================= HPU PT BRIDGE CONFIGURATION ON RANK = 0 ============= 
 PT_HPU_LAZY_MODE = 0
 PT_HPU_RECIPE_CACHE_CONFIG = ,false,1024,false
 PT_HPU_MAX_COMPOUND_OP_SIZE = 9223372036854775807
 PT_HPU_LAZY_ACC_PAR_MODE = 1
 PT_HPU_ENABLE_REFINE_DYNAMIC_SHAPES = 0
 PT_HPU_EAGER_PIPELINE_ENABLE = 1
 PT_HPU_EAGER_COLLECTIVE_PIPELINE_ENABLE = 1
 PT_HPU_ENABLE_LAZY_COLLECTIVES = 1
---------------------------: System Configuration :---------------------------
Num CPU Cores : 224
CPU RAM       : 1007 GB
------------------------------------------------------------------------------
============================= HPU SW VERSION ====================================== 
 HB_BUILD_VER = 1.22.0-425
============================= HPU PT BRIDGE CONFIGURATION ON RANK = 0 ============= 
 PT_HPU_LAZY_MODE = 0
 PT_HPU_RECIPE_CACHE_CONFIG = ,false,1024,false
 PT_HPU_MAX_COMPOUND_OP_SIZE = 9223372036854775807
 PT_HPU_LAZY_ACC_PAR_MODE = 1
 PT_HPU_ENABLE_REFINE_DYNAMIC_SHAPES = 0
 PT_HPU_EAGER_PIPELINE_ENABLE = 1
 PT_HPU_EAGER_COLLECTIVE_PIPELINE_ENABLE = 1
 PT_HPU_ENABLE_LAZY_COLLECTIVES = 1
---------------------------: System Configuration :---------------------------
Num CPU Cores : 224
CPU RAM       : 1007 GB
------------------------------------------------------------------------------
INFO 08-09 13:51:24 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[1], buffer_handle=(1, 4194304, 6, 'psm_4caee60a'), local_subscribe_addr='ipc:///tmp/aae49329-508c-479e-84cb-1ab770c59d1c', remote_subscribe_addr=None, remote_addr_ipv6=False)
INFO 08-09 13:51:24 [parallel_state.py:1065] rank 0 in world size 2 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(VllmWorkerProcess pid=8946)[0;0m INFO 08-09 13:51:24 [parallel_state.py:1065] rank 1 in world size 2 is assigned as DP rank 0, PP rank 0, TP rank 1, EP rank 1
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  2.21it/s]
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.14it/s]
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.22it/s]

INFO 08-09 13:51:27 [default_loader.py:272] Loading weights took 1.72 seconds
[1;36m(VllmWorkerProcess pid=8946)[0;0m INFO 08-09 13:51:27 [default_loader.py:272] Loading weights took 1.68 seconds
INFO 08-09 13:51:27 [hpu_model_runner.py:1184] Pre-loading model weights on hpu:0 took 6.277 GiB of device memory (6.284 GiB/126.5 GiB used) and 49.47 MiB of host memory (150.1 GiB/1007 GiB used)
[1;36m(VllmWorkerProcess pid=8946)[0;0m INFO 08-09 13:51:27 [hpu_model_runner.py:1184] Pre-loading model weights on hpu:0 took 6.277 GiB of device memory (6.284 GiB/126.5 GiB used) and -17.49 MiB of host memory (150.1 GiB/1007 GiB used)
INFO 08-09 13:51:27 [hpu_model_runner.py:1266] Wrapping in HPU Graph took 0 B of device memory (6.284 GiB/126.5 GiB used) and -4.73 MiB of host memory (150.1 GiB/1007 GiB used)
[1;36m(VllmWorkerProcess pid=8946)[0;0m INFO 08-09 13:51:27 [hpu_model_runner.py:1266] Wrapping in HPU Graph took 0 B of device memory (6.284 GiB/126.5 GiB used) and -5.875 MiB of host memory (150.1 GiB/1007 GiB used)
INFO 08-09 13:51:28 [hpu_model_runner.py:1270] Compiling took 0 B of device memory (6.284 GiB/126.5 GiB used) and -6.941 MiB of host memory (150.1 GiB/1007 GiB used)
INFO 08-09 13:51:28 [hpu_model_runner.py:1274] Loading model weights took in total 6.277 GiB of device memory (6.284 GiB/126.5 GiB used) and 31.75 MiB of host memory (150.1 GiB/1007 GiB used)
[1;36m(VllmWorkerProcess pid=8946)[0;0m INFO 08-09 13:51:28 [hpu_model_runner.py:1270] Compiling took 0 B of device memory (6.284 GiB/126.5 GiB used) and -4.547 MiB of host memory (150.1 GiB/1007 GiB used)
[1;36m(VllmWorkerProcess pid=8946)[0;0m INFO 08-09 13:51:28 [hpu_model_runner.py:1274] Loading model weights took in total 6.277 GiB of device memory (6.284 GiB/126.5 GiB used) and 29.71 MiB of host memory (150.1 GiB/1007 GiB used)
WARNING 08-09 13:51:28 [common.py:139] Prompt bucket for (1, 4096, 0) was not prepared. Adding new bucket: (1, 4096, 0)
[1;36m(VllmWorkerProcess pid=8946)[0;0m WARNING 08-09 13:51:28 [common.py:139] Prompt bucket for (1, 4096, 0) was not prepared. Adding new bucket: (1, 4096, 0)
[1;36m(VllmWorkerProcess pid=8946)[0;0m INFO 08-09 13:51:30 [hpu_worker.py:289] Model profiling run took 300 MiB of device memory (6.577 GiB/126.5 GiB used) and -53.05 MiB of host memory (150.1 GiB/1007 GiB used)
[1;36m(VllmWorkerProcess pid=8946)[0;0m INFO 08-09 13:51:30 [hpu_worker.py:313] Free device memory: 120 GiB, 108 GiB usable (gpu_memory_utilization=0.9), 10.8 GiB reserved for HPUGraphs (VLLM_GRAPH_RESERVED_MEM=0.1), 97.17 GiB reserved for KV cache
INFO 08-09 13:51:30 [hpu_worker.py:289] Model profiling run took 300 MiB of device memory (6.577 GiB/126.5 GiB used) and 15.94 MiB of host memory (150.1 GiB/1007 GiB used)
INFO 08-09 13:51:30 [hpu_worker.py:313] Free device memory: 120 GiB, 108 GiB usable (gpu_memory_utilization=0.9), 10.8 GiB reserved for HPUGraphs (VLLM_GRAPH_RESERVED_MEM=0.1), 97.17 GiB reserved for KV cache
INFO 08-09 13:51:31 [executor_base.py:113] # hpu blocks: 3109, # CPU blocks: 128
INFO 08-09 13:51:31 [executor_base.py:118] Maximum concurrency for 4096 tokens per request: 97.16x
INFO 08-09 13:51:31 [exponential.py:49] Prompt bucket config (min, step, max_warmup, limit) bs:[1, 2, 256, 9], seq:[128, 128, 4096, 13]
[1;36m(VllmWorkerProcess pid=8946)[0;0m INFO 08-09 13:51:31 [exponential.py:49] Prompt bucket config (min, step, max_warmup, limit) bs:[1, 2, 256, 9], seq:[128, 128, 4096, 13]
INFO 08-09 13:51:31 [common.py:117] Generated 38 prompt buckets [bs, query, num_blocks]: [(1, 128, 0), (1, 256, 0), (1, 384, 0), (1, 512, 0), (1, 640, 0), (1, 768, 0), (1, 896, 0), (1, 1024, 0), (1, 1408, 0), (1, 1792, 0), (1, 2304, 0), (1, 3072, 0), (1, 4096, 0), (2, 128, 0), (2, 256, 0), (2, 384, 0), (2, 512, 0), (2, 640, 0), (2, 768, 0), (2, 896, 0), (2, 1024, 0), (2, 1408, 0), (2, 1792, 0), (4, 128, 0), (4, 256, 0), (4, 384, 0), (4, 512, 0), (4, 640, 0), (4, 768, 0), (4, 896, 0), (4, 1024, 0), (8, 128, 0), (8, 256, 0), (8, 384, 0), (8, 512, 0), (16, 128, 0), (16, 256, 0), (32, 128, 0)]
INFO 08-09 13:51:31 [exponential.py:79] Decode bucket config (min, step, max_warmup, limit) bs:[1, 2, 256, 9], block:[128, 128, 3109, 13]
[1;36m(VllmWorkerProcess pid=8946)[0;0m INFO 08-09 13:51:31 [common.py:117] Generated 38 prompt buckets [bs, query, num_blocks]: [(1, 128, 0), (1, 256, 0), (1, 384, 0), (1, 512, 0), (1, 640, 0), (1, 768, 0), (1, 896, 0), (1, 1024, 0), (1, 1408, 0), (1, 1792, 0), (1, 2304, 0), (1, 3072, 0), (1, 4096, 0), (2, 128, 0), (2, 256, 0), (2, 384, 0), (2, 512, 0), (2, 640, 0), (2, 768, 0), (2, 896, 0), (2, 1024, 0), (2, 1408, 0), (2, 1792, 0), (4, 128, 0), (4, 256, 0), (4, 384, 0), (4, 512, 0), (4, 640, 0), (4, 768, 0), (4, 896, 0), (4, 1024, 0), (8, 128, 0), (8, 256, 0), (8, 384, 0), (8, 512, 0), (16, 128, 0), (16, 256, 0), (32, 128, 0)]
[1;36m(VllmWorkerProcess pid=8946)[0;0m INFO 08-09 13:51:31 [exponential.py:79] Decode bucket config (min, step, max_warmup, limit) bs:[1, 2, 256, 9], block:[128, 128, 3109, 13]
INFO 08-09 13:51:31 [common.py:117] Generated 117 decode buckets [bs, query, num_blocks]: [(1, 1, 128), (1, 1, 256), (1, 1, 384), (1, 1, 512), (1, 1, 640), (1, 1, 768), (1, 1, 896), (1, 1, 1024), (1, 1, 1152), (1, 1, 1408), (1, 1, 1920), (1, 1, 2432), (1, 1, 3109), (2, 1, 128), (2, 1, 256), (2, 1, 384), (2, 1, 512), (2, 1, 640), (2, 1, 768), (2, 1, 896), (2, 1, 1024), (2, 1, 1152), (2, 1, 1408), (2, 1, 1920), (2, 1, 2432), (2, 1, 3109), (4, 1, 128), (4, 1, 256), (4, 1, 384), (4, 1, 512), (4, 1, 640), (4, 1, 768), (4, 1, 896), (4, 1, 1024), (4, 1, 1152), (4, 1, 1408), (4, 1, 1920), (4, 1, 2432), (4, 1, 3109), (8, 1, 128), (8, 1, 256), (8, 1, 384), (8, 1, 512), (8, 1, 640), (8, 1, 768), (8, 1, 896), (8, 1, 1024), (8, 1, 1152), (8, 1, 1408), (8, 1, 1920), (8, 1, 2432), (8, 1, 3109), (16, 1, 128), (16, 1, 256), (16, 1, 384), (16, 1, 512), (16, 1, 640), (16, 1, 768), (16, 1, 896), (16, 1, 1024), (16, 1, 1152), (16, 1, 1408), (16, 1, 1920), (16, 1, 2432), (16, 1, 3109), (32, 1, 128), (32, 1, 256), (32, 1, 384), (32, 1, 512), (32, 1, 640), (32, 1, 768), (32, 1, 896), (32, 1, 1024), (32, 1, 1152), (32, 1, 1408), (32, 1, 1920), (32, 1, 2432), (32, 1, 3109), (64, 1, 128), (64, 1, 256), (64, 1, 384), (64, 1, 512), (64, 1, 640), (64, 1, 768), (64, 1, 896), (64, 1, 1024), (64, 1, 1152), (64, 1, 1408), (64, 1, 1920), (64, 1, 2432), (64, 1, 3109), (128, 1, 128), (128, 1, 256), (128, 1, 384), (128, 1, 512), (128, 1, 640), (128, 1, 768), (128, 1, 896), (128, 1, 1024), (128, 1, 1152), (128, 1, 1408), (128, 1, 1920), (128, 1, 2432), (128, 1, 3109), (256, 1, 128), (256, 1, 256), (256, 1, 384), (256, 1, 512), (256, 1, 640), (256, 1, 768), (256, 1, 896), (256, 1, 1024), (256, 1, 1152), (256, 1, 1408), (256, 1, 1920), (256, 1, 2432), (256, 1, 3109)]
[1;36m(VllmWorkerProcess pid=8946)[0;0m INFO 08-09 13:51:31 [common.py:117] Generated 117 decode buckets [bs, query, num_blocks]: [(1, 1, 128), (1, 1, 256), (1, 1, 384), (1, 1, 512), (1, 1, 640), (1, 1, 768), (1, 1, 896), (1, 1, 1024), (1, 1, 1152), (1, 1, 1408), (1, 1, 1920), (1, 1, 2432), (1, 1, 3109), (2, 1, 128), (2, 1, 256), (2, 1, 384), (2, 1, 512), (2, 1, 640), (2, 1, 768), (2, 1, 896), (2, 1, 1024), (2, 1, 1152), (2, 1, 1408), (2, 1, 1920), (2, 1, 2432), (2, 1, 3109), (4, 1, 128), (4, 1, 256), (4, 1, 384), (4, 1, 512), (4, 1, 640), (4, 1, 768), (4, 1, 896), (4, 1, 1024), (4, 1, 1152), (4, 1, 1408), (4, 1, 1920), (4, 1, 2432), (4, 1, 3109), (8, 1, 128), (8, 1, 256), (8, 1, 384), (8, 1, 512), (8, 1, 640), (8, 1, 768), (8, 1, 896), (8, 1, 1024), (8, 1, 1152), (8, 1, 1408), (8, 1, 1920), (8, 1, 2432), (8, 1, 3109), (16, 1, 128), (16, 1, 256), (16, 1, 384), (16, 1, 512), (16, 1, 640), (16, 1, 768), (16, 1, 896), (16, 1, 1024), (16, 1, 1152), (16, 1, 1408), (16, 1, 1920), (16, 1, 2432), (16, 1, 3109), (32, 1, 128), (32, 1, 256), (32, 1, 384), (32, 1, 512), (32, 1, 640), (32, 1, 768), (32, 1, 896), (32, 1, 1024), (32, 1, 1152), (32, 1, 1408), (32, 1, 1920), (32, 1, 2432), (32, 1, 3109), (64, 1, 128), (64, 1, 256), (64, 1, 384), (64, 1, 512), (64, 1, 640), (64, 1, 768), (64, 1, 896), (64, 1, 1024), (64, 1, 1152), (64, 1, 1408), (64, 1, 1920), (64, 1, 2432), (64, 1, 3109), (128, 1, 128), (128, 1, 256), (128, 1, 384), (128, 1, 512), (128, 1, 640), (128, 1, 768), (128, 1, 896), (128, 1, 1024), (128, 1, 1152), (128, 1, 1408), (128, 1, 1920), (128, 1, 2432), (128, 1, 3109), (256, 1, 128), (256, 1, 256), (256, 1, 384), (256, 1, 512), (256, 1, 640), (256, 1, 768), (256, 1, 896), (256, 1, 1024), (256, 1, 1152), (256, 1, 1408), (256, 1, 1920), (256, 1, 2432), (256, 1, 3109)]
[1;36m(VllmWorkerProcess pid=8946)[0;0m INFO 08-09 13:51:33 [hpu_worker.py:350] Initializing cache engine took 97.16 GiB of device memory (103.7 GiB/126.5 GiB used) and 7.989 GiB of host memory (158.1 GiB/1007 GiB used)
INFO 08-09 13:51:33 [hpu_worker.py:350] Initializing cache engine took 97.16 GiB of device memory (103.7 GiB/126.5 GiB used) and 7.97 GiB of host memory (158.1 GiB/1007 GiB used)
[1;36m(VllmWorkerProcess pid=8946)[0;0m INFO 08-09 13:51:33 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][1/38] batch_size:32 query_len:128 num_blocks:0 free_mem:22.81 GiB
INFO 08-09 13:51:33 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][1/38] batch_size:32 query_len:128 num_blocks:0 free_mem:22.81 GiB
INFO 08-09 13:51:35 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][2/38] batch_size:16 query_len:256 num_blocks:0 free_mem:22.81 GiB
[1;36m(VllmWorkerProcess pid=8946)[0;0m INFO 08-09 13:51:35 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][2/38] batch_size:16 query_len:256 num_blocks:0 free_mem:22.81 GiB
INFO 08-09 13:51:37 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][3/38] batch_size:16 query_len:128 num_blocks:0 free_mem:22.81 GiB
[1;36m(VllmWorkerProcess pid=8946)[0;0m INFO 08-09 13:51:37 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][3/38] batch_size:16 query_len:128 num_blocks:0 free_mem:22.81 GiB
[1;36m(VllmWorkerProcess pid=8946)[0;0m INFO 08-09 13:51:38 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][4/38] batch_size:8 query_len:512 num_blocks:0 free_mem:22.81 GiB
INFO 08-09 13:51:38 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][4/38] batch_size:8 query_len:512 num_blocks:0 free_mem:22.81 GiB
INFO 08-09 13:51:41 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][5/38] batch_size:8 query_len:384 num_blocks:0 free_mem:22.81 GiB
[1;36m(VllmWorkerProcess pid=8946)[0;0m INFO 08-09 13:51:41 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][5/38] batch_size:8 query_len:384 num_blocks:0 free_mem:22.81 GiB
INFO 08-09 13:51:43 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][6/38] batch_size:8 query_len:256 num_blocks:0 free_mem:22.81 GiB
[1;36m(VllmWorkerProcess pid=8946)[0;0m INFO 08-09 13:51:43 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][6/38] batch_size:8 query_len:256 num_blocks:0 free_mem:22.81 GiB
INFO 08-09 13:51:44 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][7/38] batch_size:8 query_len:128 num_blocks:0 free_mem:22.81 GiB
[1;36m(VllmWorkerProcess pid=8946)[0;0m INFO 08-09 13:51:44 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][7/38] batch_size:8 query_len:128 num_blocks:0 free_mem:22.81 GiB
INFO 08-09 13:51:46 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][8/38] batch_size:4 query_len:1024 num_blocks:0 free_mem:22.81 GiB
[1;36m(VllmWorkerProcess pid=8946)[0;0m INFO 08-09 13:51:46 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][8/38] batch_size:4 query_len:1024 num_blocks:0 free_mem:22.81 GiB
[1;36m(VllmWorkerProcess pid=8946)[0;0m INFO 08-09 13:51:49 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][9/38] batch_size:4 query_len:896 num_blocks:0 free_mem:22.81 GiB
INFO 08-09 13:51:49 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][9/38] batch_size:4 query_len:896 num_blocks:0 free_mem:22.81 GiB
INFO 08-09 13:51:51 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][10/38] batch_size:4 query_len:768 num_blocks:0 free_mem:22.81 GiB
[1;36m(VllmWorkerProcess pid=8946)[0;0m INFO 08-09 13:51:51 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][10/38] batch_size:4 query_len:768 num_blocks:0 free_mem:22.81 GiB
[1;36m(VllmWorkerProcess pid=8946)[0;0m INFO 08-09 13:51:53 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][11/38] batch_size:4 query_len:640 num_blocks:0 free_mem:22.81 GiB
INFO 08-09 13:51:53 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][11/38] batch_size:4 query_len:640 num_blocks:0 free_mem:22.81 GiB
INFO 08-09 13:51:55 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][12/38] batch_size:4 query_len:512 num_blocks:0 free_mem:22.81 GiB
[1;36m(VllmWorkerProcess pid=8946)[0;0m INFO 08-09 13:51:55 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][12/38] batch_size:4 query_len:512 num_blocks:0 free_mem:22.81 GiB
INFO 08-09 13:51:57 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][13/38] batch_size:4 query_len:384 num_blocks:0 free_mem:22.81 GiB
[1;36m(VllmWorkerProcess pid=8946)[0;0m INFO 08-09 13:51:57 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][13/38] batch_size:4 query_len:384 num_blocks:0 free_mem:22.81 GiB
INFO 08-09 13:51:59 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][14/38] batch_size:4 query_len:256 num_blocks:0 free_mem:22.81 GiB
[1;36m(VllmWorkerProcess pid=8946)[0;0m INFO 08-09 13:51:59 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][14/38] batch_size:4 query_len:256 num_blocks:0 free_mem:22.81 GiB
INFO 08-09 13:52:01 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][15/38] batch_size:4 query_len:128 num_blocks:0 free_mem:22.81 GiB
[1;36m(VllmWorkerProcess pid=8946)[0;0m INFO 08-09 13:52:01 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][15/38] batch_size:4 query_len:128 num_blocks:0 free_mem:22.81 GiB
INFO 08-09 13:52:03 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][16/38] batch_size:2 query_len:1792 num_blocks:0 free_mem:22.81 GiB
[1;36m(VllmWorkerProcess pid=8946)[0;0m INFO 08-09 13:52:03 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][16/38] batch_size:2 query_len:1792 num_blocks:0 free_mem:22.81 GiB
[1;36m(VllmWorkerProcess pid=8946)[0;0m INFO 08-09 13:52:05 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][17/38] batch_size:2 query_len:1408 num_blocks:0 free_mem:22.81 GiB
INFO 08-09 13:52:05 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][17/38] batch_size:2 query_len:1408 num_blocks:0 free_mem:22.81 GiB
INFO 08-09 13:52:08 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][18/38] batch_size:2 query_len:1024 num_blocks:0 free_mem:22.81 GiB
[1;36m(VllmWorkerProcess pid=8946)[0;0m INFO 08-09 13:52:08 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][18/38] batch_size:2 query_len:1024 num_blocks:0 free_mem:22.81 GiB
[1;36m(VllmWorkerProcess pid=8946)[0;0m INFO 08-09 13:52:10 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][19/38] batch_size:2 query_len:896 num_blocks:0 free_mem:22.81 GiB
INFO 08-09 13:52:10 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][19/38] batch_size:2 query_len:896 num_blocks:0 free_mem:22.81 GiB
INFO 08-09 13:52:12 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][20/38] batch_size:2 query_len:768 num_blocks:0 free_mem:22.81 GiB
[1;36m(VllmWorkerProcess pid=8946)[0;0m INFO 08-09 13:52:12 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][20/38] batch_size:2 query_len:768 num_blocks:0 free_mem:22.81 GiB
[1;36m(VllmWorkerProcess pid=8946)[0;0m INFO 08-09 13:52:14 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][21/38] batch_size:2 query_len:640 num_blocks:0 free_mem:22.81 GiB
INFO 08-09 13:52:14 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][21/38] batch_size:2 query_len:640 num_blocks:0 free_mem:22.81 GiB
INFO 08-09 13:52:17 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][22/38] batch_size:2 query_len:512 num_blocks:0 free_mem:22.81 GiB
[1;36m(VllmWorkerProcess pid=8946)[0;0m INFO 08-09 13:52:17 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][22/38] batch_size:2 query_len:512 num_blocks:0 free_mem:22.81 GiB
INFO 08-09 13:52:19 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][23/38] batch_size:2 query_len:384 num_blocks:0 free_mem:22.81 GiB
[1;36m(VllmWorkerProcess pid=8946)[0;0m INFO 08-09 13:52:19 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][23/38] batch_size:2 query_len:384 num_blocks:0 free_mem:22.81 GiB
INFO 08-09 13:52:21 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][24/38] batch_size:2 query_len:256 num_blocks:0 free_mem:22.81 GiB
[1;36m(VllmWorkerProcess pid=8946)[0;0m INFO 08-09 13:52:21 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][24/38] batch_size:2 query_len:256 num_blocks:0 free_mem:22.81 GiB
INFO 08-09 13:52:23 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][25/38] batch_size:2 query_len:128 num_blocks:0 free_mem:22.81 GiB
[1;36m(VllmWorkerProcess pid=8946)[0;0m INFO 08-09 13:52:23 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][25/38] batch_size:2 query_len:128 num_blocks:0 free_mem:22.81 GiB
INFO 08-09 13:52:25 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][26/38] batch_size:1 query_len:4096 num_blocks:0 free_mem:22.81 GiB
[1;36m(VllmWorkerProcess pid=8946)[0;0m INFO 08-09 13:52:25 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][26/38] batch_size:1 query_len:4096 num_blocks:0 free_mem:22.81 GiB
INFO 08-09 13:52:28 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][27/38] batch_size:1 query_len:3072 num_blocks:0 free_mem:22.81 GiB
[1;36m(VllmWorkerProcess pid=8946)[0;0m INFO 08-09 13:52:28 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][27/38] batch_size:1 query_len:3072 num_blocks:0 free_mem:22.81 GiB
INFO 08-09 13:52:31 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][28/38] batch_size:1 query_len:2304 num_blocks:0 free_mem:22.81 GiB
[1;36m(VllmWorkerProcess pid=8946)[0;0m INFO 08-09 13:52:31 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][28/38] batch_size:1 query_len:2304 num_blocks:0 free_mem:22.81 GiB
[1;36m(VllmWorkerProcess pid=8946)[0;0m INFO 08-09 13:52:33 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][29/38] batch_size:1 query_len:1792 num_blocks:0 free_mem:22.81 GiB
INFO 08-09 13:52:33 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][29/38] batch_size:1 query_len:1792 num_blocks:0 free_mem:22.81 GiB
INFO 08-09 13:52:35 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][30/38] batch_size:1 query_len:1408 num_blocks:0 free_mem:22.81 GiB
[1;36m(VllmWorkerProcess pid=8946)[0;0m INFO 08-09 13:52:35 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][30/38] batch_size:1 query_len:1408 num_blocks:0 free_mem:22.81 GiB
INFO 08-09 13:52:38 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][31/38] batch_size:1 query_len:1024 num_blocks:0 free_mem:22.81 GiB
[1;36m(VllmWorkerProcess pid=8946)[0;0m INFO 08-09 13:52:38 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][31/38] batch_size:1 query_len:1024 num_blocks:0 free_mem:22.81 GiB
INFO 08-09 13:52:40 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][32/38] batch_size:1 query_len:896 num_blocks:0 free_mem:22.81 GiB
[1;36m(VllmWorkerProcess pid=8946)[0;0m INFO 08-09 13:52:40 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][32/38] batch_size:1 query_len:896 num_blocks:0 free_mem:22.81 GiB
INFO 08-09 13:52:43 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][33/38] batch_size:1 query_len:768 num_blocks:0 free_mem:22.81 GiB
[1;36m(VllmWorkerProcess pid=8946)[0;0m INFO 08-09 13:52:43 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][33/38] batch_size:1 query_len:768 num_blocks:0 free_mem:22.81 GiB
INFO 08-09 13:52:45 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][34/38] batch_size:1 query_len:640 num_blocks:0 free_mem:22.81 GiB
[1;36m(VllmWorkerProcess pid=8946)[0;0m INFO 08-09 13:52:45 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][34/38] batch_size:1 query_len:640 num_blocks:0 free_mem:22.81 GiB
[1;36m(VllmWorkerProcess pid=8946)[0;0m INFO 08-09 13:52:47 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][35/38] batch_size:1 query_len:512 num_blocks:0 free_mem:22.81 GiB
INFO 08-09 13:52:47 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][35/38] batch_size:1 query_len:512 num_blocks:0 free_mem:22.81 GiB
INFO 08-09 13:52:50 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][36/38] batch_size:1 query_len:384 num_blocks:0 free_mem:22.81 GiB
[1;36m(VllmWorkerProcess pid=8946)[0;0m INFO 08-09 13:52:50 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][36/38] batch_size:1 query_len:384 num_blocks:0 free_mem:22.81 GiB
INFO 08-09 13:52:52 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][37/38] batch_size:1 query_len:256 num_blocks:0 free_mem:22.81 GiB
[1;36m(VllmWorkerProcess pid=8946)[0;0m INFO 08-09 13:52:52 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][37/38] batch_size:1 query_len:256 num_blocks:0 free_mem:22.81 GiB
[1;36m(VllmWorkerProcess pid=8946)[0;0m INFO 08-09 13:52:55 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][38/38] batch_size:1 query_len:128 num_blocks:0 free_mem:22.81 GiB
INFO 08-09 13:52:55 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][38/38] batch_size:1 query_len:128 num_blocks:0 free_mem:22.81 GiB
[1;36m(VllmWorkerProcess pid=8946)[0;0m INFO 08-09 13:52:57 [hpu_model_runner.py:3029] [Warmup][Graph/decode][1/117] batch_size:256 query_len:1 num_blocks:3109 free_mem:22.81 GiB
INFO 08-09 13:52:57 [hpu_model_runner.py:3029] [Warmup][Graph/decode][1/117] batch_size:256 query_len:1 num_blocks:3109 free_mem:22.81 GiB
INFO 08-09 13:53:00 [hpu_model_runner.py:3029] [Warmup][Graph/decode][2/117] batch_size:256 query_len:1 num_blocks:2432 free_mem:22.81 GiB
[1;36m(VllmWorkerProcess pid=8946)[0;0m INFO 08-09 13:53:00 [hpu_model_runner.py:3029] [Warmup][Graph/decode][2/117] batch_size:256 query_len:1 num_blocks:2432 free_mem:22.81 GiB
INFO 08-09 13:53:03 [hpu_model_runner.py:3029] [Warmup][Graph/decode][3/117] batch_size:256 query_len:1 num_blocks:1920 free_mem:22.81 GiB
[1;36m(VllmWorkerProcess pid=8946)[0;0m INFO 08-09 13:53:03 [hpu_model_runner.py:3029] [Warmup][Graph/decode][3/117] batch_size:256 query_len:1 num_blocks:1920 free_mem:22.81 GiB
[1;36m(VllmWorkerProcess pid=8946)[0;0m INFO 08-09 13:53:06 [hpu_model_runner.py:3029] [Warmup][Graph/decode][4/117] batch_size:256 query_len:1 num_blocks:1408 free_mem:22.81 GiB
INFO 08-09 13:53:06 [hpu_model_runner.py:3029] [Warmup][Graph/decode][4/117] batch_size:256 query_len:1 num_blocks:1408 free_mem:22.81 GiB
INFO 08-09 13:53:08 [hpu_model_runner.py:3029] [Warmup][Graph/decode][5/117] batch_size:256 query_len:1 num_blocks:1152 free_mem:22.81 GiB
[1;36m(VllmWorkerProcess pid=8946)[0;0m INFO 08-09 13:53:08 [hpu_model_runner.py:3029] [Warmup][Graph/decode][5/117] batch_size:256 query_len:1 num_blocks:1152 free_mem:22.81 GiB
INFO 08-09 13:53:11 [hpu_model_runner.py:3029] [Warmup][Graph/decode][6/117] batch_size:256 query_len:1 num_blocks:1024 free_mem:22.81 GiB
[1;36m(VllmWorkerProcess pid=8946)[0;0m INFO 08-09 13:53:11 [hpu_model_runner.py:3029] [Warmup][Graph/decode][6/117] batch_size:256 query_len:1 num_blocks:1024 free_mem:22.81 GiB
INFO 08-09 13:53:14 [hpu_model_runner.py:3029] [Warmup][Graph/decode][7/117] batch_size:256 query_len:1 num_blocks:896 free_mem:22.81 GiB
[1;36m(VllmWorkerProcess pid=8946)[0;0m INFO 08-09 13:53:14 [hpu_model_runner.py:3029] [Warmup][Graph/decode][7/117] batch_size:256 query_len:1 num_blocks:896 free_mem:22.81 GiB
[1;36m(VllmWorkerProcess pid=8946)[0;0m INFO 08-09 13:53:17 [hpu_model_runner.py:3029] [Warmup][Graph/decode][8/117] batch_size:256 query_len:1 num_blocks:768 free_mem:22.81 GiB
INFO 08-09 13:53:17 [hpu_model_runner.py:3029] [Warmup][Graph/decode][8/117] batch_size:256 query_len:1 num_blocks:768 free_mem:22.81 GiB
[1;36m(VllmWorkerProcess pid=8946)[0;0m INFO 08-09 13:53:20 [hpu_model_runner.py:3029] [Warmup][Graph/decode][9/117] batch_size:256 query_len:1 num_blocks:640 free_mem:22.81 GiB
INFO 08-09 13:53:20 [hpu_model_runner.py:3029] [Warmup][Graph/decode][9/117] batch_size:256 query_len:1 num_blocks:640 free_mem:22.81 GiB
INFO 08-09 13:53:23 [hpu_model_runner.py:3029] [Warmup][Graph/decode][10/117] batch_size:256 query_len:1 num_blocks:512 free_mem:22.81 GiB
[1;36m(VllmWorkerProcess pid=8946)[0;0m INFO 08-09 13:53:23 [hpu_model_runner.py:3029] [Warmup][Graph/decode][10/117] batch_size:256 query_len:1 num_blocks:512 free_mem:22.81 GiB
INFO 08-09 13:53:25 [hpu_model_runner.py:3029] [Warmup][Graph/decode][11/117] batch_size:256 query_len:1 num_blocks:384 free_mem:22.81 GiB
[1;36m(VllmWorkerProcess pid=8946)[0;0m INFO 08-09 13:53:25 [hpu_model_runner.py:3029] [Warmup][Graph/decode][11/117] batch_size:256 query_len:1 num_blocks:384 free_mem:22.81 GiB
INFO 08-09 13:53:28 [hpu_model_runner.py:3029] [Warmup][Graph/decode][12/117] batch_size:256 query_len:1 num_blocks:256 free_mem:22.81 GiB
[1;36m(VllmWorkerProcess pid=8946)[0;0m INFO 08-09 13:53:28 [hpu_model_runner.py:3029] [Warmup][Graph/decode][12/117] batch_size:256 query_len:1 num_blocks:256 free_mem:22.81 GiB
INFO 08-09 13:53:31 [hpu_model_runner.py:3029] [Warmup][Graph/decode][13/117] batch_size:256 query_len:1 num_blocks:128 free_mem:22.81 GiB
[1;36m(VllmWorkerProcess pid=8946)[0;0m INFO 08-09 13:53:31 [hpu_model_runner.py:3029] [Warmup][Graph/decode][13/117] batch_size:256 query_len:1 num_blocks:128 free_mem:22.81 GiB
INFO 08-09 13:53:34 [hpu_model_runner.py:3029] [Warmup][Graph/decode][14/117] batch_size:128 query_len:1 num_blocks:3109 free_mem:22.81 GiB
[1;36m(VllmWorkerProcess pid=8946)[0;0m INFO 08-09 13:53:34 [hpu_model_runner.py:3029] [Warmup][Graph/decode][14/117] batch_size:128 query_len:1 num_blocks:3109 free_mem:22.81 GiB
INFO 08-09 13:53:37 [hpu_model_runner.py:3029] [Warmup][Graph/decode][15/117] batch_size:128 query_len:1 num_blocks:2432 free_mem:22.81 GiB
[1;36m(VllmWorkerProcess pid=8946)[0;0m INFO 08-09 13:53:37 [hpu_model_runner.py:3029] [Warmup][Graph/decode][15/117] batch_size:128 query_len:1 num_blocks:2432 free_mem:22.81 GiB
INFO 08-09 13:53:40 [hpu_model_runner.py:3029] [Warmup][Graph/decode][16/117] batch_size:128 query_len:1 num_blocks:1920 free_mem:22.81 GiB
[1;36m(VllmWorkerProcess pid=8946)[0;0m INFO 08-09 13:53:40 [hpu_model_runner.py:3029] [Warmup][Graph/decode][16/117] batch_size:128 query_len:1 num_blocks:1920 free_mem:22.81 GiB
INFO 08-09 13:53:43 [hpu_model_runner.py:3029] [Warmup][Graph/decode][17/117] batch_size:128 query_len:1 num_blocks:1408 free_mem:22.81 GiB
[1;36m(VllmWorkerProcess pid=8946)[0;0m INFO 08-09 13:53:43 [hpu_model_runner.py:3029] [Warmup][Graph/decode][17/117] batch_size:128 query_len:1 num_blocks:1408 free_mem:22.81 GiB
INFO 08-09 13:53:46 [hpu_model_runner.py:3029] [Warmup][Graph/decode][18/117] batch_size:128 query_len:1 num_blocks:1152 free_mem:22.81 GiB
[1;36m(VllmWorkerProcess pid=8946)[0;0m INFO 08-09 13:53:46 [hpu_model_runner.py:3029] [Warmup][Graph/decode][18/117] batch_size:128 query_len:1 num_blocks:1152 free_mem:22.81 GiB
INFO 08-09 13:53:49 [hpu_model_runner.py:3029] [Warmup][Graph/decode][19/117] batch_size:128 query_len:1 num_blocks:1024 free_mem:22.81 GiB
[1;36m(VllmWorkerProcess pid=8946)[0;0m INFO 08-09 13:53:49 [hpu_model_runner.py:3029] [Warmup][Graph/decode][19/117] batch_size:128 query_len:1 num_blocks:1024 free_mem:22.81 GiB
INFO 08-09 13:53:53 [hpu_model_runner.py:3029] [Warmup][Graph/decode][20/117] batch_size:128 query_len:1 num_blocks:896 free_mem:22.81 GiB
[1;36m(VllmWorkerProcess pid=8946)[0;0m INFO 08-09 13:53:53 [hpu_model_runner.py:3029] [Warmup][Graph/decode][20/117] batch_size:128 query_len:1 num_blocks:896 free_mem:22.81 GiB
INFO 08-09 13:53:56 [hpu_model_runner.py:3029] [Warmup][Graph/decode][21/117] batch_size:128 query_len:1 num_blocks:768 free_mem:22.81 GiB
[1;36m(VllmWorkerProcess pid=8946)[0;0m INFO 08-09 13:53:56 [hpu_model_runner.py:3029] [Warmup][Graph/decode][21/117] batch_size:128 query_len:1 num_blocks:768 free_mem:22.81 GiB
[1;36m(VllmWorkerProcess pid=8946)[0;0m INFO 08-09 13:53:59 [hpu_model_runner.py:3029] [Warmup][Graph/decode][22/117] batch_size:128 query_len:1 num_blocks:640 free_mem:22.81 GiB
INFO 08-09 13:53:59 [hpu_model_runner.py:3029] [Warmup][Graph/decode][22/117] batch_size:128 query_len:1 num_blocks:640 free_mem:22.81 GiB
[1;36m(VllmWorkerProcess pid=8946)[0;0m INFO 08-09 13:54:02 [hpu_model_runner.py:3029] [Warmup][Graph/decode][23/117] batch_size:128 query_len:1 num_blocks:512 free_mem:22.81 GiB
INFO 08-09 13:54:02 [hpu_model_runner.py:3029] [Warmup][Graph/decode][23/117] batch_size:128 query_len:1 num_blocks:512 free_mem:22.81 GiB
INFO 08-09 13:54:05 [hpu_model_runner.py:3029] [Warmup][Graph/decode][24/117] batch_size:128 query_len:1 num_blocks:384 free_mem:22.81 GiB
[1;36m(VllmWorkerProcess pid=8946)[0;0m INFO 08-09 13:54:05 [hpu_model_runner.py:3029] [Warmup][Graph/decode][24/117] batch_size:128 query_len:1 num_blocks:384 free_mem:22.81 GiB
INFO 08-09 13:54:08 [hpu_model_runner.py:3029] [Warmup][Graph/decode][25/117] batch_size:128 query_len:1 num_blocks:256 free_mem:22.81 GiB
[1;36m(VllmWorkerProcess pid=8946)[0;0m INFO 08-09 13:54:08 [hpu_model_runner.py:3029] [Warmup][Graph/decode][25/117] batch_size:128 query_len:1 num_blocks:256 free_mem:22.81 GiB
INFO 08-09 13:54:11 [hpu_model_runner.py:3029] [Warmup][Graph/decode][26/117] batch_size:128 query_len:1 num_blocks:128 free_mem:22.81 GiB
[1;36m(VllmWorkerProcess pid=8946)[0;0m INFO 08-09 13:54:11 [hpu_model_runner.py:3029] [Warmup][Graph/decode][26/117] batch_size:128 query_len:1 num_blocks:128 free_mem:22.81 GiB
INFO 08-09 13:54:14 [hpu_model_runner.py:3029] [Warmup][Graph/decode][27/117] batch_size:64 query_len:1 num_blocks:3109 free_mem:22.81 GiB
[1;36m(VllmWorkerProcess pid=8946)[0;0m INFO 08-09 13:54:14 [hpu_model_runner.py:3029] [Warmup][Graph/decode][27/117] batch_size:64 query_len:1 num_blocks:3109 free_mem:22.81 GiB
INFO 08-09 13:54:18 [hpu_model_runner.py:3029] [Warmup][Graph/decode][28/117] batch_size:64 query_len:1 num_blocks:2432 free_mem:22.81 GiB
[1;36m(VllmWorkerProcess pid=8946)[0;0m INFO 08-09 13:54:18 [hpu_model_runner.py:3029] [Warmup][Graph/decode][28/117] batch_size:64 query_len:1 num_blocks:2432 free_mem:22.81 GiB
INFO 08-09 13:54:21 [hpu_model_runner.py:3029] [Warmup][Graph/decode][29/117] batch_size:64 query_len:1 num_blocks:1920 free_mem:22.81 GiB
[1;36m(VllmWorkerProcess pid=8946)[0;0m INFO 08-09 13:54:21 [hpu_model_runner.py:3029] [Warmup][Graph/decode][29/117] batch_size:64 query_len:1 num_blocks:1920 free_mem:22.81 GiB
INFO 08-09 13:54:25 [hpu_model_runner.py:3029] [Warmup][Graph/decode][30/117] batch_size:64 query_len:1 num_blocks:1408 free_mem:22.81 GiB
[1;36m(VllmWorkerProcess pid=8946)[0;0m INFO 08-09 13:54:25 [hpu_model_runner.py:3029] [Warmup][Graph/decode][30/117] batch_size:64 query_len:1 num_blocks:1408 free_mem:22.81 GiB
[1;36m(VllmWorkerProcess pid=8946)[0;0m INFO 08-09 13:54:28 [hpu_model_runner.py:3029] [Warmup][Graph/decode][31/117] batch_size:64 query_len:1 num_blocks:1152 free_mem:22.81 GiB
INFO 08-09 13:54:28 [hpu_model_runner.py:3029] [Warmup][Graph/decode][31/117] batch_size:64 query_len:1 num_blocks:1152 free_mem:22.81 GiB
[1;36m(VllmWorkerProcess pid=8946)[0;0m INFO 08-09 13:54:32 [hpu_model_runner.py:3029] [Warmup][Graph/decode][32/117] batch_size:64 query_len:1 num_blocks:1024 free_mem:22.81 GiB
INFO 08-09 13:54:32 [hpu_model_runner.py:3029] [Warmup][Graph/decode][32/117] batch_size:64 query_len:1 num_blocks:1024 free_mem:22.81 GiB
[1;36m(VllmWorkerProcess pid=8946)[0;0m INFO 08-09 13:54:35 [hpu_model_runner.py:3029] [Warmup][Graph/decode][33/117] batch_size:64 query_len:1 num_blocks:896 free_mem:22.81 GiB
INFO 08-09 13:54:35 [hpu_model_runner.py:3029] [Warmup][Graph/decode][33/117] batch_size:64 query_len:1 num_blocks:896 free_mem:22.81 GiB
[1;36m(VllmWorkerProcess pid=8946)[0;0m INFO 08-09 13:54:39 [hpu_model_runner.py:3029] [Warmup][Graph/decode][34/117] batch_size:64 query_len:1 num_blocks:768 free_mem:22.81 GiB
INFO 08-09 13:54:39 [hpu_model_runner.py:3029] [Warmup][Graph/decode][34/117] batch_size:64 query_len:1 num_blocks:768 free_mem:22.81 GiB
INFO 08-09 13:54:42 [hpu_model_runner.py:3029] [Warmup][Graph/decode][35/117] batch_size:64 query_len:1 num_blocks:640 free_mem:22.81 GiB
[1;36m(VllmWorkerProcess pid=8946)[0;0m INFO 08-09 13:54:42 [hpu_model_runner.py:3029] [Warmup][Graph/decode][35/117] batch_size:64 query_len:1 num_blocks:640 free_mem:22.81 GiB
INFO 08-09 13:54:46 [hpu_model_runner.py:3029] [Warmup][Graph/decode][36/117] batch_size:64 query_len:1 num_blocks:512 free_mem:22.81 GiB
[1;36m(VllmWorkerProcess pid=8946)[0;0m INFO 08-09 13:54:46 [hpu_model_runner.py:3029] [Warmup][Graph/decode][36/117] batch_size:64 query_len:1 num_blocks:512 free_mem:22.81 GiB
INFO 08-09 13:54:49 [hpu_model_runner.py:3029] [Warmup][Graph/decode][37/117] batch_size:64 query_len:1 num_blocks:384 free_mem:22.81 GiB
[1;36m(VllmWorkerProcess pid=8946)[0;0m INFO 08-09 13:54:49 [hpu_model_runner.py:3029] [Warmup][Graph/decode][37/117] batch_size:64 query_len:1 num_blocks:384 free_mem:22.81 GiB
[1;36m(VllmWorkerProcess pid=8946)[0;0m INFO 08-09 13:54:53 [hpu_model_runner.py:3029] [Warmup][Graph/decode][38/117] batch_size:64 query_len:1 num_blocks:256 free_mem:22.81 GiB
INFO 08-09 13:54:53 [hpu_model_runner.py:3029] [Warmup][Graph/decode][38/117] batch_size:64 query_len:1 num_blocks:256 free_mem:22.81 GiB
[1;36m(VllmWorkerProcess pid=8946)[0;0m INFO 08-09 13:54:56 [hpu_model_runner.py:3029] [Warmup][Graph/decode][39/117] batch_size:64 query_len:1 num_blocks:128 free_mem:22.81 GiB
INFO 08-09 13:54:56 [hpu_model_runner.py:3029] [Warmup][Graph/decode][39/117] batch_size:64 query_len:1 num_blocks:128 free_mem:22.81 GiB
INFO 08-09 13:55:00 [hpu_model_runner.py:3029] [Warmup][Graph/decode][40/117] batch_size:32 query_len:1 num_blocks:3109 free_mem:22.81 GiB
[1;36m(VllmWorkerProcess pid=8946)[0;0m INFO 08-09 13:55:00 [hpu_model_runner.py:3029] [Warmup][Graph/decode][40/117] batch_size:32 query_len:1 num_blocks:3109 free_mem:22.81 GiB
[1;36m(VllmWorkerProcess pid=8946)[0;0m INFO 08-09 13:55:03 [hpu_model_runner.py:3029] [Warmup][Graph/decode][41/117] batch_size:32 query_len:1 num_blocks:2432 free_mem:22.81 GiB
INFO 08-09 13:55:03 [hpu_model_runner.py:3029] [Warmup][Graph/decode][41/117] batch_size:32 query_len:1 num_blocks:2432 free_mem:22.81 GiB
INFO 08-09 13:55:07 [hpu_model_runner.py:3029] [Warmup][Graph/decode][42/117] batch_size:32 query_len:1 num_blocks:1920 free_mem:22.81 GiB
[1;36m(VllmWorkerProcess pid=8946)[0;0m INFO 08-09 13:55:07 [hpu_model_runner.py:3029] [Warmup][Graph/decode][42/117] batch_size:32 query_len:1 num_blocks:1920 free_mem:22.81 GiB
[1;36m(VllmWorkerProcess pid=8946)[0;0m INFO 08-09 13:55:11 [hpu_model_runner.py:3029] [Warmup][Graph/decode][43/117] batch_size:32 query_len:1 num_blocks:1408 free_mem:22.81 GiB
INFO 08-09 13:55:11 [hpu_model_runner.py:3029] [Warmup][Graph/decode][43/117] batch_size:32 query_len:1 num_blocks:1408 free_mem:22.81 GiB
[1;36m(VllmWorkerProcess pid=8946)[0;0m INFO 08-09 13:55:14 [hpu_model_runner.py:3029] [Warmup][Graph/decode][44/117] batch_size:32 query_len:1 num_blocks:1152 free_mem:22.81 GiB
INFO 08-09 13:55:14 [hpu_model_runner.py:3029] [Warmup][Graph/decode][44/117] batch_size:32 query_len:1 num_blocks:1152 free_mem:22.81 GiB
[1;36m(VllmWorkerProcess pid=8946)[0;0m INFO 08-09 13:55:18 [hpu_model_runner.py:3029] [Warmup][Graph/decode][45/117] batch_size:32 query_len:1 num_blocks:1024 free_mem:22.81 GiB
INFO 08-09 13:55:18 [hpu_model_runner.py:3029] [Warmup][Graph/decode][45/117] batch_size:32 query_len:1 num_blocks:1024 free_mem:22.81 GiB
INFO 08-09 13:55:22 [hpu_model_runner.py:3029] [Warmup][Graph/decode][46/117] batch_size:32 query_len:1 num_blocks:896 free_mem:22.81 GiB
[1;36m(VllmWorkerProcess pid=8946)[0;0m INFO 08-09 13:55:22 [hpu_model_runner.py:3029] [Warmup][Graph/decode][46/117] batch_size:32 query_len:1 num_blocks:896 free_mem:22.81 GiB
INFO 08-09 13:55:26 [hpu_model_runner.py:3029] [Warmup][Graph/decode][47/117] batch_size:32 query_len:1 num_blocks:768 free_mem:22.81 GiB
[1;36m(VllmWorkerProcess pid=8946)[0;0m INFO 08-09 13:55:26 [hpu_model_runner.py:3029] [Warmup][Graph/decode][47/117] batch_size:32 query_len:1 num_blocks:768 free_mem:22.81 GiB
[1;36m(VllmWorkerProcess pid=8946)[0;0m INFO 08-09 13:55:30 [hpu_model_runner.py:3029] [Warmup][Graph/decode][48/117] batch_size:32 query_len:1 num_blocks:640 free_mem:22.81 GiB
INFO 08-09 13:55:30 [hpu_model_runner.py:3029] [Warmup][Graph/decode][48/117] batch_size:32 query_len:1 num_blocks:640 free_mem:22.81 GiB
INFO 08-09 13:55:34 [hpu_model_runner.py:3029] [Warmup][Graph/decode][49/117] batch_size:32 query_len:1 num_blocks:512 free_mem:22.81 GiB
[1;36m(VllmWorkerProcess pid=8946)[0;0m INFO 08-09 13:55:34 [hpu_model_runner.py:3029] [Warmup][Graph/decode][49/117] batch_size:32 query_len:1 num_blocks:512 free_mem:22.81 GiB
INFO 08-09 13:55:37 [hpu_model_runner.py:3029] [Warmup][Graph/decode][50/117] batch_size:32 query_len:1 num_blocks:384 free_mem:22.81 GiB
[1;36m(VllmWorkerProcess pid=8946)[0;0m INFO 08-09 13:55:37 [hpu_model_runner.py:3029] [Warmup][Graph/decode][50/117] batch_size:32 query_len:1 num_blocks:384 free_mem:22.81 GiB
INFO 08-09 13:55:41 [hpu_model_runner.py:3029] [Warmup][Graph/decode][51/117] batch_size:32 query_len:1 num_blocks:256 free_mem:22.81 GiB
[1;36m(VllmWorkerProcess pid=8946)[0;0m INFO 08-09 13:55:41 [hpu_model_runner.py:3029] [Warmup][Graph/decode][51/117] batch_size:32 query_len:1 num_blocks:256 free_mem:22.81 GiB
INFO 08-09 13:55:45 [hpu_model_runner.py:3029] [Warmup][Graph/decode][52/117] batch_size:32 query_len:1 num_blocks:128 free_mem:22.81 GiB
[1;36m(VllmWorkerProcess pid=8946)[0;0m INFO 08-09 13:55:45 [hpu_model_runner.py:3029] [Warmup][Graph/decode][52/117] batch_size:32 query_len:1 num_blocks:128 free_mem:22.81 GiB
INFO 08-09 13:55:48 [hpu_model_runner.py:3029] [Warmup][Graph/decode][53/117] batch_size:16 query_len:1 num_blocks:3109 free_mem:22.81 GiB
[1;36m(VllmWorkerProcess pid=8946)[0;0m INFO 08-09 13:55:48 [hpu_model_runner.py:3029] [Warmup][Graph/decode][53/117] batch_size:16 query_len:1 num_blocks:3109 free_mem:22.81 GiB
INFO 08-09 13:55:53 [hpu_model_runner.py:3029] [Warmup][Graph/decode][54/117] batch_size:16 query_len:1 num_blocks:2432 free_mem:22.81 GiB
[1;36m(VllmWorkerProcess pid=8946)[0;0m INFO 08-09 13:55:53 [hpu_model_runner.py:3029] [Warmup][Graph/decode][54/117] batch_size:16 query_len:1 num_blocks:2432 free_mem:22.81 GiB
INFO 08-09 13:55:57 [hpu_model_runner.py:3029] [Warmup][Graph/decode][55/117] batch_size:16 query_len:1 num_blocks:1920 free_mem:22.81 GiB
[1;36m(VllmWorkerProcess pid=8946)[0;0m INFO 08-09 13:55:57 [hpu_model_runner.py:3029] [Warmup][Graph/decode][55/117] batch_size:16 query_len:1 num_blocks:1920 free_mem:22.81 GiB
INFO 08-09 13:56:01 [hpu_model_runner.py:3029] [Warmup][Graph/decode][56/117] batch_size:16 query_len:1 num_blocks:1408 free_mem:22.81 GiB
[1;36m(VllmWorkerProcess pid=8946)[0;0m INFO 08-09 13:56:01 [hpu_model_runner.py:3029] [Warmup][Graph/decode][56/117] batch_size:16 query_len:1 num_blocks:1408 free_mem:22.81 GiB
INFO 08-09 13:56:05 [hpu_model_runner.py:3029] [Warmup][Graph/decode][57/117] batch_size:16 query_len:1 num_blocks:1152 free_mem:22.81 GiB
[1;36m(VllmWorkerProcess pid=8946)[0;0m INFO 08-09 13:56:05 [hpu_model_runner.py:3029] [Warmup][Graph/decode][57/117] batch_size:16 query_len:1 num_blocks:1152 free_mem:22.81 GiB
INFO 08-09 13:56:09 [hpu_model_runner.py:3029] [Warmup][Graph/decode][58/117] batch_size:16 query_len:1 num_blocks:1024 free_mem:22.81 GiB
[1;36m(VllmWorkerProcess pid=8946)[0;0m INFO 08-09 13:56:09 [hpu_model_runner.py:3029] [Warmup][Graph/decode][58/117] batch_size:16 query_len:1 num_blocks:1024 free_mem:22.81 GiB
INFO 08-09 13:56:13 [hpu_model_runner.py:3029] [Warmup][Graph/decode][59/117] batch_size:16 query_len:1 num_blocks:896 free_mem:22.81 GiB
[1;36m(VllmWorkerProcess pid=8946)[0;0m INFO 08-09 13:56:13 [hpu_model_runner.py:3029] [Warmup][Graph/decode][59/117] batch_size:16 query_len:1 num_blocks:896 free_mem:22.81 GiB
INFO 08-09 13:56:17 [hpu_model_runner.py:3029] [Warmup][Graph/decode][60/117] batch_size:16 query_len:1 num_blocks:768 free_mem:22.81 GiB
[1;36m(VllmWorkerProcess pid=8946)[0;0m INFO 08-09 13:56:17 [hpu_model_runner.py:3029] [Warmup][Graph/decode][60/117] batch_size:16 query_len:1 num_blocks:768 free_mem:22.81 GiB
INFO 08-09 13:56:21 [hpu_model_runner.py:3029] [Warmup][Graph/decode][61/117] batch_size:16 query_len:1 num_blocks:640 free_mem:22.81 GiB
[1;36m(VllmWorkerProcess pid=8946)[0;0m INFO 08-09 13:56:21 [hpu_model_runner.py:3029] [Warmup][Graph/decode][61/117] batch_size:16 query_len:1 num_blocks:640 free_mem:22.81 GiB
[1;36m(VllmWorkerProcess pid=8946)[0;0m INFO 08-09 13:56:26 [hpu_model_runner.py:3029] [Warmup][Graph/decode][62/117] batch_size:16 query_len:1 num_blocks:512 free_mem:22.81 GiB
INFO 08-09 13:56:26 [hpu_model_runner.py:3029] [Warmup][Graph/decode][62/117] batch_size:16 query_len:1 num_blocks:512 free_mem:22.81 GiB
INFO 08-09 13:56:30 [hpu_model_runner.py:3029] [Warmup][Graph/decode][63/117] batch_size:16 query_len:1 num_blocks:384 free_mem:22.81 GiB
[1;36m(VllmWorkerProcess pid=8946)[0;0m INFO 08-09 13:56:30 [hpu_model_runner.py:3029] [Warmup][Graph/decode][63/117] batch_size:16 query_len:1 num_blocks:384 free_mem:22.81 GiB
[1;36m(VllmWorkerProcess pid=8946)[0;0m INFO 08-09 13:56:34 [hpu_model_runner.py:3029] [Warmup][Graph/decode][64/117] batch_size:16 query_len:1 num_blocks:256 free_mem:22.81 GiB
INFO 08-09 13:56:34 [hpu_model_runner.py:3029] [Warmup][Graph/decode][64/117] batch_size:16 query_len:1 num_blocks:256 free_mem:22.81 GiB
INFO 08-09 13:56:38 [hpu_model_runner.py:3029] [Warmup][Graph/decode][65/117] batch_size:16 query_len:1 num_blocks:128 free_mem:22.81 GiB
[1;36m(VllmWorkerProcess pid=8946)[0;0m INFO 08-09 13:56:38 [hpu_model_runner.py:3029] [Warmup][Graph/decode][65/117] batch_size:16 query_len:1 num_blocks:128 free_mem:22.81 GiB
INFO 08-09 13:56:42 [hpu_model_runner.py:3029] [Warmup][Graph/decode][66/117] batch_size:8 query_len:1 num_blocks:3109 free_mem:22.81 GiB
[1;36m(VllmWorkerProcess pid=8946)[0;0m INFO 08-09 13:56:42 [hpu_model_runner.py:3029] [Warmup][Graph/decode][66/117] batch_size:8 query_len:1 num_blocks:3109 free_mem:22.81 GiB
INFO 08-09 13:56:47 [hpu_model_runner.py:3029] [Warmup][Graph/decode][67/117] batch_size:8 query_len:1 num_blocks:2432 free_mem:22.81 GiB
[1;36m(VllmWorkerProcess pid=8946)[0;0m INFO 08-09 13:56:47 [hpu_model_runner.py:3029] [Warmup][Graph/decode][67/117] batch_size:8 query_len:1 num_blocks:2432 free_mem:22.81 GiB
INFO 08-09 13:56:51 [hpu_model_runner.py:3029] [Warmup][Graph/decode][68/117] batch_size:8 query_len:1 num_blocks:1920 free_mem:22.81 GiB
[1;36m(VllmWorkerProcess pid=8946)[0;0m INFO 08-09 13:56:51 [hpu_model_runner.py:3029] [Warmup][Graph/decode][68/117] batch_size:8 query_len:1 num_blocks:1920 free_mem:22.81 GiB
INFO 08-09 13:56:56 [hpu_model_runner.py:3029] [Warmup][Graph/decode][69/117] batch_size:8 query_len:1 num_blocks:1408 free_mem:22.81 GiB
[1;36m(VllmWorkerProcess pid=8946)[0;0m INFO 08-09 13:56:56 [hpu_model_runner.py:3029] [Warmup][Graph/decode][69/117] batch_size:8 query_len:1 num_blocks:1408 free_mem:22.81 GiB
INFO 08-09 13:57:00 [hpu_model_runner.py:3029] [Warmup][Graph/decode][70/117] batch_size:8 query_len:1 num_blocks:1152 free_mem:22.81 GiB
[1;36m(VllmWorkerProcess pid=8946)[0;0m INFO 08-09 13:57:00 [hpu_model_runner.py:3029] [Warmup][Graph/decode][70/117] batch_size:8 query_len:1 num_blocks:1152 free_mem:22.81 GiB
[1;36m(VllmWorkerProcess pid=8946)[0;0m INFO 08-09 13:57:05 [hpu_model_runner.py:3029] [Warmup][Graph/decode][71/117] batch_size:8 query_len:1 num_blocks:1024 free_mem:22.81 GiB
INFO 08-09 13:57:05 [hpu_model_runner.py:3029] [Warmup][Graph/decode][71/117] batch_size:8 query_len:1 num_blocks:1024 free_mem:22.81 GiB
INFO 08-09 13:57:09 [hpu_model_runner.py:3029] [Warmup][Graph/decode][72/117] batch_size:8 query_len:1 num_blocks:896 free_mem:22.81 GiB
[1;36m(VllmWorkerProcess pid=8946)[0;0m INFO 08-09 13:57:09 [hpu_model_runner.py:3029] [Warmup][Graph/decode][72/117] batch_size:8 query_len:1 num_blocks:896 free_mem:22.81 GiB
INFO 08-09 13:57:13 [hpu_model_runner.py:3029] [Warmup][Graph/decode][73/117] batch_size:8 query_len:1 num_blocks:768 free_mem:22.81 GiB
[1;36m(VllmWorkerProcess pid=8946)[0;0m INFO 08-09 13:57:13 [hpu_model_runner.py:3029] [Warmup][Graph/decode][73/117] batch_size:8 query_len:1 num_blocks:768 free_mem:22.81 GiB
INFO 08-09 13:57:18 [hpu_model_runner.py:3029] [Warmup][Graph/decode][74/117] batch_size:8 query_len:1 num_blocks:640 free_mem:22.81 GiB
[1;36m(VllmWorkerProcess pid=8946)[0;0m INFO 08-09 13:57:18 [hpu_model_runner.py:3029] [Warmup][Graph/decode][74/117] batch_size:8 query_len:1 num_blocks:640 free_mem:22.81 GiB
[1;36m(VllmWorkerProcess pid=8946)[0;0m INFO 08-09 13:57:22 [hpu_model_runner.py:3029] [Warmup][Graph/decode][75/117] batch_size:8 query_len:1 num_blocks:512 free_mem:22.81 GiB
INFO 08-09 13:57:22 [hpu_model_runner.py:3029] [Warmup][Graph/decode][75/117] batch_size:8 query_len:1 num_blocks:512 free_mem:22.81 GiB
INFO 08-09 13:57:27 [hpu_model_runner.py:3029] [Warmup][Graph/decode][76/117] batch_size:8 query_len:1 num_blocks:384 free_mem:22.81 GiB
[1;36m(VllmWorkerProcess pid=8946)[0;0m INFO 08-09 13:57:27 [hpu_model_runner.py:3029] [Warmup][Graph/decode][76/117] batch_size:8 query_len:1 num_blocks:384 free_mem:22.81 GiB
INFO 08-09 13:57:31 [hpu_model_runner.py:3029] [Warmup][Graph/decode][77/117] batch_size:8 query_len:1 num_blocks:256 free_mem:22.81 GiB
[1;36m(VllmWorkerProcess pid=8946)[0;0m INFO 08-09 13:57:31 [hpu_model_runner.py:3029] [Warmup][Graph/decode][77/117] batch_size:8 query_len:1 num_blocks:256 free_mem:22.81 GiB
INFO 08-09 13:57:36 [hpu_model_runner.py:3029] [Warmup][Graph/decode][78/117] batch_size:8 query_len:1 num_blocks:128 free_mem:22.81 GiB
[1;36m(VllmWorkerProcess pid=8946)[0;0m INFO 08-09 13:57:36 [hpu_model_runner.py:3029] [Warmup][Graph/decode][78/117] batch_size:8 query_len:1 num_blocks:128 free_mem:22.81 GiB
INFO 08-09 13:57:40 [hpu_model_runner.py:3029] [Warmup][Graph/decode][79/117] batch_size:4 query_len:1 num_blocks:3109 free_mem:22.81 GiB
[1;36m(VllmWorkerProcess pid=8946)[0;0m INFO 08-09 13:57:40 [hpu_model_runner.py:3029] [Warmup][Graph/decode][79/117] batch_size:4 query_len:1 num_blocks:3109 free_mem:22.81 GiB
INFO 08-09 13:57:45 [hpu_model_runner.py:3029] [Warmup][Graph/decode][80/117] batch_size:4 query_len:1 num_blocks:2432 free_mem:22.81 GiB
[1;36m(VllmWorkerProcess pid=8946)[0;0m INFO 08-09 13:57:45 [hpu_model_runner.py:3029] [Warmup][Graph/decode][80/117] batch_size:4 query_len:1 num_blocks:2432 free_mem:22.81 GiB
[1;36m(VllmWorkerProcess pid=8946)[0;0m INFO 08-09 13:57:50 [hpu_model_runner.py:3029] [Warmup][Graph/decode][81/117] batch_size:4 query_len:1 num_blocks:1920 free_mem:22.81 GiB
INFO 08-09 13:57:50 [hpu_model_runner.py:3029] [Warmup][Graph/decode][81/117] batch_size:4 query_len:1 num_blocks:1920 free_mem:22.81 GiB
[1;36m(VllmWorkerProcess pid=8946)[0;0m INFO 08-09 13:57:54 [hpu_model_runner.py:3029] [Warmup][Graph/decode][82/117] batch_size:4 query_len:1 num_blocks:1408 free_mem:22.81 GiB
INFO 08-09 13:57:54 [hpu_model_runner.py:3029] [Warmup][Graph/decode][82/117] batch_size:4 query_len:1 num_blocks:1408 free_mem:22.81 GiB
[1;36m(VllmWorkerProcess pid=8946)[0;0m INFO 08-09 13:57:59 [hpu_model_runner.py:3029] [Warmup][Graph/decode][83/117] batch_size:4 query_len:1 num_blocks:1152 free_mem:22.81 GiB
INFO 08-09 13:57:59 [hpu_model_runner.py:3029] [Warmup][Graph/decode][83/117] batch_size:4 query_len:1 num_blocks:1152 free_mem:22.81 GiB
[1;36m(VllmWorkerProcess pid=8946)[0;0m INFO 08-09 13:58:04 [hpu_model_runner.py:3029] [Warmup][Graph/decode][84/117] batch_size:4 query_len:1 num_blocks:1024 free_mem:22.81 GiB
INFO 08-09 13:58:04 [hpu_model_runner.py:3029] [Warmup][Graph/decode][84/117] batch_size:4 query_len:1 num_blocks:1024 free_mem:22.81 GiB
[1;36m(VllmWorkerProcess pid=8946)[0;0m INFO 08-09 13:58:09 [hpu_model_runner.py:3029] [Warmup][Graph/decode][85/117] batch_size:4 query_len:1 num_blocks:896 free_mem:22.81 GiB
INFO 08-09 13:58:09 [hpu_model_runner.py:3029] [Warmup][Graph/decode][85/117] batch_size:4 query_len:1 num_blocks:896 free_mem:22.81 GiB
[1;36m(VllmWorkerProcess pid=8946)[0;0m INFO 08-09 13:58:13 [hpu_model_runner.py:3029] [Warmup][Graph/decode][86/117] batch_size:4 query_len:1 num_blocks:768 free_mem:22.81 GiB
INFO 08-09 13:58:13 [hpu_model_runner.py:3029] [Warmup][Graph/decode][86/117] batch_size:4 query_len:1 num_blocks:768 free_mem:22.81 GiB
[1;36m(VllmWorkerProcess pid=8946)[0;0m INFO 08-09 13:58:18 [hpu_model_runner.py:3029] [Warmup][Graph/decode][87/117] batch_size:4 query_len:1 num_blocks:640 free_mem:22.81 GiB
INFO 08-09 13:58:18 [hpu_model_runner.py:3029] [Warmup][Graph/decode][87/117] batch_size:4 query_len:1 num_blocks:640 free_mem:22.81 GiB
INFO 08-09 13:58:23 [hpu_model_runner.py:3029] [Warmup][Graph/decode][88/117] batch_size:4 query_len:1 num_blocks:512 free_mem:22.81 GiB
[1;36m(VllmWorkerProcess pid=8946)[0;0m INFO 08-09 13:58:23 [hpu_model_runner.py:3029] [Warmup][Graph/decode][88/117] batch_size:4 query_len:1 num_blocks:512 free_mem:22.81 GiB
[1;36m(VllmWorkerProcess pid=8946)[0;0m INFO 08-09 13:58:28 [hpu_model_runner.py:3029] [Warmup][Graph/decode][89/117] batch_size:4 query_len:1 num_blocks:384 free_mem:22.81 GiB
INFO 08-09 13:58:28 [hpu_model_runner.py:3029] [Warmup][Graph/decode][89/117] batch_size:4 query_len:1 num_blocks:384 free_mem:22.81 GiB
INFO 08-09 13:58:33 [hpu_model_runner.py:3029] [Warmup][Graph/decode][90/117] batch_size:4 query_len:1 num_blocks:256 free_mem:22.81 GiB
[1;36m(VllmWorkerProcess pid=8946)[0;0m INFO 08-09 13:58:33 [hpu_model_runner.py:3029] [Warmup][Graph/decode][90/117] batch_size:4 query_len:1 num_blocks:256 free_mem:22.81 GiB
INFO 08-09 13:58:37 [hpu_model_runner.py:3029] [Warmup][Graph/decode][91/117] batch_size:4 query_len:1 num_blocks:128 free_mem:22.81 GiB
[1;36m(VllmWorkerProcess pid=8946)[0;0m INFO 08-09 13:58:37 [hpu_model_runner.py:3029] [Warmup][Graph/decode][91/117] batch_size:4 query_len:1 num_blocks:128 free_mem:22.81 GiB
[1;36m(VllmWorkerProcess pid=8946)[0;0m INFO 08-09 13:58:42 [hpu_model_runner.py:3029] [Warmup][Graph/decode][92/117] batch_size:2 query_len:1 num_blocks:3109 free_mem:22.81 GiB
INFO 08-09 13:58:42 [hpu_model_runner.py:3029] [Warmup][Graph/decode][92/117] batch_size:2 query_len:1 num_blocks:3109 free_mem:22.81 GiB
INFO 08-09 13:58:48 [hpu_model_runner.py:3029] [Warmup][Graph/decode][93/117] batch_size:2 query_len:1 num_blocks:2432 free_mem:22.81 GiB
[1;36m(VllmWorkerProcess pid=8946)[0;0m INFO 08-09 13:58:48 [hpu_model_runner.py:3029] [Warmup][Graph/decode][93/117] batch_size:2 query_len:1 num_blocks:2432 free_mem:22.81 GiB
[1;36m(VllmWorkerProcess pid=8946)[0;0m INFO 08-09 13:58:53 [hpu_model_runner.py:3029] [Warmup][Graph/decode][94/117] batch_size:2 query_len:1 num_blocks:1920 free_mem:22.81 GiB
INFO 08-09 13:58:53 [hpu_model_runner.py:3029] [Warmup][Graph/decode][94/117] batch_size:2 query_len:1 num_blocks:1920 free_mem:22.81 GiB
INFO 08-09 13:58:58 [hpu_model_runner.py:3029] [Warmup][Graph/decode][95/117] batch_size:2 query_len:1 num_blocks:1408 free_mem:22.81 GiB
[1;36m(VllmWorkerProcess pid=8946)[0;0m INFO 08-09 13:58:58 [hpu_model_runner.py:3029] [Warmup][Graph/decode][95/117] batch_size:2 query_len:1 num_blocks:1408 free_mem:22.81 GiB
[1;36m(VllmWorkerProcess pid=8946)[0;0m INFO 08-09 13:59:03 [hpu_model_runner.py:3029] [Warmup][Graph/decode][96/117] batch_size:2 query_len:1 num_blocks:1152 free_mem:22.81 GiB
INFO 08-09 13:59:03 [hpu_model_runner.py:3029] [Warmup][Graph/decode][96/117] batch_size:2 query_len:1 num_blocks:1152 free_mem:22.81 GiB
INFO 08-09 13:59:08 [hpu_model_runner.py:3029] [Warmup][Graph/decode][97/117] batch_size:2 query_len:1 num_blocks:1024 free_mem:22.81 GiB
[1;36m(VllmWorkerProcess pid=8946)[0;0m INFO 08-09 13:59:08 [hpu_model_runner.py:3029] [Warmup][Graph/decode][97/117] batch_size:2 query_len:1 num_blocks:1024 free_mem:22.81 GiB
[1;36m(VllmWorkerProcess pid=8946)[0;0m INFO 08-09 13:59:13 [hpu_model_runner.py:3029] [Warmup][Graph/decode][98/117] batch_size:2 query_len:1 num_blocks:896 free_mem:22.81 GiB
INFO 08-09 13:59:13 [hpu_model_runner.py:3029] [Warmup][Graph/decode][98/117] batch_size:2 query_len:1 num_blocks:896 free_mem:22.81 GiB
[1;36m(VllmWorkerProcess pid=8946)[0;0m INFO 08-09 13:59:18 [hpu_model_runner.py:3029] [Warmup][Graph/decode][99/117] batch_size:2 query_len:1 num_blocks:768 free_mem:22.81 GiB
INFO 08-09 13:59:18 [hpu_model_runner.py:3029] [Warmup][Graph/decode][99/117] batch_size:2 query_len:1 num_blocks:768 free_mem:22.81 GiB
INFO 08-09 13:59:23 [hpu_model_runner.py:3029] [Warmup][Graph/decode][100/117] batch_size:2 query_len:1 num_blocks:640 free_mem:22.81 GiB
[1;36m(VllmWorkerProcess pid=8946)[0;0m INFO 08-09 13:59:23 [hpu_model_runner.py:3029] [Warmup][Graph/decode][100/117] batch_size:2 query_len:1 num_blocks:640 free_mem:22.81 GiB
INFO 08-09 13:59:28 [hpu_model_runner.py:3029] [Warmup][Graph/decode][101/117] batch_size:2 query_len:1 num_blocks:512 free_mem:22.81 GiB
[1;36m(VllmWorkerProcess pid=8946)[0;0m INFO 08-09 13:59:28 [hpu_model_runner.py:3029] [Warmup][Graph/decode][101/117] batch_size:2 query_len:1 num_blocks:512 free_mem:22.81 GiB
INFO 08-09 13:59:33 [hpu_model_runner.py:3029] [Warmup][Graph/decode][102/117] batch_size:2 query_len:1 num_blocks:384 free_mem:22.81 GiB
[1;36m(VllmWorkerProcess pid=8946)[0;0m INFO 08-09 13:59:33 [hpu_model_runner.py:3029] [Warmup][Graph/decode][102/117] batch_size:2 query_len:1 num_blocks:384 free_mem:22.81 GiB
INFO 08-09 13:59:38 [hpu_model_runner.py:3029] [Warmup][Graph/decode][103/117] batch_size:2 query_len:1 num_blocks:256 free_mem:22.81 GiB
[1;36m(VllmWorkerProcess pid=8946)[0;0m INFO 08-09 13:59:38 [hpu_model_runner.py:3029] [Warmup][Graph/decode][103/117] batch_size:2 query_len:1 num_blocks:256 free_mem:22.81 GiB
[1;36m(VllmWorkerProcess pid=8946)[0;0m INFO 08-09 13:59:43 [hpu_model_runner.py:3029] [Warmup][Graph/decode][104/117] batch_size:2 query_len:1 num_blocks:128 free_mem:22.81 GiB
INFO 08-09 13:59:43 [hpu_model_runner.py:3029] [Warmup][Graph/decode][104/117] batch_size:2 query_len:1 num_blocks:128 free_mem:22.81 GiB
[1;36m(VllmWorkerProcess pid=8946)[0;0m INFO 08-09 13:59:48 [hpu_model_runner.py:3029] [Warmup][Graph/decode][105/117] batch_size:1 query_len:1 num_blocks:3109 free_mem:22.81 GiB
INFO 08-09 13:59:48 [hpu_model_runner.py:3029] [Warmup][Graph/decode][105/117] batch_size:1 query_len:1 num_blocks:3109 free_mem:22.81 GiB
INFO 08-09 13:59:54 [hpu_model_runner.py:3029] [Warmup][Graph/decode][106/117] batch_size:1 query_len:1 num_blocks:2432 free_mem:22.81 GiB
[1;36m(VllmWorkerProcess pid=8946)[0;0m INFO 08-09 13:59:54 [hpu_model_runner.py:3029] [Warmup][Graph/decode][106/117] batch_size:1 query_len:1 num_blocks:2432 free_mem:22.81 GiB
[1;36m(VllmWorkerProcess pid=8946)[0;0m INFO 08-09 13:59:59 [hpu_model_runner.py:3029] [Warmup][Graph/decode][107/117] batch_size:1 query_len:1 num_blocks:1920 free_mem:22.81 GiB
INFO 08-09 13:59:59 [hpu_model_runner.py:3029] [Warmup][Graph/decode][107/117] batch_size:1 query_len:1 num_blocks:1920 free_mem:22.81 GiB
INFO 08-09 14:00:04 [hpu_model_runner.py:3029] [Warmup][Graph/decode][108/117] batch_size:1 query_len:1 num_blocks:1408 free_mem:22.81 GiB
[1;36m(VllmWorkerProcess pid=8946)[0;0m INFO 08-09 14:00:04 [hpu_model_runner.py:3029] [Warmup][Graph/decode][108/117] batch_size:1 query_len:1 num_blocks:1408 free_mem:22.81 GiB
INFO 08-09 14:00:10 [hpu_model_runner.py:3029] [Warmup][Graph/decode][109/117] batch_size:1 query_len:1 num_blocks:1152 free_mem:22.81 GiB
[1;36m(VllmWorkerProcess pid=8946)[0;0m INFO 08-09 14:00:10 [hpu_model_runner.py:3029] [Warmup][Graph/decode][109/117] batch_size:1 query_len:1 num_blocks:1152 free_mem:22.81 GiB
[1;36m(VllmWorkerProcess pid=8946)[0;0m INFO 08-09 14:00:15 [hpu_model_runner.py:3029] [Warmup][Graph/decode][110/117] batch_size:1 query_len:1 num_blocks:1024 free_mem:22.81 GiB
INFO 08-09 14:00:15 [hpu_model_runner.py:3029] [Warmup][Graph/decode][110/117] batch_size:1 query_len:1 num_blocks:1024 free_mem:22.81 GiB
INFO 08-09 14:00:20 [hpu_model_runner.py:3029] [Warmup][Graph/decode][111/117] batch_size:1 query_len:1 num_blocks:896 free_mem:22.81 GiB
[1;36m(VllmWorkerProcess pid=8946)[0;0m INFO 08-09 14:00:20 [hpu_model_runner.py:3029] [Warmup][Graph/decode][111/117] batch_size:1 query_len:1 num_blocks:896 free_mem:22.81 GiB
[1;36m(VllmWorkerProcess pid=8946)[0;0m INFO 08-09 14:00:26 [hpu_model_runner.py:3029] [Warmup][Graph/decode][112/117] batch_size:1 query_len:1 num_blocks:768 free_mem:22.81 GiB
INFO 08-09 14:00:26 [hpu_model_runner.py:3029] [Warmup][Graph/decode][112/117] batch_size:1 query_len:1 num_blocks:768 free_mem:22.81 GiB
INFO 08-09 14:00:31 [hpu_model_runner.py:3029] [Warmup][Graph/decode][113/117] batch_size:1 query_len:1 num_blocks:640 free_mem:22.81 GiB
[1;36m(VllmWorkerProcess pid=8946)[0;0m INFO 08-09 14:00:31 [hpu_model_runner.py:3029] [Warmup][Graph/decode][113/117] batch_size:1 query_len:1 num_blocks:640 free_mem:22.81 GiB
INFO 08-09 14:00:36 [hpu_model_runner.py:3029] [Warmup][Graph/decode][114/117] batch_size:1 query_len:1 num_blocks:512 free_mem:22.81 GiB
[1;36m(VllmWorkerProcess pid=8946)[0;0m INFO 08-09 14:00:36 [hpu_model_runner.py:3029] [Warmup][Graph/decode][114/117] batch_size:1 query_len:1 num_blocks:512 free_mem:22.81 GiB
[1;36m(VllmWorkerProcess pid=8946)[0;0m INFO 08-09 14:00:42 [hpu_model_runner.py:3029] [Warmup][Graph/decode][115/117] batch_size:1 query_len:1 num_blocks:384 free_mem:22.81 GiB
INFO 08-09 14:00:42 [hpu_model_runner.py:3029] [Warmup][Graph/decode][115/117] batch_size:1 query_len:1 num_blocks:384 free_mem:22.81 GiB
INFO 08-09 14:00:47 [hpu_model_runner.py:3029] [Warmup][Graph/decode][116/117] batch_size:1 query_len:1 num_blocks:256 free_mem:22.81 GiB
[1;36m(VllmWorkerProcess pid=8946)[0;0m INFO 08-09 14:00:47 [hpu_model_runner.py:3029] [Warmup][Graph/decode][116/117] batch_size:1 query_len:1 num_blocks:256 free_mem:22.81 GiB
[1;36m(VllmWorkerProcess pid=8946)[0;0m INFO 08-09 14:00:53 [hpu_model_runner.py:3029] [Warmup][Graph/decode][117/117] batch_size:1 query_len:1 num_blocks:128 free_mem:22.81 GiB
INFO 08-09 14:00:53 [hpu_model_runner.py:3029] [Warmup][Graph/decode][117/117] batch_size:1 query_len:1 num_blocks:128 free_mem:22.81 GiB
INFO 08-09 14:00:58 [hpu_model_runner.py:3152] Prompt captured:38 (100.0%) used_mem:0 B
INFO 08-09 14:00:58 [hpu_model_runner.py:3152] Decode captured:117 (100.0%) used_mem:4 KiB
INFO 08-09 14:00:58 [hpu_model_runner.py:3280] Warmup finished in 566 secs, allocated 0 B of device memory
[1;36m(VllmWorkerProcess pid=8946)[0;0m INFO 08-09 14:00:58 [hpu_model_runner.py:3152] Prompt captured:38 (100.0%) used_mem:0 B
[1;36m(VllmWorkerProcess pid=8946)[0;0m INFO 08-09 14:00:58 [hpu_model_runner.py:3152] Decode captured:117 (100.0%) used_mem:4 KiB
[1;36m(VllmWorkerProcess pid=8946)[0;0m INFO 08-09 14:00:58 [hpu_model_runner.py:3280] Warmup finished in 566 secs, allocated 0 B of device memory
INFO 08-09 14:00:58 [llm_engine.py:431] init engine (profile, create kv cache, warmup model) took 570.24 seconds
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
WARNING 08-09 14:00:58 [config.py:1364] Default sampling parameters have been overridden by the model's Hugging Face generation config recommended from the model creator. If this is not intended, please relaunch vLLM instance with `--generation-config vllm`.
INFO 08-09 14:00:58 [serving_chat.py:118] Using default chat sampling params from model: {'temperature': 0.9, 'top_p': 0.6}
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
INFO 08-09 14:00:58 [serving_completion.py:66] Using default completion sampling params from model: {'temperature': 0.9, 'top_p': 0.6}
INFO 08-09 14:00:58 [api_server.py:1351] Starting vLLM API server 0 on http://0.0.0.0:8000
INFO 08-09 14:00:58 [launcher.py:29] Available routes are:
INFO 08-09 14:00:58 [launcher.py:37] Route: /openapi.json, Methods: GET, HEAD
INFO 08-09 14:00:58 [launcher.py:37] Route: /docs, Methods: GET, HEAD
INFO 08-09 14:00:58 [launcher.py:37] Route: /docs/oauth2-redirect, Methods: GET, HEAD
INFO 08-09 14:00:58 [launcher.py:37] Route: /redoc, Methods: GET, HEAD
INFO 08-09 14:00:58 [launcher.py:37] Route: /health, Methods: GET
INFO 08-09 14:00:58 [launcher.py:37] Route: /load, Methods: GET
INFO 08-09 14:00:58 [launcher.py:37] Route: /ping, Methods: POST
INFO 08-09 14:00:58 [launcher.py:37] Route: /ping, Methods: GET
INFO 08-09 14:00:58 [launcher.py:37] Route: /tokenize, Methods: POST
INFO 08-09 14:00:58 [launcher.py:37] Route: /detokenize, Methods: POST
INFO 08-09 14:00:58 [launcher.py:37] Route: /v1/models, Methods: GET
INFO 08-09 14:00:58 [launcher.py:37] Route: /version, Methods: GET
INFO 08-09 14:00:58 [launcher.py:37] Route: /v1/chat/completions, Methods: POST
INFO 08-09 14:00:58 [launcher.py:37] Route: /v1/completions, Methods: POST
INFO 08-09 14:00:58 [launcher.py:37] Route: /v1/embeddings, Methods: POST
INFO 08-09 14:00:58 [launcher.py:37] Route: /pooling, Methods: POST
INFO 08-09 14:00:58 [launcher.py:37] Route: /classify, Methods: POST
INFO 08-09 14:00:58 [launcher.py:37] Route: /score, Methods: POST
INFO 08-09 14:00:58 [launcher.py:37] Route: /v1/score, Methods: POST
INFO 08-09 14:00:58 [launcher.py:37] Route: /v1/audio/transcriptions, Methods: POST
INFO 08-09 14:00:58 [launcher.py:37] Route: /rerank, Methods: POST
INFO 08-09 14:00:58 [launcher.py:37] Route: /v1/rerank, Methods: POST
INFO 08-09 14:00:58 [launcher.py:37] Route: /v2/rerank, Methods: POST
INFO 08-09 14:00:58 [launcher.py:37] Route: /invocations, Methods: POST
INFO 08-09 14:00:58 [launcher.py:37] Route: /metrics, Methods: GET
INFO:     Started server process [8616]
INFO:     Waiting for application startup.
INFO:     Application startup complete.
INFO 08-09 14:05:21 [logger.py:43] Received request cmpl-cddc6a9d2f5f42c48f580e9cb43f5365-0: prompt: 'What is LLaMA 2 and how does it work?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 365, 5661, 1529, 29871, 29906, 322, 920, 947, 372, 664, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:50078 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:05:21 [engine.py:331] Added request cmpl-cddc6a9d2f5f42c48f580e9cb43f5365-0.
INFO 08-09 14:05:23 [metrics.py:417] Avg prompt throughput: 2.8 tokens/s, Avg generation throughput: 3.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:05:28 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 123.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-09 14:05:31 [logger.py:43] Received request cmpl-d53ef2a8e525406692eb69fb94d2c2b3-0: prompt: 'Explain the architecture of a transformer model.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 12027, 7420, 278, 11258, 310, 263, 4327, 261, 1904, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:34732 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:05:31 [engine.py:331] Added request cmpl-d53ef2a8e525406692eb69fb94d2c2b3-0.
INFO 08-09 14:05:33 [metrics.py:417] Avg prompt throughput: 2.2 tokens/s, Avg generation throughput: 124.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:05:38 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 127.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 08-09 14:05:39 [logger.py:43] Received request cmpl-2d60b97e6aff4e6ca6eea187fefd8adf-0: prompt: 'What are the core differences between CNN and RNN?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 526, 278, 7136, 12651, 1546, 29696, 322, 390, 10262, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:46794 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:05:39 [engine.py:331] Added request cmpl-2d60b97e6aff4e6ca6eea187fefd8adf-0.
INFO 08-09 14:05:43 [metrics.py:417] Avg prompt throughput: 2.4 tokens/s, Avg generation throughput: 129.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-09 14:05:47 [logger.py:43] Received request cmpl-f02c3f9f985a4954a766f8dbd1eb152e-0: prompt: 'Describe how attention mechanism improves translation models.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 20355, 915, 920, 8570, 13336, 4857, 1960, 13962, 4733, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:52030 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:05:47 [engine.py:331] Added request cmpl-f02c3f9f985a4954a766f8dbd1eb152e-0.
INFO 08-09 14:05:48 [metrics.py:417] Avg prompt throughput: 2.2 tokens/s, Avg generation throughput: 128.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:05:53 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 124.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 08-09 14:05:55 [logger.py:43] Received request cmpl-064f16773211467692a1a13c5c67822d-0: prompt: 'What is the role of positional encoding in transformers?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 278, 6297, 310, 2602, 284, 8025, 297, 4327, 414, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:52032 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:05:55 [engine.py:331] Added request cmpl-064f16773211467692a1a13c5c67822d-0.
INFO 08-09 14:05:58 [metrics.py:417] Avg prompt throughput: 2.6 tokens/s, Avg generation throughput: 130.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-09 14:06:03 [logger.py:43] Received request cmpl-28c8e618d9344105bc97849ef8148872-0: prompt: 'How does self-attention differ from cross-attention?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 947, 1583, 29899, 1131, 2509, 1163, 515, 4891, 29899, 1131, 2509, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:52554 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:06:03 [engine.py:331] Added request cmpl-28c8e618d9344105bc97849ef8148872-0.
INFO 08-09 14:06:03 [metrics.py:417] Avg prompt throughput: 2.8 tokens/s, Avg generation throughput: 126.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:06:08 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 123.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-09 14:06:11 [logger.py:43] Received request cmpl-3a0622dde6924b72a7c5684f1a6820bc-0: prompt: 'Explain the concept of tokenization in NLP.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 12027, 7420, 278, 6964, 310, 5993, 2133, 297, 405, 13208, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:47798 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:06:11 [engine.py:331] Added request cmpl-3a0622dde6924b72a7c5684f1a6820bc-0.
INFO 08-09 14:06:13 [metrics.py:417] Avg prompt throughput: 2.4 tokens/s, Avg generation throughput: 104.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:06:18 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 112.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 08-09 14:06:20 [logger.py:43] Received request cmpl-27fd3427df234f9eaf9848e97989b092-0: prompt: 'What is the vanishing gradient problem and how to mitigate it?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 278, 1109, 14424, 16030, 1108, 322, 920, 304, 1380, 335, 403, 372, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:55420 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:06:20 [engine.py:331] Added request cmpl-27fd3427df234f9eaf9848e97989b092-0.
INFO 08-09 14:06:23 [metrics.py:417] Avg prompt throughput: 3.2 tokens/s, Avg generation throughput: 109.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:06:28 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 116.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 08-09 14:06:29 [logger.py:43] Received request cmpl-82c39127a86e4e0d82986aa54a393c5d-0: prompt: 'Describe how Gaudi architecture differs from NVIDIA GPUs.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 20355, 915, 920, 10415, 4749, 11258, 2923, 414, 515, 405, 13044, 10764, 22796, 29879, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:46932 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:06:29 [engine.py:331] Added request cmpl-82c39127a86e4e0d82986aa54a393c5d-0.
INFO 08-09 14:06:33 [metrics.py:417] Avg prompt throughput: 3.2 tokens/s, Avg generation throughput: 118.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-09 14:06:37 [logger.py:43] Received request cmpl-7ddf2caf34424f159e82ff1126893ebb-0: prompt: 'What is model parallelism and how does it help scale large models?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 1904, 8943, 1608, 322, 920, 947, 372, 1371, 6287, 2919, 4733, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:34526 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:06:37 [engine.py:331] Added request cmpl-7ddf2caf34424f159e82ff1126893ebb-0.
INFO 08-09 14:06:39 [metrics.py:417] Avg prompt throughput: 3.0 tokens/s, Avg generation throughput: 119.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:06:44 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 94.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-09 14:06:47 [logger.py:43] Received request cmpl-86e3e9f36bd0448da36be1bd61c902a3-0: prompt: 'Explain the difference between GPT and BERT architectures.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 12027, 7420, 278, 4328, 1546, 402, 7982, 322, 350, 20161, 6956, 1973, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:57048 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:06:47 [engine.py:331] Added request cmpl-86e3e9f36bd0448da36be1bd61c902a3-0.
INFO 08-09 14:06:49 [metrics.py:417] Avg prompt throughput: 2.8 tokens/s, Avg generation throughput: 122.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:06:54 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 120.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 08-09 14:06:55 [logger.py:43] Received request cmpl-6d92966321c740adbd6b3b64e67bf6b0-0: prompt: 'How does prompt engineering affect language model outputs?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 947, 9508, 21639, 6602, 4086, 1904, 14391, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:57050 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:06:55 [engine.py:331] Added request cmpl-6d92966321c740adbd6b3b64e67bf6b0-0.
INFO 08-09 14:06:59 [metrics.py:417] Avg prompt throughput: 2.0 tokens/s, Avg generation throughput: 129.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-09 14:07:00 [logger.py:43] Received request cmpl-28e94acff8e84a8da194ac56fc37b063-0: prompt: 'What is the function of layer normalization in deep networks?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 278, 740, 310, 7546, 4226, 2133, 297, 6483, 14379, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:57244 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:07:00 [engine.py:331] Added request cmpl-28e94acff8e84a8da194ac56fc37b063-0.
INFO 08-09 14:07:04 [metrics.py:417] Avg prompt throughput: 2.6 tokens/s, Avg generation throughput: 116.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-09 14:07:09 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 125.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 08-09 14:07:09 [logger.py:43] Received request cmpl-0677faee69a34e26a78aa92586d5fc88-0: prompt: 'Explain causal language modeling with examples.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 12027, 7420, 3269, 284, 4086, 1904, 292, 411, 6455, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:45712 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:07:09 [engine.py:331] Added request cmpl-0677faee69a34e26a78aa92586d5fc88-0.
INFO 08-09 14:07:14 [metrics.py:417] Avg prompt throughput: 2.2 tokens/s, Avg generation throughput: 125.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-09 14:07:15 [logger.py:43] Received request cmpl-1fdbf12472754a92a5e3a21bc7a644cd-0: prompt: 'What is quantization and how does it improve inference performance?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 4323, 2133, 322, 920, 947, 372, 11157, 27262, 4180, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:45726 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:07:15 [engine.py:331] Added request cmpl-1fdbf12472754a92a5e3a21bc7a644cd-0.
INFO 08-09 14:07:19 [metrics.py:417] Avg prompt throughput: 2.6 tokens/s, Avg generation throughput: 131.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-09 14:07:22 [logger.py:43] Received request cmpl-fba06d3f38c445a0a3f18ee0e2ad6bfe-0: prompt: 'Describe how fine-tuning works for large language models.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 20355, 915, 920, 2691, 29899, 29873, 27964, 1736, 363, 2919, 4086, 4733, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:49480 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:07:22 [engine.py:331] Added request cmpl-fba06d3f38c445a0a3f18ee0e2ad6bfe-0.
INFO 08-09 14:07:24 [metrics.py:417] Avg prompt throughput: 2.8 tokens/s, Avg generation throughput: 123.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:07:29 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 99.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-09 14:07:31 [logger.py:43] Received request cmpl-37d301e3415c40e9a710655a4e8db991-0: prompt: 'What are LoRA adapters and how do they help in parameter-efficient training?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 526, 4309, 4717, 594, 481, 2153, 322, 920, 437, 896, 1371, 297, 3443, 29899, 8462, 6694, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:38924 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:07:31 [engine.py:331] Added request cmpl-37d301e3415c40e9a710655a4e8db991-0.
INFO 08-09 14:07:34 [metrics.py:417] Avg prompt throughput: 3.8 tokens/s, Avg generation throughput: 124.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:07:39 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 112.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 08-09 14:07:40 [logger.py:43] Received request cmpl-7ae6201852694d18ae2dcda8f9debcce-0: prompt: 'Explain the purpose of rotary positional embeddings.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 12027, 7420, 278, 6437, 310, 5731, 653, 2602, 284, 8297, 29881, 886, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:34778 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:07:40 [engine.py:331] Added request cmpl-7ae6201852694d18ae2dcda8f9debcce-0.
INFO 08-09 14:07:44 [metrics.py:417] Avg prompt throughput: 2.8 tokens/s, Avg generation throughput: 104.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:07:49 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 114.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 08-09 14:07:49 [logger.py:43] Received request cmpl-8d65242d93104d02b8dbd9bf106f4690-0: prompt: 'How does DeepSpeed improve model training efficiency?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 947, 21784, 26539, 11157, 1904, 6694, 19201, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:50016 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:07:49 [engine.py:331] Added request cmpl-8d65242d93104d02b8dbd9bf106f4690-0.
INFO 08-09 14:07:54 [metrics.py:417] Avg prompt throughput: 2.0 tokens/s, Avg generation throughput: 119.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-09 14:07:58 [logger.py:43] Received request cmpl-02bd73b9a9094c8086603af64bd2ca98-0: prompt: 'Compare data parallelism and pipeline parallelism.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 3831, 598, 848, 8943, 1608, 322, 16439, 8943, 1608, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:53172 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:07:58 [engine.py:331] Added request cmpl-02bd73b9a9094c8086603af64bd2ca98-0.
INFO 08-09 14:07:59 [metrics.py:417] Avg prompt throughput: 2.2 tokens/s, Avg generation throughput: 115.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:08:04 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 115.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-09 14:08:06 [logger.py:43] Received request cmpl-86bb51454eb04213be4ea130abf870ad-0: prompt: 'What is ALiBi positional encoding?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 14445, 29875, 20517, 2602, 284, 8025, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:52508 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:08:06 [engine.py:331] Added request cmpl-86bb51454eb04213be4ea130abf870ad-0.
INFO 08-09 14:08:09 [metrics.py:417] Avg prompt throughput: 2.0 tokens/s, Avg generation throughput: 120.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:08:14 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 123.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 08-09 14:08:15 [logger.py:43] Received request cmpl-a219e80797874fc880ab026b70bd9dbc-0: prompt: 'Describe the purpose of HF Transformers library.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 20355, 915, 278, 6437, 310, 379, 29943, 4103, 689, 414, 3489, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:52510 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:08:15 [engine.py:331] Added request cmpl-a219e80797874fc880ab026b70bd9dbc-0.
INFO 08-09 14:08:19 [metrics.py:417] Avg prompt throughput: 2.6 tokens/s, Avg generation throughput: 110.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-09 14:08:24 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 103.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 08-09 14:08:24 [logger.py:43] Received request cmpl-a671025e61494a1592349ec7dc5c53f7-0: prompt: 'How do attention heads capture contextual meaning?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 437, 8570, 15883, 10446, 3030, 950, 6593, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:50068 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:08:24 [engine.py:331] Added request cmpl-a671025e61494a1592349ec7dc5c53f7-0.
INFO 08-09 14:08:29 [metrics.py:417] Avg prompt throughput: 2.0 tokens/s, Avg generation throughput: 124.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-09 14:08:32 [logger.py:43] Received request cmpl-5f7c503ed2184d96bb3989befc783b31-0: prompt: 'What is dynamic batching in inference servers?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 7343, 9853, 292, 297, 27262, 12424, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:57842 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:08:32 [engine.py:331] Added request cmpl-5f7c503ed2184d96bb3989befc783b31-0.
INFO 08-09 14:08:34 [metrics.py:417] Avg prompt throughput: 2.0 tokens/s, Avg generation throughput: 117.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:08:37 [logger.py:43] Received request cmpl-1978198185c645cfb4a463c82cb2d6ab-0: prompt: 'Explain greedy decoding vs beam search vs sampling.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 12027, 7420, 1395, 7584, 1602, 3689, 7186, 22913, 2740, 7186, 23460, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:47384 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:08:37 [engine.py:331] Added request cmpl-1978198185c645cfb4a463c82cb2d6ab-0.
INFO 08-09 14:08:39 [metrics.py:417] Avg prompt throughput: 2.6 tokens/s, Avg generation throughput: 123.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:08:41 [logger.py:43] Received request cmpl-9f5b6527aaea4752a103a95a5e5b1463-0: prompt: 'What is perplexity in language models and why is it important?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 639, 10709, 537, 297, 4086, 4733, 322, 2020, 338, 372, 4100, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:47394 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:08:41 [engine.py:331] Added request cmpl-9f5b6527aaea4752a103a95a5e5b1463-0.
INFO 08-09 14:08:44 [metrics.py:417] Avg prompt throughput: 3.0 tokens/s, Avg generation throughput: 124.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-09 14:08:48 [logger.py:43] Received request cmpl-38cc775715b34cddb1daa66bf4c94c6f-0: prompt: 'How does mixed precision training benefit model training?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 947, 12849, 16716, 6694, 14169, 1904, 6694, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:39522 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:08:48 [engine.py:331] Added request cmpl-38cc775715b34cddb1daa66bf4c94c6f-0.
INFO 08-09 14:08:49 [metrics.py:417] Avg prompt throughput: 2.0 tokens/s, Avg generation throughput: 126.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:08:54 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 122.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-09 14:08:57 [logger.py:43] Received request cmpl-1934c774f0804eb49fd8769f31682e10-0: prompt: 'What is the function of the softmax layer in transformers?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 278, 740, 310, 278, 4964, 3317, 7546, 297, 4327, 414, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:49894 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:08:57 [engine.py:331] Added request cmpl-1934c774f0804eb49fd8769f31682e10-0.
INFO 08-09 14:08:59 [metrics.py:417] Avg prompt throughput: 2.8 tokens/s, Avg generation throughput: 116.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:09:04 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 104.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 08-09 14:09:06 [logger.py:43] Received request cmpl-0fae5cacde0140658454fb9d65a3324a-0: prompt: "Explain embeddings and how they're used in NLP models.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 12027, 7420, 8297, 29881, 886, 322, 920, 896, 29915, 276, 1304, 297, 405, 13208, 4733, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:50036 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:09:06 [engine.py:331] Added request cmpl-0fae5cacde0140658454fb9d65a3324a-0.
INFO 08-09 14:09:09 [metrics.py:417] Avg prompt throughput: 3.4 tokens/s, Avg generation throughput: 110.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:09:14 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 102.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 08-09 14:09:15 [logger.py:43] Received request cmpl-986afe06a6244f088019adc761314d7d-0: prompt: "Describe the function of the tokenizer's vocabulary.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 20355, 915, 278, 740, 310, 278, 5993, 3950, 29915, 29879, 7931, 370, 352, 653, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:50042 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:09:15 [engine.py:331] Added request cmpl-986afe06a6244f088019adc761314d7d-0.
INFO 08-09 14:09:18 [logger.py:43] Received request cmpl-d99cbb417329466fbeca356c1da804c6-0: prompt: 'What is causal masking in transformer decoders?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 3269, 284, 11105, 292, 297, 4327, 261, 1602, 397, 414, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:43448 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:09:18 [engine.py:331] Added request cmpl-d99cbb417329466fbeca356c1da804c6-0.
INFO 08-09 14:09:19 [metrics.py:417] Avg prompt throughput: 6.0 tokens/s, Avg generation throughput: 120.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:09:24 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 118.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-09 14:09:26 [logger.py:43] Received request cmpl-409b2ea5668d479dbc0cb79d335ec70d-0: prompt: 'How does gradient checkpointing reduce memory usage?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 947, 16030, 1423, 3149, 292, 10032, 3370, 8744, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:42264 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:09:26 [engine.py:331] Added request cmpl-409b2ea5668d479dbc0cb79d335ec70d-0.
INFO 08-09 14:09:29 [metrics.py:417] Avg prompt throughput: 2.2 tokens/s, Avg generation throughput: 117.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:09:34 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 125.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 08-09 14:09:34 [logger.py:43] Received request cmpl-b0a32a7cf4fb401396972a6815be388a-0: prompt: 'Explain the difference between encoder-decoder and decoder-only models.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 12027, 7420, 278, 4328, 1546, 2094, 6119, 29899, 7099, 6119, 322, 1602, 6119, 29899, 6194, 4733, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:42278 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:09:34 [engine.py:331] Added request cmpl-b0a32a7cf4fb401396972a6815be388a-0.
INFO 08-09 14:09:39 [metrics.py:417] Avg prompt throughput: 3.6 tokens/s, Avg generation throughput: 112.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-09 14:09:43 [logger.py:43] Received request cmpl-a2122c5bf1194196ae0175fb3283c919-0: prompt: 'What is zero-shot vs few-shot vs fine-tuned inference?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 5225, 29899, 8962, 7186, 2846, 29899, 8962, 7186, 2691, 29899, 29873, 348, 287, 27262, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:50916 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:09:43 [engine.py:331] Added request cmpl-a2122c5bf1194196ae0175fb3283c919-0.
INFO 08-09 14:09:44 [metrics.py:417] Avg prompt throughput: 3.6 tokens/s, Avg generation throughput: 112.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:09:49 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 117.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-09 14:09:52 [logger.py:43] Received request cmpl-e29eaa1b3f1049c5a6c0d80d8794dbee-0: prompt: 'What are prompt templates and why do they matter?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 526, 9508, 17475, 322, 2020, 437, 896, 4383, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:52600 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:09:52 [engine.py:331] Added request cmpl-e29eaa1b3f1049c5a6c0d80d8794dbee-0.
INFO 08-09 14:09:54 [metrics.py:417] Avg prompt throughput: 2.2 tokens/s, Avg generation throughput: 104.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:09:59 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 121.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 08-09 14:10:00 [logger.py:43] Received request cmpl-03c261c91ba5480a899cbc90c59ace74-0: prompt: 'Explain the role of BOS and EOS tokens in transformers.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 12027, 7420, 278, 6297, 310, 350, 3267, 322, 382, 3267, 18897, 297, 4327, 414, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:52382 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:10:00 [engine.py:331] Added request cmpl-03c261c91ba5480a899cbc90c59ace74-0.
INFO 08-09 14:10:04 [metrics.py:417] Avg prompt throughput: 3.2 tokens/s, Avg generation throughput: 125.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-09 14:10:09 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 99.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 08-09 14:10:10 [logger.py:43] Received request cmpl-61c2f4267577440daac7b5466402783c-0: prompt: 'What is a floating-point tensor and how is it represented?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 263, 16526, 29899, 3149, 12489, 322, 920, 338, 372, 9875, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:37576 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:10:10 [engine.py:331] Added request cmpl-61c2f4267577440daac7b5466402783c-0.
INFO 08-09 14:10:14 [metrics.py:417] Avg prompt throughput: 2.8 tokens/s, Avg generation throughput: 101.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-09 14:10:19 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 92.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 08-09 14:10:20 [logger.py:43] Received request cmpl-d15b3dedfe9f4375bc3f2644f4af0907-0: prompt: 'What are hypernetworks in neural architectures?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 526, 11266, 11618, 29879, 297, 19677, 6956, 1973, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:43600 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:10:20 [engine.py:331] Added request cmpl-d15b3dedfe9f4375bc3f2644f4af0907-0.
INFO 08-09 14:10:24 [metrics.py:417] Avg prompt throughput: 2.2 tokens/s, Avg generation throughput: 106.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-09 14:10:29 [logger.py:43] Received request cmpl-0047344dbbd644838a36f64079c7323e-0: prompt: 'Explain MoE (Mixture of Experts) architecture.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 12027, 7420, 4546, 29923, 313, 29924, 29875, 15546, 310, 28224, 1372, 29897, 11258, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:40558 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:10:29 [engine.py:331] Added request cmpl-0047344dbbd644838a36f64079c7323e-0.
INFO 08-09 14:10:29 [metrics.py:417] Avg prompt throughput: 3.0 tokens/s, Avg generation throughput: 118.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:10:34 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 117.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-09 14:10:37 [logger.py:43] Received request cmpl-3fddf01937614dbe9bc34d03b7d00f98-0: prompt: 'What is distillation in model compression?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 1320, 453, 362, 297, 1904, 24221, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:55872 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:10:37 [engine.py:331] Added request cmpl-3fddf01937614dbe9bc34d03b7d00f98-0.
INFO 08-09 14:10:39 [metrics.py:417] Avg prompt throughput: 2.0 tokens/s, Avg generation throughput: 112.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:10:44 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 113.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-09 14:10:46 [logger.py:43] Received request cmpl-f1db9ce38b7b4d77b70b3c259065aa3b-0: prompt: 'How do you implement top-k and top-p sampling?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 437, 366, 2334, 2246, 29899, 29895, 322, 2246, 29899, 29886, 23460, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:54314 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:10:46 [engine.py:331] Added request cmpl-f1db9ce38b7b4d77b70b3c259065aa3b-0.
INFO 08-09 14:10:49 [metrics.py:417] Avg prompt throughput: 2.8 tokens/s, Avg generation throughput: 115.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:10:54 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 113.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 08-09 14:10:55 [logger.py:43] Received request cmpl-9b81ba330a0049cca501478814862ab0-0: prompt: 'What is a vector database and how is it used with LLMs?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 263, 4608, 2566, 322, 920, 338, 372, 1304, 411, 365, 26369, 29879, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:54316 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:10:55 [engine.py:331] Added request cmpl-9b81ba330a0049cca501478814862ab0-0.
INFO 08-09 14:10:59 [metrics.py:417] Avg prompt throughput: 3.2 tokens/s, Avg generation throughput: 125.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-09 14:11:03 [logger.py:43] Received request cmpl-c54832a4abb54e05b2db831984d572ed-0: prompt: 'Describe how you would use RAG (Retrieval Augmented Generation).', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 20355, 915, 920, 366, 723, 671, 390, 10051, 313, 8015, 2546, 791, 22333, 358, 287, 28203, 467], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:43960 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:11:03 [engine.py:331] Added request cmpl-c54832a4abb54e05b2db831984d572ed-0.
INFO 08-09 14:11:04 [metrics.py:417] Avg prompt throughput: 3.6 tokens/s, Avg generation throughput: 129.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:11:09 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 114.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-09 14:11:12 [logger.py:43] Received request cmpl-28ff50ef2665417b8d599fd86c8770e2-0: prompt: 'What is streaming inference and how is it used?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 24820, 27262, 322, 920, 338, 372, 1304, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:50258 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:11:12 [engine.py:331] Added request cmpl-28ff50ef2665417b8d599fd86c8770e2-0.
INFO 08-09 14:11:14 [metrics.py:417] Avg prompt throughput: 2.2 tokens/s, Avg generation throughput: 106.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:11:19 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 113.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 08-09 14:11:21 [logger.py:43] Received request cmpl-fbdeac6fb8e04d9cabdc9e4ebeedeffd-0: prompt: 'How does TTFT relate to user-perceived latency?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 947, 323, 29911, 7818, 29279, 304, 1404, 29899, 546, 346, 2347, 23316, 1270, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:44778 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:11:21 [engine.py:331] Added request cmpl-fbdeac6fb8e04d9cabdc9e4ebeedeffd-0.
INFO 08-09 14:11:24 [metrics.py:417] Avg prompt throughput: 3.2 tokens/s, Avg generation throughput: 89.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:11:29 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 117.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 08-09 14:11:30 [logger.py:43] Received request cmpl-1418a0639fa248f5a8e2c304d9a18077-0: prompt: 'Explain transformer decoder blocks layer-by-layer.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 12027, 7420, 4327, 261, 1602, 6119, 10930, 7546, 29899, 1609, 29899, 13148, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:40762 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:11:30 [engine.py:331] Added request cmpl-1418a0639fa248f5a8e2c304d9a18077-0.
INFO 08-09 14:11:34 [metrics.py:417] Avg prompt throughput: 2.8 tokens/s, Avg generation throughput: 129.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-09 14:11:36 [logger.py:43] Received request cmpl-f2c6049a88254fcc8003c670258a9941-0: prompt: 'What is a KV cache in transformer inference?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 263, 476, 29963, 7090, 297, 4327, 261, 27262, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:42872 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:11:36 [engine.py:331] Added request cmpl-f2c6049a88254fcc8003c670258a9941-0.
INFO 08-09 14:11:39 [metrics.py:417] Avg prompt throughput: 2.4 tokens/s, Avg generation throughput: 117.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:11:44 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 125.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 08-09 14:11:45 [logger.py:43] Received request cmpl-e6b98d8b956d488f8d8939917f3d3e9f-0: prompt: 'Describe token-by-token generation and its challenges.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 20355, 915, 5993, 29899, 1609, 29899, 6979, 12623, 322, 967, 18066, 267, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:42878 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:11:45 [engine.py:331] Added request cmpl-e6b98d8b956d488f8d8939917f3d3e9f-0.
INFO 08-09 14:11:49 [metrics.py:417] Avg prompt throughput: 2.8 tokens/s, Avg generation throughput: 128.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-09 14:11:52 [logger.py:43] Received request cmpl-a7b287ff97b64305942aeb8272e1e894-0: prompt: 'How does LLaMA 2 compare to GPT-3.5?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 947, 365, 5661, 1529, 29871, 29906, 7252, 304, 402, 7982, 29899, 29941, 29889, 29945, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:46588 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:11:52 [engine.py:331] Added request cmpl-a7b287ff97b64305942aeb8272e1e894-0.
INFO 08-09 14:11:54 [metrics.py:417] Avg prompt throughput: 3.4 tokens/s, Avg generation throughput: 128.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:11:59 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 119.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 08-09 14:12:01 [logger.py:43] Received request cmpl-2cfeb05db91e40ae9111ee97090256ed-0: prompt: 'Explain the training data pipeline for a large model.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 12027, 7420, 278, 6694, 848, 16439, 363, 263, 2919, 1904, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:33986 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:12:01 [engine.py:331] Added request cmpl-2cfeb05db91e40ae9111ee97090256ed-0.
INFO 08-09 14:12:04 [metrics.py:417] Avg prompt throughput: 2.4 tokens/s, Avg generation throughput: 105.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:12:09 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 113.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 08-09 14:12:10 [logger.py:43] Received request cmpl-ef36a0bdecc34c1d9c2aa8681822625d-0: prompt: 'What are the ethical concerns with generative models?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 526, 278, 11314, 936, 21838, 411, 1176, 1230, 4733, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:37862 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:12:10 [engine.py:331] Added request cmpl-ef36a0bdecc34c1d9c2aa8681822625d-0.
INFO 08-09 14:12:14 [metrics.py:417] Avg prompt throughput: 2.4 tokens/s, Avg generation throughput: 100.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-09 14:12:19 [logger.py:43] Received request cmpl-930cfa04121e4c16bf2fe23c4b1044be-0: prompt: 'How do transformers process multilingual text?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 437, 4327, 414, 1889, 1773, 6504, 950, 1426, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:37470 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:12:19 [engine.py:331] Added request cmpl-930cfa04121e4c16bf2fe23c4b1044be-0.
INFO 08-09 14:12:19 [metrics.py:417] Avg prompt throughput: 2.2 tokens/s, Avg generation throughput: 127.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:12:24 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 126.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-09 14:12:26 [logger.py:43] Received request cmpl-a425f641da1c4710a7f11dac2a8778c4-0: prompt: 'What are flash attention mechanisms?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 526, 11013, 8570, 7208, 12903, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:41576 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:12:26 [engine.py:331] Added request cmpl-a425f641da1c4710a7f11dac2a8778c4-0.
INFO 08-09 14:12:29 [metrics.py:417] Avg prompt throughput: 1.6 tokens/s, Avg generation throughput: 123.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:12:34 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 112.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 08-09 14:12:35 [logger.py:43] Received request cmpl-2b159289bd1b4f4f993ef7536bfb54db-0: prompt: 'Describe how memory-efficient attention works.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 20355, 915, 920, 3370, 29899, 8462, 8570, 1736, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:41586 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:12:35 [engine.py:331] Added request cmpl-2b159289bd1b4f4f993ef7536bfb54db-0.
INFO 08-09 14:12:39 [metrics.py:417] Avg prompt throughput: 2.0 tokens/s, Avg generation throughput: 126.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-09 14:12:43 [logger.py:43] Received request cmpl-c4952006474f45c8a742d6f5997362f1-0: prompt: 'What is inference throughput and how is it calculated?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 27262, 1549, 649, 322, 920, 338, 372, 12833, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:58682 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:12:43 [engine.py:331] Added request cmpl-c4952006474f45c8a742d6f5997362f1-0.
INFO 08-09 14:12:44 [metrics.py:417] Avg prompt throughput: 2.4 tokens/s, Avg generation throughput: 120.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:12:49 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 117.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-09 14:12:51 [logger.py:43] Received request cmpl-33a371f31d374766b7f0bed63500e80f-0: prompt: 'Explain how masked self-attention works in BERT.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 12027, 7420, 920, 11105, 287, 1583, 29899, 1131, 2509, 1736, 297, 350, 20161, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:60836 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:12:51 [engine.py:331] Added request cmpl-33a371f31d374766b7f0bed63500e80f-0.
INFO 08-09 14:12:54 [metrics.py:417] Avg prompt throughput: 3.0 tokens/s, Avg generation throughput: 126.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:12:58 [logger.py:43] Received request cmpl-12c51d3eb4d5474c813497a39fa4c87c-0: prompt: 'How do instruction-tuned models differ from base models?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 437, 15278, 29899, 29873, 348, 287, 4733, 1163, 515, 2967, 4733, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:55228 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:12:58 [engine.py:331] Added request cmpl-12c51d3eb4d5474c813497a39fa4c87c-0.
INFO 08-09 14:12:59 [metrics.py:417] Avg prompt throughput: 2.8 tokens/s, Avg generation throughput: 109.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:13:04 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 112.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-09 14:13:07 [logger.py:43] Received request cmpl-dc67d2d2cce041f8850c803b8b8f0598-0: prompt: 'Describe an RLHF (Reinforcement Learning from Human Feedback) process.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 20355, 915, 385, 390, 29931, 29950, 29943, 313, 1123, 262, 1454, 13561, 29257, 515, 12968, 5169, 287, 1627, 29897, 1889, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:38284 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:13:07 [engine.py:331] Added request cmpl-dc67d2d2cce041f8850c803b8b8f0598-0.
INFO 08-09 14:13:09 [metrics.py:417] Avg prompt throughput: 4.4 tokens/s, Avg generation throughput: 98.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:13:14 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 103.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-09 14:13:17 [logger.py:43] Received request cmpl-5f13832d146c442db13a965f00abab41-0: prompt: 'What is SFT (Supervised Fine-Tuning) in LLM training?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 317, 7818, 313, 19111, 11292, 28896, 29899, 29911, 27964, 29897, 297, 365, 26369, 6694, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:45312 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:13:17 [engine.py:331] Added request cmpl-5f13832d146c442db13a965f00abab41-0.
INFO 08-09 14:13:19 [metrics.py:417] Avg prompt throughput: 3.6 tokens/s, Avg generation throughput: 115.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:13:24 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 104.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 08-09 14:13:26 [logger.py:43] Received request cmpl-d14518a841ad45d0a59fd884522f3c9e-0: prompt: 'Explain the role of HPU in Gaudi chips.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 12027, 7420, 278, 6297, 310, 379, 7056, 297, 10415, 4749, 521, 4512, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:45388 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:13:26 [engine.py:331] Added request cmpl-d14518a841ad45d0a59fd884522f3c9e-0.
INFO 08-09 14:13:29 [metrics.py:417] Avg prompt throughput: 2.8 tokens/s, Avg generation throughput: 112.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:13:34 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 128.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 08-09 14:13:34 [logger.py:43] Received request cmpl-23665dcb5c754eecbad8531c6d172ac1-0: prompt: 'What is a causal attention mask?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 263, 3269, 284, 8570, 11105, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:45398 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:13:34 [engine.py:331] Added request cmpl-23665dcb5c754eecbad8531c6d172ac1-0.
INFO 08-09 14:13:39 [metrics.py:417] Avg prompt throughput: 1.8 tokens/s, Avg generation throughput: 120.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-09 14:13:42 [logger.py:43] Received request cmpl-4525f6755e574326ba2ca130b101572c-0: prompt: 'Describe the HF `generate()` function in Transformers.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 20355, 915, 278, 379, 29943, 421, 17158, 2555, 740, 297, 4103, 689, 414, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:55040 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:13:42 [engine.py:331] Added request cmpl-4525f6755e574326ba2ca130b101572c-0.
INFO 08-09 14:13:44 [metrics.py:417] Avg prompt throughput: 3.0 tokens/s, Avg generation throughput: 115.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:13:49 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 113.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-09 14:13:51 [logger.py:43] Received request cmpl-b6369034c0194b0eadc2927e78972680-0: prompt: 'Explain differences between T5, BART, and GPT models.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 12027, 7420, 12651, 1546, 323, 29945, 29892, 350, 8322, 29892, 322, 402, 7982, 4733, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:47812 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:13:51 [engine.py:331] Added request cmpl-b6369034c0194b0eadc2927e78972680-0.
INFO 08-09 14:13:54 [metrics.py:417] Avg prompt throughput: 3.2 tokens/s, Avg generation throughput: 108.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:13:59 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 100.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 08-09 14:14:01 [logger.py:43] Received request cmpl-ca8cccb89c4e423f86cac988b5429239-0: prompt: 'What is model checkpointing and resumption?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 1904, 1423, 3149, 292, 322, 620, 28069, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:54972 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:14:01 [engine.py:331] Added request cmpl-ca8cccb89c4e423f86cac988b5429239-0.
INFO 08-09 14:14:04 [metrics.py:417] Avg prompt throughput: 2.2 tokens/s, Avg generation throughput: 116.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:14:09 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 124.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 08-09 14:14:09 [logger.py:43] Received request cmpl-7eca194b7f8e44888a30962563ac7d8f-0: prompt: 'Describe how checkpoint sharding works in distributed training.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 20355, 915, 920, 1423, 3149, 528, 20272, 1736, 297, 13235, 6694, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:38244 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:14:09 [engine.py:331] Added request cmpl-7eca194b7f8e44888a30962563ac7d8f-0.
INFO 08-09 14:14:14 [metrics.py:417] Avg prompt throughput: 2.6 tokens/s, Avg generation throughput: 133.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-09 14:14:17 [logger.py:43] Received request cmpl-65904d9315db4a8b817f0fd14722e4cc-0: prompt: 'How are model weights stored in HuggingFace format?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 526, 1904, 18177, 6087, 297, 379, 688, 3460, 23360, 3402, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:41108 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:14:17 [engine.py:331] Added request cmpl-65904d9315db4a8b817f0fd14722e4cc-0.
INFO 08-09 14:14:19 [metrics.py:417] Avg prompt throughput: 2.6 tokens/s, Avg generation throughput: 117.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:14:24 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 122.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 08-09 14:14:26 [logger.py:43] Received request cmpl-36c682c52d364524a8e63b8f08fa2877-0: prompt: 'Explain the impact of batch size on training and inference.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 12027, 7420, 278, 10879, 310, 9853, 2159, 373, 6694, 322, 27262, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:41120 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:14:26 [engine.py:331] Added request cmpl-36c682c52d364524a8e63b8f08fa2877-0.
INFO 08-09 14:14:29 [metrics.py:417] Avg prompt throughput: 2.6 tokens/s, Avg generation throughput: 104.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-09 14:14:34 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 95.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 08-09 14:14:36 [logger.py:43] Received request cmpl-cb6b97f5b81448b2b516518255840ca4-0: prompt: 'What are fused kernels and why are they fast?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 526, 285, 3880, 413, 824, 1379, 322, 2020, 526, 896, 5172, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:38762 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:14:36 [engine.py:331] Added request cmpl-cb6b97f5b81448b2b516518255840ca4-0.
INFO 08-09 14:14:39 [metrics.py:417] Avg prompt throughput: 2.8 tokens/s, Avg generation throughput: 106.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-09 14:14:44 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 119.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 08-09 14:14:44 [logger.py:43] Received request cmpl-c177a16560004702be8294060c415588-0: prompt: 'How does the `AutoModelForCausalLM` class work?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 947, 278, 421, 12300, 3195, 2831, 29907, 1485, 284, 26369, 29952, 770, 664, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:38778 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:14:44 [engine.py:331] Added request cmpl-c177a16560004702be8294060c415588-0.
INFO 08-09 14:14:49 [metrics.py:417] Avg prompt throughput: 3.2 tokens/s, Avg generation throughput: 111.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-09 14:14:53 [logger.py:43] Received request cmpl-91771ae6880a4511812ee530a3936e8d-0: prompt: 'What are tensor parallel and tensor slicing?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 526, 12489, 8943, 322, 12489, 269, 506, 292, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:50192 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:14:53 [engine.py:331] Added request cmpl-91771ae6880a4511812ee530a3936e8d-0.
INFO 08-09 14:14:54 [metrics.py:417] Avg prompt throughput: 2.2 tokens/s, Avg generation throughput: 113.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:14:59 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 111.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-09 14:15:02 [logger.py:43] Received request cmpl-062c3080879d4d878a0995c3d7cdacf9-0: prompt: 'Describe the key steps of model serving using vLLM.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 20355, 915, 278, 1820, 6576, 310, 1904, 16330, 773, 325, 2208, 29924, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:42270 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:15:02 [engine.py:331] Added request cmpl-062c3080879d4d878a0995c3d7cdacf9-0.
INFO 08-09 14:15:04 [metrics.py:417] Avg prompt throughput: 2.8 tokens/s, Avg generation throughput: 115.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:15:09 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 108.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 08-09 14:15:09 [logger.py:43] Received request cmpl-59d6d39fca3749a99c0b2778f7b36eac-0: prompt: 'What are inference graphs in deep learning?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 526, 27262, 18445, 297, 6483, 6509, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:48952 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:15:09 [engine.py:331] Added request cmpl-59d6d39fca3749a99c0b2778f7b36eac-0.
INFO 08-09 14:15:14 [metrics.py:417] Avg prompt throughput: 1.8 tokens/s, Avg generation throughput: 114.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-09 14:15:18 [logger.py:43] Received request cmpl-dea4f95e4b994ca098bf1a5f77639806-0: prompt: 'What is a streaming response in OpenAI-style APIs?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 263, 24820, 2933, 297, 4673, 23869, 29899, 3293, 23649, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:36564 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:15:18 [engine.py:331] Added request cmpl-dea4f95e4b994ca098bf1a5f77639806-0.
INFO 08-09 14:15:19 [metrics.py:417] Avg prompt throughput: 2.6 tokens/s, Avg generation throughput: 112.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:15:24 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 97.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-09 14:15:27 [logger.py:43] Received request cmpl-46826c7d48b04a71afe831d6fafb973c-0: prompt: 'How does speculative decoding reduce latency?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 947, 1580, 28524, 1602, 3689, 10032, 23316, 1270, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:49648 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:15:27 [engine.py:331] Added request cmpl-46826c7d48b04a71afe831d6fafb973c-0.
INFO 08-09 14:15:29 [metrics.py:417] Avg prompt throughput: 2.2 tokens/s, Avg generation throughput: 116.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:15:34 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 111.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-09 14:15:36 [logger.py:43] Received request cmpl-61365b5bebf44514bc75ae3b1cba2868-0: prompt: 'What is JAX and how is it different from PyTorch?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 435, 6604, 322, 920, 338, 372, 1422, 515, 10772, 29911, 25350, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:48360 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:15:36 [engine.py:331] Added request cmpl-61365b5bebf44514bc75ae3b1cba2868-0.
INFO 08-09 14:15:39 [metrics.py:417] Avg prompt throughput: 3.0 tokens/s, Avg generation throughput: 126.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-09 14:15:44 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 115.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 08-09 14:15:44 [logger.py:43] Received request cmpl-9f675a19a1ae4d68982135952e0e288e-0: prompt: 'Describe lazy loading of weights in vLLM.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 20355, 915, 17366, 8363, 310, 18177, 297, 325, 2208, 29924, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:48366 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:15:44 [engine.py:331] Added request cmpl-9f675a19a1ae4d68982135952e0e288e-0.
INFO 08-09 14:15:49 [metrics.py:417] Avg prompt throughput: 2.4 tokens/s, Avg generation throughput: 131.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-09 14:15:52 [logger.py:43] Received request cmpl-8d2c86f57ce840eb826e749964e4b0b5-0: prompt: 'Explain the role of SynapseAI in Gaudi software stack.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 12027, 7420, 278, 6297, 310, 10829, 481, 344, 23869, 297, 10415, 4749, 7047, 5096, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:39438 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:15:52 [engine.py:331] Added request cmpl-8d2c86f57ce840eb826e749964e4b0b5-0.
INFO 08-09 14:15:54 [metrics.py:417] Avg prompt throughput: 3.2 tokens/s, Avg generation throughput: 128.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:15:59 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 111.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 08-09 14:16:00 [logger.py:43] Received request cmpl-4724671a0a944d35b3fd791ea575b3c4-0: prompt: 'What are the differences between HPU and GPU tensors?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 526, 278, 12651, 1546, 379, 7056, 322, 22796, 25187, 943, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:50814 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:16:00 [engine.py:331] Added request cmpl-4724671a0a944d35b3fd791ea575b3c4-0.
INFO 08-09 14:16:04 [metrics.py:417] Avg prompt throughput: 2.6 tokens/s, Avg generation throughput: 112.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-09 14:16:07 [logger.py:43] Received request cmpl-b4ec579006984d4da1adba1e6280ad3c-0: prompt: 'What are the limitations of LLaMA 2?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 526, 278, 27028, 310, 365, 5661, 1529, 29871, 29906, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:51124 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:16:07 [engine.py:331] Added request cmpl-b4ec579006984d4da1adba1e6280ad3c-0.
INFO 08-09 14:16:09 [metrics.py:417] Avg prompt throughput: 2.4 tokens/s, Avg generation throughput: 107.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:16:14 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 115.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 08-09 14:16:16 [logger.py:43] Received request cmpl-48d52db930f045f1baf9885bc88a0a8c-0: prompt: 'How is multilingual performance evaluated in LLMs?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 338, 1773, 6504, 950, 4180, 19030, 297, 365, 26369, 29879, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:42660 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:16:16 [engine.py:331] Added request cmpl-48d52db930f045f1baf9885bc88a0a8c-0.
INFO 08-09 14:16:17 [logger.py:43] Received request cmpl-8a44cd11303e49f5bcb14115d442bda9-0: prompt: 'Describe an example use of prompt tuning.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 20355, 915, 385, 1342, 671, 310, 9508, 18515, 292, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:42662 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:16:17 [engine.py:331] Added request cmpl-8a44cd11303e49f5bcb14115d442bda9-0.
INFO 08-09 14:16:19 [metrics.py:417] Avg prompt throughput: 4.8 tokens/s, Avg generation throughput: 114.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:16:24 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 116.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 08-09 14:16:26 [logger.py:43] Received request cmpl-62288da8feff46e9b310d1330114c15f-0: prompt: 'What is the attention bottleneck and how to address it?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 278, 8570, 18046, 29880, 1600, 384, 322, 920, 304, 3211, 372, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:44032 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:16:26 [engine.py:331] Added request cmpl-62288da8feff46e9b310d1330114c15f-0.
INFO 08-09 14:16:29 [metrics.py:417] Avg prompt throughput: 3.0 tokens/s, Avg generation throughput: 123.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-09 14:16:34 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 110.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 08-09 14:16:35 [logger.py:43] Received request cmpl-ceaae7bd4685486e80b841b0d31d76ca-0: prompt: 'How does token alignment affect downstream tasks?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 947, 5993, 22239, 6602, 1623, 5461, 9595, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:44046 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:16:35 [engine.py:331] Added request cmpl-ceaae7bd4685486e80b841b0d31d76ca-0.
INFO 08-09 14:16:39 [metrics.py:417] Avg prompt throughput: 2.0 tokens/s, Avg generation throughput: 94.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-09 14:16:44 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 112.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 08-09 14:16:44 [logger.py:43] Received request cmpl-08e5f6f90e1c4bbbaac3805cf1a7ea0e-0: prompt: 'What are the challenges in inference on edge devices?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 526, 278, 18066, 267, 297, 27262, 373, 7636, 9224, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:54094 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:16:44 [engine.py:331] Added request cmpl-08e5f6f90e1c4bbbaac3805cf1a7ea0e-0.
INFO 08-09 14:16:49 [metrics.py:417] Avg prompt throughput: 2.4 tokens/s, Avg generation throughput: 116.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-09 14:16:53 [logger.py:43] Received request cmpl-8ded552b4558493190c4ec4beacd74d0-0: prompt: 'Explain the principle behind rotary embeddings.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 12027, 7420, 278, 12502, 5742, 5731, 653, 8297, 29881, 886, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:39076 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:16:53 [engine.py:331] Added request cmpl-8ded552b4558493190c4ec4beacd74d0-0.
INFO 08-09 14:16:53 [logger.py:43] Received request cmpl-1bbc8cc3192c496382b48ef0b6f679fe-0: prompt: 'How does temperature affect language model output?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 947, 10430, 6602, 4086, 1904, 1962, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:39090 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:16:53 [engine.py:331] Added request cmpl-1bbc8cc3192c496382b48ef0b6f679fe-0.
INFO 08-09 14:16:54 [metrics.py:417] Avg prompt throughput: 4.2 tokens/s, Avg generation throughput: 124.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:16:59 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 125.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-09 14:17:01 [logger.py:43] Received request cmpl-078e95340a174862abe9f1df613ea8ef-0: prompt: 'What are stop sequences in generation APIs?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 526, 5040, 15602, 297, 12623, 23649, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:40492 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:17:01 [engine.py:331] Added request cmpl-078e95340a174862abe9f1df613ea8ef-0.
INFO 08-09 14:17:04 [metrics.py:417] Avg prompt throughput: 1.8 tokens/s, Avg generation throughput: 125.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:17:09 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 99.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 08-09 14:17:11 [logger.py:43] Received request cmpl-a95a0842b16a4d7bb42341c8dcb95684-0: prompt: 'Describe token streaming latency optimization.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 20355, 915, 5993, 24820, 23316, 1270, 13883, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:34542 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:17:11 [engine.py:331] Added request cmpl-a95a0842b16a4d7bb42341c8dcb95684-0.
INFO 08-09 14:17:14 [metrics.py:417] Avg prompt throughput: 1.8 tokens/s, Avg generation throughput: 114.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-09 14:17:19 [logger.py:43] Received request cmpl-36cd03cc73a844708c74a4ec2f6f7138-0: prompt: 'How to evaluate hallucinations in language models?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 304, 14707, 12713, 1682, 262, 800, 297, 4086, 4733, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:57624 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:17:19 [engine.py:331] Added request cmpl-36cd03cc73a844708c74a4ec2f6f7138-0.
INFO 08-09 14:17:19 [metrics.py:417] Avg prompt throughput: 2.4 tokens/s, Avg generation throughput: 128.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:17:24 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 123.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-09 14:17:27 [logger.py:43] Received request cmpl-de4a334c59144f4aa171cf184fb4ec6f-0: prompt: 'What is the purpose of `bos_token_id`?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 278, 6437, 310, 421, 27737, 29918, 6979, 29918, 333, 6522], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:39232 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:17:27 [engine.py:331] Added request cmpl-de4a334c59144f4aa171cf184fb4ec6f-0.
INFO 08-09 14:17:29 [logger.py:43] Received request cmpl-6640a81e96de4b0abf40a69beb9d4cca-0: prompt: 'How are fused attention layers implemented?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 526, 285, 3880, 8570, 15359, 8762, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:39234 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:17:29 [engine.py:331] Added request cmpl-6640a81e96de4b0abf40a69beb9d4cca-0.
INFO 08-09 14:17:29 [metrics.py:417] Avg prompt throughput: 4.4 tokens/s, Avg generation throughput: 120.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:17:34 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 133.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-09 14:17:36 [logger.py:43] Received request cmpl-44c9f8edc1404e6b8a3047abbd419046-0: prompt: 'Describe a pipeline for deploying LLaMA 2 in production.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 20355, 915, 263, 16439, 363, 7246, 292, 365, 5661, 1529, 29871, 29906, 297, 5802, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:56736 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:17:36 [engine.py:331] Added request cmpl-44c9f8edc1404e6b8a3047abbd419046-0.
INFO 08-09 14:17:39 [metrics.py:417] Avg prompt throughput: 3.2 tokens/s, Avg generation throughput: 113.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:17:42 [logger.py:43] Received request cmpl-af0c50d021cb4b95ae25ac619b6ab835-0: prompt: 'How does instruction following improve model usability?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 947, 15278, 1494, 11157, 1904, 502, 3097, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:56742 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:17:42 [engine.py:331] Added request cmpl-af0c50d021cb4b95ae25ac619b6ab835-0.
INFO 08-09 14:17:44 [metrics.py:417] Avg prompt throughput: 2.0 tokens/s, Avg generation throughput: 118.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:17:49 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 93.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-09 14:17:52 [logger.py:43] Received request cmpl-149d3616375440b79107ffa63938474a-0: prompt: 'What is a sequence-to-sequence model?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 263, 5665, 29899, 517, 29899, 16506, 1904, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:52834 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:17:52 [engine.py:331] Added request cmpl-149d3616375440b79107ffa63938474a-0.
INFO 08-09 14:17:54 [metrics.py:417] Avg prompt throughput: 2.2 tokens/s, Avg generation throughput: 110.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:17:59 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 112.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 08-09 14:18:01 [logger.py:43] Received request cmpl-d226bab2484b4f8896dc72c7921423a6-0: prompt: 'What is the purpose of `pad_token_id` in generation?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 278, 6437, 310, 421, 8305, 29918, 6979, 29918, 333, 29952, 297, 12623, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:57644 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:18:01 [engine.py:331] Added request cmpl-d226bab2484b4f8896dc72c7921423a6-0.
INFO 08-09 14:18:03 [logger.py:43] Received request cmpl-833260a9a17b4b2bba85cae2eb67fce4-0: prompt: 'Explain the concept of auto-regressive generation.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 12027, 7420, 278, 6964, 310, 4469, 29899, 276, 3663, 573, 12623, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:57658 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:18:03 [engine.py:331] Added request cmpl-833260a9a17b4b2bba85cae2eb67fce4-0.
INFO 08-09 14:18:04 [logger.py:43] Received request cmpl-a5c1d5e7af974d259ce5088f4f143329-0: prompt: 'How is top-p sampling different from nucleus sampling?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 338, 2246, 29899, 29886, 23460, 1422, 515, 22699, 375, 23460, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:57662 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:18:04 [engine.py:331] Added request cmpl-a5c1d5e7af974d259ce5088f4f143329-0.
INFO 08-09 14:18:04 [metrics.py:417] Avg prompt throughput: 8.4 tokens/s, Avg generation throughput: 120.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:18:09 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 114.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-09 14:18:13 [logger.py:43] Received request cmpl-a50e6a0b9aeb4a9b9f6004ec7aa19707-0: prompt: 'What are decoding strategies for safe responses?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 526, 1602, 3689, 16650, 583, 363, 9109, 20890, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:39372 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:18:13 [engine.py:331] Added request cmpl-a50e6a0b9aeb4a9b9f6004ec7aa19707-0.
INFO 08-09 14:18:14 [metrics.py:417] Avg prompt throughput: 2.2 tokens/s, Avg generation throughput: 123.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:18:19 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 110.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-09 14:18:21 [logger.py:43] Received request cmpl-e03b91418c71411b967905fba7eb9fab-0: prompt: 'How are model prompts structured in ChatML?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 526, 1904, 9508, 29879, 2281, 2955, 297, 678, 271, 1988, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:35284 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:18:21 [engine.py:331] Added request cmpl-e03b91418c71411b967905fba7eb9fab-0.
INFO 08-09 14:18:24 [metrics.py:417] Avg prompt throughput: 2.6 tokens/s, Avg generation throughput: 112.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:18:29 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 117.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 08-09 14:18:30 [logger.py:43] Received request cmpl-4095e52872d549949510aa2443ccc3e7-0: prompt: 'What is a system prompt in chat inference?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 263, 1788, 9508, 297, 13563, 27262, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:40594 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:18:30 [engine.py:331] Added request cmpl-4095e52872d549949510aa2443ccc3e7-0.
INFO 08-09 14:18:34 [metrics.py:417] Avg prompt throughput: 2.0 tokens/s, Avg generation throughput: 116.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-09 14:18:39 [logger.py:43] Received request cmpl-a43a26bbb93e4c479fd8efc7a2579a75-0: prompt: 'How do you batch requests for high throughput?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 437, 366, 9853, 7274, 363, 1880, 1549, 649, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:36014 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:18:39 [engine.py:331] Added request cmpl-a43a26bbb93e4c479fd8efc7a2579a75-0.
INFO 08-09 14:18:39 [metrics.py:417] Avg prompt throughput: 2.2 tokens/s, Avg generation throughput: 113.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:18:44 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 114.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-09 14:18:47 [logger.py:43] Received request cmpl-944060c8f3034161bfeaf90607eeec4a-0: prompt: 'Explain model serving with `transformers + vLLM`.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 12027, 7420, 1904, 16330, 411, 421, 9067, 414, 718, 325, 2208, 29924, 1412], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:40222 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:18:47 [engine.py:331] Added request cmpl-944060c8f3034161bfeaf90607eeec4a-0.
INFO 08-09 14:18:49 [logger.py:43] Received request cmpl-95195458b3bd49b0bee26fc041a86270-0: prompt: 'What is Triton Inference Server?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 323, 768, 265, 512, 1659, 5656, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:40238 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:18:49 [engine.py:331] Added request cmpl-95195458b3bd49b0bee26fc041a86270-0.
INFO 08-09 14:18:49 [metrics.py:417] Avg prompt throughput: 4.8 tokens/s, Avg generation throughput: 128.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:18:54 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 123.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-09 14:18:57 [logger.py:43] Received request cmpl-ff198258a7e649c0b1284064a70c5ef6-0: prompt: 'Describe the significance of large context windows.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 20355, 915, 278, 26002, 310, 2919, 3030, 5417, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:47838 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:18:57 [engine.py:331] Added request cmpl-ff198258a7e649c0b1284064a70c5ef6-0.
INFO 08-09 14:18:59 [metrics.py:417] Avg prompt throughput: 2.0 tokens/s, Avg generation throughput: 116.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:19:04 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 122.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 08-09 14:19:06 [logger.py:43] Received request cmpl-3abec35cc2854e638788c6ddb31e6e51-0: prompt: 'What is dynamic quantization in transformers?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 7343, 4323, 2133, 297, 4327, 414, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:56654 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:19:06 [engine.py:331] Added request cmpl-3abec35cc2854e638788c6ddb31e6e51-0.
INFO 08-09 14:19:09 [metrics.py:417] Avg prompt throughput: 2.0 tokens/s, Avg generation throughput: 112.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-09 14:19:14 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 104.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 08-09 14:19:15 [logger.py:43] Received request cmpl-e945861309b04b2ea1e94fc8bc79c881-0: prompt: 'How to prevent prompt injection in LLMs?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 304, 5557, 9508, 20859, 297, 365, 26369, 29879, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:56660 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:19:15 [engine.py:331] Added request cmpl-e945861309b04b2ea1e94fc8bc79c881-0.
INFO 08-09 14:19:19 [metrics.py:417] Avg prompt throughput: 2.2 tokens/s, Avg generation throughput: 120.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-09 14:19:23 [logger.py:43] Received request cmpl-f7b6be2052d745d69a3a667c8766e503-0: prompt: 'What are attention scores and how are they computed?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 526, 8570, 19435, 322, 920, 526, 896, 15712, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:36850 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:19:23 [engine.py:331] Added request cmpl-f7b6be2052d745d69a3a667c8766e503-0.
INFO 08-09 14:19:24 [metrics.py:417] Avg prompt throughput: 2.2 tokens/s, Avg generation throughput: 128.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:19:29 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 116.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-09 14:19:31 [logger.py:43] Received request cmpl-782b7e6a6955429ca9bdd70cc7051aa1-0: prompt: 'What are residual connections in transformers?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 526, 10995, 950, 12368, 297, 4327, 414, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:46240 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:19:31 [engine.py:331] Added request cmpl-782b7e6a6955429ca9bdd70cc7051aa1-0.
INFO 08-09 14:19:34 [metrics.py:417] Avg prompt throughput: 2.0 tokens/s, Avg generation throughput: 115.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:19:39 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 106.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 08-09 14:19:40 [logger.py:43] Received request cmpl-66ba711c46f94d6ea869b0230d3ad1a8-0: prompt: 'How to handle long sequences in generation?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 304, 4386, 1472, 15602, 297, 12623, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:55040 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:19:40 [engine.py:331] Added request cmpl-66ba711c46f94d6ea869b0230d3ad1a8-0.
INFO 08-09 14:19:44 [metrics.py:417] Avg prompt throughput: 1.8 tokens/s, Avg generation throughput: 127.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-09 14:19:49 [logger.py:43] Received request cmpl-3ff5dbfadd5249e8bc5e0908fb97c8ae-0: prompt: 'Explain the use of flash attention v2.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 12027, 7420, 278, 671, 310, 11013, 8570, 325, 29906, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:39630 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:19:49 [engine.py:331] Added request cmpl-3ff5dbfadd5249e8bc5e0908fb97c8ae-0.
INFO 08-09 14:19:49 [metrics.py:417] Avg prompt throughput: 2.2 tokens/s, Avg generation throughput: 116.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:19:54 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 129.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-09 14:19:57 [logger.py:43] Received request cmpl-e71b98d6abae4f269f8f5b956f98e07b-0: prompt: 'What is a greedy search in inference?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 263, 1395, 7584, 2740, 297, 27262, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:46346 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:19:57 [engine.py:331] Added request cmpl-e71b98d6abae4f269f8f5b956f98e07b-0.
INFO 08-09 14:19:59 [metrics.py:417] Avg prompt throughput: 2.0 tokens/s, Avg generation throughput: 129.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:20:04 [logger.py:43] Received request cmpl-a012d0fa3831451e948d076d75be9ec9-0: prompt: 'How do checkpoints improve fault tolerance?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 437, 1423, 9748, 11157, 12570, 20341, 749, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:46362 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:20:04 [engine.py:331] Added request cmpl-a012d0fa3831451e948d076d75be9ec9-0.
INFO 08-09 14:20:04 [metrics.py:417] Avg prompt throughput: 2.0 tokens/s, Avg generation throughput: 131.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:20:09 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 118.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-09 14:20:13 [logger.py:43] Received request cmpl-240340a040974f8fbcc797379b5ac0d9-0: prompt: 'Describe how text-to-text generation works.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 20355, 915, 920, 1426, 29899, 517, 29899, 726, 12623, 1736, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:51114 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:20:13 [engine.py:331] Added request cmpl-240340a040974f8fbcc797379b5ac0d9-0.
INFO 08-09 14:20:13 [logger.py:43] Received request cmpl-68e30a99f0f842df953301e4f1b47bd7-0: prompt: 'Explain cosine similarity in embeddings.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 12027, 7420, 6776, 457, 29501, 297, 8297, 29881, 886, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:51130 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:20:13 [engine.py:331] Added request cmpl-68e30a99f0f842df953301e4f1b47bd7-0.
INFO 08-09 14:20:14 [metrics.py:417] Avg prompt throughput: 4.6 tokens/s, Avg generation throughput: 123.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:20:17 [logger.py:43] Received request cmpl-081c416f7d93463fb6315010c7f9f01f-0: prompt: 'What is token-wise parallelism?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 5993, 29899, 3538, 8943, 1608, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:47258 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:20:17 [engine.py:331] Added request cmpl-081c416f7d93463fb6315010c7f9f01f-0.
INFO 08-09 14:20:19 [metrics.py:417] Avg prompt throughput: 1.8 tokens/s, Avg generation throughput: 131.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:20:24 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 122.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 08-09 14:20:25 [logger.py:43] Received request cmpl-9bf2f806febc4c109fd7d54f5169c33d-0: prompt: 'How do shared embeddings help in decoder-only models?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 437, 7258, 8297, 29881, 886, 1371, 297, 1602, 6119, 29899, 6194, 4733, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:47270 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:20:25 [engine.py:331] Added request cmpl-9bf2f806febc4c109fd7d54f5169c33d-0.
INFO 08-09 14:20:29 [metrics.py:417] Avg prompt throughput: 3.0 tokens/s, Avg generation throughput: 113.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-09 14:20:34 [logger.py:43] Received request cmpl-2877ba4475154b27ad4904156c9ee85a-0: prompt: 'Explain how to measure latency in model output.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 12027, 7420, 920, 304, 5645, 23316, 1270, 297, 1904, 1962, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:39976 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:20:34 [engine.py:331] Added request cmpl-2877ba4475154b27ad4904156c9ee85a-0.
INFO 08-09 14:20:34 [metrics.py:417] Avg prompt throughput: 2.4 tokens/s, Avg generation throughput: 99.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:20:39 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 115.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-09 14:20:43 [logger.py:43] Received request cmpl-4f8be1b2616d4edcaf77cc0b6d28f928-0: prompt: 'What is the purpose of dropout during training?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 278, 6437, 310, 5768, 449, 2645, 6694, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:48716 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:20:43 [engine.py:331] Added request cmpl-4f8be1b2616d4edcaf77cc0b6d28f928-0.
INFO 08-09 14:20:44 [metrics.py:417] Avg prompt throughput: 2.2 tokens/s, Avg generation throughput: 98.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:20:49 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 122.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-09 14:20:52 [logger.py:43] Received request cmpl-cbe33f4fcf594d8f8b2d468f3fd35c23-0: prompt: 'What is the role of GELU activation in transformers?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 278, 6297, 310, 402, 6670, 29965, 26229, 297, 4327, 414, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:56440 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:20:52 [engine.py:331] Added request cmpl-cbe33f4fcf594d8f8b2d468f3fd35c23-0.
INFO 08-09 14:20:54 [metrics.py:417] Avg prompt throughput: 2.8 tokens/s, Avg generation throughput: 114.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:20:59 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 114.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 08-09 14:21:01 [logger.py:43] Received request cmpl-99a81903505641e9906a1cd19c790108-0: prompt: 'Explain forward and backward pass in transformers.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 12027, 7420, 6375, 322, 1250, 1328, 1209, 297, 4327, 414, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:53850 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:21:01 [engine.py:331] Added request cmpl-99a81903505641e9906a1cd19c790108-0.
INFO 08-09 14:21:04 [metrics.py:417] Avg prompt throughput: 2.4 tokens/s, Avg generation throughput: 128.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-09 14:21:08 [logger.py:43] Received request cmpl-e5500e4a8b874768a883cc251a4f83cc-0: prompt: 'What is speculative decoding and how does it help?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 1580, 28524, 1602, 3689, 322, 920, 947, 372, 1371, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:36296 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:21:08 [engine.py:331] Added request cmpl-e5500e4a8b874768a883cc251a4f83cc-0.
INFO 08-09 14:21:09 [metrics.py:417] Avg prompt throughput: 2.6 tokens/s, Avg generation throughput: 132.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:21:14 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 132.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 08-09 14:21:16 [logger.py:43] Received request cmpl-5d9666784353457e8c8dcf412da153d1-0: prompt: 'How to optimize HPU memory usage during inference?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 304, 24656, 379, 7056, 3370, 8744, 2645, 27262, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:42744 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:21:16 [engine.py:331] Added request cmpl-5d9666784353457e8c8dcf412da153d1-0.
INFO 08-09 14:21:19 [metrics.py:417] Avg prompt throughput: 2.2 tokens/s, Avg generation throughput: 124.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-09 14:21:24 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 107.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 08-09 14:21:25 [logger.py:43] Received request cmpl-11f5f52848854dda8bdbf2fc13af4781-0: prompt: 'Explain profiling for LLM inference performance.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 12027, 7420, 20077, 292, 363, 365, 26369, 27262, 4180, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:42748 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:21:25 [engine.py:331] Added request cmpl-11f5f52848854dda8bdbf2fc13af4781-0.
INFO 08-09 14:21:29 [metrics.py:417] Avg prompt throughput: 2.2 tokens/s, Avg generation throughput: 120.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-09 14:21:33 [logger.py:43] Received request cmpl-3660aca217a34b51b3a4a199a57cdd41-0: prompt: 'How to compute inter-token latency per request?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 304, 10272, 1006, 29899, 6979, 23316, 1270, 639, 2009, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:47928 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:21:33 [engine.py:331] Added request cmpl-3660aca217a34b51b3a4a199a57cdd41-0.
INFO 08-09 14:21:34 [metrics.py:417] Avg prompt throughput: 2.4 tokens/s, Avg generation throughput: 116.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:21:39 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 124.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 08-09 14:21:41 [logger.py:43] Received request cmpl-b7f7ea2cb3c24087b53e275b44b36664-0: prompt: 'What is the difference between BFloat16 and Float16?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 278, 4328, 1546, 350, 11031, 29896, 29953, 322, 27842, 29896, 29953, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:54184 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:21:41 [engine.py:331] Added request cmpl-b7f7ea2cb3c24087b53e275b44b36664-0.
INFO 08-09 14:21:44 [metrics.py:417] Avg prompt throughput: 3.0 tokens/s, Avg generation throughput: 113.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:21:48 [logger.py:43] Received request cmpl-d5225f19f2df49d8bf25bbff885d3462-0: prompt: 'What are logits and how are they interpreted?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 526, 1480, 1169, 322, 920, 526, 896, 21551, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:50956 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:21:48 [engine.py:331] Added request cmpl-d5225f19f2df49d8bf25bbff885d3462-0.
INFO 08-09 14:21:49 [metrics.py:417] Avg prompt throughput: 2.2 tokens/s, Avg generation throughput: 127.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:21:54 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 130.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 08-09 14:21:56 [logger.py:43] Received request cmpl-4b5c7137d011465ab9c2f4ce3171c2f9-0: prompt: 'What is log probability in token prediction?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 1480, 6976, 297, 5993, 18988, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:55664 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:21:56 [engine.py:331] Added request cmpl-4b5c7137d011465ab9c2f4ce3171c2f9-0.
INFO 08-09 14:21:59 [metrics.py:417] Avg prompt throughput: 1.8 tokens/s, Avg generation throughput: 130.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-09 14:22:04 [logger.py:43] Received request cmpl-e2350a6cd0cc4784b5390b0e5072fbd3-0: prompt: 'How to use `TextStreamer` for streaming output?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 304, 671, 421, 1626, 3835, 261, 29952, 363, 24820, 1962, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:55670 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:22:04 [engine.py:331] Added request cmpl-e2350a6cd0cc4784b5390b0e5072fbd3-0.
INFO 08-09 14:22:04 [metrics.py:417] Avg prompt throughput: 2.6 tokens/s, Avg generation throughput: 122.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:22:07 [logger.py:43] Received request cmpl-5c4b655092724f30af0a96f12b6e4633-0: prompt: 'How to tokenize and detokenize a prompt manually?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 304, 5993, 675, 322, 1439, 4476, 675, 263, 9508, 7522, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:41184 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:22:07 [engine.py:331] Added request cmpl-5c4b655092724f30af0a96f12b6e4633-0.
INFO 08-09 14:22:09 [metrics.py:417] Avg prompt throughput: 2.6 tokens/s, Avg generation throughput: 120.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:22:11 [logger.py:43] Received request cmpl-e02e7b16d3b34c24a906508ac321b4b8-0: prompt: 'Describe how a prompt flows through the transformer layers.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 20355, 915, 920, 263, 9508, 24536, 1549, 278, 4327, 261, 15359, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:41188 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:22:11 [engine.py:331] Added request cmpl-e02e7b16d3b34c24a906508ac321b4b8-0.
INFO 08-09 14:22:14 [metrics.py:417] Avg prompt throughput: 2.6 tokens/s, Avg generation throughput: 102.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-09 14:22:19 [logger.py:43] Received request cmpl-c8cdbff41fc64a639c4a9f458f8c2d6b-0: prompt: 'How to handle EOS in streaming responses?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 304, 4386, 382, 3267, 297, 24820, 20890, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:53654 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:22:19 [engine.py:331] Added request cmpl-c8cdbff41fc64a639c4a9f458f8c2d6b-0.
INFO 08-09 14:22:19 [metrics.py:417] Avg prompt throughput: 2.0 tokens/s, Avg generation throughput: 128.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:22:24 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 125.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-09 14:22:28 [logger.py:43] Received request cmpl-20904ab7effb461fac7ccd9d89bc176a-0: prompt: 'What is a model config JSON in HF models?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 263, 1904, 2295, 4663, 297, 379, 29943, 4733, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:37860 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:22:28 [engine.py:331] Added request cmpl-20904ab7effb461fac7ccd9d89bc176a-0.
INFO 08-09 14:22:29 [metrics.py:417] Avg prompt throughput: 2.4 tokens/s, Avg generation throughput: 109.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:22:34 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 129.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 08-09 14:22:35 [logger.py:43] Received request cmpl-dacab8c0514c40a382c56ec98684752d-0: prompt: 'What is kv_cache reuse in decoder models?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 10908, 29918, 8173, 24270, 297, 1602, 6119, 4733, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:37868 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:22:35 [engine.py:331] Added request cmpl-dacab8c0514c40a382c56ec98684752d-0.
INFO 08-09 14:22:39 [metrics.py:417] Avg prompt throughput: 2.4 tokens/s, Avg generation throughput: 128.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-09 14:22:44 [logger.py:43] Received request cmpl-aea92e63851546b286c3b62cc947a851-0: prompt: 'Describe a scenario of real-time chatbot streaming.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 20355, 915, 263, 10483, 310, 1855, 29899, 2230, 13563, 7451, 24820, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:49062 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:22:44 [engine.py:331] Added request cmpl-aea92e63851546b286c3b62cc947a851-0.
INFO 08-09 14:22:44 [metrics.py:417] Avg prompt throughput: 2.6 tokens/s, Avg generation throughput: 112.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:22:49 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 109.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-09 14:22:53 [logger.py:43] Received request cmpl-39198be59c5342d8b4e1a6dbfcd72b78-0: prompt: 'How to compute throughput with variable prompts?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 304, 10272, 1549, 649, 411, 2286, 9508, 29879, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:36342 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:22:53 [engine.py:331] Added request cmpl-39198be59c5342d8b4e1a6dbfcd72b78-0.
INFO 08-09 14:22:54 [metrics.py:417] Avg prompt throughput: 2.2 tokens/s, Avg generation throughput: 117.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:22:59 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 124.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 08-09 14:23:01 [logger.py:43] Received request cmpl-369367f888f8484a8b6c89b7a8423a06-0: prompt: 'How does `max_new_tokens` differ from `max_length`?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 947, 421, 3317, 29918, 1482, 29918, 517, 12360, 29952, 1163, 515, 421, 3317, 29918, 2848, 6522], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:45148 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:23:01 [engine.py:331] Added request cmpl-369367f888f8484a8b6c89b7a8423a06-0.
INFO 08-09 14:23:04 [metrics.py:417] Avg prompt throughput: 3.6 tokens/s, Avg generation throughput: 121.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-09 14:23:09 [logger.py:43] Received request cmpl-21cb41aca5e24dd69076c064006f38a6-0: prompt: 'Explain the difference between loss and perplexity.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 12027, 7420, 278, 4328, 1546, 6410, 322, 639, 10709, 537, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:35854 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:23:09 [engine.py:331] Added request cmpl-21cb41aca5e24dd69076c064006f38a6-0.
INFO 08-09 14:23:09 [metrics.py:417] Avg prompt throughput: 2.4 tokens/s, Avg generation throughput: 117.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:23:14 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 119.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-09 14:23:17 [logger.py:43] Received request cmpl-93533cbc40ba4a4598ea0a0dbdd4be6e-0: prompt: 'How to log benchmark results in a structured CSV?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 304, 1480, 23513, 2582, 297, 263, 2281, 2955, 16874, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:58550 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:23:17 [engine.py:331] Added request cmpl-93533cbc40ba4a4598ea0a0dbdd4be6e-0.
INFO 08-09 14:23:19 [metrics.py:417] Avg prompt throughput: 2.4 tokens/s, Avg generation throughput: 124.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:23:24 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 113.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 08-09 14:23:26 [logger.py:43] Received request cmpl-42bd9ac1c07d4739b8c080e6fa73cb37-0: prompt: 'What are attention heads and how do they specialize?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 526, 8570, 15883, 322, 920, 437, 896, 4266, 675, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:41646 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:23:26 [engine.py:331] Added request cmpl-42bd9ac1c07d4739b8c080e6fa73cb37-0.
INFO 08-09 14:23:29 [metrics.py:417] Avg prompt throughput: 2.4 tokens/s, Avg generation throughput: 120.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-09 14:23:34 [logger.py:43] Received request cmpl-ff9db8a955654d76bb3eae770f54e8e5-0: prompt: 'How to ensure reproducible benchmarking?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 304, 9801, 9483, 15520, 23513, 292, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:41650 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:23:34 [engine.py:331] Added request cmpl-ff9db8a955654d76bb3eae770f54e8e5-0.
INFO 08-09 14:23:34 [metrics.py:417] Avg prompt throughput: 1.8 tokens/s, Avg generation throughput: 125.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:23:39 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 104.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-09 14:23:43 [logger.py:43] Received request cmpl-c8afec3e9bfb409cb9096101b94c3caf-0: prompt: 'What are model shards and when are they used?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 526, 1904, 528, 3163, 322, 746, 526, 896, 1304, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:54514 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:23:43 [engine.py:331] Added request cmpl-c8afec3e9bfb409cb9096101b94c3caf-0.
INFO 08-09 14:23:44 [metrics.py:417] Avg prompt throughput: 2.4 tokens/s, Avg generation throughput: 123.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:23:49 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 129.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 08-09 14:23:51 [logger.py:43] Received request cmpl-c260a07f8376461ebe41ab8c166b2dc0-0: prompt: 'Explain the relationship between prompt length and latency.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 12027, 7420, 278, 9443, 1546, 9508, 3309, 322, 23316, 1270, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:50678 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:23:51 [engine.py:331] Added request cmpl-c260a07f8376461ebe41ab8c166b2dc0-0.
INFO 08-09 14:23:54 [metrics.py:417] Avg prompt throughput: 2.4 tokens/s, Avg generation throughput: 120.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-09 14:23:59 [logger.py:43] Received request cmpl-4006e878e1114f6fb8e168629c0c7e78-0: prompt: 'What is batch inference and why is it faster?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 9853, 27262, 322, 2020, 338, 372, 8473, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:60040 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:23:59 [engine.py:331] Added request cmpl-4006e878e1114f6fb8e168629c0c7e78-0.
INFO 08-09 14:23:59 [metrics.py:417] Avg prompt throughput: 2.2 tokens/s, Avg generation throughput: 132.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:24:02 [logger.py:43] Received request cmpl-bf545f6c9a6443d19170eafef8e1b235-0: prompt: 'What are rotary sin-cos embeddings?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 526, 5731, 653, 4457, 29899, 3944, 8297, 29881, 886, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:60052 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:24:02 [engine.py:331] Added request cmpl-bf545f6c9a6443d19170eafef8e1b235-0.
INFO 08-09 14:24:04 [metrics.py:417] Avg prompt throughput: 2.4 tokens/s, Avg generation throughput: 119.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:24:09 [logger.py:43] Received request cmpl-25a16fc7623e41f482f9f87b2d0d5821-0: prompt: "Describe HuggingFace's `AutoTokenizer` features.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 20355, 915, 379, 688, 3460, 23360, 29915, 29879, 421, 12300, 6066, 3950, 29952, 5680, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:55640 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:24:09 [engine.py:331] Added request cmpl-25a16fc7623e41f482f9f87b2d0d5821-0.
INFO 08-09 14:24:09 [metrics.py:417] Avg prompt throughput: 3.2 tokens/s, Avg generation throughput: 132.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:24:14 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 113.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-09 14:24:18 [logger.py:43] Received request cmpl-c5476ce1bbde4528bdd818b8e5077d90-0: prompt: 'How does positional encoding interact with attention?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 947, 2602, 284, 8025, 16254, 411, 8570, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:51126 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:24:18 [engine.py:331] Added request cmpl-c5476ce1bbde4528bdd818b8e5077d90-0.
INFO 08-09 14:24:19 [metrics.py:417] Avg prompt throughput: 2.0 tokens/s, Avg generation throughput: 121.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:24:24 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 110.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-09 14:24:27 [logger.py:43] Received request cmpl-15c6d8b4d4e548e4abfe1ee929cb5240-0: prompt: 'What is a streaming callback in Transformers?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 263, 24820, 6939, 297, 4103, 689, 414, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:38470 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:24:27 [engine.py:331] Added request cmpl-15c6d8b4d4e548e4abfe1ee929cb5240-0.
INFO 08-09 14:24:29 [metrics.py:417] Avg prompt throughput: 2.2 tokens/s, Avg generation throughput: 118.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:24:35 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 107.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 08-09 14:24:36 [logger.py:43] Received request cmpl-0d67613740a04f0f9ce1bcadd7240b52-0: prompt: 'How to benchmark models using curl and Python?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 304, 23513, 4733, 773, 11051, 322, 5132, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:43862 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:24:36 [engine.py:331] Added request cmpl-0d67613740a04f0f9ce1bcadd7240b52-0.
INFO 08-09 14:24:40 [metrics.py:417] Avg prompt throughput: 2.0 tokens/s, Avg generation throughput: 134.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-09 14:24:43 [logger.py:43] Received request cmpl-4a19050cf2514c50b2ba29bc284a5405-0: prompt: 'How to parse streaming responses from vLLM?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 304, 6088, 24820, 20890, 515, 325, 2208, 29924, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:43872 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:24:43 [engine.py:331] Added request cmpl-4a19050cf2514c50b2ba29bc284a5405-0.
INFO 08-09 14:24:45 [metrics.py:417] Avg prompt throughput: 2.2 tokens/s, Avg generation throughput: 133.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:24:50 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 119.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 08-09 14:24:51 [logger.py:43] Received request cmpl-581dcee916f743b8b839d9d984fbeb20-0: prompt: 'How does deep caching work in inference?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 947, 6483, 22488, 664, 297, 27262, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:56938 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:24:51 [engine.py:331] Added request cmpl-581dcee916f743b8b839d9d984fbeb20-0.
INFO 08-09 14:24:55 [metrics.py:417] Avg prompt throughput: 1.8 tokens/s, Avg generation throughput: 110.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:24:55 [logger.py:43] Received request cmpl-567c27a4edce4114ae8806be226b0f58-0: prompt: 'What is a prompt-truncation error?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 263, 9508, 29899, 509, 4661, 362, 1059, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:56944 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:24:55 [engine.py:331] Added request cmpl-567c27a4edce4114ae8806be226b0f58-0.
INFO 08-09 14:25:00 [metrics.py:417] Avg prompt throughput: 2.2 tokens/s, Avg generation throughput: 133.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-09 14:25:03 [logger.py:43] Received request cmpl-11d5897211c9443baeef07159d7ec7c3-0: prompt: 'Explain the role of tokenizer config files.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 12027, 7420, 278, 6297, 310, 5993, 3950, 2295, 2066, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:38816 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:25:03 [engine.py:331] Added request cmpl-11d5897211c9443baeef07159d7ec7c3-0.
INFO 08-09 14:25:05 [metrics.py:417] Avg prompt throughput: 2.2 tokens/s, Avg generation throughput: 114.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:25:10 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 120.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-09 14:25:12 [logger.py:43] Received request cmpl-776382fd2f51435dbeec1d2f21b43247-0: prompt: 'How to batch multiple prompts into one request?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 304, 9853, 2999, 9508, 29879, 964, 697, 2009, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:55020 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:25:12 [engine.py:331] Added request cmpl-776382fd2f51435dbeec1d2f21b43247-0.
INFO 08-09 14:25:15 [metrics.py:417] Avg prompt throughput: 2.2 tokens/s, Avg generation throughput: 118.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:25:20 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 120.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 08-09 14:25:20 [logger.py:43] Received request cmpl-c4b06ea3abbe4d48a1ee3301cbe2f24f-0: prompt: 'What is a chat template in HuggingFace?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 263, 13563, 4472, 297, 379, 688, 3460, 23360, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:53234 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:25:20 [engine.py:331] Added request cmpl-c4b06ea3abbe4d48a1ee3301cbe2f24f-0.
INFO 08-09 14:25:25 [metrics.py:417] Avg prompt throughput: 2.4 tokens/s, Avg generation throughput: 115.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-09 14:25:29 [logger.py:43] Received request cmpl-ea15b34c483944469958fefea95fafbe-0: prompt: 'What is inference latency and why is it important?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 27262, 23316, 1270, 322, 2020, 338, 372, 4100, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:42852 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:25:29 [engine.py:331] Added request cmpl-ea15b34c483944469958fefea95fafbe-0.
INFO 08-09 14:25:30 [metrics.py:417] Avg prompt throughput: 2.4 tokens/s, Avg generation throughput: 110.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:25:35 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 131.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-09 14:25:37 [logger.py:43] Received request cmpl-8e9abe5abc4a4d078ddd3360b58920bd-0: prompt: 'How to reduce memory footprint of LLaMA models?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 304, 10032, 3370, 3661, 2158, 310, 365, 5661, 1529, 4733, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:58888 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:25:37 [engine.py:331] Added request cmpl-8e9abe5abc4a4d078ddd3360b58920bd-0.
INFO 08-09 14:25:40 [metrics.py:417] Avg prompt throughput: 2.6 tokens/s, Avg generation throughput: 135.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-09 14:25:44 [logger.py:43] Received request cmpl-c277db91252e46f88d0623527b5c631b-0: prompt: 'What is the difference between `generation_config.json` and `config.json`?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 278, 4328, 1546, 421, 4738, 362, 29918, 2917, 29889, 3126, 29952, 322, 421, 2917, 29889, 3126, 6522], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:58892 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:25:44 [engine.py:331] Added request cmpl-c277db91252e46f88d0623527b5c631b-0.
INFO 08-09 14:25:45 [metrics.py:417] Avg prompt throughput: 4.0 tokens/s, Avg generation throughput: 131.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:25:50 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 107.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-09 14:25:54 [logger.py:43] Received request cmpl-3a5951c078724ee1bc5bb8a42102448a-0: prompt: 'What is a quantized model and how is it served?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 263, 4323, 1891, 1904, 322, 920, 338, 372, 6766, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:39442 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:25:54 [engine.py:331] Added request cmpl-3a5951c078724ee1bc5bb8a42102448a-0.
INFO 08-09 14:25:55 [metrics.py:417] Avg prompt throughput: 2.6 tokens/s, Avg generation throughput: 111.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:26:00 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 126.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 08-09 14:26:01 [logger.py:43] Received request cmpl-4122f5dad36a4d509ee0e946af272e03-0: prompt: 'How to handle prompt overflow in generation?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 304, 4386, 9508, 11969, 297, 12623, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:54552 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:26:01 [engine.py:331] Added request cmpl-4122f5dad36a4d509ee0e946af272e03-0.
INFO 08-09 14:26:05 [metrics.py:417] Avg prompt throughput: 1.8 tokens/s, Avg generation throughput: 127.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-09 14:26:09 [logger.py:43] Received request cmpl-d579e14cd8ce49098eff21971e361492-0: prompt: 'What is warmup time in inference pipelines?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 14294, 786, 931, 297, 27262, 8450, 24210, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:51702 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:26:09 [engine.py:331] Added request cmpl-d579e14cd8ce49098eff21971e361492-0.
INFO 08-09 14:26:10 [metrics.py:417] Avg prompt throughput: 2.2 tokens/s, Avg generation throughput: 123.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:26:15 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 110.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-09 14:26:18 [logger.py:43] Received request cmpl-26365746a48544c989fa1c352245bde5-0: prompt: 'How to stream JSON tokens using curl?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 304, 4840, 4663, 18897, 773, 11051, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:42002 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:26:18 [engine.py:331] Added request cmpl-26365746a48544c989fa1c352245bde5-0.
INFO 08-09 14:26:20 [metrics.py:417] Avg prompt throughput: 1.8 tokens/s, Avg generation throughput: 126.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:26:25 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 123.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 08-09 14:26:26 [logger.py:43] Received request cmpl-ec06be62413e47fd88a5ab2b34268b3a-0: prompt: 'What is a `generate_with_streaming()` wrapper?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 263, 421, 17158, 29918, 2541, 29918, 5461, 292, 2555, 14476, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:33356 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:26:26 [engine.py:331] Added request cmpl-ec06be62413e47fd88a5ab2b34268b3a-0.
INFO 08-09 14:26:28 [logger.py:43] Received request cmpl-901a71fc06f54f82a63024ec4940bd1b-0: prompt: 'How to evaluate TTFT and ITL for multiple prompts?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 304, 14707, 323, 29911, 7818, 322, 13315, 29931, 363, 2999, 9508, 29879, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:33366 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:26:28 [engine.py:331] Added request cmpl-901a71fc06f54f82a63024ec4940bd1b-0.
INFO 08-09 14:26:30 [metrics.py:417] Avg prompt throughput: 5.8 tokens/s, Avg generation throughput: 124.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:26:35 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 128.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 08-09 14:26:36 [logger.py:43] Received request cmpl-39ccc47ff41b42d7a3495265a1a1c40c-0: prompt: 'What are server cold starts in model serving?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 526, 1923, 11220, 8665, 297, 1904, 16330, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:41700 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:26:36 [engine.py:331] Added request cmpl-39ccc47ff41b42d7a3495265a1a1c40c-0.
INFO 08-09 14:26:40 [metrics.py:417] Avg prompt throughput: 2.0 tokens/s, Avg generation throughput: 116.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-09 14:26:45 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 98.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 08-09 14:26:45 [logger.py:43] Received request cmpl-527b5452a52e43498a6c7083b7f480a1-0: prompt: 'Explain how to avoid padding inefficiencies.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 12027, 7420, 920, 304, 4772, 7164, 297, 29872, 2416, 819, 2478, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:41702 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:26:45 [engine.py:331] Added request cmpl-527b5452a52e43498a6c7083b7f480a1-0.
INFO 08-09 14:26:50 [metrics.py:417] Avg prompt throughput: 2.6 tokens/s, Avg generation throughput: 114.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-09 14:26:55 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 93.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 08-09 14:26:55 [logger.py:43] Received request cmpl-c4037ed18c2346edb46238d1416673fc-0: prompt: 'How does tokenizer pre-processing affect TTFT?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 947, 5993, 3950, 758, 29899, 19170, 6602, 323, 29911, 7818, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:37338 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:26:55 [engine.py:331] Added request cmpl-c4037ed18c2346edb46238d1416673fc-0.
INFO 08-09 14:27:00 [metrics.py:417] Avg prompt throughput: 2.6 tokens/s, Avg generation throughput: 125.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-09 14:27:03 [logger.py:43] Received request cmpl-d7cef1e07a8f42e3ab7f4d74a50aa182-0: prompt: 'How to track per-prompt latency in logs?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 304, 5702, 639, 29899, 14032, 415, 23316, 1270, 297, 10748, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:36744 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:27:03 [engine.py:331] Added request cmpl-d7cef1e07a8f42e3ab7f4d74a50aa182-0.
INFO 08-09 14:27:05 [metrics.py:417] Avg prompt throughput: 2.6 tokens/s, Avg generation throughput: 123.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:27:10 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 124.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 08-09 14:27:12 [logger.py:43] Received request cmpl-a29d82c429584a6a80a8ce570862b9bd-0: prompt: 'Explain streaming benchmarks with high concurrency.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 12027, 7420, 24820, 23513, 29879, 411, 1880, 3022, 10880, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:52214 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:27:12 [engine.py:331] Added request cmpl-a29d82c429584a6a80a8ce570862b9bd-0.
INFO 08-09 14:27:15 [metrics.py:417] Avg prompt throughput: 2.2 tokens/s, Avg generation throughput: 111.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:27:20 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 125.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 08-09 14:27:20 [logger.py:43] Received request cmpl-f553ce5138b34de4b7bf15e77cb60c4a-0: prompt: 'How to calculate power usage per prompt?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 304, 8147, 3081, 8744, 639, 9508, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:54226 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:27:20 [engine.py:331] Added request cmpl-f553ce5138b34de4b7bf15e77cb60c4a-0.
INFO 08-09 14:27:25 [metrics.py:417] Avg prompt throughput: 1.8 tokens/s, Avg generation throughput: 115.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-09 14:27:29 [logger.py:43] Received request cmpl-05ef757499cd46a4b646efcbf476f50f-0: prompt: 'How does the tokenizer handle emojis and symbols?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 947, 278, 5993, 3950, 4386, 953, 3848, 275, 322, 15072, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:56458 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:27:29 [engine.py:331] Added request cmpl-05ef757499cd46a4b646efcbf476f50f-0.
INFO 08-09 14:27:30 [metrics.py:417] Avg prompt throughput: 2.6 tokens/s, Avg generation throughput: 108.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:27:35 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 104.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-09 14:27:39 [logger.py:43] Received request cmpl-e4896cba68094614beb3072739f909d9-0: prompt: 'How does model size affect inference latency?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 947, 1904, 2159, 6602, 27262, 23316, 1270, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:44998 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:27:39 [engine.py:331] Added request cmpl-e4896cba68094614beb3072739f909d9-0.
INFO 08-09 14:27:40 [metrics.py:417] Avg prompt throughput: 2.0 tokens/s, Avg generation throughput: 94.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:27:45 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 105.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-09 14:27:48 [logger.py:43] Received request cmpl-e4c4521ac978430d950361b1dbb572df-0: prompt: 'How to debug slow streaming responses?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 304, 4744, 5232, 24820, 20890, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:40582 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:27:48 [engine.py:331] Added request cmpl-e4c4521ac978430d950361b1dbb572df-0.
INFO 08-09 14:27:50 [metrics.py:417] Avg prompt throughput: 1.6 tokens/s, Avg generation throughput: 125.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:27:55 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 130.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 08-09 14:27:55 [logger.py:43] Received request cmpl-e891d27f97464181948c2dc125f84373-0: prompt: 'What causes generation to stall midway?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 9946, 12623, 304, 380, 497, 7145, 1582, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:40598 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:27:55 [engine.py:331] Added request cmpl-e891d27f97464181948c2dc125f84373-0.
INFO 08-09 14:28:00 [metrics.py:417] Avg prompt throughput: 2.0 tokens/s, Avg generation throughput: 129.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-09 14:28:03 [logger.py:43] Received request cmpl-7d7b7c75605b44d5a2541d5e0c8c3047-0: prompt: 'How to load a model on HPU using HF Transformers?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 304, 2254, 263, 1904, 373, 379, 7056, 773, 379, 29943, 4103, 689, 414, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:38124 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:28:03 [engine.py:331] Added request cmpl-7d7b7c75605b44d5a2541d5e0c8c3047-0.
INFO 08-09 14:28:05 [metrics.py:417] Avg prompt throughput: 3.2 tokens/s, Avg generation throughput: 125.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:28:10 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 113.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 08-09 14:28:12 [logger.py:43] Received request cmpl-a797c93ac0e349679a301eadd2ab83fd-0: prompt: 'How to pass prompt + history into a chat model?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 304, 1209, 9508, 718, 4955, 964, 263, 13563, 1904, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:54116 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:28:12 [engine.py:331] Added request cmpl-a797c93ac0e349679a301eadd2ab83fd-0.
INFO 08-09 14:28:15 [metrics.py:417] Avg prompt throughput: 2.4 tokens/s, Avg generation throughput: 112.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:28:20 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 93.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 08-09 14:28:22 [logger.py:43] Received request cmpl-0eaeb9fde56e4a43a16e376e5c8a2d9a-0: prompt: 'What is an OpenAI-compatible inference API?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 385, 4673, 23869, 29899, 23712, 27262, 3450, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:60140 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:28:22 [engine.py:331] Added request cmpl-0eaeb9fde56e4a43a16e376e5c8a2d9a-0.
INFO 08-09 14:28:25 [metrics.py:417] Avg prompt throughput: 2.2 tokens/s, Avg generation throughput: 100.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:28:30 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 108.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 08-09 14:28:31 [logger.py:43] Received request cmpl-366421e16d074b7399e16cf20cbf05bc-0: prompt: 'How to handle decoding errors in stream?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 304, 4386, 1602, 3689, 4436, 297, 4840, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:44832 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:28:31 [engine.py:331] Added request cmpl-366421e16d074b7399e16cf20cbf05bc-0.
INFO 08-09 14:28:35 [metrics.py:417] Avg prompt throughput: 2.0 tokens/s, Avg generation throughput: 113.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-09 14:28:36 [logger.py:43] Received request cmpl-0773a722f77b42e3bfa3fc4d69da8886-0: prompt: 'What are temperature vs randomness in sampling?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 526, 10430, 7186, 4036, 2264, 297, 23460, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:49606 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:28:36 [engine.py:331] Added request cmpl-0773a722f77b42e3bfa3fc4d69da8886-0.
INFO 08-09 14:28:40 [metrics.py:417] Avg prompt throughput: 2.0 tokens/s, Avg generation throughput: 92.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:28:44 [logger.py:43] Received request cmpl-e5c086354b1442aca42a34d2078efba6-0: prompt: 'Explain token logprobs in output.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 12027, 7420, 5993, 1480, 771, 5824, 297, 1962, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:49614 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:28:44 [engine.py:331] Added request cmpl-e5c086354b1442aca42a34d2078efba6-0.
INFO 08-09 14:28:45 [metrics.py:417] Avg prompt throughput: 2.0 tokens/s, Avg generation throughput: 120.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:28:47 [logger.py:43] Received request cmpl-3b8e40f2b820477da0eb6f7b4edf102e-0: prompt: 'How to measure and reduce TTFT on Gaudi?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 304, 5645, 322, 10032, 323, 29911, 7818, 373, 10415, 4749, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:53010 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:28:47 [engine.py:331] Added request cmpl-3b8e40f2b820477da0eb6f7b4edf102e-0.
INFO 08-09 14:28:50 [metrics.py:417] Avg prompt throughput: 2.6 tokens/s, Avg generation throughput: 112.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:28:55 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 102.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 08-09 14:28:56 [logger.py:43] Received request cmpl-fd0a6a8efb6a40448493e33a97a958cb-0: prompt: 'Explain the internals of HF `generate()` loop.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 12027, 7420, 278, 2836, 1338, 310, 379, 29943, 421, 17158, 2555, 2425, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:54744 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:28:56 [engine.py:331] Added request cmpl-fd0a6a8efb6a40448493e33a97a958cb-0.
INFO 08-09 14:29:00 [metrics.py:417] Avg prompt throughput: 2.8 tokens/s, Avg generation throughput: 110.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:29:05 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 119.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 08-09 14:29:05 [logger.py:43] Received request cmpl-5e1db96fb5a5488ab8ea61b7278b4a2c-0: prompt: 'What is speculative token generation?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 1580, 28524, 5993, 12623, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:54758 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:29:05 [engine.py:331] Added request cmpl-5e1db96fb5a5488ab8ea61b7278b4a2c-0.
INFO 08-09 14:29:10 [metrics.py:417] Avg prompt throughput: 1.6 tokens/s, Avg generation throughput: 125.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-09 14:29:13 [logger.py:43] Received request cmpl-a4e5c0a25b9c470b882d17e35b6b4bcc-0: prompt: 'What are possible failure points in vLLM serving?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 526, 1950, 10672, 3291, 297, 325, 2208, 29924, 16330, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:44958 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:29:13 [engine.py:331] Added request cmpl-a4e5c0a25b9c470b882d17e35b6b4bcc-0.
INFO 08-09 14:29:15 [metrics.py:417] Avg prompt throughput: 2.4 tokens/s, Avg generation throughput: 118.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:29:20 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 114.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-09 14:29:23 [logger.py:43] Received request cmpl-1b0832876b0a44498e9c202d0cf94e4b-0: prompt: 'How to catch stream decoding errors gracefully?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 304, 4380, 4840, 1602, 3689, 4436, 17659, 3730, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:57538 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:29:23 [engine.py:331] Added request cmpl-1b0832876b0a44498e9c202d0cf94e4b-0.
INFO 08-09 14:29:25 [metrics.py:417] Avg prompt throughput: 2.2 tokens/s, Avg generation throughput: 98.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:29:30 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 118.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 08-09 14:29:32 [logger.py:43] Received request cmpl-921e92487fe6464d8823148cbfb85fd5-0: prompt: 'How to enable/disable kv caching in transformers?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 304, 9025, 29914, 20472, 10908, 22488, 297, 4327, 414, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:46796 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:29:32 [engine.py:331] Added request cmpl-921e92487fe6464d8823148cbfb85fd5-0.
INFO 08-09 14:29:34 [logger.py:43] Received request cmpl-be7f7c55b44f4c44af2f83a85b371ac3-0: prompt: 'What are challenges of multilingual streaming inference?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 526, 18066, 267, 310, 1773, 6504, 950, 24820, 27262, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:46798 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:29:34 [engine.py:331] Added request cmpl-be7f7c55b44f4c44af2f83a85b371ac3-0.
INFO 08-09 14:29:35 [metrics.py:417] Avg prompt throughput: 4.8 tokens/s, Avg generation throughput: 100.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:29:40 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 114.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-09 14:29:43 [logger.py:43] Received request cmpl-937146dad377484499e168a9e42d03ef-0: prompt: 'How to benchmark with different model precisions?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 304, 23513, 411, 1422, 1904, 12132, 1080, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:52778 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:29:43 [engine.py:331] Added request cmpl-937146dad377484499e168a9e42d03ef-0.
INFO 08-09 14:29:45 [metrics.py:417] Avg prompt throughput: 2.0 tokens/s, Avg generation throughput: 102.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:29:50 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 105.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-09 14:29:52 [logger.py:43] Received request cmpl-dec36630f3eb40bda69fa2e65cd94371-0: prompt: 'What is the tradeoff between token latency and batch size?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 278, 11302, 2696, 1546, 5993, 23316, 1270, 322, 9853, 2159, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:48632 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:29:52 [engine.py:331] Added request cmpl-dec36630f3eb40bda69fa2e65cd94371-0.
INFO 08-09 14:29:54 [logger.py:43] Received request cmpl-a560e2bb9d1f4b9bb441e7d6937fb67c-0: prompt: 'Describe the advantages and limitations of transformer-based neural networks.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 20355, 915, 278, 25486, 322, 27028, 310, 4327, 261, 29899, 6707, 19677, 14379, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:48644 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:29:54 [engine.py:331] Added request cmpl-a560e2bb9d1f4b9bb441e7d6937fb67c-0.
INFO 08-09 14:29:55 [metrics.py:417] Avg prompt throughput: 5.8 tokens/s, Avg generation throughput: 123.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:30:00 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 122.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-09 14:30:02 [logger.py:43] Received request cmpl-1cb593c8e2a74bc3b6bbed93d9b2db32-0: prompt: 'Explain the concept of overfitting in machine learning and how to prevent it.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 12027, 7420, 278, 6964, 310, 975, 29888, 5367, 297, 4933, 6509, 322, 920, 304, 5557, 372, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:54032 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:30:02 [engine.py:331] Added request cmpl-1cb593c8e2a74bc3b6bbed93d9b2db32-0.
INFO 08-09 14:30:05 [metrics.py:417] Avg prompt throughput: 3.6 tokens/s, Avg generation throughput: 126.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:30:10 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 124.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 08-09 14:30:10 [logger.py:43] Received request cmpl-788a6b3364e74732b297b00b8ba9435d-0: prompt: 'How does the attention mechanism work in transformer models?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 947, 278, 8570, 13336, 664, 297, 4327, 261, 4733, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:40414 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:30:10 [engine.py:331] Added request cmpl-788a6b3364e74732b297b00b8ba9435d-0.
INFO 08-09 14:30:15 [metrics.py:417] Avg prompt throughput: 2.4 tokens/s, Avg generation throughput: 127.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-09 14:30:18 [logger.py:43] Received request cmpl-0627779ee80543fead6425ca7ac2e4c8-0: prompt: 'Summarize the main differences between supervised and unsupervised learning.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 6991, 3034, 675, 278, 1667, 12651, 1546, 2428, 11292, 322, 443, 9136, 11292, 6509, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:55742 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:30:18 [engine.py:331] Added request cmpl-0627779ee80543fead6425ca7ac2e4c8-0.
INFO 08-09 14:30:20 [metrics.py:417] Avg prompt throughput: 3.2 tokens/s, Avg generation throughput: 120.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:30:25 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 107.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-09 14:30:27 [logger.py:43] Received request cmpl-343f85a7418340f688187fae7f0b8f1a-0: prompt: 'What are the ethical concerns associated with deploying large language models?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 526, 278, 11314, 936, 21838, 6942, 411, 7246, 292, 2919, 4086, 4733, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:35746 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:30:27 [engine.py:331] Added request cmpl-343f85a7418340f688187fae7f0b8f1a-0.
INFO 08-09 14:30:30 [metrics.py:417] Avg prompt throughput: 3.0 tokens/s, Avg generation throughput: 107.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:30:35 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 110.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 08-09 14:30:36 [logger.py:43] Received request cmpl-a40c2f7e398d4716beb7eab68eeb944c-0: prompt: 'Explain how gradient descent works in training deep neural networks.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 12027, 7420, 920, 16030, 26815, 1736, 297, 6694, 6483, 19677, 14379, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:55138 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:30:36 [engine.py:331] Added request cmpl-a40c2f7e398d4716beb7eab68eeb944c-0.
INFO 08-09 14:30:40 [metrics.py:417] Avg prompt throughput: 2.6 tokens/s, Avg generation throughput: 112.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-09 14:30:45 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 120.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 08-09 14:30:45 [logger.py:43] Received request cmpl-eafd4d1d76424715b190a37f0b928ef7-0: prompt: 'Describe the role of activation functions in deep learning.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 20355, 915, 278, 6297, 310, 26229, 3168, 297, 6483, 6509, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:55154 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:30:45 [engine.py:331] Added request cmpl-eafd4d1d76424715b190a37f0b928ef7-0.
INFO 08-09 14:30:50 [metrics.py:417] Avg prompt throughput: 2.4 tokens/s, Avg generation throughput: 126.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-09 14:30:53 [logger.py:43] Received request cmpl-18b0cb09701a4863aaebbab9981ae164-0: prompt: 'How can transfer learning improve model performance on small datasets?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 508, 6782, 6509, 11157, 1904, 4180, 373, 2319, 20035, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:52052 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:30:53 [engine.py:331] Added request cmpl-18b0cb09701a4863aaebbab9981ae164-0.
INFO 08-09 14:30:55 [metrics.py:417] Avg prompt throughput: 2.4 tokens/s, Avg generation throughput: 123.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:31:00 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 123.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 08-09 14:31:01 [logger.py:43] Received request cmpl-809fc201c03b4eb592f55e40e52201da-0: prompt: 'Explain the concept of reinforcement learning with an example.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 12027, 7420, 278, 6964, 310, 15561, 1454, 13561, 6509, 411, 385, 1342, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:36560 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:31:01 [engine.py:331] Added request cmpl-809fc201c03b4eb592f55e40e52201da-0.
INFO 08-09 14:31:05 [metrics.py:417] Avg prompt throughput: 2.8 tokens/s, Avg generation throughput: 130.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-09 14:31:09 [logger.py:43] Received request cmpl-0d70e408b08d420580360f343a5cb25e-0: prompt: 'What are the main components of a convolutional neural network (CNN)?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 526, 278, 1667, 7117, 310, 263, 26851, 284, 19677, 3564, 313, 29907, 10262, 6877], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:54352 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:31:09 [engine.py:331] Added request cmpl-0d70e408b08d420580360f343a5cb25e-0.
INFO 08-09 14:31:10 [metrics.py:417] Avg prompt throughput: 3.2 tokens/s, Avg generation throughput: 117.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:31:15 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 123.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-09 14:31:17 [logger.py:43] Received request cmpl-f58ba2aa7cf1431cad48f12697c51753-0: prompt: 'Describe how large language models like GPT or LLaMA are trained and fine-tuned.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 20355, 915, 920, 2919, 4086, 4733, 763, 402, 7982, 470, 365, 5661, 1529, 526, 16370, 322, 2691, 29899, 29873, 348, 287, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:42708 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:31:17 [engine.py:331] Added request cmpl-f58ba2aa7cf1431cad48f12697c51753-0.
INFO 08-09 14:31:20 [metrics.py:417] Avg prompt throughput: 4.6 tokens/s, Avg generation throughput: 120.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:31:25 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 103.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 08-09 14:31:26 [logger.py:43] Received request cmpl-b875b2038cdb4f4e87d72fee0348a973-0: prompt: 'Compare and contrast BERT and GPT in terms of architecture and use cases.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 3831, 598, 322, 12814, 350, 20161, 322, 402, 7982, 297, 4958, 310, 11258, 322, 671, 4251, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:46172 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:31:26 [engine.py:331] Added request cmpl-b875b2038cdb4f4e87d72fee0348a973-0.
INFO 08-09 14:31:30 [metrics.py:417] Avg prompt throughput: 3.6 tokens/s, Avg generation throughput: 124.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 08-09 14:31:35 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 108.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
INFO 08-09 14:31:45 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 1.2 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 14:31:55 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
