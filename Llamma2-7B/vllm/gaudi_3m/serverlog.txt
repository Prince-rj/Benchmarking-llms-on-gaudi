Calling add_step_closure function does not have any effect. It's lazy mode only functionality. (warning logged once)
Calling mark_step function does not have any effect. It's lazy mode only functionality. (warning logged once)
Calling iter_mark_step function does not have any effect. It's lazy mode only functionality. (warning logged once)
INFO 08-09 13:53:25 [__init__.py:254] Automatically detected platform hpu.
INFO 08-09 13:53:28 [config.py:1975] Disabled the custom all-reduce kernel because it is not supported on current platform.
INFO 08-09 13:53:28 [config.py:1975] Disabled the custom all-reduce kernel because it is not supported on current platform.
INFO 08-09 13:53:29 [config.py:1975] Disabled the custom all-reduce kernel because it is not supported on current platform.
INFO 08-09 13:53:29 [config.py:1975] Disabled the custom all-reduce kernel because it is not supported on current platform.
INFO 08-09 13:53:29 [api_server.py:1289] vLLM API server version 0.6.3.dev5453+ge9c83fca1
INFO 08-09 13:53:30 [config.py:1975] Disabled the custom all-reduce kernel because it is not supported on current platform.
INFO 08-09 13:53:30 [cli_args.py:309] non-default args: {'model': '/software/data/llama_inference/Llama-2-7b-hf/', 'tensor_parallel_size': 4}
INFO 08-09 13:53:39 [config.py:822] This model supports multiple tasks: {'score', 'classify', 'generate', 'reward', 'embed'}. Defaulting to 'generate'.
WARNING 08-09 13:53:39 [config.py:3236] Your device 'hpu' doesn't support torch.float16. Falling back to torch.bfloat16 for compatibility.
WARNING 08-09 13:53:39 [config.py:3287] Casting torch.float16 to torch.bfloat16.
INFO 08-09 13:53:39 [arg_utils.py:1706] hpu is experimental on VLLM_USE_V1=1. Falling back to V0 Engine.
INFO 08-09 13:53:39 [config.py:1941] Defaulting to use mp for distributed inference
INFO 08-09 13:53:39 [config.py:1975] Disabled the custom all-reduce kernel because it is not supported on current platform.
INFO 08-09 13:53:39 [api_server.py:267] Started engine process with PID 18002
Calling add_step_closure function does not have any effect. It's lazy mode only functionality. (warning logged once)
Calling mark_step function does not have any effect. It's lazy mode only functionality. (warning logged once)
Calling iter_mark_step function does not have any effect. It's lazy mode only functionality. (warning logged once)
WARNING 08-09 13:53:41 [env_override.py:17] NCCL_CUMEM_ENABLE is set to 0, skipping override. This may increase memory overhead with cudagraph+allreduce: https://github.com/NVIDIA/nccl/issues/1234
INFO 08-09 13:53:42 [__init__.py:254] Automatically detected platform hpu.
INFO 08-09 13:53:44 [llm_engine.py:239] Initializing a V0 LLM engine (v0.6.3.dev5453+ge9c83fca1) with config: model='/software/data/llama_inference/Llama-2-7b-hf/', speculative_config=None, tokenizer='/software/data/llama_inference/Llama-2-7b-hf/', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=4, pipeline_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=hpu, decoding_config=DecodingConfig(backend='xgrammar', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=None, served_model_name=/software/data/llama_inference/Llama-2-7b-hf/, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, pooler_config=None, compilation_config={"level":0,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":[],"splitting_ops":[],"use_inductor":true,"compile_sizes":[],"inductor_compile_config":{"enable_auto_functionalized_v2":false},"inductor_passes":{},"use_cudagraph":false,"cudagraph_num_of_warmups":0,"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"max_capture_size":256,"local_cache_dir":null}, use_cached_outputs=True, 
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
WARNING 08-09 13:53:44 [multiproc_worker_utils.py:325] Reducing Torch parallelism from 104 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
WARNING 08-09 13:53:45 [rocm.py:28] Failed to import from amdsmi with ModuleNotFoundError("No module named 'amdsmi'")
WARNING 08-09 13:53:45 [rocm.py:33] Failed to import from vllm._C with ModuleNotFoundError("No module named 'vllm._C'")
WARNING 08-09 13:53:45 [rocm.py:39] Failed to import from vllm._rocm_C with ModuleNotFoundError("No module named 'vllm._rocm_C'")
WARNING 08-09 13:53:45 [utils.py:2850] Methods add_prompt_adapter,cache_config,compilation_config,current_platform,list_prompt_adapters,load_config,pin_prompt_adapter,remove_prompt_adapter,scheduler_config not implemented in <vllm.worker.hpu_worker.HPUWorker object at 0x7fa8921f7e80>
INFO 08-09 13:53:45 [runtime.py:26] Environment:
INFO 08-09 13:53:45 [runtime.py:30]     hw: gaudi3
INFO 08-09 13:53:45 [runtime.py:30]     build: 1.22.0.425
INFO 08-09 13:53:45 [runtime.py:30]     engine_version: v0
INFO 08-09 13:53:45 [runtime.py:30]     bridge_mode: eager
INFO 08-09 13:53:45 [runtime.py:30]     model_type: llama
INFO 08-09 13:53:45 [runtime.py:26] Features:
INFO 08-09 13:53:45 [runtime.py:30]     fp32_alibi_biases: True
INFO 08-09 13:53:45 [runtime.py:30]     fp32_softmax: False
INFO 08-09 13:53:45 [runtime.py:30]     fused_block_softmax_adjustment: False
INFO 08-09 13:53:45 [runtime.py:30]     fused_block_softmax: False
INFO 08-09 13:53:45 [runtime.py:30]     prompt_attn_impl: fsdpa_impl
INFO 08-09 13:53:45 [runtime.py:30]     skip_warmup: False
INFO 08-09 13:53:45 [runtime.py:30]     merged_prefill: False
INFO 08-09 13:53:45 [runtime.py:30]     use_contiguous_pa: True
INFO 08-09 13:53:45 [runtime.py:30]     use_delayed_sampling: True
INFO 08-09 13:53:45 [runtime.py:30]     use_bucketing: True
INFO 08-09 13:53:45 [runtime.py:30]     bucketing_strategy: exponential_bucketing
INFO 08-09 13:53:45 [runtime.py:26] User flags:
INFO 08-09 13:53:45 [runtime.py:30]     VLLM_USE_V1: False
WARNING 08-09 13:53:45 [hpu.py:135] Pin memory is not supported on HPU.
INFO 08-09 13:53:45 [hpu.py:60] Using HPUAttention backend.
Calling add_step_closure function does not have any effect. It's lazy mode only functionality. (warning logged once)
Calling mark_step function does not have any effect. It's lazy mode only functionality. (warning logged once)
Calling iter_mark_step function does not have any effect. It's lazy mode only functionality. (warning logged once)
Calling add_step_closure function does not have any effect. It's lazy mode only functionality. (warning logged once)
Calling mark_step function does not have any effect. It's lazy mode only functionality. (warning logged once)
Calling iter_mark_step function does not have any effect. It's lazy mode only functionality. (warning logged once)
Calling add_step_closure function does not have any effect. It's lazy mode only functionality. (warning logged once)
Calling mark_step function does not have any effect. It's lazy mode only functionality. (warning logged once)
Calling iter_mark_step function does not have any effect. It's lazy mode only functionality. (warning logged once)
WARNING 08-09 13:53:47 [env_override.py:17] NCCL_CUMEM_ENABLE is set to 0, skipping override. This may increase memory overhead with cudagraph+allreduce: https://github.com/NVIDIA/nccl/issues/1234
WARNING 08-09 13:53:47 [env_override.py:17] NCCL_CUMEM_ENABLE is set to 0, skipping override. This may increase memory overhead with cudagraph+allreduce: https://github.com/NVIDIA/nccl/issues/1234
WARNING 08-09 13:53:47 [env_override.py:17] NCCL_CUMEM_ENABLE is set to 0, skipping override. This may increase memory overhead with cudagraph+allreduce: https://github.com/NVIDIA/nccl/issues/1234
INFO 08-09 13:53:48 [__init__.py:254] Automatically detected platform hpu.
INFO 08-09 13:53:48 [__init__.py:254] Automatically detected platform hpu.
INFO 08-09 13:53:48 [__init__.py:254] Automatically detected platform hpu.
[1;36m(VllmWorkerProcess pid=18140)[0;0m INFO 08-09 13:53:50 [multiproc_worker_utils.py:228] Worker ready; awaiting tasks
[1;36m(VllmWorkerProcess pid=18139)[0;0m INFO 08-09 13:53:50 [multiproc_worker_utils.py:228] Worker ready; awaiting tasks
[1;36m(VllmWorkerProcess pid=18138)[0;0m INFO 08-09 13:53:50 [multiproc_worker_utils.py:228] Worker ready; awaiting tasks
[1;36m(VllmWorkerProcess pid=18138)[0;0m WARNING 08-09 13:53:50 [rocm.py:28] Failed to import from amdsmi with ModuleNotFoundError("No module named 'amdsmi'")
[1;36m(VllmWorkerProcess pid=18138)[0;0m WARNING 08-09 13:53:50 [rocm.py:33] Failed to import from vllm._C with ModuleNotFoundError("No module named 'vllm._C'")
[1;36m(VllmWorkerProcess pid=18138)[0;0m WARNING 08-09 13:53:50 [rocm.py:39] Failed to import from vllm._rocm_C with ModuleNotFoundError("No module named 'vllm._rocm_C'")
[1;36m(VllmWorkerProcess pid=18140)[0;0m WARNING 08-09 13:53:50 [rocm.py:28] Failed to import from amdsmi with ModuleNotFoundError("No module named 'amdsmi'")
[1;36m(VllmWorkerProcess pid=18140)[0;0m WARNING 08-09 13:53:50 [rocm.py:33] Failed to import from vllm._C with ModuleNotFoundError("No module named 'vllm._C'")
[1;36m(VllmWorkerProcess pid=18140)[0;0m WARNING 08-09 13:53:50 [rocm.py:39] Failed to import from vllm._rocm_C with ModuleNotFoundError("No module named 'vllm._rocm_C'")
[1;36m(VllmWorkerProcess pid=18139)[0;0m WARNING 08-09 13:53:50 [rocm.py:28] Failed to import from amdsmi with ModuleNotFoundError("No module named 'amdsmi'")
[1;36m(VllmWorkerProcess pid=18139)[0;0m WARNING 08-09 13:53:50 [rocm.py:33] Failed to import from vllm._C with ModuleNotFoundError("No module named 'vllm._C'")
[1;36m(VllmWorkerProcess pid=18139)[0;0m WARNING 08-09 13:53:50 [rocm.py:39] Failed to import from vllm._rocm_C with ModuleNotFoundError("No module named 'vllm._rocm_C'")
[1;36m(VllmWorkerProcess pid=18138)[0;0m WARNING 08-09 13:53:50 [utils.py:2850] Methods add_prompt_adapter,cache_config,compilation_config,current_platform,list_prompt_adapters,load_config,pin_prompt_adapter,remove_prompt_adapter,scheduler_config not implemented in <vllm.worker.hpu_worker.HPUWorker object at 0x7f172e5a92d0>
[1;36m(VllmWorkerProcess pid=18140)[0;0m WARNING 08-09 13:53:50 [utils.py:2850] Methods add_prompt_adapter,cache_config,compilation_config,current_platform,list_prompt_adapters,load_config,pin_prompt_adapter,remove_prompt_adapter,scheduler_config not implemented in <vllm.worker.hpu_worker.HPUWorker object at 0x7f364e3493f0>
[1;36m(VllmWorkerProcess pid=18139)[0;0m WARNING 08-09 13:53:50 [utils.py:2850] Methods add_prompt_adapter,cache_config,compilation_config,current_platform,list_prompt_adapters,load_config,pin_prompt_adapter,remove_prompt_adapter,scheduler_config not implemented in <vllm.worker.hpu_worker.HPUWorker object at 0x7f4ef0b6d330>
[1;36m(VllmWorkerProcess pid=18139)[0;0m INFO 08-09 13:53:51 [runtime.py:26] Environment:
[1;36m(VllmWorkerProcess pid=18139)[0;0m INFO 08-09 13:53:51 [runtime.py:30]     hw: gaudi3
[1;36m(VllmWorkerProcess pid=18139)[0;0m INFO 08-09 13:53:51 [runtime.py:30]     build: 1.22.0.425
[1;36m(VllmWorkerProcess pid=18139)[0;0m INFO 08-09 13:53:51 [runtime.py:30]     engine_version: v0
[1;36m(VllmWorkerProcess pid=18139)[0;0m INFO 08-09 13:53:51 [runtime.py:30]     bridge_mode: eager
[1;36m(VllmWorkerProcess pid=18139)[0;0m INFO 08-09 13:53:51 [runtime.py:30]     model_type: llama
[1;36m(VllmWorkerProcess pid=18139)[0;0m INFO 08-09 13:53:51 [runtime.py:26] Features:
[1;36m(VllmWorkerProcess pid=18139)[0;0m INFO 08-09 13:53:51 [runtime.py:30]     fp32_alibi_biases: True
[1;36m(VllmWorkerProcess pid=18139)[0;0m INFO 08-09 13:53:51 [runtime.py:30]     fp32_softmax: False
[1;36m(VllmWorkerProcess pid=18139)[0;0m INFO 08-09 13:53:51 [runtime.py:30]     fused_block_softmax_adjustment: False
[1;36m(VllmWorkerProcess pid=18139)[0;0m INFO 08-09 13:53:51 [runtime.py:30]     fused_block_softmax: False
[1;36m(VllmWorkerProcess pid=18139)[0;0m INFO 08-09 13:53:51 [runtime.py:30]     prompt_attn_impl: fsdpa_impl
[1;36m(VllmWorkerProcess pid=18139)[0;0m INFO 08-09 13:53:51 [runtime.py:30]     skip_warmup: False
[1;36m(VllmWorkerProcess pid=18139)[0;0m INFO 08-09 13:53:51 [runtime.py:30]     merged_prefill: False
[1;36m(VllmWorkerProcess pid=18139)[0;0m INFO 08-09 13:53:51 [runtime.py:30]     use_contiguous_pa: True
[1;36m(VllmWorkerProcess pid=18139)[0;0m INFO 08-09 13:53:51 [runtime.py:30]     use_delayed_sampling: True
[1;36m(VllmWorkerProcess pid=18139)[0;0m INFO 08-09 13:53:51 [runtime.py:30]     use_bucketing: True
[1;36m(VllmWorkerProcess pid=18139)[0;0m INFO 08-09 13:53:51 [runtime.py:30]     bucketing_strategy: exponential_bucketing
[1;36m(VllmWorkerProcess pid=18139)[0;0m INFO 08-09 13:53:51 [runtime.py:26] User flags:
[1;36m(VllmWorkerProcess pid=18139)[0;0m INFO 08-09 13:53:51 [runtime.py:30]     VLLM_USE_V1: False
[1;36m(VllmWorkerProcess pid=18139)[0;0m WARNING 08-09 13:53:51 [hpu.py:135] Pin memory is not supported on HPU.
[1;36m(VllmWorkerProcess pid=18139)[0;0m INFO 08-09 13:53:51 [hpu.py:60] Using HPUAttention backend.
[1;36m(VllmWorkerProcess pid=18140)[0;0m INFO 08-09 13:53:51 [runtime.py:26] Environment:
[1;36m(VllmWorkerProcess pid=18140)[0;0m INFO 08-09 13:53:51 [runtime.py:30]     hw: gaudi3
[1;36m(VllmWorkerProcess pid=18140)[0;0m INFO 08-09 13:53:51 [runtime.py:30]     build: 1.22.0.425
[1;36m(VllmWorkerProcess pid=18140)[0;0m INFO 08-09 13:53:51 [runtime.py:30]     engine_version: v0
[1;36m(VllmWorkerProcess pid=18140)[0;0m INFO 08-09 13:53:51 [runtime.py:30]     bridge_mode: eager
[1;36m(VllmWorkerProcess pid=18140)[0;0m INFO 08-09 13:53:51 [runtime.py:30]     model_type: llama
[1;36m(VllmWorkerProcess pid=18140)[0;0m INFO 08-09 13:53:51 [runtime.py:26] Features:
[1;36m(VllmWorkerProcess pid=18140)[0;0m INFO 08-09 13:53:51 [runtime.py:30]     fp32_alibi_biases: True
[1;36m(VllmWorkerProcess pid=18140)[0;0m INFO 08-09 13:53:51 [runtime.py:30]     fp32_softmax: False
[1;36m(VllmWorkerProcess pid=18140)[0;0m INFO 08-09 13:53:51 [runtime.py:30]     fused_block_softmax_adjustment: False
[1;36m(VllmWorkerProcess pid=18140)[0;0m INFO 08-09 13:53:51 [runtime.py:30]     fused_block_softmax: False
[1;36m(VllmWorkerProcess pid=18140)[0;0m INFO 08-09 13:53:51 [runtime.py:30]     prompt_attn_impl: fsdpa_impl
[1;36m(VllmWorkerProcess pid=18140)[0;0m INFO 08-09 13:53:51 [runtime.py:30]     skip_warmup: False
[1;36m(VllmWorkerProcess pid=18140)[0;0m INFO 08-09 13:53:51 [runtime.py:30]     merged_prefill: False
[1;36m(VllmWorkerProcess pid=18140)[0;0m INFO 08-09 13:53:51 [runtime.py:30]     use_contiguous_pa: True
[1;36m(VllmWorkerProcess pid=18140)[0;0m INFO 08-09 13:53:51 [runtime.py:30]     use_delayed_sampling: True
[1;36m(VllmWorkerProcess pid=18140)[0;0m INFO 08-09 13:53:51 [runtime.py:30]     use_bucketing: True
[1;36m(VllmWorkerProcess pid=18140)[0;0m INFO 08-09 13:53:51 [runtime.py:30]     bucketing_strategy: exponential_bucketing
[1;36m(VllmWorkerProcess pid=18140)[0;0m INFO 08-09 13:53:51 [runtime.py:26] User flags:
[1;36m(VllmWorkerProcess pid=18140)[0;0m INFO 08-09 13:53:51 [runtime.py:30]     VLLM_USE_V1: False
[1;36m(VllmWorkerProcess pid=18140)[0;0m WARNING 08-09 13:53:51 [hpu.py:135] Pin memory is not supported on HPU.
[1;36m(VllmWorkerProcess pid=18140)[0;0m INFO 08-09 13:53:51 [hpu.py:60] Using HPUAttention backend.
[1;36m(VllmWorkerProcess pid=18138)[0;0m INFO 08-09 13:53:51 [runtime.py:26] Environment:
[1;36m(VllmWorkerProcess pid=18138)[0;0m INFO 08-09 13:53:51 [runtime.py:30]     hw: gaudi3
[1;36m(VllmWorkerProcess pid=18138)[0;0m INFO 08-09 13:53:51 [runtime.py:30]     build: 1.22.0.425
[1;36m(VllmWorkerProcess pid=18138)[0;0m INFO 08-09 13:53:51 [runtime.py:30]     engine_version: v0
[1;36m(VllmWorkerProcess pid=18138)[0;0m INFO 08-09 13:53:51 [runtime.py:30]     bridge_mode: eager
[1;36m(VllmWorkerProcess pid=18138)[0;0m INFO 08-09 13:53:51 [runtime.py:30]     model_type: llama
[1;36m(VllmWorkerProcess pid=18138)[0;0m INFO 08-09 13:53:51 [runtime.py:26] Features:
[1;36m(VllmWorkerProcess pid=18138)[0;0m INFO 08-09 13:53:51 [runtime.py:30]     fp32_alibi_biases: True
[1;36m(VllmWorkerProcess pid=18138)[0;0m INFO 08-09 13:53:51 [runtime.py:30]     fp32_softmax: False
[1;36m(VllmWorkerProcess pid=18138)[0;0m INFO 08-09 13:53:51 [runtime.py:30]     fused_block_softmax_adjustment: False
[1;36m(VllmWorkerProcess pid=18138)[0;0m INFO 08-09 13:53:51 [runtime.py:30]     fused_block_softmax: False
[1;36m(VllmWorkerProcess pid=18138)[0;0m INFO 08-09 13:53:51 [runtime.py:30]     prompt_attn_impl: fsdpa_impl
[1;36m(VllmWorkerProcess pid=18138)[0;0m INFO 08-09 13:53:51 [runtime.py:30]     skip_warmup: False
[1;36m(VllmWorkerProcess pid=18138)[0;0m INFO 08-09 13:53:51 [runtime.py:30]     merged_prefill: False
[1;36m(VllmWorkerProcess pid=18138)[0;0m INFO 08-09 13:53:51 [runtime.py:30]     use_contiguous_pa: True
[1;36m(VllmWorkerProcess pid=18138)[0;0m INFO 08-09 13:53:51 [runtime.py:30]     use_delayed_sampling: True
[1;36m(VllmWorkerProcess pid=18138)[0;0m INFO 08-09 13:53:51 [runtime.py:30]     use_bucketing: True
[1;36m(VllmWorkerProcess pid=18138)[0;0m INFO 08-09 13:53:51 [runtime.py:30]     bucketing_strategy: exponential_bucketing
[1;36m(VllmWorkerProcess pid=18138)[0;0m INFO 08-09 13:53:51 [runtime.py:26] User flags:
[1;36m(VllmWorkerProcess pid=18138)[0;0m INFO 08-09 13:53:51 [runtime.py:30]     VLLM_USE_V1: False
[1;36m(VllmWorkerProcess pid=18138)[0;0m WARNING 08-09 13:53:51 [hpu.py:135] Pin memory is not supported on HPU.
[1;36m(VllmWorkerProcess pid=18138)[0;0m INFO 08-09 13:53:51 [hpu.py:60] Using HPUAttention backend.
============================= HPU SW VERSION ====================================== 
 HB_BUILD_VER = 1.22.0-425
============================= HPU PT BRIDGE CONFIGURATION ON RANK = 0 ============= 
 PT_HPU_LAZY_MODE = 0
 PT_HPU_RECIPE_CACHE_CONFIG = ,false,1024,false
 PT_HPU_MAX_COMPOUND_OP_SIZE = 9223372036854775807
 PT_HPU_LAZY_ACC_PAR_MODE = 1
 PT_HPU_ENABLE_REFINE_DYNAMIC_SHAPES = 0
 PT_HPU_EAGER_PIPELINE_ENABLE = 1
 PT_HPU_EAGER_COLLECTIVE_PIPELINE_ENABLE = 1
 PT_HPU_ENABLE_LAZY_COLLECTIVES = 1
---------------------------: System Configuration :---------------------------
Num CPU Cores : 224
CPU RAM       : 1007 GB
------------------------------------------------------------------------------
============================= HPU SW VERSION ====================================== 
 HB_BUILD_VER = 1.22.0-425
============================= HPU PT BRIDGE CONFIGURATION ON RANK = 0 ============= 
 PT_HPU_LAZY_MODE = 0
 PT_HPU_RECIPE_CACHE_CONFIG = ,false,1024,false
 PT_HPU_MAX_COMPOUND_OP_SIZE = 9223372036854775807
 PT_HPU_LAZY_ACC_PAR_MODE = 1
 PT_HPU_ENABLE_REFINE_DYNAMIC_SHAPES = 0
 PT_HPU_EAGER_PIPELINE_ENABLE = 1
 PT_HPU_EAGER_COLLECTIVE_PIPELINE_ENABLE = 1
 PT_HPU_ENABLE_LAZY_COLLECTIVES = 1
---------------------------: System Configuration :---------------------------
Num CPU Cores : 224
CPU RAM       : 1007 GB
------------------------------------------------------------------------------
============================= HPU SW VERSION ====================================== 
 HB_BUILD_VER = 1.22.0-425
============================= HPU PT BRIDGE CONFIGURATION ON RANK = 0 ============= 
 PT_HPU_LAZY_MODE = 0
 PT_HPU_RECIPE_CACHE_CONFIG = ,false,1024,false
 PT_HPU_MAX_COMPOUND_OP_SIZE = 9223372036854775807
 PT_HPU_LAZY_ACC_PAR_MODE = 1
 PT_HPU_ENABLE_REFINE_DYNAMIC_SHAPES = 0
 PT_HPU_EAGER_PIPELINE_ENABLE = 1
 PT_HPU_EAGER_COLLECTIVE_PIPELINE_ENABLE = 1
 PT_HPU_ENABLE_LAZY_COLLECTIVES = 1
---------------------------: System Configuration :---------------------------
Num CPU Cores : 224
CPU RAM       : 1007 GB
------------------------------------------------------------------------------
============================= HPU SW VERSION ====================================== 
 HB_BUILD_VER = 1.22.0-425
============================= HPU PT BRIDGE CONFIGURATION ON RANK = 0 ============= 
 PT_HPU_LAZY_MODE = 0
 PT_HPU_RECIPE_CACHE_CONFIG = ,false,1024,false
 PT_HPU_MAX_COMPOUND_OP_SIZE = 9223372036854775807
 PT_HPU_LAZY_ACC_PAR_MODE = 1
 PT_HPU_ENABLE_REFINE_DYNAMIC_SHAPES = 0
 PT_HPU_EAGER_PIPELINE_ENABLE = 1
 PT_HPU_EAGER_COLLECTIVE_PIPELINE_ENABLE = 1
 PT_HPU_ENABLE_LAZY_COLLECTIVES = 1
---------------------------: System Configuration :---------------------------
Num CPU Cores : 224
CPU RAM       : 1007 GB
------------------------------------------------------------------------------
INFO 08-09 13:53:53 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[1, 2, 3], buffer_handle=(3, 4194304, 6, 'psm_4409ecf5'), local_subscribe_addr='ipc:///tmp/0d24291a-8192-4706-bc0d-80668c82b3bc', remote_subscribe_addr=None, remote_addr_ipv6=False)
INFO 08-09 13:53:53 [parallel_state.py:1065] rank 0 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(VllmWorkerProcess pid=18138)[0;0m INFO 08-09 13:53:53 [parallel_state.py:1065] rank 1 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 1, EP rank 1
[1;36m(VllmWorkerProcess pid=18140)[0;0m INFO 08-09 13:53:53 [parallel_state.py:1065] rank 3 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 3, EP rank 3
[1;36m(VllmWorkerProcess pid=18139)[0;0m INFO 08-09 13:53:53 [parallel_state.py:1065] rank 2 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 2, EP rank 2
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  3.71it/s]
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:00<00:00,  1.88it/s]
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:00<00:00,  2.03it/s]

INFO 08-09 13:53:55 [default_loader.py:272] Loading weights took 1.10 seconds
[1;36m(VllmWorkerProcess pid=18139)[0;0m INFO 08-09 13:53:55 [default_loader.py:272] Loading weights took 1.14 seconds
INFO 08-09 13:53:55 [hpu_model_runner.py:1184] Pre-loading model weights on hpu:0 took 3.139 GiB of device memory (3.146 GiB/126.5 GiB used) and 132.7 MiB of host memory (146.9 GiB/1007 GiB used)
[1;36m(VllmWorkerProcess pid=18140)[0;0m INFO 08-09 13:53:55 [default_loader.py:272] Loading weights took 1.27 seconds
[1;36m(VllmWorkerProcess pid=18139)[0;0m INFO 08-09 13:53:55 [hpu_model_runner.py:1184] Pre-loading model weights on hpu:0 took 3.139 GiB of device memory (3.146 GiB/126.5 GiB used) and 104 MiB of host memory (146.9 GiB/1007 GiB used)
[1;36m(VllmWorkerProcess pid=18138)[0;0m INFO 08-09 13:53:55 [default_loader.py:272] Loading weights took 1.22 seconds
[1;36m(VllmWorkerProcess pid=18140)[0;0m INFO 08-09 13:53:55 [hpu_model_runner.py:1184] Pre-loading model weights on hpu:0 took 3.139 GiB of device memory (3.146 GiB/126.5 GiB used) and 102.4 MiB of host memory (146.9 GiB/1007 GiB used)
INFO 08-09 13:53:55 [hpu_model_runner.py:1266] Wrapping in HPU Graph took 0 B of device memory (3.146 GiB/126.5 GiB used) and 1.137 MiB of host memory (146.9 GiB/1007 GiB used)
[1;36m(VllmWorkerProcess pid=18139)[0;0m INFO 08-09 13:53:55 [hpu_model_runner.py:1266] Wrapping in HPU Graph took 0 B of device memory (3.146 GiB/126.5 GiB used) and 1.137 MiB of host memory (146.9 GiB/1007 GiB used)
[1;36m(VllmWorkerProcess pid=18138)[0;0m INFO 08-09 13:53:55 [hpu_model_runner.py:1184] Pre-loading model weights on hpu:0 took 3.139 GiB of device memory (3.146 GiB/126.5 GiB used) and 68.42 MiB of host memory (146.9 GiB/1007 GiB used)
[1;36m(VllmWorkerProcess pid=18140)[0;0m INFO 08-09 13:53:55 [hpu_model_runner.py:1266] Wrapping in HPU Graph took 0 B of device memory (3.146 GiB/126.5 GiB used) and 884 KiB of host memory (146.9 GiB/1007 GiB used)
INFO 08-09 13:53:56 [hpu_model_runner.py:1270] Compiling took 0 B of device memory (3.146 GiB/126.5 GiB used) and 240 KiB of host memory (146.9 GiB/1007 GiB used)
[1;36m(VllmWorkerProcess pid=18140)[0;0m INFO 08-09 13:53:56 [hpu_model_runner.py:1270] Compiling took 0 B of device memory (3.146 GiB/126.5 GiB used) and 456 KiB of host memory (146.9 GiB/1007 GiB used)
[1;36m(VllmWorkerProcess pid=18139)[0;0m INFO 08-09 13:53:56 [hpu_model_runner.py:1270] Compiling took 0 B of device memory (3.146 GiB/126.5 GiB used) and 456 KiB of host memory (146.9 GiB/1007 GiB used)
[1;36m(VllmWorkerProcess pid=18140)[0;0m INFO 08-09 13:53:56 [hpu_model_runner.py:1274] Loading model weights took in total 3.139 GiB of device memory (3.146 GiB/126.5 GiB used) and 126.1 MiB of host memory (146.9 GiB/1007 GiB used)
[1;36m(VllmWorkerProcess pid=18138)[0;0m INFO 08-09 13:53:56 [hpu_model_runner.py:1266] Wrapping in HPU Graph took 0 B of device memory (3.146 GiB/126.5 GiB used) and -716 KiB of host memory (146.9 GiB/1007 GiB used)
[1;36m(VllmWorkerProcess pid=18139)[0;0m INFO 08-09 13:53:56 [hpu_model_runner.py:1274] Loading model weights took in total 3.139 GiB of device memory (3.146 GiB/126.5 GiB used) and 124.7 MiB of host memory (146.9 GiB/1007 GiB used)
INFO 08-09 13:53:56 [hpu_model_runner.py:1274] Loading model weights took in total 3.139 GiB of device memory (3.146 GiB/126.5 GiB used) and 126.6 MiB of host memory (146.9 GiB/1007 GiB used)
[1;36m(VllmWorkerProcess pid=18138)[0;0m INFO 08-09 13:53:56 [hpu_model_runner.py:1270] Compiling took 0 B of device memory (3.146 GiB/126.5 GiB used) and 12 KiB of host memory (146.9 GiB/1007 GiB used)
[1;36m(VllmWorkerProcess pid=18138)[0;0m INFO 08-09 13:53:57 [hpu_model_runner.py:1274] Loading model weights took in total 3.139 GiB of device memory (3.146 GiB/126.5 GiB used) and 102 MiB of host memory (146.9 GiB/1007 GiB used)
WARNING 08-09 13:53:57 [common.py:139] Prompt bucket for (1, 4096, 0) was not prepared. Adding new bucket: (1, 4096, 0)
[1;36m(VllmWorkerProcess pid=18139)[0;0m WARNING 08-09 13:53:57 [common.py:139] Prompt bucket for (1, 4096, 0) was not prepared. Adding new bucket: (1, 4096, 0)
[1;36m(VllmWorkerProcess pid=18140)[0;0m WARNING 08-09 13:53:57 [common.py:139] Prompt bucket for (1, 4096, 0) was not prepared. Adding new bucket: (1, 4096, 0)
[1;36m(VllmWorkerProcess pid=18138)[0;0m WARNING 08-09 13:53:57 [common.py:139] Prompt bucket for (1, 4096, 0) was not prepared. Adding new bucket: (1, 4096, 0)
[1;36m(VllmWorkerProcess pid=18140)[0;0m INFO 08-09 13:53:59 [hpu_worker.py:289] Model profiling run took 184 MiB of device memory (3.326 GiB/126.5 GiB used) and 140.7 MiB of host memory (147.1 GiB/1007 GiB used)
[1;36m(VllmWorkerProcess pid=18140)[0;0m INFO 08-09 13:53:59 [hpu_worker.py:313] Free device memory: 123.2 GiB, 110.9 GiB usable (gpu_memory_utilization=0.9), 11.09 GiB reserved for HPUGraphs (VLLM_GRAPH_RESERVED_MEM=0.1), 99.8 GiB reserved for KV cache
[1;36m(VllmWorkerProcess pid=18139)[0;0m INFO 08-09 13:53:59 [hpu_worker.py:289] Model profiling run took 184 MiB of device memory (3.326 GiB/126.5 GiB used) and 132.3 MiB of host memory (147.1 GiB/1007 GiB used)
[1;36m(VllmWorkerProcess pid=18139)[0;0m INFO 08-09 13:53:59 [hpu_worker.py:313] Free device memory: 123.2 GiB, 110.9 GiB usable (gpu_memory_utilization=0.9), 11.09 GiB reserved for HPUGraphs (VLLM_GRAPH_RESERVED_MEM=0.1), 99.8 GiB reserved for KV cache
[1;36m(VllmWorkerProcess pid=18138)[0;0m INFO 08-09 13:53:59 [hpu_worker.py:289] Model profiling run took 184 MiB of device memory (3.326 GiB/126.5 GiB used) and 129.7 MiB of host memory (147.1 GiB/1007 GiB used)
[1;36m(VllmWorkerProcess pid=18138)[0;0m INFO 08-09 13:53:59 [hpu_worker.py:313] Free device memory: 123.2 GiB, 110.9 GiB usable (gpu_memory_utilization=0.9), 11.09 GiB reserved for HPUGraphs (VLLM_GRAPH_RESERVED_MEM=0.1), 99.8 GiB reserved for KV cache
INFO 08-09 13:53:59 [hpu_worker.py:289] Model profiling run took 184 MiB of device memory (3.326 GiB/126.5 GiB used) and 144.5 MiB of host memory (147.1 GiB/1007 GiB used)
INFO 08-09 13:53:59 [hpu_worker.py:313] Free device memory: 123.2 GiB, 110.9 GiB usable (gpu_memory_utilization=0.9), 11.09 GiB reserved for HPUGraphs (VLLM_GRAPH_RESERVED_MEM=0.1), 99.8 GiB reserved for KV cache
INFO 08-09 13:54:00 [executor_base.py:113] # hpu blocks: 6387, # CPU blocks: 256
INFO 08-09 13:54:00 [executor_base.py:118] Maximum concurrency for 4096 tokens per request: 199.59x
[1;36m(VllmWorkerProcess pid=18139)[0;0m INFO 08-09 13:54:00 [exponential.py:49] Prompt bucket config (min, step, max_warmup, limit) bs:[1, 2, 256, 9], seq:[128, 128, 4096, 13]
INFO 08-09 13:54:00 [exponential.py:49] Prompt bucket config (min, step, max_warmup, limit) bs:[1, 2, 256, 9], seq:[128, 128, 4096, 13]
[1;36m(VllmWorkerProcess pid=18140)[0;0m INFO 08-09 13:54:00 [exponential.py:49] Prompt bucket config (min, step, max_warmup, limit) bs:[1, 2, 256, 9], seq:[128, 128, 4096, 13]
[1;36m(VllmWorkerProcess pid=18139)[0;0m INFO 08-09 13:54:00 [common.py:117] Generated 38 prompt buckets [bs, query, num_blocks]: [(1, 128, 0), (1, 256, 0), (1, 384, 0), (1, 512, 0), (1, 640, 0), (1, 768, 0), (1, 896, 0), (1, 1024, 0), (1, 1408, 0), (1, 1792, 0), (1, 2304, 0), (1, 3072, 0), (1, 4096, 0), (2, 128, 0), (2, 256, 0), (2, 384, 0), (2, 512, 0), (2, 640, 0), (2, 768, 0), (2, 896, 0), (2, 1024, 0), (2, 1408, 0), (2, 1792, 0), (4, 128, 0), (4, 256, 0), (4, 384, 0), (4, 512, 0), (4, 640, 0), (4, 768, 0), (4, 896, 0), (4, 1024, 0), (8, 128, 0), (8, 256, 0), (8, 384, 0), (8, 512, 0), (16, 128, 0), (16, 256, 0), (32, 128, 0)]
[1;36m(VllmWorkerProcess pid=18139)[0;0m INFO 08-09 13:54:00 [exponential.py:79] Decode bucket config (min, step, max_warmup, limit) bs:[1, 2, 256, 9], block:[128, 128, 6387, 14]
[1;36m(VllmWorkerProcess pid=18140)[0;0m INFO 08-09 13:54:00 [common.py:117] Generated 38 prompt buckets [bs, query, num_blocks]: [(1, 128, 0), (1, 256, 0), (1, 384, 0), (1, 512, 0), (1, 640, 0), (1, 768, 0), (1, 896, 0), (1, 1024, 0), (1, 1408, 0), (1, 1792, 0), (1, 2304, 0), (1, 3072, 0), (1, 4096, 0), (2, 128, 0), (2, 256, 0), (2, 384, 0), (2, 512, 0), (2, 640, 0), (2, 768, 0), (2, 896, 0), (2, 1024, 0), (2, 1408, 0), (2, 1792, 0), (4, 128, 0), (4, 256, 0), (4, 384, 0), (4, 512, 0), (4, 640, 0), (4, 768, 0), (4, 896, 0), (4, 1024, 0), (8, 128, 0), (8, 256, 0), (8, 384, 0), (8, 512, 0), (16, 128, 0), (16, 256, 0), (32, 128, 0)]
INFO 08-09 13:54:00 [common.py:117] Generated 38 prompt buckets [bs, query, num_blocks]: [(1, 128, 0), (1, 256, 0), (1, 384, 0), (1, 512, 0), (1, 640, 0), (1, 768, 0), (1, 896, 0), (1, 1024, 0), (1, 1408, 0), (1, 1792, 0), (1, 2304, 0), (1, 3072, 0), (1, 4096, 0), (2, 128, 0), (2, 256, 0), (2, 384, 0), (2, 512, 0), (2, 640, 0), (2, 768, 0), (2, 896, 0), (2, 1024, 0), (2, 1408, 0), (2, 1792, 0), (4, 128, 0), (4, 256, 0), (4, 384, 0), (4, 512, 0), (4, 640, 0), (4, 768, 0), (4, 896, 0), (4, 1024, 0), (8, 128, 0), (8, 256, 0), (8, 384, 0), (8, 512, 0), (16, 128, 0), (16, 256, 0), (32, 128, 0)]
[1;36m(VllmWorkerProcess pid=18140)[0;0m INFO 08-09 13:54:00 [exponential.py:79] Decode bucket config (min, step, max_warmup, limit) bs:[1, 2, 256, 9], block:[128, 128, 6387, 14]
INFO 08-09 13:54:00 [exponential.py:79] Decode bucket config (min, step, max_warmup, limit) bs:[1, 2, 256, 9], block:[128, 128, 6387, 14]
[1;36m(VllmWorkerProcess pid=18138)[0;0m INFO 08-09 13:54:00 [exponential.py:49] Prompt bucket config (min, step, max_warmup, limit) bs:[1, 2, 256, 9], seq:[128, 128, 4096, 13]
[1;36m(VllmWorkerProcess pid=18139)[0;0m INFO 08-09 13:54:00 [common.py:117] Generated 126 decode buckets [bs, query, num_blocks]: [(1, 1, 128), (1, 1, 256), (1, 1, 384), (1, 1, 512), (1, 1, 640), (1, 1, 768), (1, 1, 896), (1, 1, 1152), (1, 1, 1536), (1, 1, 1920), (1, 1, 2688), (1, 1, 3584), (1, 1, 4736), (1, 1, 6387), (2, 1, 128), (2, 1, 256), (2, 1, 384), (2, 1, 512), (2, 1, 640), (2, 1, 768), (2, 1, 896), (2, 1, 1152), (2, 1, 1536), (2, 1, 1920), (2, 1, 2688), (2, 1, 3584), (2, 1, 4736), (2, 1, 6387), (4, 1, 128), (4, 1, 256), (4, 1, 384), (4, 1, 512), (4, 1, 640), (4, 1, 768), (4, 1, 896), (4, 1, 1152), (4, 1, 1536), (4, 1, 1920), (4, 1, 2688), (4, 1, 3584), (4, 1, 4736), (4, 1, 6387), (8, 1, 128), (8, 1, 256), (8, 1, 384), (8, 1, 512), (8, 1, 640), (8, 1, 768), (8, 1, 896), (8, 1, 1152), (8, 1, 1536), (8, 1, 1920), (8, 1, 2688), (8, 1, 3584), (8, 1, 4736), (8, 1, 6387), (16, 1, 128), (16, 1, 256), (16, 1, 384), (16, 1, 512), (16, 1, 640), (16, 1, 768), (16, 1, 896), (16, 1, 1152), (16, 1, 1536), (16, 1, 1920), (16, 1, 2688), (16, 1, 3584), (16, 1, 4736), (16, 1, 6387), (32, 1, 128), (32, 1, 256), (32, 1, 384), (32, 1, 512), (32, 1, 640), (32, 1, 768), (32, 1, 896), (32, 1, 1152), (32, 1, 1536), (32, 1, 1920), (32, 1, 2688), (32, 1, 3584), (32, 1, 4736), (32, 1, 6387), (64, 1, 128), (64, 1, 256), (64, 1, 384), (64, 1, 512), (64, 1, 640), (64, 1, 768), (64, 1, 896), (64, 1, 1152), (64, 1, 1536), (64, 1, 1920), (64, 1, 2688), (64, 1, 3584), (64, 1, 4736), (64, 1, 6387), (128, 1, 128), (128, 1, 256), (128, 1, 384), (128, 1, 512), (128, 1, 640), (128, 1, 768), (128, 1, 896), (128, 1, 1152), (128, 1, 1536), (128, 1, 1920), (128, 1, 2688), (128, 1, 3584), (128, 1, 4736), (128, 1, 6387), (256, 1, 128), (256, 1, 256), (256, 1, 384), (256, 1, 512), (256, 1, 640), (256, 1, 768), (256, 1, 896), (256, 1, 1152), (256, 1, 1536), (256, 1, 1920), (256, 1, 2688), (256, 1, 3584), (256, 1, 4736), (256, 1, 6387)]
INFO 08-09 13:54:00 [common.py:117] Generated 126 decode buckets [bs, query, num_blocks]: [(1, 1, 128), (1, 1, 256), (1, 1, 384), (1, 1, 512), (1, 1, 640), (1, 1, 768), (1, 1, 896), (1, 1, 1152), (1, 1, 1536), (1, 1, 1920), (1, 1, 2688), (1, 1, 3584), (1, 1, 4736), (1, 1, 6387), (2, 1, 128), (2, 1, 256), (2, 1, 384), (2, 1, 512), (2, 1, 640), (2, 1, 768), (2, 1, 896), (2, 1, 1152), (2, 1, 1536), (2, 1, 1920), (2, 1, 2688), (2, 1, 3584), (2, 1, 4736), (2, 1, 6387), (4, 1, 128), (4, 1, 256), (4, 1, 384), (4, 1, 512), (4, 1, 640), (4, 1, 768), (4, 1, 896), (4, 1, 1152), (4, 1, 1536), (4, 1, 1920), (4, 1, 2688), (4, 1, 3584), (4, 1, 4736), (4, 1, 6387), (8, 1, 128), (8, 1, 256), (8, 1, 384), (8, 1, 512), (8, 1, 640), (8, 1, 768), (8, 1, 896), (8, 1, 1152), (8, 1, 1536), (8, 1, 1920), (8, 1, 2688), (8, 1, 3584), (8, 1, 4736), (8, 1, 6387), (16, 1, 128), (16, 1, 256), (16, 1, 384), (16, 1, 512), (16, 1, 640), (16, 1, 768), (16, 1, 896), (16, 1, 1152), (16, 1, 1536), (16, 1, 1920), (16, 1, 2688), (16, 1, 3584), (16, 1, 4736), (16, 1, 6387), (32, 1, 128), (32, 1, 256), (32, 1, 384), (32, 1, 512), (32, 1, 640), (32, 1, 768), (32, 1, 896), (32, 1, 1152), (32, 1, 1536), (32, 1, 1920), (32, 1, 2688), (32, 1, 3584), (32, 1, 4736), (32, 1, 6387), (64, 1, 128), (64, 1, 256), (64, 1, 384), (64, 1, 512), (64, 1, 640), (64, 1, 768), (64, 1, 896), (64, 1, 1152), (64, 1, 1536), (64, 1, 1920), (64, 1, 2688), (64, 1, 3584), (64, 1, 4736), (64, 1, 6387), (128, 1, 128), (128, 1, 256), (128, 1, 384), (128, 1, 512), (128, 1, 640), (128, 1, 768), (128, 1, 896), (128, 1, 1152), (128, 1, 1536), (128, 1, 1920), (128, 1, 2688), (128, 1, 3584), (128, 1, 4736), (128, 1, 6387), (256, 1, 128), (256, 1, 256), (256, 1, 384), (256, 1, 512), (256, 1, 640), (256, 1, 768), (256, 1, 896), (256, 1, 1152), (256, 1, 1536), (256, 1, 1920), (256, 1, 2688), (256, 1, 3584), (256, 1, 4736), (256, 1, 6387)]
[1;36m(VllmWorkerProcess pid=18140)[0;0m INFO 08-09 13:54:00 [common.py:117] Generated 126 decode buckets [bs, query, num_blocks]: [(1, 1, 128), (1, 1, 256), (1, 1, 384), (1, 1, 512), (1, 1, 640), (1, 1, 768), (1, 1, 896), (1, 1, 1152), (1, 1, 1536), (1, 1, 1920), (1, 1, 2688), (1, 1, 3584), (1, 1, 4736), (1, 1, 6387), (2, 1, 128), (2, 1, 256), (2, 1, 384), (2, 1, 512), (2, 1, 640), (2, 1, 768), (2, 1, 896), (2, 1, 1152), (2, 1, 1536), (2, 1, 1920), (2, 1, 2688), (2, 1, 3584), (2, 1, 4736), (2, 1, 6387), (4, 1, 128), (4, 1, 256), (4, 1, 384), (4, 1, 512), (4, 1, 640), (4, 1, 768), (4, 1, 896), (4, 1, 1152), (4, 1, 1536), (4, 1, 1920), (4, 1, 2688), (4, 1, 3584), (4, 1, 4736), (4, 1, 6387), (8, 1, 128), (8, 1, 256), (8, 1, 384), (8, 1, 512), (8, 1, 640), (8, 1, 768), (8, 1, 896), (8, 1, 1152), (8, 1, 1536), (8, 1, 1920), (8, 1, 2688), (8, 1, 3584), (8, 1, 4736), (8, 1, 6387), (16, 1, 128), (16, 1, 256), (16, 1, 384), (16, 1, 512), (16, 1, 640), (16, 1, 768), (16, 1, 896), (16, 1, 1152), (16, 1, 1536), (16, 1, 1920), (16, 1, 2688), (16, 1, 3584), (16, 1, 4736), (16, 1, 6387), (32, 1, 128), (32, 1, 256), (32, 1, 384), (32, 1, 512), (32, 1, 640), (32, 1, 768), (32, 1, 896), (32, 1, 1152), (32, 1, 1536), (32, 1, 1920), (32, 1, 2688), (32, 1, 3584), (32, 1, 4736), (32, 1, 6387), (64, 1, 128), (64, 1, 256), (64, 1, 384), (64, 1, 512), (64, 1, 640), (64, 1, 768), (64, 1, 896), (64, 1, 1152), (64, 1, 1536), (64, 1, 1920), (64, 1, 2688), (64, 1, 3584), (64, 1, 4736), (64, 1, 6387), (128, 1, 128), (128, 1, 256), (128, 1, 384), (128, 1, 512), (128, 1, 640), (128, 1, 768), (128, 1, 896), (128, 1, 1152), (128, 1, 1536), (128, 1, 1920), (128, 1, 2688), (128, 1, 3584), (128, 1, 4736), (128, 1, 6387), (256, 1, 128), (256, 1, 256), (256, 1, 384), (256, 1, 512), (256, 1, 640), (256, 1, 768), (256, 1, 896), (256, 1, 1152), (256, 1, 1536), (256, 1, 1920), (256, 1, 2688), (256, 1, 3584), (256, 1, 4736), (256, 1, 6387)]
[1;36m(VllmWorkerProcess pid=18138)[0;0m INFO 08-09 13:54:00 [common.py:117] Generated 38 prompt buckets [bs, query, num_blocks]: [(1, 128, 0), (1, 256, 0), (1, 384, 0), (1, 512, 0), (1, 640, 0), (1, 768, 0), (1, 896, 0), (1, 1024, 0), (1, 1408, 0), (1, 1792, 0), (1, 2304, 0), (1, 3072, 0), (1, 4096, 0), (2, 128, 0), (2, 256, 0), (2, 384, 0), (2, 512, 0), (2, 640, 0), (2, 768, 0), (2, 896, 0), (2, 1024, 0), (2, 1408, 0), (2, 1792, 0), (4, 128, 0), (4, 256, 0), (4, 384, 0), (4, 512, 0), (4, 640, 0), (4, 768, 0), (4, 896, 0), (4, 1024, 0), (8, 128, 0), (8, 256, 0), (8, 384, 0), (8, 512, 0), (16, 128, 0), (16, 256, 0), (32, 128, 0)]
[1;36m(VllmWorkerProcess pid=18138)[0;0m INFO 08-09 13:54:00 [exponential.py:79] Decode bucket config (min, step, max_warmup, limit) bs:[1, 2, 256, 9], block:[128, 128, 6387, 14]
[1;36m(VllmWorkerProcess pid=18138)[0;0m INFO 08-09 13:54:00 [common.py:117] Generated 126 decode buckets [bs, query, num_blocks]: [(1, 1, 128), (1, 1, 256), (1, 1, 384), (1, 1, 512), (1, 1, 640), (1, 1, 768), (1, 1, 896), (1, 1, 1152), (1, 1, 1536), (1, 1, 1920), (1, 1, 2688), (1, 1, 3584), (1, 1, 4736), (1, 1, 6387), (2, 1, 128), (2, 1, 256), (2, 1, 384), (2, 1, 512), (2, 1, 640), (2, 1, 768), (2, 1, 896), (2, 1, 1152), (2, 1, 1536), (2, 1, 1920), (2, 1, 2688), (2, 1, 3584), (2, 1, 4736), (2, 1, 6387), (4, 1, 128), (4, 1, 256), (4, 1, 384), (4, 1, 512), (4, 1, 640), (4, 1, 768), (4, 1, 896), (4, 1, 1152), (4, 1, 1536), (4, 1, 1920), (4, 1, 2688), (4, 1, 3584), (4, 1, 4736), (4, 1, 6387), (8, 1, 128), (8, 1, 256), (8, 1, 384), (8, 1, 512), (8, 1, 640), (8, 1, 768), (8, 1, 896), (8, 1, 1152), (8, 1, 1536), (8, 1, 1920), (8, 1, 2688), (8, 1, 3584), (8, 1, 4736), (8, 1, 6387), (16, 1, 128), (16, 1, 256), (16, 1, 384), (16, 1, 512), (16, 1, 640), (16, 1, 768), (16, 1, 896), (16, 1, 1152), (16, 1, 1536), (16, 1, 1920), (16, 1, 2688), (16, 1, 3584), (16, 1, 4736), (16, 1, 6387), (32, 1, 128), (32, 1, 256), (32, 1, 384), (32, 1, 512), (32, 1, 640), (32, 1, 768), (32, 1, 896), (32, 1, 1152), (32, 1, 1536), (32, 1, 1920), (32, 1, 2688), (32, 1, 3584), (32, 1, 4736), (32, 1, 6387), (64, 1, 128), (64, 1, 256), (64, 1, 384), (64, 1, 512), (64, 1, 640), (64, 1, 768), (64, 1, 896), (64, 1, 1152), (64, 1, 1536), (64, 1, 1920), (64, 1, 2688), (64, 1, 3584), (64, 1, 4736), (64, 1, 6387), (128, 1, 128), (128, 1, 256), (128, 1, 384), (128, 1, 512), (128, 1, 640), (128, 1, 768), (128, 1, 896), (128, 1, 1152), (128, 1, 1536), (128, 1, 1920), (128, 1, 2688), (128, 1, 3584), (128, 1, 4736), (128, 1, 6387), (256, 1, 128), (256, 1, 256), (256, 1, 384), (256, 1, 512), (256, 1, 640), (256, 1, 768), (256, 1, 896), (256, 1, 1152), (256, 1, 1536), (256, 1, 1920), (256, 1, 2688), (256, 1, 3584), (256, 1, 4736), (256, 1, 6387)]
[1;36m(VllmWorkerProcess pid=18140)[0;0m INFO 08-09 13:54:01 [hpu_worker.py:350] Initializing cache engine took 99.8 GiB of device memory (103.1 GiB/126.5 GiB used) and 16.04 GiB of host memory (163.1 GiB/1007 GiB used)
[1;36m(VllmWorkerProcess pid=18139)[0;0m INFO 08-09 13:54:01 [hpu_worker.py:350] Initializing cache engine took 99.8 GiB of device memory (103.1 GiB/126.5 GiB used) and 15.97 GiB of host memory (163.1 GiB/1007 GiB used)
[1;36m(VllmWorkerProcess pid=18138)[0;0m INFO 08-09 13:54:02 [hpu_worker.py:350] Initializing cache engine took 99.8 GiB of device memory (103.1 GiB/126.5 GiB used) and 15.56 GiB of host memory (163.1 GiB/1007 GiB used)
INFO 08-09 13:54:02 [hpu_worker.py:350] Initializing cache engine took 99.8 GiB of device memory (103.1 GiB/126.5 GiB used) and 15.57 GiB of host memory (163.1 GiB/1007 GiB used)
INFO 08-09 13:54:02 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][1/38] batch_size:32 query_len:128 num_blocks:0 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18140)[0;0m INFO 08-09 13:54:02 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][1/38] batch_size:32 query_len:128 num_blocks:0 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18138)[0;0m INFO 08-09 13:54:02 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][1/38] batch_size:32 query_len:128 num_blocks:0 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18139)[0;0m INFO 08-09 13:54:02 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][1/38] batch_size:32 query_len:128 num_blocks:0 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18140)[0;0m INFO 08-09 13:54:04 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][2/38] batch_size:16 query_len:256 num_blocks:0 free_mem:23.42 GiB
INFO 08-09 13:54:04 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][2/38] batch_size:16 query_len:256 num_blocks:0 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18139)[0;0m INFO 08-09 13:54:04 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][2/38] batch_size:16 query_len:256 num_blocks:0 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18138)[0;0m INFO 08-09 13:54:04 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][2/38] batch_size:16 query_len:256 num_blocks:0 free_mem:23.42 GiB
INFO 08-09 13:54:06 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][3/38] batch_size:16 query_len:128 num_blocks:0 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18140)[0;0m INFO 08-09 13:54:06 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][3/38] batch_size:16 query_len:128 num_blocks:0 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18139)[0;0m INFO 08-09 13:54:06 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][3/38] batch_size:16 query_len:128 num_blocks:0 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18138)[0;0m INFO 08-09 13:54:06 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][3/38] batch_size:16 query_len:128 num_blocks:0 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18140)[0;0m INFO 08-09 13:54:08 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][4/38] batch_size:8 query_len:512 num_blocks:0 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18139)[0;0m INFO 08-09 13:54:08 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][4/38] batch_size:8 query_len:512 num_blocks:0 free_mem:23.42 GiB
INFO 08-09 13:54:08 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][4/38] batch_size:8 query_len:512 num_blocks:0 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18138)[0;0m INFO 08-09 13:54:08 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][4/38] batch_size:8 query_len:512 num_blocks:0 free_mem:23.42 GiB
INFO 08-09 13:54:11 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][5/38] batch_size:8 query_len:384 num_blocks:0 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18140)[0;0m INFO 08-09 13:54:11 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][5/38] batch_size:8 query_len:384 num_blocks:0 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18138)[0;0m INFO 08-09 13:54:11 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][5/38] batch_size:8 query_len:384 num_blocks:0 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18139)[0;0m INFO 08-09 13:54:11 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][5/38] batch_size:8 query_len:384 num_blocks:0 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18140)[0;0m INFO 08-09 13:54:13 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][6/38] batch_size:8 query_len:256 num_blocks:0 free_mem:23.42 GiB
INFO 08-09 13:54:13 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][6/38] batch_size:8 query_len:256 num_blocks:0 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18139)[0;0m INFO 08-09 13:54:13 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][6/38] batch_size:8 query_len:256 num_blocks:0 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18138)[0;0m INFO 08-09 13:54:13 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][6/38] batch_size:8 query_len:256 num_blocks:0 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18140)[0;0m INFO 08-09 13:54:15 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][7/38] batch_size:8 query_len:128 num_blocks:0 free_mem:23.42 GiB
INFO 08-09 13:54:15 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][7/38] batch_size:8 query_len:128 num_blocks:0 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18138)[0;0m INFO 08-09 13:54:15 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][7/38] batch_size:8 query_len:128 num_blocks:0 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18139)[0;0m INFO 08-09 13:54:15 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][7/38] batch_size:8 query_len:128 num_blocks:0 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18140)[0;0m INFO 08-09 13:54:18 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][8/38] batch_size:4 query_len:1024 num_blocks:0 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18139)[0;0m INFO 08-09 13:54:18 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][8/38] batch_size:4 query_len:1024 num_blocks:0 free_mem:23.42 GiB
INFO 08-09 13:54:18 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][8/38] batch_size:4 query_len:1024 num_blocks:0 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18138)[0;0m INFO 08-09 13:54:18 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][8/38] batch_size:4 query_len:1024 num_blocks:0 free_mem:23.42 GiB
INFO 08-09 13:54:21 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][9/38] batch_size:4 query_len:896 num_blocks:0 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18140)[0;0m INFO 08-09 13:54:21 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][9/38] batch_size:4 query_len:896 num_blocks:0 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18139)[0;0m INFO 08-09 13:54:21 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][9/38] batch_size:4 query_len:896 num_blocks:0 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18138)[0;0m INFO 08-09 13:54:21 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][9/38] batch_size:4 query_len:896 num_blocks:0 free_mem:23.42 GiB
INFO 08-09 13:54:23 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][10/38] batch_size:4 query_len:768 num_blocks:0 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18140)[0;0m INFO 08-09 13:54:23 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][10/38] batch_size:4 query_len:768 num_blocks:0 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18138)[0;0m INFO 08-09 13:54:23 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][10/38] batch_size:4 query_len:768 num_blocks:0 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18139)[0;0m INFO 08-09 13:54:23 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][10/38] batch_size:4 query_len:768 num_blocks:0 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18140)[0;0m INFO 08-09 13:54:25 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][11/38] batch_size:4 query_len:640 num_blocks:0 free_mem:23.42 GiB
INFO 08-09 13:54:25 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][11/38] batch_size:4 query_len:640 num_blocks:0 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18139)[0;0m INFO 08-09 13:54:25 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][11/38] batch_size:4 query_len:640 num_blocks:0 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18138)[0;0m INFO 08-09 13:54:25 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][11/38] batch_size:4 query_len:640 num_blocks:0 free_mem:23.42 GiB
INFO 08-09 13:54:28 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][12/38] batch_size:4 query_len:512 num_blocks:0 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18140)[0;0m INFO 08-09 13:54:28 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][12/38] batch_size:4 query_len:512 num_blocks:0 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18139)[0;0m INFO 08-09 13:54:28 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][12/38] batch_size:4 query_len:512 num_blocks:0 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18138)[0;0m INFO 08-09 13:54:28 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][12/38] batch_size:4 query_len:512 num_blocks:0 free_mem:23.42 GiB
INFO 08-09 13:54:31 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][13/38] batch_size:4 query_len:384 num_blocks:0 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18140)[0;0m INFO 08-09 13:54:31 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][13/38] batch_size:4 query_len:384 num_blocks:0 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18139)[0;0m INFO 08-09 13:54:31 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][13/38] batch_size:4 query_len:384 num_blocks:0 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18138)[0;0m INFO 08-09 13:54:31 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][13/38] batch_size:4 query_len:384 num_blocks:0 free_mem:23.42 GiB
INFO 08-09 13:54:33 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][14/38] batch_size:4 query_len:256 num_blocks:0 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18140)[0;0m INFO 08-09 13:54:33 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][14/38] batch_size:4 query_len:256 num_blocks:0 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18139)[0;0m INFO 08-09 13:54:33 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][14/38] batch_size:4 query_len:256 num_blocks:0 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18138)[0;0m INFO 08-09 13:54:33 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][14/38] batch_size:4 query_len:256 num_blocks:0 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18140)[0;0m INFO 08-09 13:54:35 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][15/38] batch_size:4 query_len:128 num_blocks:0 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18139)[0;0m INFO 08-09 13:54:35 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][15/38] batch_size:4 query_len:128 num_blocks:0 free_mem:23.42 GiB
INFO 08-09 13:54:35 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][15/38] batch_size:4 query_len:128 num_blocks:0 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18138)[0;0m INFO 08-09 13:54:35 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][15/38] batch_size:4 query_len:128 num_blocks:0 free_mem:23.42 GiB
INFO 08-09 13:54:38 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][16/38] batch_size:2 query_len:1792 num_blocks:0 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18140)[0;0m INFO 08-09 13:54:38 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][16/38] batch_size:2 query_len:1792 num_blocks:0 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18139)[0;0m INFO 08-09 13:54:38 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][16/38] batch_size:2 query_len:1792 num_blocks:0 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18138)[0;0m INFO 08-09 13:54:38 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][16/38] batch_size:2 query_len:1792 num_blocks:0 free_mem:23.42 GiB
INFO 08-09 13:54:41 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][17/38] batch_size:2 query_len:1408 num_blocks:0 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18140)[0;0m INFO 08-09 13:54:41 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][17/38] batch_size:2 query_len:1408 num_blocks:0 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18138)[0;0m INFO 08-09 13:54:41 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][17/38] batch_size:2 query_len:1408 num_blocks:0 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18139)[0;0m INFO 08-09 13:54:41 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][17/38] batch_size:2 query_len:1408 num_blocks:0 free_mem:23.42 GiB
INFO 08-09 13:54:44 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][18/38] batch_size:2 query_len:1024 num_blocks:0 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18140)[0;0m INFO 08-09 13:54:44 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][18/38] batch_size:2 query_len:1024 num_blocks:0 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18139)[0;0m INFO 08-09 13:54:44 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][18/38] batch_size:2 query_len:1024 num_blocks:0 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18138)[0;0m INFO 08-09 13:54:44 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][18/38] batch_size:2 query_len:1024 num_blocks:0 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18140)[0;0m INFO 08-09 13:54:46 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][19/38] batch_size:2 query_len:896 num_blocks:0 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18139)[0;0m INFO 08-09 13:54:46 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][19/38] batch_size:2 query_len:896 num_blocks:0 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18138)[0;0m INFO 08-09 13:54:46 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][19/38] batch_size:2 query_len:896 num_blocks:0 free_mem:23.42 GiB
INFO 08-09 13:54:46 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][19/38] batch_size:2 query_len:896 num_blocks:0 free_mem:23.42 GiB
INFO 08-09 13:54:49 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][20/38] batch_size:2 query_len:768 num_blocks:0 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18140)[0;0m INFO 08-09 13:54:49 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][20/38] batch_size:2 query_len:768 num_blocks:0 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18139)[0;0m INFO 08-09 13:54:49 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][20/38] batch_size:2 query_len:768 num_blocks:0 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18138)[0;0m INFO 08-09 13:54:49 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][20/38] batch_size:2 query_len:768 num_blocks:0 free_mem:23.42 GiB
INFO 08-09 13:54:52 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][21/38] batch_size:2 query_len:640 num_blocks:0 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18140)[0;0m INFO 08-09 13:54:52 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][21/38] batch_size:2 query_len:640 num_blocks:0 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18139)[0;0m INFO 08-09 13:54:52 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][21/38] batch_size:2 query_len:640 num_blocks:0 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18138)[0;0m INFO 08-09 13:54:52 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][21/38] batch_size:2 query_len:640 num_blocks:0 free_mem:23.42 GiB
INFO 08-09 13:54:54 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][22/38] batch_size:2 query_len:512 num_blocks:0 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18140)[0;0m INFO 08-09 13:54:54 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][22/38] batch_size:2 query_len:512 num_blocks:0 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18139)[0;0m INFO 08-09 13:54:54 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][22/38] batch_size:2 query_len:512 num_blocks:0 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18138)[0;0m INFO 08-09 13:54:54 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][22/38] batch_size:2 query_len:512 num_blocks:0 free_mem:23.42 GiB
INFO 08-09 13:54:57 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][23/38] batch_size:2 query_len:384 num_blocks:0 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18140)[0;0m INFO 08-09 13:54:57 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][23/38] batch_size:2 query_len:384 num_blocks:0 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18139)[0;0m INFO 08-09 13:54:57 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][23/38] batch_size:2 query_len:384 num_blocks:0 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18138)[0;0m INFO 08-09 13:54:57 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][23/38] batch_size:2 query_len:384 num_blocks:0 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18140)[0;0m INFO 08-09 13:54:59 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][24/38] batch_size:2 query_len:256 num_blocks:0 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18139)[0;0m INFO 08-09 13:54:59 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][24/38] batch_size:2 query_len:256 num_blocks:0 free_mem:23.42 GiB
INFO 08-09 13:54:59 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][24/38] batch_size:2 query_len:256 num_blocks:0 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18138)[0;0m INFO 08-09 13:54:59 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][24/38] batch_size:2 query_len:256 num_blocks:0 free_mem:23.42 GiB
INFO 08-09 13:55:02 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][25/38] batch_size:2 query_len:128 num_blocks:0 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18140)[0;0m INFO 08-09 13:55:02 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][25/38] batch_size:2 query_len:128 num_blocks:0 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18138)[0;0m INFO 08-09 13:55:02 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][25/38] batch_size:2 query_len:128 num_blocks:0 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18139)[0;0m INFO 08-09 13:55:02 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][25/38] batch_size:2 query_len:128 num_blocks:0 free_mem:23.42 GiB
INFO 08-09 13:55:05 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][26/38] batch_size:1 query_len:4096 num_blocks:0 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18138)[0;0m INFO 08-09 13:55:05 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][26/38] batch_size:1 query_len:4096 num_blocks:0 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18139)[0;0m INFO 08-09 13:55:05 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][26/38] batch_size:1 query_len:4096 num_blocks:0 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18140)[0;0m INFO 08-09 13:55:05 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][26/38] batch_size:1 query_len:4096 num_blocks:0 free_mem:23.42 GiB
INFO 08-09 13:55:07 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][27/38] batch_size:1 query_len:3072 num_blocks:0 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18140)[0;0m INFO 08-09 13:55:07 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][27/38] batch_size:1 query_len:3072 num_blocks:0 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18139)[0;0m INFO 08-09 13:55:07 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][27/38] batch_size:1 query_len:3072 num_blocks:0 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18138)[0;0m INFO 08-09 13:55:07 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][27/38] batch_size:1 query_len:3072 num_blocks:0 free_mem:23.42 GiB
INFO 08-09 13:55:10 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][28/38] batch_size:1 query_len:2304 num_blocks:0 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18140)[0;0m INFO 08-09 13:55:10 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][28/38] batch_size:1 query_len:2304 num_blocks:0 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18138)[0;0m INFO 08-09 13:55:10 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][28/38] batch_size:1 query_len:2304 num_blocks:0 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18139)[0;0m INFO 08-09 13:55:10 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][28/38] batch_size:1 query_len:2304 num_blocks:0 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18140)[0;0m INFO 08-09 13:55:13 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][29/38] batch_size:1 query_len:1792 num_blocks:0 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18139)[0;0m INFO 08-09 13:55:13 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][29/38] batch_size:1 query_len:1792 num_blocks:0 free_mem:23.42 GiB
INFO 08-09 13:55:13 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][29/38] batch_size:1 query_len:1792 num_blocks:0 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18138)[0;0m INFO 08-09 13:55:13 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][29/38] batch_size:1 query_len:1792 num_blocks:0 free_mem:23.42 GiB
INFO 08-09 13:55:16 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][30/38] batch_size:1 query_len:1408 num_blocks:0 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18140)[0;0m INFO 08-09 13:55:16 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][30/38] batch_size:1 query_len:1408 num_blocks:0 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18139)[0;0m INFO 08-09 13:55:16 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][30/38] batch_size:1 query_len:1408 num_blocks:0 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18138)[0;0m INFO 08-09 13:55:16 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][30/38] batch_size:1 query_len:1408 num_blocks:0 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18140)[0;0m INFO 08-09 13:55:19 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][31/38] batch_size:1 query_len:1024 num_blocks:0 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18139)[0;0m INFO 08-09 13:55:19 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][31/38] batch_size:1 query_len:1024 num_blocks:0 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18138)[0;0m INFO 08-09 13:55:19 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][31/38] batch_size:1 query_len:1024 num_blocks:0 free_mem:23.42 GiB
INFO 08-09 13:55:19 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][31/38] batch_size:1 query_len:1024 num_blocks:0 free_mem:23.42 GiB
INFO 08-09 13:55:21 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][32/38] batch_size:1 query_len:896 num_blocks:0 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18140)[0;0m INFO 08-09 13:55:21 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][32/38] batch_size:1 query_len:896 num_blocks:0 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18139)[0;0m INFO 08-09 13:55:21 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][32/38] batch_size:1 query_len:896 num_blocks:0 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18138)[0;0m INFO 08-09 13:55:21 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][32/38] batch_size:1 query_len:896 num_blocks:0 free_mem:23.42 GiB
INFO 08-09 13:55:24 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][33/38] batch_size:1 query_len:768 num_blocks:0 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18139)[0;0m INFO 08-09 13:55:24 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][33/38] batch_size:1 query_len:768 num_blocks:0 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18140)[0;0m INFO 08-09 13:55:24 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][33/38] batch_size:1 query_len:768 num_blocks:0 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18138)[0;0m INFO 08-09 13:55:24 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][33/38] batch_size:1 query_len:768 num_blocks:0 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18139)[0;0m INFO 08-09 13:55:27 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][34/38] batch_size:1 query_len:640 num_blocks:0 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18140)[0;0m INFO 08-09 13:55:27 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][34/38] batch_size:1 query_len:640 num_blocks:0 free_mem:23.42 GiB
INFO 08-09 13:55:27 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][34/38] batch_size:1 query_len:640 num_blocks:0 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18138)[0;0m INFO 08-09 13:55:27 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][34/38] batch_size:1 query_len:640 num_blocks:0 free_mem:23.42 GiB
INFO 08-09 13:55:30 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][35/38] batch_size:1 query_len:512 num_blocks:0 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18139)[0;0m INFO 08-09 13:55:30 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][35/38] batch_size:1 query_len:512 num_blocks:0 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18140)[0;0m INFO 08-09 13:55:30 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][35/38] batch_size:1 query_len:512 num_blocks:0 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18138)[0;0m INFO 08-09 13:55:30 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][35/38] batch_size:1 query_len:512 num_blocks:0 free_mem:23.42 GiB
INFO 08-09 13:55:33 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][36/38] batch_size:1 query_len:384 num_blocks:0 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18138)[0;0m INFO 08-09 13:55:33 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][36/38] batch_size:1 query_len:384 num_blocks:0 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18140)[0;0m INFO 08-09 13:55:33 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][36/38] batch_size:1 query_len:384 num_blocks:0 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18139)[0;0m INFO 08-09 13:55:33 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][36/38] batch_size:1 query_len:384 num_blocks:0 free_mem:23.42 GiB
INFO 08-09 13:55:36 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][37/38] batch_size:1 query_len:256 num_blocks:0 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18140)[0;0m INFO 08-09 13:55:36 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][37/38] batch_size:1 query_len:256 num_blocks:0 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18139)[0;0m INFO 08-09 13:55:36 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][37/38] batch_size:1 query_len:256 num_blocks:0 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18138)[0;0m INFO 08-09 13:55:36 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][37/38] batch_size:1 query_len:256 num_blocks:0 free_mem:23.42 GiB
INFO 08-09 13:55:39 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][38/38] batch_size:1 query_len:128 num_blocks:0 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18140)[0;0m INFO 08-09 13:55:39 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][38/38] batch_size:1 query_len:128 num_blocks:0 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18139)[0;0m INFO 08-09 13:55:39 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][38/38] batch_size:1 query_len:128 num_blocks:0 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18138)[0;0m INFO 08-09 13:55:39 [hpu_model_runner.py:3029] [Warmup][Graph/prompt][38/38] batch_size:1 query_len:128 num_blocks:0 free_mem:23.42 GiB
INFO 08-09 13:55:42 [hpu_model_runner.py:3029] [Warmup][Graph/decode][1/126] batch_size:256 query_len:1 num_blocks:6387 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18140)[0;0m INFO 08-09 13:55:42 [hpu_model_runner.py:3029] [Warmup][Graph/decode][1/126] batch_size:256 query_len:1 num_blocks:6387 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18139)[0;0m INFO 08-09 13:55:42 [hpu_model_runner.py:3029] [Warmup][Graph/decode][1/126] batch_size:256 query_len:1 num_blocks:6387 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18138)[0;0m INFO 08-09 13:55:42 [hpu_model_runner.py:3029] [Warmup][Graph/decode][1/126] batch_size:256 query_len:1 num_blocks:6387 free_mem:23.42 GiB
INFO 08-09 13:55:45 [hpu_model_runner.py:3029] [Warmup][Graph/decode][2/126] batch_size:256 query_len:1 num_blocks:4736 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18140)[0;0m INFO 08-09 13:55:45 [hpu_model_runner.py:3029] [Warmup][Graph/decode][2/126] batch_size:256 query_len:1 num_blocks:4736 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18139)[0;0m INFO 08-09 13:55:45 [hpu_model_runner.py:3029] [Warmup][Graph/decode][2/126] batch_size:256 query_len:1 num_blocks:4736 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18138)[0;0m INFO 08-09 13:55:45 [hpu_model_runner.py:3029] [Warmup][Graph/decode][2/126] batch_size:256 query_len:1 num_blocks:4736 free_mem:23.42 GiB
INFO 08-09 13:55:48 [hpu_model_runner.py:3029] [Warmup][Graph/decode][3/126] batch_size:256 query_len:1 num_blocks:3584 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18140)[0;0m INFO 08-09 13:55:48 [hpu_model_runner.py:3029] [Warmup][Graph/decode][3/126] batch_size:256 query_len:1 num_blocks:3584 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18139)[0;0m INFO 08-09 13:55:48 [hpu_model_runner.py:3029] [Warmup][Graph/decode][3/126] batch_size:256 query_len:1 num_blocks:3584 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18138)[0;0m INFO 08-09 13:55:48 [hpu_model_runner.py:3029] [Warmup][Graph/decode][3/126] batch_size:256 query_len:1 num_blocks:3584 free_mem:23.42 GiB
INFO 08-09 13:55:52 [hpu_model_runner.py:3029] [Warmup][Graph/decode][4/126] batch_size:256 query_len:1 num_blocks:2688 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18140)[0;0m INFO 08-09 13:55:52 [hpu_model_runner.py:3029] [Warmup][Graph/decode][4/126] batch_size:256 query_len:1 num_blocks:2688 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18139)[0;0m INFO 08-09 13:55:52 [hpu_model_runner.py:3029] [Warmup][Graph/decode][4/126] batch_size:256 query_len:1 num_blocks:2688 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18138)[0;0m INFO 08-09 13:55:52 [hpu_model_runner.py:3029] [Warmup][Graph/decode][4/126] batch_size:256 query_len:1 num_blocks:2688 free_mem:23.42 GiB
INFO 08-09 13:55:55 [hpu_model_runner.py:3029] [Warmup][Graph/decode][5/126] batch_size:256 query_len:1 num_blocks:1920 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18138)[0;0m INFO 08-09 13:55:55 [hpu_model_runner.py:3029] [Warmup][Graph/decode][5/126] batch_size:256 query_len:1 num_blocks:1920 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18140)[0;0m INFO 08-09 13:55:55 [hpu_model_runner.py:3029] [Warmup][Graph/decode][5/126] batch_size:256 query_len:1 num_blocks:1920 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18139)[0;0m INFO 08-09 13:55:55 [hpu_model_runner.py:3029] [Warmup][Graph/decode][5/126] batch_size:256 query_len:1 num_blocks:1920 free_mem:23.42 GiB
INFO 08-09 13:55:58 [hpu_model_runner.py:3029] [Warmup][Graph/decode][6/126] batch_size:256 query_len:1 num_blocks:1536 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18140)[0;0m INFO 08-09 13:55:58 [hpu_model_runner.py:3029] [Warmup][Graph/decode][6/126] batch_size:256 query_len:1 num_blocks:1536 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18138)[0;0m INFO 08-09 13:55:58 [hpu_model_runner.py:3029] [Warmup][Graph/decode][6/126] batch_size:256 query_len:1 num_blocks:1536 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18139)[0;0m INFO 08-09 13:55:58 [hpu_model_runner.py:3029] [Warmup][Graph/decode][6/126] batch_size:256 query_len:1 num_blocks:1536 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18140)[0;0m INFO 08-09 13:56:02 [hpu_model_runner.py:3029] [Warmup][Graph/decode][7/126] batch_size:256 query_len:1 num_blocks:1152 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18138)[0;0m INFO 08-09 13:56:02 [hpu_model_runner.py:3029] [Warmup][Graph/decode][7/126] batch_size:256 query_len:1 num_blocks:1152 free_mem:23.42 GiB
INFO 08-09 13:56:02 [hpu_model_runner.py:3029] [Warmup][Graph/decode][7/126] batch_size:256 query_len:1 num_blocks:1152 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18139)[0;0m INFO 08-09 13:56:02 [hpu_model_runner.py:3029] [Warmup][Graph/decode][7/126] batch_size:256 query_len:1 num_blocks:1152 free_mem:23.42 GiB
INFO 08-09 13:56:05 [hpu_model_runner.py:3029] [Warmup][Graph/decode][8/126] batch_size:256 query_len:1 num_blocks:896 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18140)[0;0m INFO 08-09 13:56:05 [hpu_model_runner.py:3029] [Warmup][Graph/decode][8/126] batch_size:256 query_len:1 num_blocks:896 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18139)[0;0m INFO 08-09 13:56:05 [hpu_model_runner.py:3029] [Warmup][Graph/decode][8/126] batch_size:256 query_len:1 num_blocks:896 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18138)[0;0m INFO 08-09 13:56:05 [hpu_model_runner.py:3029] [Warmup][Graph/decode][8/126] batch_size:256 query_len:1 num_blocks:896 free_mem:23.42 GiB
INFO 08-09 13:56:09 [hpu_model_runner.py:3029] [Warmup][Graph/decode][9/126] batch_size:256 query_len:1 num_blocks:768 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18140)[0;0m INFO 08-09 13:56:09 [hpu_model_runner.py:3029] [Warmup][Graph/decode][9/126] batch_size:256 query_len:1 num_blocks:768 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18139)[0;0m INFO 08-09 13:56:09 [hpu_model_runner.py:3029] [Warmup][Graph/decode][9/126] batch_size:256 query_len:1 num_blocks:768 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18138)[0;0m INFO 08-09 13:56:09 [hpu_model_runner.py:3029] [Warmup][Graph/decode][9/126] batch_size:256 query_len:1 num_blocks:768 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18140)[0;0m INFO 08-09 13:56:12 [hpu_model_runner.py:3029] [Warmup][Graph/decode][10/126] batch_size:256 query_len:1 num_blocks:640 free_mem:23.42 GiB
INFO 08-09 13:56:12 [hpu_model_runner.py:3029] [Warmup][Graph/decode][10/126] batch_size:256 query_len:1 num_blocks:640 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18139)[0;0m INFO 08-09 13:56:12 [hpu_model_runner.py:3029] [Warmup][Graph/decode][10/126] batch_size:256 query_len:1 num_blocks:640 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18138)[0;0m INFO 08-09 13:56:12 [hpu_model_runner.py:3029] [Warmup][Graph/decode][10/126] batch_size:256 query_len:1 num_blocks:640 free_mem:23.42 GiB
INFO 08-09 13:56:15 [hpu_model_runner.py:3029] [Warmup][Graph/decode][11/126] batch_size:256 query_len:1 num_blocks:512 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18139)[0;0m INFO 08-09 13:56:15 [hpu_model_runner.py:3029] [Warmup][Graph/decode][11/126] batch_size:256 query_len:1 num_blocks:512 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18138)[0;0m INFO 08-09 13:56:15 [hpu_model_runner.py:3029] [Warmup][Graph/decode][11/126] batch_size:256 query_len:1 num_blocks:512 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18140)[0;0m INFO 08-09 13:56:15 [hpu_model_runner.py:3029] [Warmup][Graph/decode][11/126] batch_size:256 query_len:1 num_blocks:512 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18140)[0;0m INFO 08-09 13:56:19 [hpu_model_runner.py:3029] [Warmup][Graph/decode][12/126] batch_size:256 query_len:1 num_blocks:384 free_mem:23.42 GiB
INFO 08-09 13:56:19 [hpu_model_runner.py:3029] [Warmup][Graph/decode][12/126] batch_size:256 query_len:1 num_blocks:384 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18139)[0;0m INFO 08-09 13:56:19 [hpu_model_runner.py:3029] [Warmup][Graph/decode][12/126] batch_size:256 query_len:1 num_blocks:384 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18138)[0;0m INFO 08-09 13:56:19 [hpu_model_runner.py:3029] [Warmup][Graph/decode][12/126] batch_size:256 query_len:1 num_blocks:384 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18140)[0;0m INFO 08-09 13:56:23 [hpu_model_runner.py:3029] [Warmup][Graph/decode][13/126] batch_size:256 query_len:1 num_blocks:256 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18139)[0;0m INFO 08-09 13:56:23 [hpu_model_runner.py:3029] [Warmup][Graph/decode][13/126] batch_size:256 query_len:1 num_blocks:256 free_mem:23.42 GiB
INFO 08-09 13:56:23 [hpu_model_runner.py:3029] [Warmup][Graph/decode][13/126] batch_size:256 query_len:1 num_blocks:256 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18138)[0;0m INFO 08-09 13:56:23 [hpu_model_runner.py:3029] [Warmup][Graph/decode][13/126] batch_size:256 query_len:1 num_blocks:256 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18140)[0;0m INFO 08-09 13:56:26 [hpu_model_runner.py:3029] [Warmup][Graph/decode][14/126] batch_size:256 query_len:1 num_blocks:128 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18139)[0;0m INFO 08-09 13:56:26 [hpu_model_runner.py:3029] [Warmup][Graph/decode][14/126] batch_size:256 query_len:1 num_blocks:128 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18138)[0;0m INFO 08-09 13:56:26 [hpu_model_runner.py:3029] [Warmup][Graph/decode][14/126] batch_size:256 query_len:1 num_blocks:128 free_mem:23.42 GiB
INFO 08-09 13:56:26 [hpu_model_runner.py:3029] [Warmup][Graph/decode][14/126] batch_size:256 query_len:1 num_blocks:128 free_mem:23.42 GiB
INFO 08-09 13:56:30 [hpu_model_runner.py:3029] [Warmup][Graph/decode][15/126] batch_size:128 query_len:1 num_blocks:6387 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18140)[0;0m INFO 08-09 13:56:30 [hpu_model_runner.py:3029] [Warmup][Graph/decode][15/126] batch_size:128 query_len:1 num_blocks:6387 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18138)[0;0m INFO 08-09 13:56:30 [hpu_model_runner.py:3029] [Warmup][Graph/decode][15/126] batch_size:128 query_len:1 num_blocks:6387 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18139)[0;0m INFO 08-09 13:56:30 [hpu_model_runner.py:3029] [Warmup][Graph/decode][15/126] batch_size:128 query_len:1 num_blocks:6387 free_mem:23.42 GiB
INFO 08-09 13:56:33 [hpu_model_runner.py:3029] [Warmup][Graph/decode][16/126] batch_size:128 query_len:1 num_blocks:4736 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18139)[0;0m INFO 08-09 13:56:33 [hpu_model_runner.py:3029] [Warmup][Graph/decode][16/126] batch_size:128 query_len:1 num_blocks:4736 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18140)[0;0m INFO 08-09 13:56:33 [hpu_model_runner.py:3029] [Warmup][Graph/decode][16/126] batch_size:128 query_len:1 num_blocks:4736 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18138)[0;0m INFO 08-09 13:56:33 [hpu_model_runner.py:3029] [Warmup][Graph/decode][16/126] batch_size:128 query_len:1 num_blocks:4736 free_mem:23.42 GiB
INFO 08-09 13:56:37 [hpu_model_runner.py:3029] [Warmup][Graph/decode][17/126] batch_size:128 query_len:1 num_blocks:3584 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18139)[0;0m INFO 08-09 13:56:37 [hpu_model_runner.py:3029] [Warmup][Graph/decode][17/126] batch_size:128 query_len:1 num_blocks:3584 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18140)[0;0m INFO 08-09 13:56:37 [hpu_model_runner.py:3029] [Warmup][Graph/decode][17/126] batch_size:128 query_len:1 num_blocks:3584 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18138)[0;0m INFO 08-09 13:56:37 [hpu_model_runner.py:3029] [Warmup][Graph/decode][17/126] batch_size:128 query_len:1 num_blocks:3584 free_mem:23.42 GiB
INFO 08-09 13:56:41 [hpu_model_runner.py:3029] [Warmup][Graph/decode][18/126] batch_size:128 query_len:1 num_blocks:2688 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18139)[0;0m INFO 08-09 13:56:41 [hpu_model_runner.py:3029] [Warmup][Graph/decode][18/126] batch_size:128 query_len:1 num_blocks:2688 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18140)[0;0m INFO 08-09 13:56:41 [hpu_model_runner.py:3029] [Warmup][Graph/decode][18/126] batch_size:128 query_len:1 num_blocks:2688 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18138)[0;0m INFO 08-09 13:56:41 [hpu_model_runner.py:3029] [Warmup][Graph/decode][18/126] batch_size:128 query_len:1 num_blocks:2688 free_mem:23.42 GiB
INFO 08-09 13:56:44 [hpu_model_runner.py:3029] [Warmup][Graph/decode][19/126] batch_size:128 query_len:1 num_blocks:1920 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18139)[0;0m INFO 08-09 13:56:44 [hpu_model_runner.py:3029] [Warmup][Graph/decode][19/126] batch_size:128 query_len:1 num_blocks:1920 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18140)[0;0m INFO 08-09 13:56:44 [hpu_model_runner.py:3029] [Warmup][Graph/decode][19/126] batch_size:128 query_len:1 num_blocks:1920 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18138)[0;0m INFO 08-09 13:56:44 [hpu_model_runner.py:3029] [Warmup][Graph/decode][19/126] batch_size:128 query_len:1 num_blocks:1920 free_mem:23.42 GiB
INFO 08-09 13:56:48 [hpu_model_runner.py:3029] [Warmup][Graph/decode][20/126] batch_size:128 query_len:1 num_blocks:1536 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18139)[0;0m INFO 08-09 13:56:48 [hpu_model_runner.py:3029] [Warmup][Graph/decode][20/126] batch_size:128 query_len:1 num_blocks:1536 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18140)[0;0m INFO 08-09 13:56:48 [hpu_model_runner.py:3029] [Warmup][Graph/decode][20/126] batch_size:128 query_len:1 num_blocks:1536 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18138)[0;0m INFO 08-09 13:56:48 [hpu_model_runner.py:3029] [Warmup][Graph/decode][20/126] batch_size:128 query_len:1 num_blocks:1536 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18140)[0;0m INFO 08-09 13:56:52 [hpu_model_runner.py:3029] [Warmup][Graph/decode][21/126] batch_size:128 query_len:1 num_blocks:1152 free_mem:23.42 GiB
INFO 08-09 13:56:52 [hpu_model_runner.py:3029] [Warmup][Graph/decode][21/126] batch_size:128 query_len:1 num_blocks:1152 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18139)[0;0m INFO 08-09 13:56:52 [hpu_model_runner.py:3029] [Warmup][Graph/decode][21/126] batch_size:128 query_len:1 num_blocks:1152 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18138)[0;0m INFO 08-09 13:56:52 [hpu_model_runner.py:3029] [Warmup][Graph/decode][21/126] batch_size:128 query_len:1 num_blocks:1152 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18140)[0;0m INFO 08-09 13:56:56 [hpu_model_runner.py:3029] [Warmup][Graph/decode][22/126] batch_size:128 query_len:1 num_blocks:896 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18138)[0;0m INFO 08-09 13:56:56 [hpu_model_runner.py:3029] [Warmup][Graph/decode][22/126] batch_size:128 query_len:1 num_blocks:896 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18139)[0;0m INFO 08-09 13:56:56 [hpu_model_runner.py:3029] [Warmup][Graph/decode][22/126] batch_size:128 query_len:1 num_blocks:896 free_mem:23.42 GiB
INFO 08-09 13:56:56 [hpu_model_runner.py:3029] [Warmup][Graph/decode][22/126] batch_size:128 query_len:1 num_blocks:896 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18138)[0;0m INFO 08-09 13:57:01 [hpu_model_runner.py:3029] [Warmup][Graph/decode][23/126] batch_size:128 query_len:1 num_blocks:768 free_mem:23.42 GiB
INFO 08-09 13:57:01 [hpu_model_runner.py:3029] [Warmup][Graph/decode][23/126] batch_size:128 query_len:1 num_blocks:768 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18139)[0;0m INFO 08-09 13:57:01 [hpu_model_runner.py:3029] [Warmup][Graph/decode][23/126] batch_size:128 query_len:1 num_blocks:768 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18140)[0;0m INFO 08-09 13:57:01 [hpu_model_runner.py:3029] [Warmup][Graph/decode][23/126] batch_size:128 query_len:1 num_blocks:768 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18138)[0;0m INFO 08-09 13:57:05 [hpu_model_runner.py:3029] [Warmup][Graph/decode][24/126] batch_size:128 query_len:1 num_blocks:640 free_mem:23.42 GiB
INFO 08-09 13:57:05 [hpu_model_runner.py:3029] [Warmup][Graph/decode][24/126] batch_size:128 query_len:1 num_blocks:640 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18140)[0;0m INFO 08-09 13:57:05 [hpu_model_runner.py:3029] [Warmup][Graph/decode][24/126] batch_size:128 query_len:1 num_blocks:640 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18139)[0;0m INFO 08-09 13:57:05 [hpu_model_runner.py:3029] [Warmup][Graph/decode][24/126] batch_size:128 query_len:1 num_blocks:640 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18140)[0;0m INFO 08-09 13:57:09 [hpu_model_runner.py:3029] [Warmup][Graph/decode][25/126] batch_size:128 query_len:1 num_blocks:512 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18138)[0;0m INFO 08-09 13:57:09 [hpu_model_runner.py:3029] [Warmup][Graph/decode][25/126] batch_size:128 query_len:1 num_blocks:512 free_mem:23.42 GiB
INFO 08-09 13:57:09 [hpu_model_runner.py:3029] [Warmup][Graph/decode][25/126] batch_size:128 query_len:1 num_blocks:512 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18139)[0;0m INFO 08-09 13:57:09 [hpu_model_runner.py:3029] [Warmup][Graph/decode][25/126] batch_size:128 query_len:1 num_blocks:512 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18138)[0;0m INFO 08-09 13:57:14 [hpu_model_runner.py:3029] [Warmup][Graph/decode][26/126] batch_size:128 query_len:1 num_blocks:384 free_mem:23.42 GiB
INFO 08-09 13:57:14 [hpu_model_runner.py:3029] [Warmup][Graph/decode][26/126] batch_size:128 query_len:1 num_blocks:384 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18140)[0;0m INFO 08-09 13:57:14 [hpu_model_runner.py:3029] [Warmup][Graph/decode][26/126] batch_size:128 query_len:1 num_blocks:384 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18139)[0;0m INFO 08-09 13:57:14 [hpu_model_runner.py:3029] [Warmup][Graph/decode][26/126] batch_size:128 query_len:1 num_blocks:384 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18138)[0;0m INFO 08-09 13:57:18 [hpu_model_runner.py:3029] [Warmup][Graph/decode][27/126] batch_size:128 query_len:1 num_blocks:256 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18139)[0;0m INFO 08-09 13:57:18 [hpu_model_runner.py:3029] [Warmup][Graph/decode][27/126] batch_size:128 query_len:1 num_blocks:256 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18140)[0;0m INFO 08-09 13:57:18 [hpu_model_runner.py:3029] [Warmup][Graph/decode][27/126] batch_size:128 query_len:1 num_blocks:256 free_mem:23.42 GiB
INFO 08-09 13:57:18 [hpu_model_runner.py:3029] [Warmup][Graph/decode][27/126] batch_size:128 query_len:1 num_blocks:256 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18138)[0;0m INFO 08-09 13:57:22 [hpu_model_runner.py:3029] [Warmup][Graph/decode][28/126] batch_size:128 query_len:1 num_blocks:128 free_mem:23.42 GiB
INFO 08-09 13:57:22 [hpu_model_runner.py:3029] [Warmup][Graph/decode][28/126] batch_size:128 query_len:1 num_blocks:128 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18140)[0;0m INFO 08-09 13:57:22 [hpu_model_runner.py:3029] [Warmup][Graph/decode][28/126] batch_size:128 query_len:1 num_blocks:128 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18139)[0;0m INFO 08-09 13:57:22 [hpu_model_runner.py:3029] [Warmup][Graph/decode][28/126] batch_size:128 query_len:1 num_blocks:128 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18140)[0;0m INFO 08-09 13:57:26 [hpu_model_runner.py:3029] [Warmup][Graph/decode][29/126] batch_size:64 query_len:1 num_blocks:6387 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18138)[0;0m INFO 08-09 13:57:26 [hpu_model_runner.py:3029] [Warmup][Graph/decode][29/126] batch_size:64 query_len:1 num_blocks:6387 free_mem:23.42 GiB
INFO 08-09 13:57:26 [hpu_model_runner.py:3029] [Warmup][Graph/decode][29/126] batch_size:64 query_len:1 num_blocks:6387 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18139)[0;0m INFO 08-09 13:57:26 [hpu_model_runner.py:3029] [Warmup][Graph/decode][29/126] batch_size:64 query_len:1 num_blocks:6387 free_mem:23.42 GiB
INFO 08-09 13:57:31 [hpu_model_runner.py:3029] [Warmup][Graph/decode][30/126] batch_size:64 query_len:1 num_blocks:4736 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18138)[0;0m INFO 08-09 13:57:31 [hpu_model_runner.py:3029] [Warmup][Graph/decode][30/126] batch_size:64 query_len:1 num_blocks:4736 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18140)[0;0m INFO 08-09 13:57:31 [hpu_model_runner.py:3029] [Warmup][Graph/decode][30/126] batch_size:64 query_len:1 num_blocks:4736 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18139)[0;0m INFO 08-09 13:57:31 [hpu_model_runner.py:3029] [Warmup][Graph/decode][30/126] batch_size:64 query_len:1 num_blocks:4736 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18140)[0;0m INFO 08-09 13:57:36 [hpu_model_runner.py:3029] [Warmup][Graph/decode][31/126] batch_size:64 query_len:1 num_blocks:3584 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18138)[0;0m INFO 08-09 13:57:36 [hpu_model_runner.py:3029] [Warmup][Graph/decode][31/126] batch_size:64 query_len:1 num_blocks:3584 free_mem:23.42 GiB
INFO 08-09 13:57:36 [hpu_model_runner.py:3029] [Warmup][Graph/decode][31/126] batch_size:64 query_len:1 num_blocks:3584 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18139)[0;0m INFO 08-09 13:57:36 [hpu_model_runner.py:3029] [Warmup][Graph/decode][31/126] batch_size:64 query_len:1 num_blocks:3584 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18138)[0;0m INFO 08-09 13:57:40 [hpu_model_runner.py:3029] [Warmup][Graph/decode][32/126] batch_size:64 query_len:1 num_blocks:2688 free_mem:23.42 GiB
INFO 08-09 13:57:40 [hpu_model_runner.py:3029] [Warmup][Graph/decode][32/126] batch_size:64 query_len:1 num_blocks:2688 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18140)[0;0m INFO 08-09 13:57:40 [hpu_model_runner.py:3029] [Warmup][Graph/decode][32/126] batch_size:64 query_len:1 num_blocks:2688 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18139)[0;0m INFO 08-09 13:57:40 [hpu_model_runner.py:3029] [Warmup][Graph/decode][32/126] batch_size:64 query_len:1 num_blocks:2688 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18138)[0;0m INFO 08-09 13:57:45 [hpu_model_runner.py:3029] [Warmup][Graph/decode][33/126] batch_size:64 query_len:1 num_blocks:1920 free_mem:23.42 GiB
INFO 08-09 13:57:45 [hpu_model_runner.py:3029] [Warmup][Graph/decode][33/126] batch_size:64 query_len:1 num_blocks:1920 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18140)[0;0m INFO 08-09 13:57:45 [hpu_model_runner.py:3029] [Warmup][Graph/decode][33/126] batch_size:64 query_len:1 num_blocks:1920 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18139)[0;0m INFO 08-09 13:57:45 [hpu_model_runner.py:3029] [Warmup][Graph/decode][33/126] batch_size:64 query_len:1 num_blocks:1920 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18140)[0;0m INFO 08-09 13:57:49 [hpu_model_runner.py:3029] [Warmup][Graph/decode][34/126] batch_size:64 query_len:1 num_blocks:1536 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18138)[0;0m INFO 08-09 13:57:49 [hpu_model_runner.py:3029] [Warmup][Graph/decode][34/126] batch_size:64 query_len:1 num_blocks:1536 free_mem:23.42 GiB
INFO 08-09 13:57:49 [hpu_model_runner.py:3029] [Warmup][Graph/decode][34/126] batch_size:64 query_len:1 num_blocks:1536 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18139)[0;0m INFO 08-09 13:57:49 [hpu_model_runner.py:3029] [Warmup][Graph/decode][34/126] batch_size:64 query_len:1 num_blocks:1536 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18140)[0;0m INFO 08-09 13:57:54 [hpu_model_runner.py:3029] [Warmup][Graph/decode][35/126] batch_size:64 query_len:1 num_blocks:1152 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18138)[0;0m INFO 08-09 13:57:54 [hpu_model_runner.py:3029] [Warmup][Graph/decode][35/126] batch_size:64 query_len:1 num_blocks:1152 free_mem:23.42 GiB
INFO 08-09 13:57:54 [hpu_model_runner.py:3029] [Warmup][Graph/decode][35/126] batch_size:64 query_len:1 num_blocks:1152 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18139)[0;0m INFO 08-09 13:57:54 [hpu_model_runner.py:3029] [Warmup][Graph/decode][35/126] batch_size:64 query_len:1 num_blocks:1152 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18138)[0;0m INFO 08-09 13:57:59 [hpu_model_runner.py:3029] [Warmup][Graph/decode][36/126] batch_size:64 query_len:1 num_blocks:896 free_mem:23.42 GiB
INFO 08-09 13:57:59 [hpu_model_runner.py:3029] [Warmup][Graph/decode][36/126] batch_size:64 query_len:1 num_blocks:896 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18139)[0;0m INFO 08-09 13:57:59 [hpu_model_runner.py:3029] [Warmup][Graph/decode][36/126] batch_size:64 query_len:1 num_blocks:896 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18140)[0;0m INFO 08-09 13:57:59 [hpu_model_runner.py:3029] [Warmup][Graph/decode][36/126] batch_size:64 query_len:1 num_blocks:896 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18138)[0;0m INFO 08-09 13:58:03 [hpu_model_runner.py:3029] [Warmup][Graph/decode][37/126] batch_size:64 query_len:1 num_blocks:768 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18139)[0;0m INFO 08-09 13:58:03 [hpu_model_runner.py:3029] [Warmup][Graph/decode][37/126] batch_size:64 query_len:1 num_blocks:768 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18140)[0;0m INFO 08-09 13:58:03 [hpu_model_runner.py:3029] [Warmup][Graph/decode][37/126] batch_size:64 query_len:1 num_blocks:768 free_mem:23.42 GiB
INFO 08-09 13:58:03 [hpu_model_runner.py:3029] [Warmup][Graph/decode][37/126] batch_size:64 query_len:1 num_blocks:768 free_mem:23.42 GiB
INFO 08-09 13:58:08 [hpu_model_runner.py:3029] [Warmup][Graph/decode][38/126] batch_size:64 query_len:1 num_blocks:640 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18138)[0;0m INFO 08-09 13:58:08 [hpu_model_runner.py:3029] [Warmup][Graph/decode][38/126] batch_size:64 query_len:1 num_blocks:640 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18140)[0;0m INFO 08-09 13:58:08 [hpu_model_runner.py:3029] [Warmup][Graph/decode][38/126] batch_size:64 query_len:1 num_blocks:640 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18139)[0;0m INFO 08-09 13:58:08 [hpu_model_runner.py:3029] [Warmup][Graph/decode][38/126] batch_size:64 query_len:1 num_blocks:640 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18140)[0;0m INFO 08-09 13:58:13 [hpu_model_runner.py:3029] [Warmup][Graph/decode][39/126] batch_size:64 query_len:1 num_blocks:512 free_mem:23.42 GiB
INFO 08-09 13:58:13 [hpu_model_runner.py:3029] [Warmup][Graph/decode][39/126] batch_size:64 query_len:1 num_blocks:512 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18138)[0;0m INFO 08-09 13:58:13 [hpu_model_runner.py:3029] [Warmup][Graph/decode][39/126] batch_size:64 query_len:1 num_blocks:512 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18139)[0;0m INFO 08-09 13:58:13 [hpu_model_runner.py:3029] [Warmup][Graph/decode][39/126] batch_size:64 query_len:1 num_blocks:512 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18140)[0;0m INFO 08-09 13:58:17 [hpu_model_runner.py:3029] [Warmup][Graph/decode][40/126] batch_size:64 query_len:1 num_blocks:384 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18138)[0;0m INFO 08-09 13:58:17 [hpu_model_runner.py:3029] [Warmup][Graph/decode][40/126] batch_size:64 query_len:1 num_blocks:384 free_mem:23.42 GiB
INFO 08-09 13:58:17 [hpu_model_runner.py:3029] [Warmup][Graph/decode][40/126] batch_size:64 query_len:1 num_blocks:384 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18139)[0;0m INFO 08-09 13:58:17 [hpu_model_runner.py:3029] [Warmup][Graph/decode][40/126] batch_size:64 query_len:1 num_blocks:384 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18140)[0;0m INFO 08-09 13:58:22 [hpu_model_runner.py:3029] [Warmup][Graph/decode][41/126] batch_size:64 query_len:1 num_blocks:256 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18138)[0;0m INFO 08-09 13:58:22 [hpu_model_runner.py:3029] [Warmup][Graph/decode][41/126] batch_size:64 query_len:1 num_blocks:256 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18139)[0;0m INFO 08-09 13:58:22 [hpu_model_runner.py:3029] [Warmup][Graph/decode][41/126] batch_size:64 query_len:1 num_blocks:256 free_mem:23.42 GiB
INFO 08-09 13:58:22 [hpu_model_runner.py:3029] [Warmup][Graph/decode][41/126] batch_size:64 query_len:1 num_blocks:256 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18140)[0;0m INFO 08-09 13:58:27 [hpu_model_runner.py:3029] [Warmup][Graph/decode][42/126] batch_size:64 query_len:1 num_blocks:128 free_mem:23.42 GiB
INFO 08-09 13:58:27 [hpu_model_runner.py:3029] [Warmup][Graph/decode][42/126] batch_size:64 query_len:1 num_blocks:128 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18139)[0;0m INFO 08-09 13:58:27 [hpu_model_runner.py:3029] [Warmup][Graph/decode][42/126] batch_size:64 query_len:1 num_blocks:128 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18138)[0;0m INFO 08-09 13:58:27 [hpu_model_runner.py:3029] [Warmup][Graph/decode][42/126] batch_size:64 query_len:1 num_blocks:128 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18138)[0;0m INFO 08-09 13:58:32 [hpu_model_runner.py:3029] [Warmup][Graph/decode][43/126] batch_size:32 query_len:1 num_blocks:6387 free_mem:23.42 GiB
INFO 08-09 13:58:32 [hpu_model_runner.py:3029] [Warmup][Graph/decode][43/126] batch_size:32 query_len:1 num_blocks:6387 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18140)[0;0m INFO 08-09 13:58:32 [hpu_model_runner.py:3029] [Warmup][Graph/decode][43/126] batch_size:32 query_len:1 num_blocks:6387 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18139)[0;0m INFO 08-09 13:58:32 [hpu_model_runner.py:3029] [Warmup][Graph/decode][43/126] batch_size:32 query_len:1 num_blocks:6387 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18140)[0;0m INFO 08-09 13:58:37 [hpu_model_runner.py:3029] [Warmup][Graph/decode][44/126] batch_size:32 query_len:1 num_blocks:4736 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18138)[0;0m INFO 08-09 13:58:37 [hpu_model_runner.py:3029] [Warmup][Graph/decode][44/126] batch_size:32 query_len:1 num_blocks:4736 free_mem:23.42 GiB
INFO 08-09 13:58:37 [hpu_model_runner.py:3029] [Warmup][Graph/decode][44/126] batch_size:32 query_len:1 num_blocks:4736 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18139)[0;0m INFO 08-09 13:58:37 [hpu_model_runner.py:3029] [Warmup][Graph/decode][44/126] batch_size:32 query_len:1 num_blocks:4736 free_mem:23.42 GiB
INFO 08-09 13:58:42 [hpu_model_runner.py:3029] [Warmup][Graph/decode][45/126] batch_size:32 query_len:1 num_blocks:3584 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18138)[0;0m INFO 08-09 13:58:42 [hpu_model_runner.py:3029] [Warmup][Graph/decode][45/126] batch_size:32 query_len:1 num_blocks:3584 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18140)[0;0m INFO 08-09 13:58:42 [hpu_model_runner.py:3029] [Warmup][Graph/decode][45/126] batch_size:32 query_len:1 num_blocks:3584 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18139)[0;0m INFO 08-09 13:58:42 [hpu_model_runner.py:3029] [Warmup][Graph/decode][45/126] batch_size:32 query_len:1 num_blocks:3584 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18140)[0;0m INFO 08-09 13:58:47 [hpu_model_runner.py:3029] [Warmup][Graph/decode][46/126] batch_size:32 query_len:1 num_blocks:2688 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18138)[0;0m INFO 08-09 13:58:47 [hpu_model_runner.py:3029] [Warmup][Graph/decode][46/126] batch_size:32 query_len:1 num_blocks:2688 free_mem:23.42 GiB
INFO 08-09 13:58:47 [hpu_model_runner.py:3029] [Warmup][Graph/decode][46/126] batch_size:32 query_len:1 num_blocks:2688 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18139)[0;0m INFO 08-09 13:58:47 [hpu_model_runner.py:3029] [Warmup][Graph/decode][46/126] batch_size:32 query_len:1 num_blocks:2688 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18140)[0;0m INFO 08-09 13:58:52 [hpu_model_runner.py:3029] [Warmup][Graph/decode][47/126] batch_size:32 query_len:1 num_blocks:1920 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18138)[0;0m INFO 08-09 13:58:52 [hpu_model_runner.py:3029] [Warmup][Graph/decode][47/126] batch_size:32 query_len:1 num_blocks:1920 free_mem:23.42 GiB
INFO 08-09 13:58:52 [hpu_model_runner.py:3029] [Warmup][Graph/decode][47/126] batch_size:32 query_len:1 num_blocks:1920 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18139)[0;0m INFO 08-09 13:58:52 [hpu_model_runner.py:3029] [Warmup][Graph/decode][47/126] batch_size:32 query_len:1 num_blocks:1920 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18140)[0;0m INFO 08-09 13:58:57 [hpu_model_runner.py:3029] [Warmup][Graph/decode][48/126] batch_size:32 query_len:1 num_blocks:1536 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18138)[0;0m INFO 08-09 13:58:57 [hpu_model_runner.py:3029] [Warmup][Graph/decode][48/126] batch_size:32 query_len:1 num_blocks:1536 free_mem:23.42 GiB
INFO 08-09 13:58:57 [hpu_model_runner.py:3029] [Warmup][Graph/decode][48/126] batch_size:32 query_len:1 num_blocks:1536 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18139)[0;0m INFO 08-09 13:58:57 [hpu_model_runner.py:3029] [Warmup][Graph/decode][48/126] batch_size:32 query_len:1 num_blocks:1536 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18140)[0;0m INFO 08-09 13:59:02 [hpu_model_runner.py:3029] [Warmup][Graph/decode][49/126] batch_size:32 query_len:1 num_blocks:1152 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18139)[0;0m INFO 08-09 13:59:02 [hpu_model_runner.py:3029] [Warmup][Graph/decode][49/126] batch_size:32 query_len:1 num_blocks:1152 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18138)[0;0m INFO 08-09 13:59:02 [hpu_model_runner.py:3029] [Warmup][Graph/decode][49/126] batch_size:32 query_len:1 num_blocks:1152 free_mem:23.42 GiB
INFO 08-09 13:59:02 [hpu_model_runner.py:3029] [Warmup][Graph/decode][49/126] batch_size:32 query_len:1 num_blocks:1152 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18138)[0;0m INFO 08-09 13:59:07 [hpu_model_runner.py:3029] [Warmup][Graph/decode][50/126] batch_size:32 query_len:1 num_blocks:896 free_mem:23.42 GiB
INFO 08-09 13:59:07 [hpu_model_runner.py:3029] [Warmup][Graph/decode][50/126] batch_size:32 query_len:1 num_blocks:896 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18140)[0;0m INFO 08-09 13:59:07 [hpu_model_runner.py:3029] [Warmup][Graph/decode][50/126] batch_size:32 query_len:1 num_blocks:896 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18139)[0;0m INFO 08-09 13:59:07 [hpu_model_runner.py:3029] [Warmup][Graph/decode][50/126] batch_size:32 query_len:1 num_blocks:896 free_mem:23.42 GiB
INFO 08-09 13:59:12 [hpu_model_runner.py:3029] [Warmup][Graph/decode][51/126] batch_size:32 query_len:1 num_blocks:768 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18140)[0;0m INFO 08-09 13:59:12 [hpu_model_runner.py:3029] [Warmup][Graph/decode][51/126] batch_size:32 query_len:1 num_blocks:768 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18138)[0;0m INFO 08-09 13:59:12 [hpu_model_runner.py:3029] [Warmup][Graph/decode][51/126] batch_size:32 query_len:1 num_blocks:768 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18139)[0;0m INFO 08-09 13:59:12 [hpu_model_runner.py:3029] [Warmup][Graph/decode][51/126] batch_size:32 query_len:1 num_blocks:768 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18138)[0;0m INFO 08-09 13:59:17 [hpu_model_runner.py:3029] [Warmup][Graph/decode][52/126] batch_size:32 query_len:1 num_blocks:640 free_mem:23.42 GiB
INFO 08-09 13:59:17 [hpu_model_runner.py:3029] [Warmup][Graph/decode][52/126] batch_size:32 query_len:1 num_blocks:640 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18140)[0;0m INFO 08-09 13:59:17 [hpu_model_runner.py:3029] [Warmup][Graph/decode][52/126] batch_size:32 query_len:1 num_blocks:640 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18139)[0;0m INFO 08-09 13:59:17 [hpu_model_runner.py:3029] [Warmup][Graph/decode][52/126] batch_size:32 query_len:1 num_blocks:640 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18138)[0;0m INFO 08-09 13:59:22 [hpu_model_runner.py:3029] [Warmup][Graph/decode][53/126] batch_size:32 query_len:1 num_blocks:512 free_mem:23.42 GiB
INFO 08-09 13:59:22 [hpu_model_runner.py:3029] [Warmup][Graph/decode][53/126] batch_size:32 query_len:1 num_blocks:512 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18140)[0;0m INFO 08-09 13:59:22 [hpu_model_runner.py:3029] [Warmup][Graph/decode][53/126] batch_size:32 query_len:1 num_blocks:512 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18139)[0;0m INFO 08-09 13:59:22 [hpu_model_runner.py:3029] [Warmup][Graph/decode][53/126] batch_size:32 query_len:1 num_blocks:512 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18138)[0;0m INFO 08-09 13:59:27 [hpu_model_runner.py:3029] [Warmup][Graph/decode][54/126] batch_size:32 query_len:1 num_blocks:384 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18139)[0;0m INFO 08-09 13:59:27 [hpu_model_runner.py:3029] [Warmup][Graph/decode][54/126] batch_size:32 query_len:1 num_blocks:384 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18140)[0;0m INFO 08-09 13:59:27 [hpu_model_runner.py:3029] [Warmup][Graph/decode][54/126] batch_size:32 query_len:1 num_blocks:384 free_mem:23.42 GiB
INFO 08-09 13:59:27 [hpu_model_runner.py:3029] [Warmup][Graph/decode][54/126] batch_size:32 query_len:1 num_blocks:384 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18138)[0;0m INFO 08-09 13:59:32 [hpu_model_runner.py:3029] [Warmup][Graph/decode][55/126] batch_size:32 query_len:1 num_blocks:256 free_mem:23.42 GiB
INFO 08-09 13:59:32 [hpu_model_runner.py:3029] [Warmup][Graph/decode][55/126] batch_size:32 query_len:1 num_blocks:256 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18140)[0;0m INFO 08-09 13:59:32 [hpu_model_runner.py:3029] [Warmup][Graph/decode][55/126] batch_size:32 query_len:1 num_blocks:256 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18139)[0;0m INFO 08-09 13:59:32 [hpu_model_runner.py:3029] [Warmup][Graph/decode][55/126] batch_size:32 query_len:1 num_blocks:256 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18139)[0;0m INFO 08-09 13:59:38 [hpu_model_runner.py:3029] [Warmup][Graph/decode][56/126] batch_size:32 query_len:1 num_blocks:128 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18138)[0;0m INFO 08-09 13:59:38 [hpu_model_runner.py:3029] [Warmup][Graph/decode][56/126] batch_size:32 query_len:1 num_blocks:128 free_mem:23.42 GiB
INFO 08-09 13:59:38 [hpu_model_runner.py:3029] [Warmup][Graph/decode][56/126] batch_size:32 query_len:1 num_blocks:128 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18140)[0;0m INFO 08-09 13:59:38 [hpu_model_runner.py:3029] [Warmup][Graph/decode][56/126] batch_size:32 query_len:1 num_blocks:128 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18139)[0;0m INFO 08-09 13:59:43 [hpu_model_runner.py:3029] [Warmup][Graph/decode][57/126] batch_size:16 query_len:1 num_blocks:6387 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18138)[0;0m INFO 08-09 13:59:43 [hpu_model_runner.py:3029] [Warmup][Graph/decode][57/126] batch_size:16 query_len:1 num_blocks:6387 free_mem:23.42 GiB
INFO 08-09 13:59:43 [hpu_model_runner.py:3029] [Warmup][Graph/decode][57/126] batch_size:16 query_len:1 num_blocks:6387 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18140)[0;0m INFO 08-09 13:59:43 [hpu_model_runner.py:3029] [Warmup][Graph/decode][57/126] batch_size:16 query_len:1 num_blocks:6387 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18138)[0;0m INFO 08-09 13:59:49 [hpu_model_runner.py:3029] [Warmup][Graph/decode][58/126] batch_size:16 query_len:1 num_blocks:4736 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18139)[0;0m INFO 08-09 13:59:49 [hpu_model_runner.py:3029] [Warmup][Graph/decode][58/126] batch_size:16 query_len:1 num_blocks:4736 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18140)[0;0m INFO 08-09 13:59:49 [hpu_model_runner.py:3029] [Warmup][Graph/decode][58/126] batch_size:16 query_len:1 num_blocks:4736 free_mem:23.42 GiB
INFO 08-09 13:59:49 [hpu_model_runner.py:3029] [Warmup][Graph/decode][58/126] batch_size:16 query_len:1 num_blocks:4736 free_mem:23.42 GiB
INFO 08-09 13:59:54 [hpu_model_runner.py:3029] [Warmup][Graph/decode][59/126] batch_size:16 query_len:1 num_blocks:3584 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18138)[0;0m INFO 08-09 13:59:54 [hpu_model_runner.py:3029] [Warmup][Graph/decode][59/126] batch_size:16 query_len:1 num_blocks:3584 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18140)[0;0m INFO 08-09 13:59:54 [hpu_model_runner.py:3029] [Warmup][Graph/decode][59/126] batch_size:16 query_len:1 num_blocks:3584 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18139)[0;0m INFO 08-09 13:59:54 [hpu_model_runner.py:3029] [Warmup][Graph/decode][59/126] batch_size:16 query_len:1 num_blocks:3584 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18138)[0;0m INFO 08-09 13:59:59 [hpu_model_runner.py:3029] [Warmup][Graph/decode][60/126] batch_size:16 query_len:1 num_blocks:2688 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18139)[0;0m INFO 08-09 13:59:59 [hpu_model_runner.py:3029] [Warmup][Graph/decode][60/126] batch_size:16 query_len:1 num_blocks:2688 free_mem:23.42 GiB
INFO 08-09 13:59:59 [hpu_model_runner.py:3029] [Warmup][Graph/decode][60/126] batch_size:16 query_len:1 num_blocks:2688 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18140)[0;0m INFO 08-09 13:59:59 [hpu_model_runner.py:3029] [Warmup][Graph/decode][60/126] batch_size:16 query_len:1 num_blocks:2688 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18140)[0;0m INFO 08-09 14:00:05 [hpu_model_runner.py:3029] [Warmup][Graph/decode][61/126] batch_size:16 query_len:1 num_blocks:1920 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18138)[0;0m INFO 08-09 14:00:05 [hpu_model_runner.py:3029] [Warmup][Graph/decode][61/126] batch_size:16 query_len:1 num_blocks:1920 free_mem:23.42 GiB
INFO 08-09 14:00:05 [hpu_model_runner.py:3029] [Warmup][Graph/decode][61/126] batch_size:16 query_len:1 num_blocks:1920 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18139)[0;0m INFO 08-09 14:00:05 [hpu_model_runner.py:3029] [Warmup][Graph/decode][61/126] batch_size:16 query_len:1 num_blocks:1920 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18138)[0;0m INFO 08-09 14:00:10 [hpu_model_runner.py:3029] [Warmup][Graph/decode][62/126] batch_size:16 query_len:1 num_blocks:1536 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18139)[0;0m INFO 08-09 14:00:10 [hpu_model_runner.py:3029] [Warmup][Graph/decode][62/126] batch_size:16 query_len:1 num_blocks:1536 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18140)[0;0m INFO 08-09 14:00:10 [hpu_model_runner.py:3029] [Warmup][Graph/decode][62/126] batch_size:16 query_len:1 num_blocks:1536 free_mem:23.42 GiB
INFO 08-09 14:00:10 [hpu_model_runner.py:3029] [Warmup][Graph/decode][62/126] batch_size:16 query_len:1 num_blocks:1536 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18138)[0;0m INFO 08-09 14:00:16 [hpu_model_runner.py:3029] [Warmup][Graph/decode][63/126] batch_size:16 query_len:1 num_blocks:1152 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18140)[0;0m INFO 08-09 14:00:16 [hpu_model_runner.py:3029] [Warmup][Graph/decode][63/126] batch_size:16 query_len:1 num_blocks:1152 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18139)[0;0m INFO 08-09 14:00:16 [hpu_model_runner.py:3029] [Warmup][Graph/decode][63/126] batch_size:16 query_len:1 num_blocks:1152 free_mem:23.42 GiB
INFO 08-09 14:00:16 [hpu_model_runner.py:3029] [Warmup][Graph/decode][63/126] batch_size:16 query_len:1 num_blocks:1152 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18138)[0;0m INFO 08-09 14:00:21 [hpu_model_runner.py:3029] [Warmup][Graph/decode][64/126] batch_size:16 query_len:1 num_blocks:896 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18140)[0;0m INFO 08-09 14:00:21 [hpu_model_runner.py:3029] [Warmup][Graph/decode][64/126] batch_size:16 query_len:1 num_blocks:896 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18139)[0;0m INFO 08-09 14:00:21 [hpu_model_runner.py:3029] [Warmup][Graph/decode][64/126] batch_size:16 query_len:1 num_blocks:896 free_mem:23.42 GiB
INFO 08-09 14:00:21 [hpu_model_runner.py:3029] [Warmup][Graph/decode][64/126] batch_size:16 query_len:1 num_blocks:896 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18138)[0;0m INFO 08-09 14:00:27 [hpu_model_runner.py:3029] [Warmup][Graph/decode][65/126] batch_size:16 query_len:1 num_blocks:768 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18139)[0;0m INFO 08-09 14:00:27 [hpu_model_runner.py:3029] [Warmup][Graph/decode][65/126] batch_size:16 query_len:1 num_blocks:768 free_mem:23.42 GiB
INFO 08-09 14:00:27 [hpu_model_runner.py:3029] [Warmup][Graph/decode][65/126] batch_size:16 query_len:1 num_blocks:768 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18140)[0;0m INFO 08-09 14:00:27 [hpu_model_runner.py:3029] [Warmup][Graph/decode][65/126] batch_size:16 query_len:1 num_blocks:768 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18140)[0;0m INFO 08-09 14:00:32 [hpu_model_runner.py:3029] [Warmup][Graph/decode][66/126] batch_size:16 query_len:1 num_blocks:640 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18138)[0;0m INFO 08-09 14:00:32 [hpu_model_runner.py:3029] [Warmup][Graph/decode][66/126] batch_size:16 query_len:1 num_blocks:640 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18139)[0;0m INFO 08-09 14:00:32 [hpu_model_runner.py:3029] [Warmup][Graph/decode][66/126] batch_size:16 query_len:1 num_blocks:640 free_mem:23.42 GiB
INFO 08-09 14:00:32 [hpu_model_runner.py:3029] [Warmup][Graph/decode][66/126] batch_size:16 query_len:1 num_blocks:640 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18140)[0;0m INFO 08-09 14:00:38 [hpu_model_runner.py:3029] [Warmup][Graph/decode][67/126] batch_size:16 query_len:1 num_blocks:512 free_mem:23.42 GiB
INFO 08-09 14:00:38 [hpu_model_runner.py:3029] [Warmup][Graph/decode][67/126] batch_size:16 query_len:1 num_blocks:512 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18138)[0;0m INFO 08-09 14:00:38 [hpu_model_runner.py:3029] [Warmup][Graph/decode][67/126] batch_size:16 query_len:1 num_blocks:512 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18139)[0;0m INFO 08-09 14:00:38 [hpu_model_runner.py:3029] [Warmup][Graph/decode][67/126] batch_size:16 query_len:1 num_blocks:512 free_mem:23.42 GiB
INFO 08-09 14:00:43 [hpu_model_runner.py:3029] [Warmup][Graph/decode][68/126] batch_size:16 query_len:1 num_blocks:384 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18138)[0;0m INFO 08-09 14:00:43 [hpu_model_runner.py:3029] [Warmup][Graph/decode][68/126] batch_size:16 query_len:1 num_blocks:384 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18140)[0;0m INFO 08-09 14:00:43 [hpu_model_runner.py:3029] [Warmup][Graph/decode][68/126] batch_size:16 query_len:1 num_blocks:384 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18139)[0;0m INFO 08-09 14:00:43 [hpu_model_runner.py:3029] [Warmup][Graph/decode][68/126] batch_size:16 query_len:1 num_blocks:384 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18140)[0;0m INFO 08-09 14:00:49 [hpu_model_runner.py:3029] [Warmup][Graph/decode][69/126] batch_size:16 query_len:1 num_blocks:256 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18139)[0;0m INFO 08-09 14:00:49 [hpu_model_runner.py:3029] [Warmup][Graph/decode][69/126] batch_size:16 query_len:1 num_blocks:256 free_mem:23.42 GiB
INFO 08-09 14:00:49 [hpu_model_runner.py:3029] [Warmup][Graph/decode][69/126] batch_size:16 query_len:1 num_blocks:256 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18138)[0;0m INFO 08-09 14:00:49 [hpu_model_runner.py:3029] [Warmup][Graph/decode][69/126] batch_size:16 query_len:1 num_blocks:256 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18140)[0;0m INFO 08-09 14:00:54 [hpu_model_runner.py:3029] [Warmup][Graph/decode][70/126] batch_size:16 query_len:1 num_blocks:128 free_mem:23.42 GiB
INFO 08-09 14:00:54 [hpu_model_runner.py:3029] [Warmup][Graph/decode][70/126] batch_size:16 query_len:1 num_blocks:128 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18139)[0;0m INFO 08-09 14:00:54 [hpu_model_runner.py:3029] [Warmup][Graph/decode][70/126] batch_size:16 query_len:1 num_blocks:128 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18138)[0;0m INFO 08-09 14:00:54 [hpu_model_runner.py:3029] [Warmup][Graph/decode][70/126] batch_size:16 query_len:1 num_blocks:128 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18140)[0;0m INFO 08-09 14:01:00 [hpu_model_runner.py:3029] [Warmup][Graph/decode][71/126] batch_size:8 query_len:1 num_blocks:6387 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18139)[0;0m INFO 08-09 14:01:00 [hpu_model_runner.py:3029] [Warmup][Graph/decode][71/126] batch_size:8 query_len:1 num_blocks:6387 free_mem:23.42 GiB
INFO 08-09 14:01:00 [hpu_model_runner.py:3029] [Warmup][Graph/decode][71/126] batch_size:8 query_len:1 num_blocks:6387 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18138)[0;0m INFO 08-09 14:01:00 [hpu_model_runner.py:3029] [Warmup][Graph/decode][71/126] batch_size:8 query_len:1 num_blocks:6387 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18140)[0;0m INFO 08-09 14:01:06 [hpu_model_runner.py:3029] [Warmup][Graph/decode][72/126] batch_size:8 query_len:1 num_blocks:4736 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18139)[0;0m INFO 08-09 14:01:06 [hpu_model_runner.py:3029] [Warmup][Graph/decode][72/126] batch_size:8 query_len:1 num_blocks:4736 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18138)[0;0m INFO 08-09 14:01:06 [hpu_model_runner.py:3029] [Warmup][Graph/decode][72/126] batch_size:8 query_len:1 num_blocks:4736 free_mem:23.42 GiB
INFO 08-09 14:01:06 [hpu_model_runner.py:3029] [Warmup][Graph/decode][72/126] batch_size:8 query_len:1 num_blocks:4736 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18140)[0;0m INFO 08-09 14:01:11 [hpu_model_runner.py:3029] [Warmup][Graph/decode][73/126] batch_size:8 query_len:1 num_blocks:3584 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18138)[0;0m INFO 08-09 14:01:11 [hpu_model_runner.py:3029] [Warmup][Graph/decode][73/126] batch_size:8 query_len:1 num_blocks:3584 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18139)[0;0m INFO 08-09 14:01:11 [hpu_model_runner.py:3029] [Warmup][Graph/decode][73/126] batch_size:8 query_len:1 num_blocks:3584 free_mem:23.42 GiB
INFO 08-09 14:01:11 [hpu_model_runner.py:3029] [Warmup][Graph/decode][73/126] batch_size:8 query_len:1 num_blocks:3584 free_mem:23.42 GiB
INFO 08-09 14:01:17 [hpu_model_runner.py:3029] [Warmup][Graph/decode][74/126] batch_size:8 query_len:1 num_blocks:2688 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18138)[0;0m INFO 08-09 14:01:17 [hpu_model_runner.py:3029] [Warmup][Graph/decode][74/126] batch_size:8 query_len:1 num_blocks:2688 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18140)[0;0m INFO 08-09 14:01:17 [hpu_model_runner.py:3029] [Warmup][Graph/decode][74/126] batch_size:8 query_len:1 num_blocks:2688 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18139)[0;0m INFO 08-09 14:01:17 [hpu_model_runner.py:3029] [Warmup][Graph/decode][74/126] batch_size:8 query_len:1 num_blocks:2688 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18139)[0;0m INFO 08-09 14:01:23 [hpu_model_runner.py:3029] [Warmup][Graph/decode][75/126] batch_size:8 query_len:1 num_blocks:1920 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18140)[0;0m INFO 08-09 14:01:23 [hpu_model_runner.py:3029] [Warmup][Graph/decode][75/126] batch_size:8 query_len:1 num_blocks:1920 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18138)[0;0m INFO 08-09 14:01:23 [hpu_model_runner.py:3029] [Warmup][Graph/decode][75/126] batch_size:8 query_len:1 num_blocks:1920 free_mem:23.42 GiB
INFO 08-09 14:01:23 [hpu_model_runner.py:3029] [Warmup][Graph/decode][75/126] batch_size:8 query_len:1 num_blocks:1920 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18140)[0;0m INFO 08-09 14:01:29 [hpu_model_runner.py:3029] [Warmup][Graph/decode][76/126] batch_size:8 query_len:1 num_blocks:1536 free_mem:23.42 GiB
INFO 08-09 14:01:29 [hpu_model_runner.py:3029] [Warmup][Graph/decode][76/126] batch_size:8 query_len:1 num_blocks:1536 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18138)[0;0m INFO 08-09 14:01:29 [hpu_model_runner.py:3029] [Warmup][Graph/decode][76/126] batch_size:8 query_len:1 num_blocks:1536 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18139)[0;0m INFO 08-09 14:01:29 [hpu_model_runner.py:3029] [Warmup][Graph/decode][76/126] batch_size:8 query_len:1 num_blocks:1536 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18140)[0;0m INFO 08-09 14:01:35 [hpu_model_runner.py:3029] [Warmup][Graph/decode][77/126] batch_size:8 query_len:1 num_blocks:1152 free_mem:23.42 GiB
INFO 08-09 14:01:35 [hpu_model_runner.py:3029] [Warmup][Graph/decode][77/126] batch_size:8 query_len:1 num_blocks:1152 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18139)[0;0m INFO 08-09 14:01:35 [hpu_model_runner.py:3029] [Warmup][Graph/decode][77/126] batch_size:8 query_len:1 num_blocks:1152 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18138)[0;0m INFO 08-09 14:01:35 [hpu_model_runner.py:3029] [Warmup][Graph/decode][77/126] batch_size:8 query_len:1 num_blocks:1152 free_mem:23.42 GiB
INFO 08-09 14:01:41 [hpu_model_runner.py:3029] [Warmup][Graph/decode][78/126] batch_size:8 query_len:1 num_blocks:896 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18140)[0;0m INFO 08-09 14:01:41 [hpu_model_runner.py:3029] [Warmup][Graph/decode][78/126] batch_size:8 query_len:1 num_blocks:896 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18139)[0;0m INFO 08-09 14:01:41 [hpu_model_runner.py:3029] [Warmup][Graph/decode][78/126] batch_size:8 query_len:1 num_blocks:896 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18138)[0;0m INFO 08-09 14:01:41 [hpu_model_runner.py:3029] [Warmup][Graph/decode][78/126] batch_size:8 query_len:1 num_blocks:896 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18140)[0;0m INFO 08-09 14:01:47 [hpu_model_runner.py:3029] [Warmup][Graph/decode][79/126] batch_size:8 query_len:1 num_blocks:768 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18138)[0;0m INFO 08-09 14:01:47 [hpu_model_runner.py:3029] [Warmup][Graph/decode][79/126] batch_size:8 query_len:1 num_blocks:768 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18139)[0;0m INFO 08-09 14:01:47 [hpu_model_runner.py:3029] [Warmup][Graph/decode][79/126] batch_size:8 query_len:1 num_blocks:768 free_mem:23.42 GiB
INFO 08-09 14:01:47 [hpu_model_runner.py:3029] [Warmup][Graph/decode][79/126] batch_size:8 query_len:1 num_blocks:768 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18140)[0;0m INFO 08-09 14:01:53 [hpu_model_runner.py:3029] [Warmup][Graph/decode][80/126] batch_size:8 query_len:1 num_blocks:640 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18138)[0;0m INFO 08-09 14:01:53 [hpu_model_runner.py:3029] [Warmup][Graph/decode][80/126] batch_size:8 query_len:1 num_blocks:640 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18139)[0;0m INFO 08-09 14:01:53 [hpu_model_runner.py:3029] [Warmup][Graph/decode][80/126] batch_size:8 query_len:1 num_blocks:640 free_mem:23.42 GiB
INFO 08-09 14:01:53 [hpu_model_runner.py:3029] [Warmup][Graph/decode][80/126] batch_size:8 query_len:1 num_blocks:640 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18138)[0;0m INFO 08-09 14:01:58 [hpu_model_runner.py:3029] [Warmup][Graph/decode][81/126] batch_size:8 query_len:1 num_blocks:512 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18140)[0;0m INFO 08-09 14:01:58 [hpu_model_runner.py:3029] [Warmup][Graph/decode][81/126] batch_size:8 query_len:1 num_blocks:512 free_mem:23.42 GiB
INFO 08-09 14:01:58 [hpu_model_runner.py:3029] [Warmup][Graph/decode][81/126] batch_size:8 query_len:1 num_blocks:512 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18139)[0;0m INFO 08-09 14:01:58 [hpu_model_runner.py:3029] [Warmup][Graph/decode][81/126] batch_size:8 query_len:1 num_blocks:512 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18140)[0;0m INFO 08-09 14:02:04 [hpu_model_runner.py:3029] [Warmup][Graph/decode][82/126] batch_size:8 query_len:1 num_blocks:384 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18138)[0;0m INFO 08-09 14:02:04 [hpu_model_runner.py:3029] [Warmup][Graph/decode][82/126] batch_size:8 query_len:1 num_blocks:384 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18139)[0;0m INFO 08-09 14:02:04 [hpu_model_runner.py:3029] [Warmup][Graph/decode][82/126] batch_size:8 query_len:1 num_blocks:384 free_mem:23.42 GiB
INFO 08-09 14:02:04 [hpu_model_runner.py:3029] [Warmup][Graph/decode][82/126] batch_size:8 query_len:1 num_blocks:384 free_mem:23.42 GiB
INFO 08-09 14:02:10 [hpu_model_runner.py:3029] [Warmup][Graph/decode][83/126] batch_size:8 query_len:1 num_blocks:256 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18140)[0;0m INFO 08-09 14:02:10 [hpu_model_runner.py:3029] [Warmup][Graph/decode][83/126] batch_size:8 query_len:1 num_blocks:256 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18139)[0;0m INFO 08-09 14:02:10 [hpu_model_runner.py:3029] [Warmup][Graph/decode][83/126] batch_size:8 query_len:1 num_blocks:256 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18138)[0;0m INFO 08-09 14:02:10 [hpu_model_runner.py:3029] [Warmup][Graph/decode][83/126] batch_size:8 query_len:1 num_blocks:256 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18138)[0;0m INFO 08-09 14:02:16 [hpu_model_runner.py:3029] [Warmup][Graph/decode][84/126] batch_size:8 query_len:1 num_blocks:128 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18139)[0;0m INFO 08-09 14:02:16 [hpu_model_runner.py:3029] [Warmup][Graph/decode][84/126] batch_size:8 query_len:1 num_blocks:128 free_mem:23.42 GiB
INFO 08-09 14:02:16 [hpu_model_runner.py:3029] [Warmup][Graph/decode][84/126] batch_size:8 query_len:1 num_blocks:128 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18140)[0;0m INFO 08-09 14:02:16 [hpu_model_runner.py:3029] [Warmup][Graph/decode][84/126] batch_size:8 query_len:1 num_blocks:128 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18138)[0;0m INFO 08-09 14:02:22 [hpu_model_runner.py:3029] [Warmup][Graph/decode][85/126] batch_size:4 query_len:1 num_blocks:6387 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18139)[0;0m INFO 08-09 14:02:22 [hpu_model_runner.py:3029] [Warmup][Graph/decode][85/126] batch_size:4 query_len:1 num_blocks:6387 free_mem:23.42 GiB
INFO 08-09 14:02:22 [hpu_model_runner.py:3029] [Warmup][Graph/decode][85/126] batch_size:4 query_len:1 num_blocks:6387 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18140)[0;0m INFO 08-09 14:02:22 [hpu_model_runner.py:3029] [Warmup][Graph/decode][85/126] batch_size:4 query_len:1 num_blocks:6387 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18140)[0;0m INFO 08-09 14:02:28 [hpu_model_runner.py:3029] [Warmup][Graph/decode][86/126] batch_size:4 query_len:1 num_blocks:4736 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18138)[0;0m INFO 08-09 14:02:28 [hpu_model_runner.py:3029] [Warmup][Graph/decode][86/126] batch_size:4 query_len:1 num_blocks:4736 free_mem:23.42 GiB
INFO 08-09 14:02:28 [hpu_model_runner.py:3029] [Warmup][Graph/decode][86/126] batch_size:4 query_len:1 num_blocks:4736 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18139)[0;0m INFO 08-09 14:02:28 [hpu_model_runner.py:3029] [Warmup][Graph/decode][86/126] batch_size:4 query_len:1 num_blocks:4736 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18139)[0;0m INFO 08-09 14:02:34 [hpu_model_runner.py:3029] [Warmup][Graph/decode][87/126] batch_size:4 query_len:1 num_blocks:3584 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18140)[0;0m INFO 08-09 14:02:34 [hpu_model_runner.py:3029] [Warmup][Graph/decode][87/126] batch_size:4 query_len:1 num_blocks:3584 free_mem:23.42 GiB
INFO 08-09 14:02:34 [hpu_model_runner.py:3029] [Warmup][Graph/decode][87/126] batch_size:4 query_len:1 num_blocks:3584 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18138)[0;0m INFO 08-09 14:02:34 [hpu_model_runner.py:3029] [Warmup][Graph/decode][87/126] batch_size:4 query_len:1 num_blocks:3584 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18138)[0;0m INFO 08-09 14:02:41 [hpu_model_runner.py:3029] [Warmup][Graph/decode][88/126] batch_size:4 query_len:1 num_blocks:2688 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18139)[0;0m INFO 08-09 14:02:41 [hpu_model_runner.py:3029] [Warmup][Graph/decode][88/126] batch_size:4 query_len:1 num_blocks:2688 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18140)[0;0m INFO 08-09 14:02:41 [hpu_model_runner.py:3029] [Warmup][Graph/decode][88/126] batch_size:4 query_len:1 num_blocks:2688 free_mem:23.42 GiB
INFO 08-09 14:02:41 [hpu_model_runner.py:3029] [Warmup][Graph/decode][88/126] batch_size:4 query_len:1 num_blocks:2688 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18140)[0;0m INFO 08-09 14:02:47 [hpu_model_runner.py:3029] [Warmup][Graph/decode][89/126] batch_size:4 query_len:1 num_blocks:1920 free_mem:23.42 GiB
INFO 08-09 14:02:47 [hpu_model_runner.py:3029] [Warmup][Graph/decode][89/126] batch_size:4 query_len:1 num_blocks:1920 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18138)[0;0m INFO 08-09 14:02:47 [hpu_model_runner.py:3029] [Warmup][Graph/decode][89/126] batch_size:4 query_len:1 num_blocks:1920 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18139)[0;0m INFO 08-09 14:02:47 [hpu_model_runner.py:3029] [Warmup][Graph/decode][89/126] batch_size:4 query_len:1 num_blocks:1920 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18140)[0;0m INFO 08-09 14:02:53 [hpu_model_runner.py:3029] [Warmup][Graph/decode][90/126] batch_size:4 query_len:1 num_blocks:1536 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18138)[0;0m INFO 08-09 14:02:53 [hpu_model_runner.py:3029] [Warmup][Graph/decode][90/126] batch_size:4 query_len:1 num_blocks:1536 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18139)[0;0m INFO 08-09 14:02:53 [hpu_model_runner.py:3029] [Warmup][Graph/decode][90/126] batch_size:4 query_len:1 num_blocks:1536 free_mem:23.42 GiB
INFO 08-09 14:02:53 [hpu_model_runner.py:3029] [Warmup][Graph/decode][90/126] batch_size:4 query_len:1 num_blocks:1536 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18138)[0;0m INFO 08-09 14:02:59 [hpu_model_runner.py:3029] [Warmup][Graph/decode][91/126] batch_size:4 query_len:1 num_blocks:1152 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18139)[0;0m INFO 08-09 14:02:59 [hpu_model_runner.py:3029] [Warmup][Graph/decode][91/126] batch_size:4 query_len:1 num_blocks:1152 free_mem:23.42 GiB
INFO 08-09 14:02:59 [hpu_model_runner.py:3029] [Warmup][Graph/decode][91/126] batch_size:4 query_len:1 num_blocks:1152 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18140)[0;0m INFO 08-09 14:02:59 [hpu_model_runner.py:3029] [Warmup][Graph/decode][91/126] batch_size:4 query_len:1 num_blocks:1152 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18140)[0;0m INFO 08-09 14:03:05 [hpu_model_runner.py:3029] [Warmup][Graph/decode][92/126] batch_size:4 query_len:1 num_blocks:896 free_mem:23.42 GiB
INFO 08-09 14:03:05 [hpu_model_runner.py:3029] [Warmup][Graph/decode][92/126] batch_size:4 query_len:1 num_blocks:896 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18138)[0;0m INFO 08-09 14:03:05 [hpu_model_runner.py:3029] [Warmup][Graph/decode][92/126] batch_size:4 query_len:1 num_blocks:896 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18139)[0;0m INFO 08-09 14:03:05 [hpu_model_runner.py:3029] [Warmup][Graph/decode][92/126] batch_size:4 query_len:1 num_blocks:896 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18138)[0;0m INFO 08-09 14:03:11 [hpu_model_runner.py:3029] [Warmup][Graph/decode][93/126] batch_size:4 query_len:1 num_blocks:768 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18139)[0;0m INFO 08-09 14:03:11 [hpu_model_runner.py:3029] [Warmup][Graph/decode][93/126] batch_size:4 query_len:1 num_blocks:768 free_mem:23.42 GiB
INFO 08-09 14:03:11 [hpu_model_runner.py:3029] [Warmup][Graph/decode][93/126] batch_size:4 query_len:1 num_blocks:768 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18140)[0;0m INFO 08-09 14:03:11 [hpu_model_runner.py:3029] [Warmup][Graph/decode][93/126] batch_size:4 query_len:1 num_blocks:768 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18140)[0;0m INFO 08-09 14:03:17 [hpu_model_runner.py:3029] [Warmup][Graph/decode][94/126] batch_size:4 query_len:1 num_blocks:640 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18138)[0;0m INFO 08-09 14:03:17 [hpu_model_runner.py:3029] [Warmup][Graph/decode][94/126] batch_size:4 query_len:1 num_blocks:640 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18139)[0;0m INFO 08-09 14:03:17 [hpu_model_runner.py:3029] [Warmup][Graph/decode][94/126] batch_size:4 query_len:1 num_blocks:640 free_mem:23.42 GiB
INFO 08-09 14:03:17 [hpu_model_runner.py:3029] [Warmup][Graph/decode][94/126] batch_size:4 query_len:1 num_blocks:640 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18138)[0;0m INFO 08-09 14:03:24 [hpu_model_runner.py:3029] [Warmup][Graph/decode][95/126] batch_size:4 query_len:1 num_blocks:512 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18140)[0;0m INFO 08-09 14:03:24 [hpu_model_runner.py:3029] [Warmup][Graph/decode][95/126] batch_size:4 query_len:1 num_blocks:512 free_mem:23.42 GiB
INFO 08-09 14:03:24 [hpu_model_runner.py:3029] [Warmup][Graph/decode][95/126] batch_size:4 query_len:1 num_blocks:512 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18139)[0;0m INFO 08-09 14:03:24 [hpu_model_runner.py:3029] [Warmup][Graph/decode][95/126] batch_size:4 query_len:1 num_blocks:512 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18138)[0;0m INFO 08-09 14:03:30 [hpu_model_runner.py:3029] [Warmup][Graph/decode][96/126] batch_size:4 query_len:1 num_blocks:384 free_mem:23.42 GiB
INFO 08-09 14:03:30 [hpu_model_runner.py:3029] [Warmup][Graph/decode][96/126] batch_size:4 query_len:1 num_blocks:384 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18140)[0;0m INFO 08-09 14:03:30 [hpu_model_runner.py:3029] [Warmup][Graph/decode][96/126] batch_size:4 query_len:1 num_blocks:384 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18139)[0;0m INFO 08-09 14:03:30 [hpu_model_runner.py:3029] [Warmup][Graph/decode][96/126] batch_size:4 query_len:1 num_blocks:384 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18138)[0;0m INFO 08-09 14:03:36 [hpu_model_runner.py:3029] [Warmup][Graph/decode][97/126] batch_size:4 query_len:1 num_blocks:256 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18140)[0;0m INFO 08-09 14:03:36 [hpu_model_runner.py:3029] [Warmup][Graph/decode][97/126] batch_size:4 query_len:1 num_blocks:256 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18139)[0;0m INFO 08-09 14:03:36 [hpu_model_runner.py:3029] [Warmup][Graph/decode][97/126] batch_size:4 query_len:1 num_blocks:256 free_mem:23.42 GiB
INFO 08-09 14:03:36 [hpu_model_runner.py:3029] [Warmup][Graph/decode][97/126] batch_size:4 query_len:1 num_blocks:256 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18138)[0;0m INFO 08-09 14:03:43 [hpu_model_runner.py:3029] [Warmup][Graph/decode][98/126] batch_size:4 query_len:1 num_blocks:128 free_mem:23.42 GiB
INFO 08-09 14:03:43 [hpu_model_runner.py:3029] [Warmup][Graph/decode][98/126] batch_size:4 query_len:1 num_blocks:128 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18140)[0;0m INFO 08-09 14:03:43 [hpu_model_runner.py:3029] [Warmup][Graph/decode][98/126] batch_size:4 query_len:1 num_blocks:128 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18139)[0;0m INFO 08-09 14:03:43 [hpu_model_runner.py:3029] [Warmup][Graph/decode][98/126] batch_size:4 query_len:1 num_blocks:128 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18140)[0;0m INFO 08-09 14:03:49 [hpu_model_runner.py:3029] [Warmup][Graph/decode][99/126] batch_size:2 query_len:1 num_blocks:6387 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18138)[0;0m INFO 08-09 14:03:49 [hpu_model_runner.py:3029] [Warmup][Graph/decode][99/126] batch_size:2 query_len:1 num_blocks:6387 free_mem:23.42 GiB
INFO 08-09 14:03:49 [hpu_model_runner.py:3029] [Warmup][Graph/decode][99/126] batch_size:2 query_len:1 num_blocks:6387 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18139)[0;0m INFO 08-09 14:03:49 [hpu_model_runner.py:3029] [Warmup][Graph/decode][99/126] batch_size:2 query_len:1 num_blocks:6387 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18138)[0;0m INFO 08-09 14:03:55 [hpu_model_runner.py:3029] [Warmup][Graph/decode][100/126] batch_size:2 query_len:1 num_blocks:4736 free_mem:23.42 GiB
INFO 08-09 14:03:55 [hpu_model_runner.py:3029] [Warmup][Graph/decode][100/126] batch_size:2 query_len:1 num_blocks:4736 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18140)[0;0m INFO 08-09 14:03:55 [hpu_model_runner.py:3029] [Warmup][Graph/decode][100/126] batch_size:2 query_len:1 num_blocks:4736 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18139)[0;0m INFO 08-09 14:03:55 [hpu_model_runner.py:3029] [Warmup][Graph/decode][100/126] batch_size:2 query_len:1 num_blocks:4736 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18138)[0;0m INFO 08-09 14:04:02 [hpu_model_runner.py:3029] [Warmup][Graph/decode][101/126] batch_size:2 query_len:1 num_blocks:3584 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18139)[0;0m INFO 08-09 14:04:02 [hpu_model_runner.py:3029] [Warmup][Graph/decode][101/126] batch_size:2 query_len:1 num_blocks:3584 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18140)[0;0m INFO 08-09 14:04:02 [hpu_model_runner.py:3029] [Warmup][Graph/decode][101/126] batch_size:2 query_len:1 num_blocks:3584 free_mem:23.42 GiB
INFO 08-09 14:04:02 [hpu_model_runner.py:3029] [Warmup][Graph/decode][101/126] batch_size:2 query_len:1 num_blocks:3584 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18139)[0;0m INFO 08-09 14:04:08 [hpu_model_runner.py:3029] [Warmup][Graph/decode][102/126] batch_size:2 query_len:1 num_blocks:2688 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18140)[0;0m INFO 08-09 14:04:08 [hpu_model_runner.py:3029] [Warmup][Graph/decode][102/126] batch_size:2 query_len:1 num_blocks:2688 free_mem:23.42 GiB
INFO 08-09 14:04:08 [hpu_model_runner.py:3029] [Warmup][Graph/decode][102/126] batch_size:2 query_len:1 num_blocks:2688 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18138)[0;0m INFO 08-09 14:04:08 [hpu_model_runner.py:3029] [Warmup][Graph/decode][102/126] batch_size:2 query_len:1 num_blocks:2688 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18140)[0;0m INFO 08-09 14:04:15 [hpu_model_runner.py:3029] [Warmup][Graph/decode][103/126] batch_size:2 query_len:1 num_blocks:1920 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18138)[0;0m INFO 08-09 14:04:15 [hpu_model_runner.py:3029] [Warmup][Graph/decode][103/126] batch_size:2 query_len:1 num_blocks:1920 free_mem:23.42 GiB
INFO 08-09 14:04:15 [hpu_model_runner.py:3029] [Warmup][Graph/decode][103/126] batch_size:2 query_len:1 num_blocks:1920 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18139)[0;0m INFO 08-09 14:04:15 [hpu_model_runner.py:3029] [Warmup][Graph/decode][103/126] batch_size:2 query_len:1 num_blocks:1920 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18140)[0;0m INFO 08-09 14:04:21 [hpu_model_runner.py:3029] [Warmup][Graph/decode][104/126] batch_size:2 query_len:1 num_blocks:1536 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18138)[0;0m INFO 08-09 14:04:21 [hpu_model_runner.py:3029] [Warmup][Graph/decode][104/126] batch_size:2 query_len:1 num_blocks:1536 free_mem:23.42 GiB
INFO 08-09 14:04:21 [hpu_model_runner.py:3029] [Warmup][Graph/decode][104/126] batch_size:2 query_len:1 num_blocks:1536 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18139)[0;0m INFO 08-09 14:04:21 [hpu_model_runner.py:3029] [Warmup][Graph/decode][104/126] batch_size:2 query_len:1 num_blocks:1536 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18140)[0;0m INFO 08-09 14:04:28 [hpu_model_runner.py:3029] [Warmup][Graph/decode][105/126] batch_size:2 query_len:1 num_blocks:1152 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18138)[0;0m INFO 08-09 14:04:28 [hpu_model_runner.py:3029] [Warmup][Graph/decode][105/126] batch_size:2 query_len:1 num_blocks:1152 free_mem:23.42 GiB
INFO 08-09 14:04:28 [hpu_model_runner.py:3029] [Warmup][Graph/decode][105/126] batch_size:2 query_len:1 num_blocks:1152 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18139)[0;0m INFO 08-09 14:04:28 [hpu_model_runner.py:3029] [Warmup][Graph/decode][105/126] batch_size:2 query_len:1 num_blocks:1152 free_mem:23.42 GiB
INFO 08-09 14:04:34 [hpu_model_runner.py:3029] [Warmup][Graph/decode][106/126] batch_size:2 query_len:1 num_blocks:896 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18140)[0;0m INFO 08-09 14:04:34 [hpu_model_runner.py:3029] [Warmup][Graph/decode][106/126] batch_size:2 query_len:1 num_blocks:896 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18139)[0;0m INFO 08-09 14:04:34 [hpu_model_runner.py:3029] [Warmup][Graph/decode][106/126] batch_size:2 query_len:1 num_blocks:896 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18138)[0;0m INFO 08-09 14:04:34 [hpu_model_runner.py:3029] [Warmup][Graph/decode][106/126] batch_size:2 query_len:1 num_blocks:896 free_mem:23.42 GiB
INFO 08-09 14:04:41 [hpu_model_runner.py:3029] [Warmup][Graph/decode][107/126] batch_size:2 query_len:1 num_blocks:768 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18138)[0;0m INFO 08-09 14:04:41 [hpu_model_runner.py:3029] [Warmup][Graph/decode][107/126] batch_size:2 query_len:1 num_blocks:768 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18140)[0;0m INFO 08-09 14:04:41 [hpu_model_runner.py:3029] [Warmup][Graph/decode][107/126] batch_size:2 query_len:1 num_blocks:768 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18139)[0;0m INFO 08-09 14:04:41 [hpu_model_runner.py:3029] [Warmup][Graph/decode][107/126] batch_size:2 query_len:1 num_blocks:768 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18138)[0;0m INFO 08-09 14:04:47 [hpu_model_runner.py:3029] [Warmup][Graph/decode][108/126] batch_size:2 query_len:1 num_blocks:640 free_mem:23.42 GiB
INFO 08-09 14:04:47 [hpu_model_runner.py:3029] [Warmup][Graph/decode][108/126] batch_size:2 query_len:1 num_blocks:640 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18139)[0;0m INFO 08-09 14:04:47 [hpu_model_runner.py:3029] [Warmup][Graph/decode][108/126] batch_size:2 query_len:1 num_blocks:640 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18140)[0;0m INFO 08-09 14:04:47 [hpu_model_runner.py:3029] [Warmup][Graph/decode][108/126] batch_size:2 query_len:1 num_blocks:640 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18140)[0;0m INFO 08-09 14:04:54 [hpu_model_runner.py:3029] [Warmup][Graph/decode][109/126] batch_size:2 query_len:1 num_blocks:512 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18138)[0;0m INFO 08-09 14:04:54 [hpu_model_runner.py:3029] [Warmup][Graph/decode][109/126] batch_size:2 query_len:1 num_blocks:512 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18139)[0;0m INFO 08-09 14:04:54 [hpu_model_runner.py:3029] [Warmup][Graph/decode][109/126] batch_size:2 query_len:1 num_blocks:512 free_mem:23.42 GiB
INFO 08-09 14:04:54 [hpu_model_runner.py:3029] [Warmup][Graph/decode][109/126] batch_size:2 query_len:1 num_blocks:512 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18140)[0;0m INFO 08-09 14:05:00 [hpu_model_runner.py:3029] [Warmup][Graph/decode][110/126] batch_size:2 query_len:1 num_blocks:384 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18139)[0;0m INFO 08-09 14:05:00 [hpu_model_runner.py:3029] [Warmup][Graph/decode][110/126] batch_size:2 query_len:1 num_blocks:384 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18138)[0;0m INFO 08-09 14:05:00 [hpu_model_runner.py:3029] [Warmup][Graph/decode][110/126] batch_size:2 query_len:1 num_blocks:384 free_mem:23.42 GiB
INFO 08-09 14:05:00 [hpu_model_runner.py:3029] [Warmup][Graph/decode][110/126] batch_size:2 query_len:1 num_blocks:384 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18138)[0;0m INFO 08-09 14:05:07 [hpu_model_runner.py:3029] [Warmup][Graph/decode][111/126] batch_size:2 query_len:1 num_blocks:256 free_mem:23.42 GiB
INFO 08-09 14:05:07 [hpu_model_runner.py:3029] [Warmup][Graph/decode][111/126] batch_size:2 query_len:1 num_blocks:256 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18140)[0;0m INFO 08-09 14:05:07 [hpu_model_runner.py:3029] [Warmup][Graph/decode][111/126] batch_size:2 query_len:1 num_blocks:256 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18139)[0;0m INFO 08-09 14:05:07 [hpu_model_runner.py:3029] [Warmup][Graph/decode][111/126] batch_size:2 query_len:1 num_blocks:256 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18138)[0;0m INFO 08-09 14:05:14 [hpu_model_runner.py:3029] [Warmup][Graph/decode][112/126] batch_size:2 query_len:1 num_blocks:128 free_mem:23.42 GiB
INFO 08-09 14:05:14 [hpu_model_runner.py:3029] [Warmup][Graph/decode][112/126] batch_size:2 query_len:1 num_blocks:128 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18140)[0;0m INFO 08-09 14:05:14 [hpu_model_runner.py:3029] [Warmup][Graph/decode][112/126] batch_size:2 query_len:1 num_blocks:128 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18139)[0;0m INFO 08-09 14:05:14 [hpu_model_runner.py:3029] [Warmup][Graph/decode][112/126] batch_size:2 query_len:1 num_blocks:128 free_mem:23.42 GiB
INFO 08-09 14:05:20 [hpu_model_runner.py:3029] [Warmup][Graph/decode][113/126] batch_size:1 query_len:1 num_blocks:6387 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18138)[0;0m INFO 08-09 14:05:20 [hpu_model_runner.py:3029] [Warmup][Graph/decode][113/126] batch_size:1 query_len:1 num_blocks:6387 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18140)[0;0m INFO 08-09 14:05:20 [hpu_model_runner.py:3029] [Warmup][Graph/decode][113/126] batch_size:1 query_len:1 num_blocks:6387 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18139)[0;0m INFO 08-09 14:05:20 [hpu_model_runner.py:3029] [Warmup][Graph/decode][113/126] batch_size:1 query_len:1 num_blocks:6387 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18139)[0;0m INFO 08-09 14:05:27 [hpu_model_runner.py:3029] [Warmup][Graph/decode][114/126] batch_size:1 query_len:1 num_blocks:4736 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18138)[0;0m INFO 08-09 14:05:27 [hpu_model_runner.py:3029] [Warmup][Graph/decode][114/126] batch_size:1 query_len:1 num_blocks:4736 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18140)[0;0m INFO 08-09 14:05:27 [hpu_model_runner.py:3029] [Warmup][Graph/decode][114/126] batch_size:1 query_len:1 num_blocks:4736 free_mem:23.42 GiB
INFO 08-09 14:05:27 [hpu_model_runner.py:3029] [Warmup][Graph/decode][114/126] batch_size:1 query_len:1 num_blocks:4736 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18138)[0;0m INFO 08-09 14:05:34 [hpu_model_runner.py:3029] [Warmup][Graph/decode][115/126] batch_size:1 query_len:1 num_blocks:3584 free_mem:23.42 GiB
INFO 08-09 14:05:34 [hpu_model_runner.py:3029] [Warmup][Graph/decode][115/126] batch_size:1 query_len:1 num_blocks:3584 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18140)[0;0m INFO 08-09 14:05:34 [hpu_model_runner.py:3029] [Warmup][Graph/decode][115/126] batch_size:1 query_len:1 num_blocks:3584 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18139)[0;0m INFO 08-09 14:05:34 [hpu_model_runner.py:3029] [Warmup][Graph/decode][115/126] batch_size:1 query_len:1 num_blocks:3584 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18140)[0;0m INFO 08-09 14:05:41 [hpu_model_runner.py:3029] [Warmup][Graph/decode][116/126] batch_size:1 query_len:1 num_blocks:2688 free_mem:23.42 GiB
INFO 08-09 14:05:41 [hpu_model_runner.py:3029] [Warmup][Graph/decode][116/126] batch_size:1 query_len:1 num_blocks:2688 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18139)[0;0m INFO 08-09 14:05:41 [hpu_model_runner.py:3029] [Warmup][Graph/decode][116/126] batch_size:1 query_len:1 num_blocks:2688 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18138)[0;0m INFO 08-09 14:05:41 [hpu_model_runner.py:3029] [Warmup][Graph/decode][116/126] batch_size:1 query_len:1 num_blocks:2688 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18138)[0;0m INFO 08-09 14:05:48 [hpu_model_runner.py:3029] [Warmup][Graph/decode][117/126] batch_size:1 query_len:1 num_blocks:1920 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18140)[0;0m INFO 08-09 14:05:48 [hpu_model_runner.py:3029] [Warmup][Graph/decode][117/126] batch_size:1 query_len:1 num_blocks:1920 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18139)[0;0m INFO 08-09 14:05:48 [hpu_model_runner.py:3029] [Warmup][Graph/decode][117/126] batch_size:1 query_len:1 num_blocks:1920 free_mem:23.42 GiB
INFO 08-09 14:05:48 [hpu_model_runner.py:3029] [Warmup][Graph/decode][117/126] batch_size:1 query_len:1 num_blocks:1920 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18139)[0;0m INFO 08-09 14:05:55 [hpu_model_runner.py:3029] [Warmup][Graph/decode][118/126] batch_size:1 query_len:1 num_blocks:1536 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18138)[0;0m INFO 08-09 14:05:55 [hpu_model_runner.py:3029] [Warmup][Graph/decode][118/126] batch_size:1 query_len:1 num_blocks:1536 free_mem:23.42 GiB
INFO 08-09 14:05:55 [hpu_model_runner.py:3029] [Warmup][Graph/decode][118/126] batch_size:1 query_len:1 num_blocks:1536 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18140)[0;0m INFO 08-09 14:05:55 [hpu_model_runner.py:3029] [Warmup][Graph/decode][118/126] batch_size:1 query_len:1 num_blocks:1536 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18140)[0;0m INFO 08-09 14:06:02 [hpu_model_runner.py:3029] [Warmup][Graph/decode][119/126] batch_size:1 query_len:1 num_blocks:1152 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18138)[0;0m INFO 08-09 14:06:02 [hpu_model_runner.py:3029] [Warmup][Graph/decode][119/126] batch_size:1 query_len:1 num_blocks:1152 free_mem:23.42 GiB
INFO 08-09 14:06:02 [hpu_model_runner.py:3029] [Warmup][Graph/decode][119/126] batch_size:1 query_len:1 num_blocks:1152 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18139)[0;0m INFO 08-09 14:06:02 [hpu_model_runner.py:3029] [Warmup][Graph/decode][119/126] batch_size:1 query_len:1 num_blocks:1152 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18138)[0;0m INFO 08-09 14:06:09 [hpu_model_runner.py:3029] [Warmup][Graph/decode][120/126] batch_size:1 query_len:1 num_blocks:896 free_mem:23.42 GiB
INFO 08-09 14:06:09 [hpu_model_runner.py:3029] [Warmup][Graph/decode][120/126] batch_size:1 query_len:1 num_blocks:896 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18140)[0;0m INFO 08-09 14:06:09 [hpu_model_runner.py:3029] [Warmup][Graph/decode][120/126] batch_size:1 query_len:1 num_blocks:896 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18139)[0;0m INFO 08-09 14:06:09 [hpu_model_runner.py:3029] [Warmup][Graph/decode][120/126] batch_size:1 query_len:1 num_blocks:896 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18138)[0;0m INFO 08-09 14:06:16 [hpu_model_runner.py:3029] [Warmup][Graph/decode][121/126] batch_size:1 query_len:1 num_blocks:768 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18139)[0;0m INFO 08-09 14:06:16 [hpu_model_runner.py:3029] [Warmup][Graph/decode][121/126] batch_size:1 query_len:1 num_blocks:768 free_mem:23.42 GiB
INFO 08-09 14:06:16 [hpu_model_runner.py:3029] [Warmup][Graph/decode][121/126] batch_size:1 query_len:1 num_blocks:768 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18140)[0;0m INFO 08-09 14:06:16 [hpu_model_runner.py:3029] [Warmup][Graph/decode][121/126] batch_size:1 query_len:1 num_blocks:768 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18140)[0;0m INFO 08-09 14:06:23 [hpu_model_runner.py:3029] [Warmup][Graph/decode][122/126] batch_size:1 query_len:1 num_blocks:640 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18138)[0;0m INFO 08-09 14:06:23 [hpu_model_runner.py:3029] [Warmup][Graph/decode][122/126] batch_size:1 query_len:1 num_blocks:640 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18139)[0;0m INFO 08-09 14:06:23 [hpu_model_runner.py:3029] [Warmup][Graph/decode][122/126] batch_size:1 query_len:1 num_blocks:640 free_mem:23.42 GiB
INFO 08-09 14:06:23 [hpu_model_runner.py:3029] [Warmup][Graph/decode][122/126] batch_size:1 query_len:1 num_blocks:640 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18138)[0;0m INFO 08-09 14:06:29 [hpu_model_runner.py:3029] [Warmup][Graph/decode][123/126] batch_size:1 query_len:1 num_blocks:512 free_mem:23.42 GiB
INFO 08-09 14:06:29 [hpu_model_runner.py:3029] [Warmup][Graph/decode][123/126] batch_size:1 query_len:1 num_blocks:512 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18140)[0;0m INFO 08-09 14:06:29 [hpu_model_runner.py:3029] [Warmup][Graph/decode][123/126] batch_size:1 query_len:1 num_blocks:512 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18139)[0;0m INFO 08-09 14:06:29 [hpu_model_runner.py:3029] [Warmup][Graph/decode][123/126] batch_size:1 query_len:1 num_blocks:512 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18138)[0;0m INFO 08-09 14:06:36 [hpu_model_runner.py:3029] [Warmup][Graph/decode][124/126] batch_size:1 query_len:1 num_blocks:384 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18140)[0;0m INFO 08-09 14:06:36 [hpu_model_runner.py:3029] [Warmup][Graph/decode][124/126] batch_size:1 query_len:1 num_blocks:384 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18139)[0;0m INFO 08-09 14:06:36 [hpu_model_runner.py:3029] [Warmup][Graph/decode][124/126] batch_size:1 query_len:1 num_blocks:384 free_mem:23.42 GiB
INFO 08-09 14:06:36 [hpu_model_runner.py:3029] [Warmup][Graph/decode][124/126] batch_size:1 query_len:1 num_blocks:384 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18138)[0;0m INFO 08-09 14:06:43 [hpu_model_runner.py:3029] [Warmup][Graph/decode][125/126] batch_size:1 query_len:1 num_blocks:256 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18139)[0;0m INFO 08-09 14:06:43 [hpu_model_runner.py:3029] [Warmup][Graph/decode][125/126] batch_size:1 query_len:1 num_blocks:256 free_mem:23.42 GiB
INFO 08-09 14:06:43 [hpu_model_runner.py:3029] [Warmup][Graph/decode][125/126] batch_size:1 query_len:1 num_blocks:256 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18140)[0;0m INFO 08-09 14:06:43 [hpu_model_runner.py:3029] [Warmup][Graph/decode][125/126] batch_size:1 query_len:1 num_blocks:256 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18138)[0;0m INFO 08-09 14:06:51 [hpu_model_runner.py:3029] [Warmup][Graph/decode][126/126] batch_size:1 query_len:1 num_blocks:128 free_mem:23.42 GiB
INFO 08-09 14:06:51 [hpu_model_runner.py:3029] [Warmup][Graph/decode][126/126] batch_size:1 query_len:1 num_blocks:128 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18140)[0;0m INFO 08-09 14:06:51 [hpu_model_runner.py:3029] [Warmup][Graph/decode][126/126] batch_size:1 query_len:1 num_blocks:128 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18139)[0;0m INFO 08-09 14:06:51 [hpu_model_runner.py:3029] [Warmup][Graph/decode][126/126] batch_size:1 query_len:1 num_blocks:128 free_mem:23.42 GiB
[1;36m(VllmWorkerProcess pid=18140)[0;0m INFO 08-09 14:06:57 [hpu_model_runner.py:3152] Prompt captured:38 (100.0%) used_mem:0 B
INFO 08-09 14:06:57 [hpu_model_runner.py:3152] Prompt captured:38 (100.0%) used_mem:0 B
[1;36m(VllmWorkerProcess pid=18140)[0;0m INFO 08-09 14:06:57 [hpu_model_runner.py:3152] Decode captured:126 (100.0%) used_mem:4 KiB
INFO 08-09 14:06:57 [hpu_model_runner.py:3152] Decode captured:126 (100.0%) used_mem:4 KiB
[1;36m(VllmWorkerProcess pid=18140)[0;0m INFO 08-09 14:06:57 [hpu_model_runner.py:3280] Warmup finished in 776 secs, allocated 0 B of device memory
[1;36m(VllmWorkerProcess pid=18138)[0;0m INFO 08-09 14:06:57 [hpu_model_runner.py:3152] Prompt captured:38 (100.0%) used_mem:0 B
[1;36m(VllmWorkerProcess pid=18139)[0;0m INFO 08-09 14:06:57 [hpu_model_runner.py:3152] Prompt captured:38 (100.0%) used_mem:0 B
[1;36m(VllmWorkerProcess pid=18138)[0;0m INFO 08-09 14:06:57 [hpu_model_runner.py:3152] Decode captured:126 (100.0%) used_mem:4 KiB
[1;36m(VllmWorkerProcess pid=18139)[0;0m INFO 08-09 14:06:57 [hpu_model_runner.py:3152] Decode captured:126 (100.0%) used_mem:4 KiB
INFO 08-09 14:06:57 [hpu_model_runner.py:3280] Warmup finished in 776 secs, allocated 0 B of device memory
[1;36m(VllmWorkerProcess pid=18138)[0;0m INFO 08-09 14:06:57 [hpu_model_runner.py:3280] Warmup finished in 776 secs, allocated 0 B of device memory
[1;36m(VllmWorkerProcess pid=18139)[0;0m INFO 08-09 14:06:57 [hpu_model_runner.py:3280] Warmup finished in 776 secs, allocated 0 B of device memory
INFO 08-09 14:06:57 [llm_engine.py:431] init engine (profile, create kv cache, warmup model) took 780.89 seconds
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
WARNING 08-09 14:06:58 [config.py:1364] Default sampling parameters have been overridden by the model's Hugging Face generation config recommended from the model creator. If this is not intended, please relaunch vLLM instance with `--generation-config vllm`.
INFO 08-09 14:06:58 [serving_chat.py:118] Using default chat sampling params from model: {'temperature': 0.9, 'top_p': 0.6}
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
INFO 08-09 14:06:58 [serving_completion.py:66] Using default completion sampling params from model: {'temperature': 0.9, 'top_p': 0.6}
INFO 08-09 14:06:58 [api_server.py:1351] Starting vLLM API server 0 on http://0.0.0.0:8000
INFO 08-09 14:06:58 [launcher.py:29] Available routes are:
INFO 08-09 14:06:58 [launcher.py:37] Route: /openapi.json, Methods: GET, HEAD
INFO 08-09 14:06:58 [launcher.py:37] Route: /docs, Methods: GET, HEAD
INFO 08-09 14:06:58 [launcher.py:37] Route: /docs/oauth2-redirect, Methods: GET, HEAD
INFO 08-09 14:06:58 [launcher.py:37] Route: /redoc, Methods: GET, HEAD
INFO 08-09 14:06:58 [launcher.py:37] Route: /health, Methods: GET
INFO 08-09 14:06:58 [launcher.py:37] Route: /load, Methods: GET
INFO 08-09 14:06:58 [launcher.py:37] Route: /ping, Methods: POST
INFO 08-09 14:06:58 [launcher.py:37] Route: /ping, Methods: GET
INFO 08-09 14:06:58 [launcher.py:37] Route: /tokenize, Methods: POST
INFO 08-09 14:06:58 [launcher.py:37] Route: /detokenize, Methods: POST
INFO 08-09 14:06:58 [launcher.py:37] Route: /v1/models, Methods: GET
INFO 08-09 14:06:58 [launcher.py:37] Route: /version, Methods: GET
INFO 08-09 14:06:58 [launcher.py:37] Route: /v1/chat/completions, Methods: POST
INFO 08-09 14:06:58 [launcher.py:37] Route: /v1/completions, Methods: POST
INFO 08-09 14:06:58 [launcher.py:37] Route: /v1/embeddings, Methods: POST
INFO 08-09 14:06:58 [launcher.py:37] Route: /pooling, Methods: POST
INFO 08-09 14:06:58 [launcher.py:37] Route: /classify, Methods: POST
INFO 08-09 14:06:58 [launcher.py:37] Route: /score, Methods: POST
INFO 08-09 14:06:58 [launcher.py:37] Route: /v1/score, Methods: POST
INFO 08-09 14:06:58 [launcher.py:37] Route: /v1/audio/transcriptions, Methods: POST
INFO 08-09 14:06:58 [launcher.py:37] Route: /rerank, Methods: POST
INFO 08-09 14:06:58 [launcher.py:37] Route: /v1/rerank, Methods: POST
INFO 08-09 14:06:58 [launcher.py:37] Route: /v2/rerank, Methods: POST
INFO 08-09 14:06:58 [launcher.py:37] Route: /invocations, Methods: POST
INFO 08-09 14:06:58 [launcher.py:37] Route: /metrics, Methods: GET
INFO:     Started server process [17728]
INFO:     Waiting for application startup.
INFO:     Application startup complete.
INFO 08-09 14:09:27 [logger.py:43] Received request cmpl-0cd7bb1d671349b1a0a260518b0d9894-0: prompt: 'What is LLaMA 2 and how does it work?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 365, 5661, 1529, 29871, 29906, 322, 920, 947, 372, 664, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:59310 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:09:27 [engine.py:331] Added request cmpl-0cd7bb1d671349b1a0a260518b0d9894-0.
INFO 08-09 14:09:28 [metrics.py:417] Avg prompt throughput: 1.3 tokens/s, Avg generation throughput: 0.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 14:09:33 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 84.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:09:39 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 100.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:09:39 [logger.py:43] Received request cmpl-4e7b420fca7f4d5e976a6bce7988f7f4-0: prompt: 'Explain the architecture of a transformer model.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 12027, 7420, 278, 11258, 310, 263, 4327, 261, 1904, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:33346 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:09:39 [engine.py:331] Added request cmpl-4e7b420fca7f4d5e976a6bce7988f7f4-0.
INFO 08-09 14:09:44 [metrics.py:417] Avg prompt throughput: 2.2 tokens/s, Avg generation throughput: 106.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:09:48 [logger.py:43] Received request cmpl-b655680d942c4eea9c3033f76afab358-0: prompt: 'What are the core differences between CNN and RNN?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 526, 278, 7136, 12651, 1546, 29696, 322, 390, 10262, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:35972 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:09:48 [engine.py:331] Added request cmpl-b655680d942c4eea9c3033f76afab358-0.
INFO 08-09 14:09:49 [metrics.py:417] Avg prompt throughput: 2.4 tokens/s, Avg generation throughput: 112.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 14:09:54 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 112.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:09:58 [logger.py:43] Received request cmpl-7e3ac44ba605473e86e8fad4343c9425-0: prompt: 'Describe how attention mechanism improves translation models.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 20355, 915, 920, 8570, 13336, 4857, 1960, 13962, 4733, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:55120 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:09:58 [engine.py:331] Added request cmpl-7e3ac44ba605473e86e8fad4343c9425-0.
INFO 08-09 14:09:59 [metrics.py:417] Avg prompt throughput: 2.2 tokens/s, Avg generation throughput: 104.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 14:10:02 [logger.py:43] Received request cmpl-1e000402077040b6a584bb2b4f664afb-0: prompt: 'What is the role of positional encoding in transformers?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 278, 6297, 310, 2602, 284, 8025, 297, 4327, 414, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:43688 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:10:02 [engine.py:331] Added request cmpl-1e000402077040b6a584bb2b4f664afb-0.
INFO 08-09 14:10:04 [metrics.py:417] Avg prompt throughput: 2.6 tokens/s, Avg generation throughput: 113.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 14:10:09 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 109.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:10:11 [logger.py:43] Received request cmpl-7de25fe34cf846a396cb01612a30c065-0: prompt: 'How does self-attention differ from cross-attention?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 947, 1583, 29899, 1131, 2509, 1163, 515, 4891, 29899, 1131, 2509, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:43694 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:10:11 [engine.py:331] Added request cmpl-7de25fe34cf846a396cb01612a30c065-0.
INFO 08-09 14:10:14 [metrics.py:417] Avg prompt throughput: 2.8 tokens/s, Avg generation throughput: 113.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:10:19 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 106.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:10:21 [logger.py:43] Received request cmpl-70527e1023724a7da0fe91de181e59c9-0: prompt: 'Explain the concept of tokenization in NLP.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 12027, 7420, 278, 6964, 310, 5993, 2133, 297, 405, 13208, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:37876 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:10:21 [engine.py:331] Added request cmpl-70527e1023724a7da0fe91de181e59c9-0.
INFO 08-09 14:10:24 [metrics.py:417] Avg prompt throughput: 2.4 tokens/s, Avg generation throughput: 103.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:10:29 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 112.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:10:30 [logger.py:43] Received request cmpl-e490c7e5afd8410a9c69b94e46ca1108-0: prompt: 'What is the vanishing gradient problem and how to mitigate it?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 278, 1109, 14424, 16030, 1108, 322, 920, 304, 1380, 335, 403, 372, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:33822 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:10:30 [engine.py:331] Added request cmpl-e490c7e5afd8410a9c69b94e46ca1108-0.
INFO 08-09 14:10:34 [metrics.py:417] Avg prompt throughput: 3.2 tokens/s, Avg generation throughput: 107.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:10:36 [logger.py:43] Received request cmpl-2be37fdf23304c55a58386cd30dfc8a6-0: prompt: 'Describe how Gaudi architecture differs from NVIDIA GPUs.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 20355, 915, 920, 10415, 4749, 11258, 2923, 414, 515, 405, 13044, 10764, 22796, 29879, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:46232 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:10:36 [engine.py:331] Added request cmpl-2be37fdf23304c55a58386cd30dfc8a6-0.
INFO 08-09 14:10:39 [metrics.py:417] Avg prompt throughput: 3.2 tokens/s, Avg generation throughput: 108.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:10:44 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 100.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:10:46 [logger.py:43] Received request cmpl-a8e3a6b1b7be4affa7d49a85fe9b3fb1-0: prompt: 'What is model parallelism and how does it help scale large models?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 1904, 8943, 1608, 322, 920, 947, 372, 1371, 6287, 2919, 4733, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:39700 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:10:46 [engine.py:331] Added request cmpl-a8e3a6b1b7be4affa7d49a85fe9b3fb1-0.
INFO 08-09 14:10:49 [metrics.py:417] Avg prompt throughput: 3.0 tokens/s, Avg generation throughput: 109.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:10:54 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 110.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:10:55 [logger.py:43] Received request cmpl-7c1ddf2fc01d463eb6aff914dea25bb9-0: prompt: 'Explain the difference between GPT and BERT architectures.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 12027, 7420, 278, 4328, 1546, 402, 7982, 322, 350, 20161, 6956, 1973, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:41914 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:10:55 [engine.py:331] Added request cmpl-7c1ddf2fc01d463eb6aff914dea25bb9-0.
INFO 08-09 14:10:59 [metrics.py:417] Avg prompt throughput: 2.8 tokens/s, Avg generation throughput: 104.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:11:04 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 106.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:11:05 [logger.py:43] Received request cmpl-1b3149fd70934e28a0feea51a2293edc-0: prompt: 'How does prompt engineering affect language model outputs?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 947, 9508, 21639, 6602, 4086, 1904, 14391, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:47504 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:11:05 [engine.py:331] Added request cmpl-1b3149fd70934e28a0feea51a2293edc-0.
INFO 08-09 14:11:09 [metrics.py:417] Avg prompt throughput: 2.0 tokens/s, Avg generation throughput: 99.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:11:14 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 110.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:11:14 [logger.py:43] Received request cmpl-dfb6ad61e9a44bb7a5afa57583b66659-0: prompt: 'What is the function of layer normalization in deep networks?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 278, 740, 310, 7546, 4226, 2133, 297, 6483, 14379, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:48294 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:11:14 [engine.py:331] Added request cmpl-dfb6ad61e9a44bb7a5afa57583b66659-0.
INFO 08-09 14:11:19 [metrics.py:417] Avg prompt throughput: 2.6 tokens/s, Avg generation throughput: 108.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:11:21 [logger.py:43] Received request cmpl-f294b1333b234f468d1ebe94816a3fc1-0: prompt: 'Explain causal language modeling with examples.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 12027, 7420, 3269, 284, 4086, 1904, 292, 411, 6455, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:38498 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:11:21 [engine.py:331] Added request cmpl-f294b1333b234f468d1ebe94816a3fc1-0.
INFO 08-09 14:11:24 [metrics.py:417] Avg prompt throughput: 2.2 tokens/s, Avg generation throughput: 108.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:11:29 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 111.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:11:31 [logger.py:43] Received request cmpl-c00300a589234dfb9ade1ecd1f7582a0-0: prompt: 'What is quantization and how does it improve inference performance?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 4323, 2133, 322, 920, 947, 372, 11157, 27262, 4180, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:38506 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:11:31 [engine.py:331] Added request cmpl-c00300a589234dfb9ade1ecd1f7582a0-0.
INFO 08-09 14:11:34 [metrics.py:417] Avg prompt throughput: 2.6 tokens/s, Avg generation throughput: 108.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:11:39 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 110.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:11:40 [logger.py:43] Received request cmpl-7c3efd734fe44ae0808becbd8c1aff3a-0: prompt: 'Describe how fine-tuning works for large language models.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 20355, 915, 920, 2691, 29899, 29873, 27964, 1736, 363, 2919, 4086, 4733, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:56926 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:11:40 [engine.py:331] Added request cmpl-7c3efd734fe44ae0808becbd8c1aff3a-0.
INFO 08-09 14:11:44 [metrics.py:417] Avg prompt throughput: 2.8 tokens/s, Avg generation throughput: 100.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:11:49 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 114.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:11:49 [logger.py:43] Received request cmpl-3e5ce96b6a484787a168ac77954856b5-0: prompt: 'What are LoRA adapters and how do they help in parameter-efficient training?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 526, 4309, 4717, 594, 481, 2153, 322, 920, 437, 896, 1371, 297, 3443, 29899, 8462, 6694, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:51294 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:11:49 [engine.py:331] Added request cmpl-3e5ce96b6a484787a168ac77954856b5-0.
INFO 08-09 14:11:52 [logger.py:43] Received request cmpl-57b5d4e45718437c81463ef3e2f30db5-0: prompt: 'Explain the purpose of rotary positional embeddings.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 12027, 7420, 278, 6437, 310, 5731, 653, 2602, 284, 8297, 29881, 886, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:50902 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:11:52 [engine.py:331] Added request cmpl-57b5d4e45718437c81463ef3e2f30db5-0.
INFO 08-09 14:11:54 [metrics.py:417] Avg prompt throughput: 6.6 tokens/s, Avg generation throughput: 103.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 14:11:59 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 111.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:12:01 [logger.py:43] Received request cmpl-9d57f78e0b064f03b5ab0f27aa1988da-0: prompt: 'How does DeepSpeed improve model training efficiency?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 947, 21784, 26539, 11157, 1904, 6694, 19201, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:47272 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:12:01 [engine.py:331] Added request cmpl-9d57f78e0b064f03b5ab0f27aa1988da-0.
INFO 08-09 14:12:04 [metrics.py:417] Avg prompt throughput: 2.0 tokens/s, Avg generation throughput: 103.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:12:09 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 108.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:12:10 [logger.py:43] Received request cmpl-cda954d9c6e34c179c7c644c203e63c3-0: prompt: 'Compare data parallelism and pipeline parallelism.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 3831, 598, 848, 8943, 1608, 322, 16439, 8943, 1608, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:47278 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:12:10 [engine.py:331] Added request cmpl-cda954d9c6e34c179c7c644c203e63c3-0.
INFO 08-09 14:12:13 [logger.py:43] Received request cmpl-1a4e1deb924642a79cb4fac1725aa04c-0: prompt: 'What is ALiBi positional encoding?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 14445, 29875, 20517, 2602, 284, 8025, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:54186 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:12:13 [engine.py:331] Added request cmpl-1a4e1deb924642a79cb4fac1725aa04c-0.
INFO 08-09 14:12:14 [metrics.py:417] Avg prompt throughput: 4.2 tokens/s, Avg generation throughput: 110.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 14:12:19 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 109.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:12:22 [logger.py:43] Received request cmpl-200be12bcb4045ccbcd79256b658afff-0: prompt: 'Describe the purpose of HF Transformers library.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 20355, 915, 278, 6437, 310, 379, 29943, 4103, 689, 414, 3489, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:50410 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:12:22 [engine.py:331] Added request cmpl-200be12bcb4045ccbcd79256b658afff-0.
INFO 08-09 14:12:24 [metrics.py:417] Avg prompt throughput: 2.6 tokens/s, Avg generation throughput: 112.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 14:12:29 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 108.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:12:31 [logger.py:43] Received request cmpl-bd7662d5c6964435b3f70722affaf94b-0: prompt: 'How do attention heads capture contextual meaning?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 437, 8570, 15883, 10446, 3030, 950, 6593, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:57730 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:12:31 [engine.py:331] Added request cmpl-bd7662d5c6964435b3f70722affaf94b-0.
INFO 08-09 14:12:34 [metrics.py:417] Avg prompt throughput: 2.0 tokens/s, Avg generation throughput: 110.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:12:39 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 106.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:12:41 [logger.py:43] Received request cmpl-a55969ffd40b469ea3aa37c0d3a4d544-0: prompt: 'What is dynamic batching in inference servers?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 7343, 9853, 292, 297, 27262, 12424, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:57744 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:12:41 [engine.py:331] Added request cmpl-a55969ffd40b469ea3aa37c0d3a4d544-0.
INFO 08-09 14:12:44 [metrics.py:417] Avg prompt throughput: 2.0 tokens/s, Avg generation throughput: 112.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:12:49 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 102.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:12:50 [logger.py:43] Received request cmpl-6fae858f60eb48698762b62abf7ee999-0: prompt: 'Explain greedy decoding vs beam search vs sampling.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 12027, 7420, 1395, 7584, 1602, 3689, 7186, 22913, 2740, 7186, 23460, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:43138 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:12:50 [engine.py:331] Added request cmpl-6fae858f60eb48698762b62abf7ee999-0.
INFO 08-09 14:12:54 [metrics.py:417] Avg prompt throughput: 2.6 tokens/s, Avg generation throughput: 108.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:12:59 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 108.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:12:59 [logger.py:43] Received request cmpl-2af9de2df2ad4dd18207f5a176c10fc7-0: prompt: 'What is perplexity in language models and why is it important?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 639, 10709, 537, 297, 4086, 4733, 322, 2020, 338, 372, 4100, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:45884 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:12:59 [engine.py:331] Added request cmpl-2af9de2df2ad4dd18207f5a176c10fc7-0.
INFO 08-09 14:13:04 [metrics.py:417] Avg prompt throughput: 3.0 tokens/s, Avg generation throughput: 103.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:13:09 [logger.py:43] Received request cmpl-7dc17d311d1041d59c03f37f3a3fb1a6-0: prompt: 'How does mixed precision training benefit model training?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 947, 12849, 16716, 6694, 14169, 1904, 6694, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:40762 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:13:09 [engine.py:331] Added request cmpl-7dc17d311d1041d59c03f37f3a3fb1a6-0.
INFO 08-09 14:13:09 [metrics.py:417] Avg prompt throughput: 2.0 tokens/s, Avg generation throughput: 105.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 14:13:14 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 112.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:13:18 [logger.py:43] Received request cmpl-4f261fabbd3e43eab2073943c3e0a459-0: prompt: 'What is the function of the softmax layer in transformers?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 278, 740, 310, 278, 4964, 3317, 7546, 297, 4327, 414, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:59940 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:13:18 [engine.py:331] Added request cmpl-4f261fabbd3e43eab2073943c3e0a459-0.
INFO 08-09 14:13:19 [metrics.py:417] Avg prompt throughput: 2.8 tokens/s, Avg generation throughput: 106.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 14:13:24 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 113.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:13:27 [logger.py:43] Received request cmpl-aefac0f9efca42e688d87f73a051f5c4-0: prompt: "Explain embeddings and how they're used in NLP models.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 12027, 7420, 8297, 29881, 886, 322, 920, 896, 29915, 276, 1304, 297, 405, 13208, 4733, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:49130 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:13:27 [engine.py:331] Added request cmpl-aefac0f9efca42e688d87f73a051f5c4-0.
INFO 08-09 14:13:29 [metrics.py:417] Avg prompt throughput: 3.4 tokens/s, Avg generation throughput: 101.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 14:13:34 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 102.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:13:37 [logger.py:43] Received request cmpl-019f59407064498da91a7c261d08246a-0: prompt: "Describe the function of the tokenizer's vocabulary.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 20355, 915, 278, 740, 310, 278, 5993, 3950, 29915, 29879, 7931, 370, 352, 653, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:53200 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:13:37 [engine.py:331] Added request cmpl-019f59407064498da91a7c261d08246a-0.
INFO 08-09 14:13:39 [metrics.py:417] Avg prompt throughput: 3.2 tokens/s, Avg generation throughput: 99.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 14:13:44 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 106.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:13:46 [logger.py:43] Received request cmpl-a7a2005609cd4eb88bb974f4dc8ee03c-0: prompt: 'What is causal masking in transformer decoders?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 3269, 284, 11105, 292, 297, 4327, 261, 1602, 397, 414, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:35074 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:13:46 [engine.py:331] Added request cmpl-a7a2005609cd4eb88bb974f4dc8ee03c-0.
INFO 08-09 14:13:49 [metrics.py:417] Avg prompt throughput: 2.8 tokens/s, Avg generation throughput: 105.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:13:54 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 110.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:13:56 [logger.py:43] Received request cmpl-8c33067b5d07414bacaf2bd5992fe465-0: prompt: 'How does gradient checkpointing reduce memory usage?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 947, 16030, 1423, 3149, 292, 10032, 3370, 8744, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:54888 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:13:56 [engine.py:331] Added request cmpl-8c33067b5d07414bacaf2bd5992fe465-0.
INFO 08-09 14:13:59 [metrics.py:417] Avg prompt throughput: 2.2 tokens/s, Avg generation throughput: 111.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:14:00 [logger.py:43] Received request cmpl-5612978ced6545eaa1d4b1a39ee7c267-0: prompt: 'Explain the difference between encoder-decoder and decoder-only models.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 12027, 7420, 278, 4328, 1546, 2094, 6119, 29899, 7099, 6119, 322, 1602, 6119, 29899, 6194, 4733, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:54890 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:14:00 [engine.py:331] Added request cmpl-5612978ced6545eaa1d4b1a39ee7c267-0.
INFO 08-09 14:14:04 [metrics.py:417] Avg prompt throughput: 3.6 tokens/s, Avg generation throughput: 109.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:14:09 [logger.py:43] Received request cmpl-6e0c81077d5f485ca5301ad1bf101328-0: prompt: 'What is zero-shot vs few-shot vs fine-tuned inference?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 5225, 29899, 8962, 7186, 2846, 29899, 8962, 7186, 2691, 29899, 29873, 348, 287, 27262, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:43060 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:14:09 [engine.py:331] Added request cmpl-6e0c81077d5f485ca5301ad1bf101328-0.
INFO 08-09 14:14:09 [metrics.py:417] Avg prompt throughput: 3.6 tokens/s, Avg generation throughput: 110.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 14:14:14 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 106.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:14:18 [logger.py:43] Received request cmpl-029c09c7534146378b61a5b253e83c6d-0: prompt: 'What are prompt templates and why do they matter?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 526, 9508, 17475, 322, 2020, 437, 896, 4383, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:56470 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:14:18 [engine.py:331] Added request cmpl-029c09c7534146378b61a5b253e83c6d-0.
INFO 08-09 14:14:19 [metrics.py:417] Avg prompt throughput: 2.2 tokens/s, Avg generation throughput: 110.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 14:14:24 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 106.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:14:27 [logger.py:43] Received request cmpl-d500d019690742b08f874644914cf3b0-0: prompt: 'Explain the role of BOS and EOS tokens in transformers.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 12027, 7420, 278, 6297, 310, 350, 3267, 322, 382, 3267, 18897, 297, 4327, 414, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:45148 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:14:27 [engine.py:331] Added request cmpl-d500d019690742b08f874644914cf3b0-0.
INFO 08-09 14:14:29 [metrics.py:417] Avg prompt throughput: 3.2 tokens/s, Avg generation throughput: 107.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 14:14:34 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 111.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:14:36 [logger.py:43] Received request cmpl-8669736864fb4499aa09eb0f0c2a6066-0: prompt: 'What is a floating-point tensor and how is it represented?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 263, 16526, 29899, 3149, 12489, 322, 920, 338, 372, 9875, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:37414 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:14:36 [engine.py:331] Added request cmpl-8669736864fb4499aa09eb0f0c2a6066-0.
INFO 08-09 14:14:39 [metrics.py:417] Avg prompt throughput: 2.8 tokens/s, Avg generation throughput: 106.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:14:44 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 109.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:14:45 [logger.py:43] Received request cmpl-fe5bcc1775f64303be955a94e10c36e0-0: prompt: 'What are hypernetworks in neural architectures?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 526, 11266, 11618, 29879, 297, 19677, 6956, 1973, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:52322 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:14:45 [engine.py:331] Added request cmpl-fe5bcc1775f64303be955a94e10c36e0-0.
INFO 08-09 14:14:49 [metrics.py:417] Avg prompt throughput: 2.2 tokens/s, Avg generation throughput: 106.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:14:54 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 115.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:14:54 [logger.py:43] Received request cmpl-875953b48088422d875d0a09d85582c6-0: prompt: 'Explain MoE (Mixture of Experts) architecture.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 12027, 7420, 4546, 29923, 313, 29924, 29875, 15546, 310, 28224, 1372, 29897, 11258, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:48576 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:14:54 [engine.py:331] Added request cmpl-875953b48088422d875d0a09d85582c6-0.
INFO 08-09 14:14:59 [metrics.py:417] Avg prompt throughput: 3.0 tokens/s, Avg generation throughput: 109.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:15:04 [logger.py:43] Received request cmpl-b6f3c1bfc46c4d6c89164fb850ad0b1e-0: prompt: 'What is distillation in model compression?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 1320, 453, 362, 297, 1904, 24221, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:48164 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:15:04 [engine.py:331] Added request cmpl-b6f3c1bfc46c4d6c89164fb850ad0b1e-0.
INFO 08-09 14:15:04 [metrics.py:417] Avg prompt throughput: 2.0 tokens/s, Avg generation throughput: 108.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 14:15:09 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 111.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:15:13 [logger.py:43] Received request cmpl-7ec79f6b70594358ae8c8cf9bcf535c0-0: prompt: 'How do you implement top-k and top-p sampling?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 437, 366, 2334, 2246, 29899, 29895, 322, 2246, 29899, 29886, 23460, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:44980 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:15:13 [engine.py:331] Added request cmpl-7ec79f6b70594358ae8c8cf9bcf535c0-0.
INFO 08-09 14:15:14 [metrics.py:417] Avg prompt throughput: 2.8 tokens/s, Avg generation throughput: 106.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 14:15:19 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 108.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:15:22 [logger.py:43] Received request cmpl-6952026788134f94895a5c9f1d564e25-0: prompt: 'What is a vector database and how is it used with LLMs?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 263, 4608, 2566, 322, 920, 338, 372, 1304, 411, 365, 26369, 29879, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:55108 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:15:22 [engine.py:331] Added request cmpl-6952026788134f94895a5c9f1d564e25-0.
INFO 08-09 14:15:24 [metrics.py:417] Avg prompt throughput: 3.2 tokens/s, Avg generation throughput: 113.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 14:15:29 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 111.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:15:31 [logger.py:43] Received request cmpl-240acc2bc8314a9f87f945233680dd77-0: prompt: 'Describe how you would use RAG (Retrieval Augmented Generation).', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 20355, 915, 920, 366, 723, 671, 390, 10051, 313, 8015, 2546, 791, 22333, 358, 287, 28203, 467], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:55110 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:15:31 [engine.py:331] Added request cmpl-240acc2bc8314a9f87f945233680dd77-0.
INFO 08-09 14:15:34 [metrics.py:417] Avg prompt throughput: 3.6 tokens/s, Avg generation throughput: 109.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:15:39 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 112.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:15:40 [logger.py:43] Received request cmpl-f34c65fd754241e39f12cafb9703e910-0: prompt: 'What is streaming inference and how is it used?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 24820, 27262, 322, 920, 338, 372, 1304, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:47894 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:15:40 [engine.py:331] Added request cmpl-f34c65fd754241e39f12cafb9703e910-0.
INFO 08-09 14:15:44 [metrics.py:417] Avg prompt throughput: 2.2 tokens/s, Avg generation throughput: 103.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:15:49 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 110.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:15:49 [logger.py:43] Received request cmpl-400427a45c8848549ab87aafbf4a6e16-0: prompt: 'How does TTFT relate to user-perceived latency?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 947, 323, 29911, 7818, 29279, 304, 1404, 29899, 546, 346, 2347, 23316, 1270, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:36260 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:15:49 [engine.py:331] Added request cmpl-400427a45c8848549ab87aafbf4a6e16-0.
INFO 08-09 14:15:54 [metrics.py:417] Avg prompt throughput: 3.2 tokens/s, Avg generation throughput: 108.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:15:58 [logger.py:43] Received request cmpl-adbf6ca3d55f44e28a18f144db119707-0: prompt: 'Explain transformer decoder blocks layer-by-layer.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 12027, 7420, 4327, 261, 1602, 6119, 10930, 7546, 29899, 1609, 29899, 13148, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:51488 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:15:58 [engine.py:331] Added request cmpl-adbf6ca3d55f44e28a18f144db119707-0.
INFO 08-09 14:15:59 [metrics.py:417] Avg prompt throughput: 2.8 tokens/s, Avg generation throughput: 109.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 14:16:04 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 110.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:16:08 [logger.py:43] Received request cmpl-084364b569f84be4ad50d5591527922b-0: prompt: 'What is a KV cache in transformer inference?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 263, 476, 29963, 7090, 297, 4327, 261, 27262, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:47710 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:16:08 [engine.py:331] Added request cmpl-084364b569f84be4ad50d5591527922b-0.
INFO 08-09 14:16:09 [metrics.py:417] Avg prompt throughput: 2.4 tokens/s, Avg generation throughput: 109.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 14:16:14 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 109.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:16:16 [logger.py:43] Received request cmpl-a19735761e3541368a9af7ba581e8b37-0: prompt: 'Describe token-by-token generation and its challenges.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 20355, 915, 5993, 29899, 1609, 29899, 6979, 12623, 322, 967, 18066, 267, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:58498 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:16:16 [engine.py:331] Added request cmpl-a19735761e3541368a9af7ba581e8b37-0.
INFO 08-09 14:16:19 [metrics.py:417] Avg prompt throughput: 2.8 tokens/s, Avg generation throughput: 109.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:16:24 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 108.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:16:26 [logger.py:43] Received request cmpl-22ffdd5db9b44b578c36e97e0e278ff6-0: prompt: 'How does LLaMA 2 compare to GPT-3.5?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 947, 365, 5661, 1529, 29871, 29906, 7252, 304, 402, 7982, 29899, 29941, 29889, 29945, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:55310 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:16:26 [engine.py:331] Added request cmpl-22ffdd5db9b44b578c36e97e0e278ff6-0.
INFO 08-09 14:16:29 [metrics.py:417] Avg prompt throughput: 3.4 tokens/s, Avg generation throughput: 112.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:16:34 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 103.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:16:35 [logger.py:43] Received request cmpl-acb640e03d9444d184126e4cee316540-0: prompt: 'Explain the training data pipeline for a large model.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 12027, 7420, 278, 6694, 848, 16439, 363, 263, 2919, 1904, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:41692 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:16:35 [engine.py:331] Added request cmpl-acb640e03d9444d184126e4cee316540-0.
INFO 08-09 14:16:39 [metrics.py:417] Avg prompt throughput: 2.4 tokens/s, Avg generation throughput: 111.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:16:44 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 105.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:16:44 [logger.py:43] Received request cmpl-be1283248a4842f2a35341d1d2954ee2-0: prompt: 'What are the ethical concerns with generative models?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 526, 278, 11314, 936, 21838, 411, 1176, 1230, 4733, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:39652 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:16:44 [engine.py:331] Added request cmpl-be1283248a4842f2a35341d1d2954ee2-0.
INFO 08-09 14:16:48 [logger.py:43] Received request cmpl-97a1b201892d484093d331bdbaa5456d-0: prompt: 'How do transformers process multilingual text?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 437, 4327, 414, 1889, 1773, 6504, 950, 1426, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:39660 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:16:48 [engine.py:331] Added request cmpl-97a1b201892d484093d331bdbaa5456d-0.
INFO 08-09 14:16:49 [metrics.py:417] Avg prompt throughput: 4.6 tokens/s, Avg generation throughput: 106.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 14:16:54 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 109.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:16:57 [logger.py:43] Received request cmpl-adad584661a848559a0ed7fb65393393-0: prompt: 'What are flash attention mechanisms?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 526, 11013, 8570, 7208, 12903, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:44074 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:16:57 [engine.py:331] Added request cmpl-adad584661a848559a0ed7fb65393393-0.
INFO 08-09 14:16:59 [metrics.py:417] Avg prompt throughput: 1.6 tokens/s, Avg generation throughput: 112.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:17:04 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 112.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:17:06 [logger.py:43] Received request cmpl-af3bfdf886794d38afef0380795ade2e-0: prompt: 'Describe how memory-efficient attention works.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 20355, 915, 920, 3370, 29899, 8462, 8570, 1736, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:46694 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:17:06 [engine.py:331] Added request cmpl-af3bfdf886794d38afef0380795ade2e-0.
INFO 08-09 14:17:09 [metrics.py:417] Avg prompt throughput: 2.0 tokens/s, Avg generation throughput: 102.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:17:14 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 110.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:17:15 [logger.py:43] Received request cmpl-d176a9592bbb463bb0077c3a43db8327-0: prompt: 'What is inference throughput and how is it calculated?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 27262, 1549, 649, 322, 920, 338, 372, 12833, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:60258 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:17:15 [engine.py:331] Added request cmpl-d176a9592bbb463bb0077c3a43db8327-0.
INFO 08-09 14:17:19 [metrics.py:417] Avg prompt throughput: 2.4 tokens/s, Avg generation throughput: 108.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:17:24 [logger.py:43] Received request cmpl-c496168552174039a7b1210ccf3ec92f-0: prompt: 'Explain how masked self-attention works in BERT.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 12027, 7420, 920, 11105, 287, 1583, 29899, 1131, 2509, 1736, 297, 350, 20161, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:50846 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:17:24 [engine.py:331] Added request cmpl-c496168552174039a7b1210ccf3ec92f-0.
INFO 08-09 14:17:24 [metrics.py:417] Avg prompt throughput: 3.0 tokens/s, Avg generation throughput: 115.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 14:17:29 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 115.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:17:33 [logger.py:43] Received request cmpl-aef2eb1c1643498bae77a71674b7dda5-0: prompt: 'How do instruction-tuned models differ from base models?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 437, 15278, 29899, 29873, 348, 287, 4733, 1163, 515, 2967, 4733, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:52910 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:17:33 [engine.py:331] Added request cmpl-aef2eb1c1643498bae77a71674b7dda5-0.
INFO 08-09 14:17:34 [metrics.py:417] Avg prompt throughput: 2.8 tokens/s, Avg generation throughput: 111.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 14:17:39 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 105.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:17:42 [logger.py:43] Received request cmpl-60eb208e4f1e4b5295232cb4e08d7381-0: prompt: 'Describe an RLHF (Reinforcement Learning from Human Feedback) process.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 20355, 915, 385, 390, 29931, 29950, 29943, 313, 1123, 262, 1454, 13561, 29257, 515, 12968, 5169, 287, 1627, 29897, 1889, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:33732 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:17:42 [engine.py:331] Added request cmpl-60eb208e4f1e4b5295232cb4e08d7381-0.
INFO 08-09 14:17:44 [metrics.py:417] Avg prompt throughput: 4.4 tokens/s, Avg generation throughput: 107.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 14:17:49 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 110.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:17:52 [logger.py:43] Received request cmpl-5c309f0667c74070bcae5a46078df86e-0: prompt: 'What is SFT (Supervised Fine-Tuning) in LLM training?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 317, 7818, 313, 19111, 11292, 28896, 29899, 29911, 27964, 29897, 297, 365, 26369, 6694, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:35612 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:17:52 [engine.py:331] Added request cmpl-5c309f0667c74070bcae5a46078df86e-0.
INFO 08-09 14:17:54 [metrics.py:417] Avg prompt throughput: 3.6 tokens/s, Avg generation throughput: 108.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:17:54 [logger.py:43] Received request cmpl-1a43426533864b3da3ad774b1c4e48e6-0: prompt: 'Explain the role of HPU in Gaudi chips.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 12027, 7420, 278, 6297, 310, 379, 7056, 297, 10415, 4749, 521, 4512, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:35626 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:17:54 [engine.py:331] Added request cmpl-1a43426533864b3da3ad774b1c4e48e6-0.
INFO 08-09 14:17:59 [metrics.py:417] Avg prompt throughput: 2.8 tokens/s, Avg generation throughput: 105.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:18:04 [logger.py:43] Received request cmpl-7f5aeeec2594453098080797d2102149-0: prompt: 'What is a causal attention mask?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 263, 3269, 284, 8570, 11105, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:33092 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:18:04 [engine.py:331] Added request cmpl-7f5aeeec2594453098080797d2102149-0.
INFO 08-09 14:18:04 [metrics.py:417] Avg prompt throughput: 1.8 tokens/s, Avg generation throughput: 106.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 14:18:09 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 105.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:18:13 [logger.py:43] Received request cmpl-4a63d5b4132b491993cd91c2b905cbbd-0: prompt: 'Describe the HF `generate()` function in Transformers.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 20355, 915, 278, 379, 29943, 421, 17158, 2555, 740, 297, 4103, 689, 414, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:46476 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:18:13 [engine.py:331] Added request cmpl-4a63d5b4132b491993cd91c2b905cbbd-0.
INFO 08-09 14:18:14 [metrics.py:417] Avg prompt throughput: 3.0 tokens/s, Avg generation throughput: 105.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 14:18:19 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 104.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:18:23 [logger.py:43] Received request cmpl-6cfd7177e5c34531a22cf27660acc812-0: prompt: 'Explain differences between T5, BART, and GPT models.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 12027, 7420, 12651, 1546, 323, 29945, 29892, 350, 8322, 29892, 322, 402, 7982, 4733, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:44628 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:18:23 [engine.py:331] Added request cmpl-6cfd7177e5c34531a22cf27660acc812-0.
INFO 08-09 14:18:24 [metrics.py:417] Avg prompt throughput: 3.2 tokens/s, Avg generation throughput: 108.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 14:18:29 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 111.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:18:32 [logger.py:43] Received request cmpl-12d2dff4310141df8cd3a86cb58940ff-0: prompt: 'What is model checkpointing and resumption?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 1904, 1423, 3149, 292, 322, 620, 28069, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:38176 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:18:32 [engine.py:331] Added request cmpl-12d2dff4310141df8cd3a86cb58940ff-0.
INFO 08-09 14:18:34 [metrics.py:417] Avg prompt throughput: 2.2 tokens/s, Avg generation throughput: 110.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:18:39 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 110.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:18:41 [logger.py:43] Received request cmpl-652bbbf6c8be4ce6b95ba43989b399fc-0: prompt: 'Describe how checkpoint sharding works in distributed training.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 20355, 915, 920, 1423, 3149, 528, 20272, 1736, 297, 13235, 6694, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:38186 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:18:41 [engine.py:331] Added request cmpl-652bbbf6c8be4ce6b95ba43989b399fc-0.
INFO 08-09 14:18:44 [metrics.py:417] Avg prompt throughput: 2.6 tokens/s, Avg generation throughput: 109.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:18:49 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 113.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:18:50 [logger.py:43] Received request cmpl-ff35c879971d4df7874c86eef94f66c1-0: prompt: 'How are model weights stored in HuggingFace format?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 526, 1904, 18177, 6087, 297, 379, 688, 3460, 23360, 3402, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:42750 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:18:50 [engine.py:331] Added request cmpl-ff35c879971d4df7874c86eef94f66c1-0.
INFO 08-09 14:18:54 [metrics.py:417] Avg prompt throughput: 2.6 tokens/s, Avg generation throughput: 107.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:18:59 [logger.py:43] Received request cmpl-6f26a2f4480d4e98a6c294fed5ee1350-0: prompt: 'Explain the impact of batch size on training and inference.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 12027, 7420, 278, 10879, 310, 9853, 2159, 373, 6694, 322, 27262, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:56770 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:18:59 [engine.py:331] Added request cmpl-6f26a2f4480d4e98a6c294fed5ee1350-0.
INFO 08-09 14:19:00 [metrics.py:417] Avg prompt throughput: 2.4 tokens/s, Avg generation throughput: 107.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 14:19:05 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 117.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:19:08 [logger.py:43] Received request cmpl-3dc9240c9e5c4a13a99a8789bb9e75b8-0: prompt: 'What are fused kernels and why are they fast?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 526, 285, 3880, 413, 824, 1379, 322, 2020, 526, 896, 5172, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:54100 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:19:08 [engine.py:331] Added request cmpl-3dc9240c9e5c4a13a99a8789bb9e75b8-0.
INFO 08-09 14:19:10 [metrics.py:417] Avg prompt throughput: 2.7 tokens/s, Avg generation throughput: 106.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 14:19:15 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 117.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:19:17 [logger.py:43] Received request cmpl-08e5ba13f8114307a5f87dd301b20f83-0: prompt: 'How does the `AutoModelForCausalLM` class work?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 947, 278, 421, 12300, 3195, 2831, 29907, 1485, 284, 26369, 29952, 770, 664, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:36976 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:19:17 [engine.py:331] Added request cmpl-08e5ba13f8114307a5f87dd301b20f83-0.
INFO 08-09 14:19:20 [metrics.py:417] Avg prompt throughput: 3.2 tokens/s, Avg generation throughput: 106.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:19:25 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 114.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:19:26 [logger.py:43] Received request cmpl-1f9e8fc2275f48a5b2bf9825066f17e8-0: prompt: 'What are tensor parallel and tensor slicing?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 526, 12489, 8943, 322, 12489, 269, 506, 292, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:56600 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:19:26 [engine.py:331] Added request cmpl-1f9e8fc2275f48a5b2bf9825066f17e8-0.
INFO 08-09 14:19:30 [metrics.py:417] Avg prompt throughput: 2.2 tokens/s, Avg generation throughput: 99.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:19:35 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 116.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:19:35 [logger.py:43] Received request cmpl-fc83b8ceeb094932bf4cef668d2282dc-0: prompt: 'Describe the key steps of model serving using vLLM.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 20355, 915, 278, 1820, 6576, 310, 1904, 16330, 773, 325, 2208, 29924, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:42114 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:19:35 [engine.py:331] Added request cmpl-fc83b8ceeb094932bf4cef668d2282dc-0.
INFO 08-09 14:19:40 [metrics.py:417] Avg prompt throughput: 2.8 tokens/s, Avg generation throughput: 101.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:19:45 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 104.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:19:45 [logger.py:43] Received request cmpl-bc45221677684c58b0002b0ccffbd9a6-0: prompt: 'What are inference graphs in deep learning?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 526, 27262, 18445, 297, 6483, 6509, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:36670 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:19:45 [engine.py:331] Added request cmpl-bc45221677684c58b0002b0ccffbd9a6-0.
INFO 08-09 14:19:50 [metrics.py:417] Avg prompt throughput: 1.8 tokens/s, Avg generation throughput: 106.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:19:54 [logger.py:43] Received request cmpl-af3d4db94d514464beb7c7b32aeca282-0: prompt: 'What is a streaming response in OpenAI-style APIs?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 263, 24820, 2933, 297, 4673, 23869, 29899, 3293, 23649, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:46634 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:19:54 [engine.py:331] Added request cmpl-af3d4db94d514464beb7c7b32aeca282-0.
INFO 08-09 14:19:55 [metrics.py:417] Avg prompt throughput: 2.6 tokens/s, Avg generation throughput: 117.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 14:20:00 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 107.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:20:03 [logger.py:43] Received request cmpl-a8532729a59c40de868e9a08df6d0e0c-0: prompt: 'How does speculative decoding reduce latency?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 947, 1580, 28524, 1602, 3689, 10032, 23316, 1270, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:59242 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:20:03 [engine.py:331] Added request cmpl-a8532729a59c40de868e9a08df6d0e0c-0.
INFO 08-09 14:20:05 [metrics.py:417] Avg prompt throughput: 2.2 tokens/s, Avg generation throughput: 113.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 14:20:10 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 105.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:20:12 [logger.py:43] Received request cmpl-f0f522d0072443af9263ecb59b604e72-0: prompt: 'What is JAX and how is it different from PyTorch?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 435, 6604, 322, 920, 338, 372, 1422, 515, 10772, 29911, 25350, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:46236 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:20:12 [engine.py:331] Added request cmpl-f0f522d0072443af9263ecb59b604e72-0.
INFO 08-09 14:20:15 [metrics.py:417] Avg prompt throughput: 3.0 tokens/s, Avg generation throughput: 116.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:20:20 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 98.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:20:21 [logger.py:43] Received request cmpl-ca2b894bc3a54eab8d092b0e0f4f2568-0: prompt: 'Describe lazy loading of weights in vLLM.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 20355, 915, 17366, 8363, 310, 18177, 297, 325, 2208, 29924, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:57716 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:20:21 [engine.py:331] Added request cmpl-ca2b894bc3a54eab8d092b0e0f4f2568-0.
INFO 08-09 14:20:23 [logger.py:43] Received request cmpl-b5effc8ee14d40a08b181dd760796f82-0: prompt: 'Explain the role of SynapseAI in Gaudi software stack.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 12027, 7420, 278, 6297, 310, 10829, 481, 344, 23869, 297, 10415, 4749, 7047, 5096, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:57732 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:20:23 [engine.py:331] Added request cmpl-b5effc8ee14d40a08b181dd760796f82-0.
INFO 08-09 14:20:25 [metrics.py:417] Avg prompt throughput: 5.6 tokens/s, Avg generation throughput: 118.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 14:20:30 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 100.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:20:33 [logger.py:43] Received request cmpl-4b67a84bb8314e28bf7173c38c9012b7-0: prompt: 'What are the differences between HPU and GPU tensors?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 526, 278, 12651, 1546, 379, 7056, 322, 22796, 25187, 943, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:55980 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:20:33 [engine.py:331] Added request cmpl-4b67a84bb8314e28bf7173c38c9012b7-0.
INFO 08-09 14:20:35 [metrics.py:417] Avg prompt throughput: 2.6 tokens/s, Avg generation throughput: 118.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:20:40 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 99.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:20:42 [logger.py:43] Received request cmpl-bd3f9aab655e4d8782f3bb873cbd27c0-0: prompt: 'What are the limitations of LLaMA 2?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 526, 278, 27028, 310, 365, 5661, 1529, 29871, 29906, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:33172 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:20:42 [engine.py:331] Added request cmpl-bd3f9aab655e4d8782f3bb873cbd27c0-0.
INFO 08-09 14:20:45 [metrics.py:417] Avg prompt throughput: 2.4 tokens/s, Avg generation throughput: 114.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:20:50 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 101.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:20:51 [logger.py:43] Received request cmpl-b27e0ebd98084c0c82e12b31fd4b8e42-0: prompt: 'How is multilingual performance evaluated in LLMs?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 338, 1773, 6504, 950, 4180, 19030, 297, 365, 26369, 29879, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:33176 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:20:51 [engine.py:331] Added request cmpl-b27e0ebd98084c0c82e12b31fd4b8e42-0.
INFO 08-09 14:20:55 [metrics.py:417] Avg prompt throughput: 2.6 tokens/s, Avg generation throughput: 115.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:21:00 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 104.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:21:00 [logger.py:43] Received request cmpl-61bba5cf4c7549738deb3b8073595190-0: prompt: 'Describe an example use of prompt tuning.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 20355, 915, 385, 1342, 671, 310, 9508, 18515, 292, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:55818 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:21:00 [engine.py:331] Added request cmpl-61bba5cf4c7549738deb3b8073595190-0.
INFO 08-09 14:21:05 [metrics.py:417] Avg prompt throughput: 2.2 tokens/s, Avg generation throughput: 116.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:21:09 [logger.py:43] Received request cmpl-47fbe03ad2c44aa09ce742b34fa52aa1-0: prompt: 'What is the attention bottleneck and how to address it?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 278, 8570, 18046, 29880, 1600, 384, 322, 920, 304, 3211, 372, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:51130 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:21:09 [engine.py:331] Added request cmpl-47fbe03ad2c44aa09ce742b34fa52aa1-0.
INFO 08-09 14:21:10 [metrics.py:417] Avg prompt throughput: 3.0 tokens/s, Avg generation throughput: 104.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 14:21:15 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 117.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:21:17 [logger.py:43] Received request cmpl-ed972a23c15a4874bd00dea1d70a8560-0: prompt: 'How does token alignment affect downstream tasks?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 947, 5993, 22239, 6602, 1623, 5461, 9595, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:53310 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:21:17 [engine.py:331] Added request cmpl-ed972a23c15a4874bd00dea1d70a8560-0.
INFO 08-09 14:21:20 [metrics.py:417] Avg prompt throughput: 2.0 tokens/s, Avg generation throughput: 102.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:21:25 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 117.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:21:26 [logger.py:43] Received request cmpl-159ab1b28b0041daab485db4a81eb981-0: prompt: 'What are the challenges in inference on edge devices?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 526, 278, 18066, 267, 297, 27262, 373, 7636, 9224, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:36718 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:21:26 [engine.py:331] Added request cmpl-159ab1b28b0041daab485db4a81eb981-0.
INFO 08-09 14:21:30 [metrics.py:417] Avg prompt throughput: 2.4 tokens/s, Avg generation throughput: 107.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:21:35 [logger.py:43] Received request cmpl-1bd89fc5f68c4073a5c932f4eb944fb4-0: prompt: 'Explain the principle behind rotary embeddings.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 12027, 7420, 278, 12502, 5742, 5731, 653, 8297, 29881, 886, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:46146 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:21:35 [engine.py:331] Added request cmpl-1bd89fc5f68c4073a5c932f4eb944fb4-0.
INFO 08-09 14:21:35 [metrics.py:417] Avg prompt throughput: 2.4 tokens/s, Avg generation throughput: 119.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 14:21:40 [logger.py:43] Received request cmpl-d96c6372d6a244c298d3338ab51ccceb-0: prompt: 'How does temperature affect language model output?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 947, 10430, 6602, 4086, 1904, 1962, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:46152 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:21:40 [engine.py:331] Added request cmpl-d96c6372d6a244c298d3338ab51ccceb-0.
INFO 08-09 14:21:40 [metrics.py:417] Avg prompt throughput: 1.8 tokens/s, Avg generation throughput: 107.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 14:21:45 [logger.py:43] Received request cmpl-d8e6048f4fe64e3da66324cdac8f2c42-0: prompt: 'What are stop sequences in generation APIs?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 526, 5040, 15602, 297, 12623, 23649, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:53624 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:21:45 [engine.py:331] Added request cmpl-d8e6048f4fe64e3da66324cdac8f2c42-0.
INFO 08-09 14:21:45 [metrics.py:417] Avg prompt throughput: 1.8 tokens/s, Avg generation throughput: 120.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 14:21:50 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 109.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:21:54 [logger.py:43] Received request cmpl-68e875f6751842269f123eaf573fd6cb-0: prompt: 'Describe token streaming latency optimization.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 20355, 915, 5993, 24820, 23316, 1270, 13883, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:34100 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:21:54 [engine.py:331] Added request cmpl-68e875f6751842269f123eaf573fd6cb-0.
INFO 08-09 14:21:55 [metrics.py:417] Avg prompt throughput: 1.8 tokens/s, Avg generation throughput: 116.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 14:22:00 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 108.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:22:01 [logger.py:43] Received request cmpl-7dd0edd0d8f44e57afb568554e491ea3-0: prompt: 'How to evaluate hallucinations in language models?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 304, 14707, 12713, 1682, 262, 800, 297, 4086, 4733, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:37690 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:22:01 [engine.py:331] Added request cmpl-7dd0edd0d8f44e57afb568554e491ea3-0.
INFO 08-09 14:22:05 [metrics.py:417] Avg prompt throughput: 2.4 tokens/s, Avg generation throughput: 116.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:22:10 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 103.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:22:10 [logger.py:43] Received request cmpl-40d47008659c47b08a5ed409301733c0-0: prompt: 'What is the purpose of `bos_token_id`?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 278, 6437, 310, 421, 27737, 29918, 6979, 29918, 333, 6522], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:37694 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:22:10 [engine.py:331] Added request cmpl-40d47008659c47b08a5ed409301733c0-0.
INFO 08-09 14:22:15 [metrics.py:417] Avg prompt throughput: 2.6 tokens/s, Avg generation throughput: 118.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:22:15 [logger.py:43] Received request cmpl-277ec6a24f404dce8c94de2f570b03c8-0: prompt: 'How are fused attention layers implemented?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 526, 285, 3880, 8570, 15359, 8762, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:50700 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:22:15 [engine.py:331] Added request cmpl-277ec6a24f404dce8c94de2f570b03c8-0.
INFO 08-09 14:22:20 [metrics.py:417] Avg prompt throughput: 1.8 tokens/s, Avg generation throughput: 102.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:22:24 [logger.py:43] Received request cmpl-6904717bc4f14974ac81f65c77519201-0: prompt: 'Describe a pipeline for deploying LLaMA 2 in production.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 20355, 915, 263, 16439, 363, 7246, 292, 365, 5661, 1529, 29871, 29906, 297, 5802, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:38572 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:22:24 [engine.py:331] Added request cmpl-6904717bc4f14974ac81f65c77519201-0.
INFO 08-09 14:22:25 [metrics.py:417] Avg prompt throughput: 3.2 tokens/s, Avg generation throughput: 116.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 14:22:30 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 105.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:22:34 [logger.py:43] Received request cmpl-7e25819299f649d99dcdc04a8f6c161b-0: prompt: 'How does instruction following improve model usability?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 947, 15278, 1494, 11157, 1904, 502, 3097, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:48818 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:22:34 [engine.py:331] Added request cmpl-7e25819299f649d99dcdc04a8f6c161b-0.
INFO 08-09 14:22:35 [metrics.py:417] Avg prompt throughput: 2.0 tokens/s, Avg generation throughput: 113.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 14:22:40 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 104.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:22:43 [logger.py:43] Received request cmpl-c7e799e4275d426195e61652a498688d-0: prompt: 'What is a sequence-to-sequence model?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 263, 5665, 29899, 517, 29899, 16506, 1904, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:43436 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:22:43 [engine.py:331] Added request cmpl-c7e799e4275d426195e61652a498688d-0.
INFO 08-09 14:22:45 [metrics.py:417] Avg prompt throughput: 2.2 tokens/s, Avg generation throughput: 115.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 14:22:50 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 99.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:22:52 [logger.py:43] Received request cmpl-b59bd904e2eb48a78695eb07a86dc24a-0: prompt: 'What is the purpose of `pad_token_id` in generation?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 278, 6437, 310, 421, 8305, 29918, 6979, 29918, 333, 29952, 297, 12623, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:41920 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:22:52 [engine.py:331] Added request cmpl-b59bd904e2eb48a78695eb07a86dc24a-0.
INFO 08-09 14:22:55 [metrics.py:417] Avg prompt throughput: 3.2 tokens/s, Avg generation throughput: 116.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:22:57 [logger.py:43] Received request cmpl-1744b04b2aab43f4aafb1cbdf51a0c46-0: prompt: 'Explain the concept of auto-regressive generation.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 12027, 7420, 278, 6964, 310, 4469, 29899, 276, 3663, 573, 12623, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:41926 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:22:57 [engine.py:331] Added request cmpl-1744b04b2aab43f4aafb1cbdf51a0c46-0.
INFO 08-09 14:23:00 [metrics.py:417] Avg prompt throughput: 2.6 tokens/s, Avg generation throughput: 98.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 14:23:05 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 118.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:23:06 [logger.py:43] Received request cmpl-2a2f85e7ad364e8f9764d54086df2322-0: prompt: 'How is top-p sampling different from nucleus sampling?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 338, 2246, 29899, 29886, 23460, 1422, 515, 22699, 375, 23460, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:34234 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:23:06 [engine.py:331] Added request cmpl-2a2f85e7ad364e8f9764d54086df2322-0.
INFO 08-09 14:23:10 [metrics.py:417] Avg prompt throughput: 2.6 tokens/s, Avg generation throughput: 98.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:23:13 [logger.py:43] Received request cmpl-71f4e931a2d14c1c81683e3f6d894995-0: prompt: 'What are decoding strategies for safe responses?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 526, 1602, 3689, 16650, 583, 363, 9109, 20890, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:58468 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:23:13 [engine.py:331] Added request cmpl-71f4e931a2d14c1c81683e3f6d894995-0.
INFO 08-09 14:23:15 [metrics.py:417] Avg prompt throughput: 2.2 tokens/s, Avg generation throughput: 111.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 14:23:20 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 109.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:23:22 [logger.py:43] Received request cmpl-a1c1c2dc385042fcb3d88ffaf582e79e-0: prompt: 'How are model prompts structured in ChatML?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 526, 1904, 9508, 29879, 2281, 2955, 297, 678, 271, 1988, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:33126 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:23:22 [engine.py:331] Added request cmpl-a1c1c2dc385042fcb3d88ffaf582e79e-0.
INFO 08-09 14:23:25 [metrics.py:417] Avg prompt throughput: 2.6 tokens/s, Avg generation throughput: 111.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:23:30 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 107.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:23:31 [logger.py:43] Received request cmpl-286d0be9dd2943ca98da8816331f2ff8-0: prompt: 'What is a system prompt in chat inference?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 263, 1788, 9508, 297, 13563, 27262, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:33136 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:23:31 [engine.py:331] Added request cmpl-286d0be9dd2943ca98da8816331f2ff8-0.
INFO 08-09 14:23:35 [metrics.py:417] Avg prompt throughput: 2.0 tokens/s, Avg generation throughput: 111.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:23:40 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 105.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:23:40 [logger.py:43] Received request cmpl-a4726cf6d34144f7964a098b79a6e7e6-0: prompt: 'How do you batch requests for high throughput?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 437, 366, 9853, 7274, 363, 1880, 1549, 649, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:41214 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:23:40 [engine.py:331] Added request cmpl-a4726cf6d34144f7964a098b79a6e7e6-0.
INFO 08-09 14:23:45 [metrics.py:417] Avg prompt throughput: 2.2 tokens/s, Avg generation throughput: 119.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:23:49 [logger.py:43] Received request cmpl-75b50e8be92f404aa1da3d62bfff79a7-0: prompt: 'Explain model serving with `transformers + vLLM`.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 12027, 7420, 1904, 16330, 411, 421, 9067, 414, 718, 325, 2208, 29924, 1412], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:42948 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:23:49 [engine.py:331] Added request cmpl-75b50e8be92f404aa1da3d62bfff79a7-0.
INFO 08-09 14:23:50 [metrics.py:417] Avg prompt throughput: 2.8 tokens/s, Avg generation throughput: 109.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 14:23:55 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 118.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:23:58 [logger.py:43] Received request cmpl-5037aaa629cf4b2bad2a95c79cb49379-0: prompt: 'What is Triton Inference Server?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 323, 768, 265, 512, 1659, 5656, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:34742 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:23:58 [engine.py:331] Added request cmpl-5037aaa629cf4b2bad2a95c79cb49379-0.
INFO 08-09 14:24:00 [metrics.py:417] Avg prompt throughput: 2.0 tokens/s, Avg generation throughput: 104.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 14:24:05 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 115.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:24:07 [logger.py:43] Received request cmpl-7c08089075a84a299869a9cd59d84984-0: prompt: 'Describe the significance of large context windows.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 20355, 915, 278, 26002, 310, 2919, 3030, 5417, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:58754 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:24:07 [engine.py:331] Added request cmpl-7c08089075a84a299869a9cd59d84984-0.
INFO 08-09 14:24:10 [metrics.py:417] Avg prompt throughput: 2.0 tokens/s, Avg generation throughput: 106.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:24:15 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 116.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:24:16 [logger.py:43] Received request cmpl-5a37790bcd314abbba4b0e425d89043a-0: prompt: 'What is dynamic quantization in transformers?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 7343, 4323, 2133, 297, 4327, 414, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:33266 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:24:16 [engine.py:331] Added request cmpl-5a37790bcd314abbba4b0e425d89043a-0.
INFO 08-09 14:24:20 [metrics.py:417] Avg prompt throughput: 2.0 tokens/s, Avg generation throughput: 105.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:24:25 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 110.2 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 14:24:25 [logger.py:43] Received request cmpl-fd44de9bb8f74f1897b13fed1bded6c5-0: prompt: 'How to prevent prompt injection in LLMs?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 304, 5557, 9508, 20859, 297, 365, 26369, 29879, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:60906 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:24:25 [engine.py:331] Added request cmpl-fd44de9bb8f74f1897b13fed1bded6c5-0.
INFO 08-09 14:24:30 [metrics.py:417] Avg prompt throughput: 2.2 tokens/s, Avg generation throughput: 101.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:24:30 [logger.py:43] Received request cmpl-0daf3f093f054366958156d65454cb71-0: prompt: 'What are attention scores and how are they computed?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 526, 8570, 19435, 322, 920, 526, 896, 15712, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:60912 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:24:30 [engine.py:331] Added request cmpl-0daf3f093f054366958156d65454cb71-0.
INFO 08-09 14:24:35 [metrics.py:417] Avg prompt throughput: 2.2 tokens/s, Avg generation throughput: 110.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:24:40 [logger.py:43] Received request cmpl-048304181f68439496fb418e60489ed9-0: prompt: 'What are residual connections in transformers?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 526, 10995, 950, 12368, 297, 4327, 414, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:60756 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:24:40 [engine.py:331] Added request cmpl-048304181f68439496fb418e60489ed9-0.
INFO 08-09 14:24:40 [metrics.py:417] Avg prompt throughput: 2.0 tokens/s, Avg generation throughput: 96.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 14:24:45 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 118.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:24:49 [logger.py:43] Received request cmpl-70078c6a70f846c3b7a128a27cb25b98-0: prompt: 'How to handle long sequences in generation?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 304, 4386, 1472, 15602, 297, 12623, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:35924 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:24:49 [engine.py:331] Added request cmpl-70078c6a70f846c3b7a128a27cb25b98-0.
INFO 08-09 14:24:50 [metrics.py:417] Avg prompt throughput: 1.8 tokens/s, Avg generation throughput: 105.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 14:24:51 [logger.py:43] Received request cmpl-f60af51c008344a9b51ee64548c04d7a-0: prompt: 'Explain the use of flash attention v2.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 12027, 7420, 278, 671, 310, 11013, 8570, 325, 29906, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:35936 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:24:51 [engine.py:331] Added request cmpl-f60af51c008344a9b51ee64548c04d7a-0.
INFO 08-09 14:24:55 [metrics.py:417] Avg prompt throughput: 2.2 tokens/s, Avg generation throughput: 119.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:25:00 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 95.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:25:00 [logger.py:43] Received request cmpl-d76ccfb9341c42c7a73cdedd0147b5eb-0: prompt: 'What is a greedy search in inference?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 263, 1395, 7584, 2740, 297, 27262, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:49812 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:25:00 [engine.py:331] Added request cmpl-d76ccfb9341c42c7a73cdedd0147b5eb-0.
INFO 08-09 14:25:02 [logger.py:43] Received request cmpl-6777dc3aa83547f1b147dab15350847a-0: prompt: 'How do checkpoints improve fault tolerance?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 437, 1423, 9748, 11157, 12570, 20341, 749, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:46724 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:25:02 [engine.py:331] Added request cmpl-6777dc3aa83547f1b147dab15350847a-0.
INFO 08-09 14:25:05 [metrics.py:417] Avg prompt throughput: 4.0 tokens/s, Avg generation throughput: 114.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:25:10 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 104.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:25:11 [logger.py:43] Received request cmpl-148daa24342b4f4287e83d9f3e7509de-0: prompt: 'Describe how text-to-text generation works.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 20355, 915, 920, 1426, 29899, 517, 29899, 726, 12623, 1736, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:49420 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:25:11 [engine.py:331] Added request cmpl-148daa24342b4f4287e83d9f3e7509de-0.
INFO 08-09 14:25:15 [metrics.py:417] Avg prompt throughput: 2.4 tokens/s, Avg generation throughput: 116.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:25:20 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 101.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:25:21 [logger.py:43] Received request cmpl-99bccdc76e864af6aaf0e6de1b1c4915-0: prompt: 'Explain cosine similarity in embeddings.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 12027, 7420, 6776, 457, 29501, 297, 8297, 29881, 886, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:49436 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:25:21 [engine.py:331] Added request cmpl-99bccdc76e864af6aaf0e6de1b1c4915-0.
INFO 08-09 14:25:25 [metrics.py:417] Avg prompt throughput: 2.2 tokens/s, Avg generation throughput: 113.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:25:30 [logger.py:43] Received request cmpl-243b66bc720647e3b18faefc766b0e66-0: prompt: 'What is token-wise parallelism?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 5993, 29899, 3538, 8943, 1608, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:40682 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:25:30 [engine.py:331] Added request cmpl-243b66bc720647e3b18faefc766b0e66-0.
INFO 08-09 14:25:30 [metrics.py:417] Avg prompt throughput: 1.8 tokens/s, Avg generation throughput: 100.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 14:25:35 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 118.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:25:39 [logger.py:43] Received request cmpl-77f59ef8abfc47a886c147e81766bc46-0: prompt: 'How do shared embeddings help in decoder-only models?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 437, 7258, 8297, 29881, 886, 1371, 297, 1602, 6119, 29899, 6194, 4733, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:48976 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:25:39 [engine.py:331] Added request cmpl-77f59ef8abfc47a886c147e81766bc46-0.
INFO 08-09 14:25:40 [metrics.py:417] Avg prompt throughput: 3.0 tokens/s, Avg generation throughput: 104.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 14:25:45 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 115.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:25:48 [logger.py:43] Received request cmpl-7f1b65b7552e45d9863be492b4b7e949-0: prompt: 'Explain how to measure latency in model output.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 12027, 7420, 920, 304, 5645, 23316, 1270, 297, 1904, 1962, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:46856 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:25:48 [engine.py:331] Added request cmpl-7f1b65b7552e45d9863be492b4b7e949-0.
INFO 08-09 14:25:50 [metrics.py:417] Avg prompt throughput: 2.4 tokens/s, Avg generation throughput: 105.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 14:25:55 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 118.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:25:57 [logger.py:43] Received request cmpl-902cc0b7b4304a0287c1feb54465bf5d-0: prompt: 'What is the purpose of dropout during training?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 278, 6437, 310, 5768, 449, 2645, 6694, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:47122 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:25:57 [engine.py:331] Added request cmpl-902cc0b7b4304a0287c1feb54465bf5d-0.
INFO 08-09 14:26:00 [metrics.py:417] Avg prompt throughput: 2.2 tokens/s, Avg generation throughput: 106.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:26:05 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 119.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:26:05 [logger.py:43] Received request cmpl-bac8cff1f1284e539b8fd5105638fb60-0: prompt: 'What is the role of GELU activation in transformers?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 278, 6297, 310, 402, 6670, 29965, 26229, 297, 4327, 414, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:54214 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:26:05 [engine.py:331] Added request cmpl-bac8cff1f1284e539b8fd5105638fb60-0.
INFO 08-09 14:26:10 [metrics.py:417] Avg prompt throughput: 2.8 tokens/s, Avg generation throughput: 111.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:26:14 [logger.py:43] Received request cmpl-e570bc323aab43569aef2273f8d503e2-0: prompt: 'Explain forward and backward pass in transformers.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 12027, 7420, 6375, 322, 1250, 1328, 1209, 297, 4327, 414, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:40678 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:26:14 [engine.py:331] Added request cmpl-e570bc323aab43569aef2273f8d503e2-0.
INFO 08-09 14:26:15 [metrics.py:417] Avg prompt throughput: 2.4 tokens/s, Avg generation throughput: 118.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 14:26:20 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 109.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:26:23 [logger.py:43] Received request cmpl-64d27976626346189704fbf212da773f-0: prompt: 'What is speculative decoding and how does it help?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 1580, 28524, 1602, 3689, 322, 920, 947, 372, 1371, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:44672 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:26:23 [engine.py:331] Added request cmpl-64d27976626346189704fbf212da773f-0.
INFO 08-09 14:26:25 [metrics.py:417] Avg prompt throughput: 2.6 tokens/s, Avg generation throughput: 114.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 14:26:30 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 107.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:26:32 [logger.py:43] Received request cmpl-283a29cd1c7c4d6cb3fc726bb17db963-0: prompt: 'How to optimize HPU memory usage during inference?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 304, 24656, 379, 7056, 3370, 8744, 2645, 27262, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:33562 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:26:32 [engine.py:331] Added request cmpl-283a29cd1c7c4d6cb3fc726bb17db963-0.
INFO 08-09 14:26:35 [metrics.py:417] Avg prompt throughput: 2.2 tokens/s, Avg generation throughput: 115.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:26:35 [logger.py:43] Received request cmpl-08b8c7b648c242f3957836ca763214f9-0: prompt: 'Explain profiling for LLM inference performance.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 12027, 7420, 20077, 292, 363, 365, 26369, 27262, 4180, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:33564 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:26:35 [engine.py:331] Added request cmpl-08b8c7b648c242f3957836ca763214f9-0.
INFO 08-09 14:26:40 [metrics.py:417] Avg prompt throughput: 2.2 tokens/s, Avg generation throughput: 106.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:26:44 [logger.py:43] Received request cmpl-f3c1bead12894c29bd57487c0b93bacd-0: prompt: 'How to compute inter-token latency per request?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 304, 10272, 1006, 29899, 6979, 23316, 1270, 639, 2009, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:44464 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:26:44 [engine.py:331] Added request cmpl-f3c1bead12894c29bd57487c0b93bacd-0.
INFO 08-09 14:26:45 [metrics.py:417] Avg prompt throughput: 2.4 tokens/s, Avg generation throughput: 115.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 14:26:49 [logger.py:43] Received request cmpl-7fb3ca0f81b64a26bee0f674f6730856-0: prompt: 'What is the difference between BFloat16 and Float16?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 278, 4328, 1546, 350, 11031, 29896, 29953, 322, 27842, 29896, 29953, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:44480 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:26:49 [engine.py:331] Added request cmpl-7fb3ca0f81b64a26bee0f674f6730856-0.
INFO 08-09 14:26:50 [metrics.py:417] Avg prompt throughput: 3.0 tokens/s, Avg generation throughput: 108.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 14:26:52 [logger.py:43] Received request cmpl-beb35c729bad426bb4ed38ac29e2e65a-0: prompt: 'What are logits and how are they interpreted?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 526, 1480, 1169, 322, 920, 526, 896, 21551, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:60518 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:26:52 [engine.py:331] Added request cmpl-beb35c729bad426bb4ed38ac29e2e65a-0.
INFO 08-09 14:26:55 [metrics.py:417] Avg prompt throughput: 2.2 tokens/s, Avg generation throughput: 118.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:26:58 [logger.py:43] Received request cmpl-d2bb9af594a04152a2f1925dd93d3dfd-0: prompt: 'What is log probability in token prediction?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 1480, 6976, 297, 5993, 18988, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:60520 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:26:58 [engine.py:331] Added request cmpl-d2bb9af594a04152a2f1925dd93d3dfd-0.
INFO 08-09 14:27:00 [metrics.py:417] Avg prompt throughput: 1.8 tokens/s, Avg generation throughput: 103.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 14:27:05 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 119.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:27:06 [logger.py:43] Received request cmpl-271a74ed85e841c68c48e22304b498fd-0: prompt: 'How to use `TextStreamer` for streaming output?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 304, 671, 421, 1626, 3835, 261, 29952, 363, 24820, 1962, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:50478 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:27:06 [engine.py:331] Added request cmpl-271a74ed85e841c68c48e22304b498fd-0.
INFO 08-09 14:27:10 [metrics.py:417] Avg prompt throughput: 2.6 tokens/s, Avg generation throughput: 107.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:27:11 [logger.py:43] Received request cmpl-8805ca1d395844fd8f5a1d062a1c2d12-0: prompt: 'How to tokenize and detokenize a prompt manually?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 304, 5993, 675, 322, 1439, 4476, 675, 263, 9508, 7522, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:55688 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:27:11 [engine.py:331] Added request cmpl-8805ca1d395844fd8f5a1d062a1c2d12-0.
INFO 08-09 14:27:15 [metrics.py:417] Avg prompt throughput: 2.6 tokens/s, Avg generation throughput: 121.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:27:20 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 102.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:27:20 [logger.py:43] Received request cmpl-a9a0fbb9f7814e5aa61b0c4d681965b1-0: prompt: 'Describe how a prompt flows through the transformer layers.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 20355, 915, 920, 263, 9508, 24536, 1549, 278, 4327, 261, 15359, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:55698 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:27:20 [engine.py:331] Added request cmpl-a9a0fbb9f7814e5aa61b0c4d681965b1-0.
INFO 08-09 14:27:25 [metrics.py:417] Avg prompt throughput: 2.6 tokens/s, Avg generation throughput: 119.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:27:29 [logger.py:43] Received request cmpl-cfdf2bc5c5824dffad4d259b4e53efdd-0: prompt: 'How to handle EOS in streaming responses?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 304, 4386, 382, 3267, 297, 24820, 20890, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:34942 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:27:29 [engine.py:331] Added request cmpl-cfdf2bc5c5824dffad4d259b4e53efdd-0.
INFO 08-09 14:27:30 [metrics.py:417] Avg prompt throughput: 2.0 tokens/s, Avg generation throughput: 103.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 14:27:35 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 120.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:27:39 [logger.py:43] Received request cmpl-9dec9bbf6c7d4f698e878fe0d680a75f-0: prompt: 'What is a model config JSON in HF models?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 263, 1904, 2295, 4663, 297, 379, 29943, 4733, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:47132 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:27:39 [engine.py:331] Added request cmpl-9dec9bbf6c7d4f698e878fe0d680a75f-0.
INFO 08-09 14:27:40 [metrics.py:417] Avg prompt throughput: 2.4 tokens/s, Avg generation throughput: 92.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 14:27:42 [logger.py:43] Received request cmpl-bb4de488ce304f109efe5f9ed3ee7790-0: prompt: 'What is kv_cache reuse in decoder models?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 10908, 29918, 8173, 24270, 297, 1602, 6119, 4733, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:53796 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:27:42 [engine.py:331] Added request cmpl-bb4de488ce304f109efe5f9ed3ee7790-0.
INFO 08-09 14:27:45 [metrics.py:417] Avg prompt throughput: 2.4 tokens/s, Avg generation throughput: 117.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:27:50 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 106.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:27:51 [logger.py:43] Received request cmpl-facc759975034365bb018b2c9a5e6063-0: prompt: 'Describe a scenario of real-time chatbot streaming.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 20355, 915, 263, 10483, 310, 1855, 29899, 2230, 13563, 7451, 24820, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:53812 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:27:51 [engine.py:331] Added request cmpl-facc759975034365bb018b2c9a5e6063-0.
INFO 08-09 14:27:55 [metrics.py:417] Avg prompt throughput: 2.6 tokens/s, Avg generation throughput: 115.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:28:00 [logger.py:43] Received request cmpl-f49e525df50241bbb46d73c9a14ce505-0: prompt: 'How to compute throughput with variable prompts?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 304, 10272, 1549, 649, 411, 2286, 9508, 29879, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:49412 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:28:00 [engine.py:331] Added request cmpl-f49e525df50241bbb46d73c9a14ce505-0.
INFO 08-09 14:28:00 [metrics.py:417] Avg prompt throughput: 2.2 tokens/s, Avg generation throughput: 106.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 14:28:05 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 115.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:28:08 [logger.py:43] Received request cmpl-654383c72b8142899549748c3be5745a-0: prompt: 'How does `max_new_tokens` differ from `max_length`?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 947, 421, 3317, 29918, 1482, 29918, 517, 12360, 29952, 1163, 515, 421, 3317, 29918, 2848, 6522], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:57292 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:28:08 [engine.py:331] Added request cmpl-654383c72b8142899549748c3be5745a-0.
INFO 08-09 14:28:10 [metrics.py:417] Avg prompt throughput: 3.6 tokens/s, Avg generation throughput: 102.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 14:28:14 [logger.py:43] Received request cmpl-9560abea89c14688b69e17cab3b84bfe-0: prompt: 'Explain the difference between loss and perplexity.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 12027, 7420, 278, 4328, 1546, 6410, 322, 639, 10709, 537, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:51004 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:28:14 [engine.py:331] Added request cmpl-9560abea89c14688b69e17cab3b84bfe-0.
INFO 08-09 14:28:15 [metrics.py:417] Avg prompt throughput: 2.4 tokens/s, Avg generation throughput: 117.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 14:28:20 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 101.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:28:24 [logger.py:43] Received request cmpl-58b5a80deb7c45a39755b147678bef01-0: prompt: 'How to log benchmark results in a structured CSV?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 304, 1480, 23513, 2582, 297, 263, 2281, 2955, 16874, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:42446 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:28:24 [engine.py:331] Added request cmpl-58b5a80deb7c45a39755b147678bef01-0.
INFO 08-09 14:28:25 [metrics.py:417] Avg prompt throughput: 2.4 tokens/s, Avg generation throughput: 109.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 14:28:30 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 105.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:28:33 [logger.py:43] Received request cmpl-ceca237fff894d5ea61dad15c5f82367-0: prompt: 'What are attention heads and how do they specialize?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 526, 8570, 15883, 322, 920, 437, 896, 4266, 675, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:55954 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:28:33 [engine.py:331] Added request cmpl-ceca237fff894d5ea61dad15c5f82367-0.
INFO 08-09 14:28:35 [metrics.py:417] Avg prompt throughput: 2.4 tokens/s, Avg generation throughput: 115.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:28:40 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 102.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:28:42 [logger.py:43] Received request cmpl-1332598b46894fe18a43c44371b36822-0: prompt: 'How to ensure reproducible benchmarking?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 304, 9801, 9483, 15520, 23513, 292, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:40602 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:28:42 [engine.py:331] Added request cmpl-1332598b46894fe18a43c44371b36822-0.
INFO 08-09 14:28:45 [metrics.py:417] Avg prompt throughput: 1.8 tokens/s, Avg generation throughput: 113.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:28:50 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 105.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:28:51 [logger.py:43] Received request cmpl-9e419c79f13e4314a40f4bed75b6bb1a-0: prompt: 'What are model shards and when are they used?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 526, 1904, 528, 3163, 322, 746, 526, 896, 1304, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:35788 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:28:51 [engine.py:331] Added request cmpl-9e419c79f13e4314a40f4bed75b6bb1a-0.
INFO 08-09 14:28:55 [metrics.py:417] Avg prompt throughput: 2.4 tokens/s, Avg generation throughput: 117.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:29:00 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 105.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:29:00 [logger.py:43] Received request cmpl-6a31b3863c2441679d0b60e96c1eff20-0: prompt: 'Explain the relationship between prompt length and latency.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 12027, 7420, 278, 9443, 1546, 9508, 3309, 322, 23316, 1270, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:35802 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:29:00 [engine.py:331] Added request cmpl-6a31b3863c2441679d0b60e96c1eff20-0.
INFO 08-09 14:29:05 [metrics.py:417] Avg prompt throughput: 2.4 tokens/s, Avg generation throughput: 119.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:29:09 [logger.py:43] Received request cmpl-7876581a51484c89bb04e709ef2865de-0: prompt: 'What is batch inference and why is it faster?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 9853, 27262, 322, 2020, 338, 372, 8473, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:50676 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:29:09 [engine.py:331] Added request cmpl-7876581a51484c89bb04e709ef2865de-0.
INFO 08-09 14:29:10 [metrics.py:417] Avg prompt throughput: 2.2 tokens/s, Avg generation throughput: 105.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 14:29:15 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 113.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:29:18 [logger.py:43] Received request cmpl-626785c4383d4cada9185539204a32a4-0: prompt: 'What are rotary sin-cos embeddings?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 526, 5731, 653, 4457, 29899, 3944, 8297, 29881, 886, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:41480 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:29:18 [engine.py:331] Added request cmpl-626785c4383d4cada9185539204a32a4-0.
INFO 08-09 14:29:20 [metrics.py:417] Avg prompt throughput: 2.4 tokens/s, Avg generation throughput: 100.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 14:29:25 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 118.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:29:28 [logger.py:43] Received request cmpl-404c01d145204628bc3d348d3407dd8c-0: prompt: "Describe HuggingFace's `AutoTokenizer` features.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 20355, 915, 379, 688, 3460, 23360, 29915, 29879, 421, 12300, 6066, 3950, 29952, 5680, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:34552 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:29:28 [engine.py:331] Added request cmpl-404c01d145204628bc3d348d3407dd8c-0.
INFO 08-09 14:29:30 [metrics.py:417] Avg prompt throughput: 3.2 tokens/s, Avg generation throughput: 100.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:29:35 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 114.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:29:37 [logger.py:43] Received request cmpl-80098f0a1df549b1afccedd31a0189b1-0: prompt: 'How does positional encoding interact with attention?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 947, 2602, 284, 8025, 16254, 411, 8570, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:46420 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:29:37 [engine.py:331] Added request cmpl-80098f0a1df549b1afccedd31a0189b1-0.
INFO 08-09 14:29:40 [metrics.py:417] Avg prompt throughput: 2.0 tokens/s, Avg generation throughput: 97.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:29:43 [logger.py:43] Received request cmpl-a250bf3736364b8aa396247283f25bdd-0: prompt: 'What is a streaming callback in Transformers?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 263, 24820, 6939, 297, 4103, 689, 414, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:36282 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:29:43 [engine.py:331] Added request cmpl-a250bf3736364b8aa396247283f25bdd-0.
INFO 08-09 14:29:45 [metrics.py:417] Avg prompt throughput: 2.2 tokens/s, Avg generation throughput: 115.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:29:50 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 107.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:29:52 [logger.py:43] Received request cmpl-e2af989b15524153b598e6887652836d-0: prompt: 'How to benchmark models using curl and Python?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 304, 23513, 4733, 773, 11051, 322, 5132, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:33312 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:29:52 [engine.py:331] Added request cmpl-e2af989b15524153b598e6887652836d-0.
INFO 08-09 14:29:55 [metrics.py:417] Avg prompt throughput: 2.0 tokens/s, Avg generation throughput: 105.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:30:00 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 103.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:30:02 [logger.py:43] Received request cmpl-32d9529fa6264c368e1874ca98edd4c2-0: prompt: 'How to parse streaming responses from vLLM?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 304, 6088, 24820, 20890, 515, 325, 2208, 29924, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:59820 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:30:02 [engine.py:331] Added request cmpl-32d9529fa6264c368e1874ca98edd4c2-0.
INFO 08-09 14:30:05 [metrics.py:417] Avg prompt throughput: 2.2 tokens/s, Avg generation throughput: 117.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:30:10 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 98.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:30:11 [logger.py:43] Received request cmpl-ba8b24a6405440b9a4dfd6067cda710d-0: prompt: 'How does deep caching work in inference?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 947, 6483, 22488, 664, 297, 27262, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:59830 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:30:11 [engine.py:331] Added request cmpl-ba8b24a6405440b9a4dfd6067cda710d-0.
INFO 08-09 14:30:15 [metrics.py:417] Avg prompt throughput: 1.8 tokens/s, Avg generation throughput: 110.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:30:20 [logger.py:43] Received request cmpl-89e3837d5f3743a980eaa8d68e31eb23-0: prompt: 'What is a prompt-truncation error?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 263, 9508, 29899, 509, 4661, 362, 1059, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:55544 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:30:20 [engine.py:331] Added request cmpl-89e3837d5f3743a980eaa8d68e31eb23-0.
INFO 08-09 14:30:20 [metrics.py:417] Avg prompt throughput: 2.2 tokens/s, Avg generation throughput: 111.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 14:30:25 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 118.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:30:28 [logger.py:43] Received request cmpl-6c3db165cc9b4db6b2b9f27eaf13a5cc-0: prompt: 'Explain the role of tokenizer config files.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 12027, 7420, 278, 6297, 310, 5993, 3950, 2295, 2066, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:43600 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:30:28 [engine.py:331] Added request cmpl-6c3db165cc9b4db6b2b9f27eaf13a5cc-0.
INFO 08-09 14:30:30 [metrics.py:417] Avg prompt throughput: 2.2 tokens/s, Avg generation throughput: 107.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 14:30:35 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 120.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:30:37 [logger.py:43] Received request cmpl-48a0c6d35a4a47f2b22a32e287d2bf97-0: prompt: 'How to batch multiple prompts into one request?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 304, 9853, 2999, 9508, 29879, 964, 697, 2009, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:43454 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:30:37 [engine.py:331] Added request cmpl-48a0c6d35a4a47f2b22a32e287d2bf97-0.
INFO 08-09 14:30:40 [metrics.py:417] Avg prompt throughput: 2.2 tokens/s, Avg generation throughput: 105.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:30:45 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 118.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:30:46 [logger.py:43] Received request cmpl-f1676b1231874576aa22fedfe950b657-0: prompt: 'What is a chat template in HuggingFace?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 263, 13563, 4472, 297, 379, 688, 3460, 23360, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:42938 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:30:46 [engine.py:331] Added request cmpl-f1676b1231874576aa22fedfe950b657-0.
INFO 08-09 14:30:50 [metrics.py:417] Avg prompt throughput: 2.4 tokens/s, Avg generation throughput: 113.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:30:55 [logger.py:43] Received request cmpl-252be2b10771402289de5f605cdf6f11-0: prompt: 'What is inference latency and why is it important?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 27262, 23316, 1270, 322, 2020, 338, 372, 4100, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:52240 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:30:55 [engine.py:331] Added request cmpl-252be2b10771402289de5f605cdf6f11-0.
INFO 08-09 14:30:55 [metrics.py:417] Avg prompt throughput: 2.4 tokens/s, Avg generation throughput: 112.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 14:31:00 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 99.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:31:04 [logger.py:43] Received request cmpl-9f64f006d0d74dfa8ce81259fd903f52-0: prompt: 'How to reduce memory footprint of LLaMA models?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 304, 10032, 3370, 3661, 2158, 310, 365, 5661, 1529, 4733, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:52820 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:31:04 [engine.py:331] Added request cmpl-9f64f006d0d74dfa8ce81259fd903f52-0.
INFO 08-09 14:31:05 [metrics.py:417] Avg prompt throughput: 2.6 tokens/s, Avg generation throughput: 119.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 14:31:10 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 115.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:31:13 [logger.py:43] Received request cmpl-5387ef4cedf4479e815db085a6e6d672-0: prompt: 'What is the difference between `generation_config.json` and `config.json`?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 278, 4328, 1546, 421, 4738, 362, 29918, 2917, 29889, 3126, 29952, 322, 421, 2917, 29889, 3126, 6522], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:47782 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:31:13 [engine.py:331] Added request cmpl-5387ef4cedf4479e815db085a6e6d672-0.
INFO 08-09 14:31:15 [metrics.py:417] Avg prompt throughput: 4.0 tokens/s, Avg generation throughput: 116.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:31:18 [logger.py:43] Received request cmpl-1aef5af6bb9843829cb47dffb6b36dcf-0: prompt: 'What is a quantized model and how is it served?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 263, 4323, 1891, 1904, 322, 920, 338, 372, 6766, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:47786 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:31:18 [engine.py:331] Added request cmpl-1aef5af6bb9843829cb47dffb6b36dcf-0.
INFO 08-09 14:31:20 [metrics.py:417] Avg prompt throughput: 2.6 tokens/s, Avg generation throughput: 100.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 14:31:25 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 112.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:31:28 [logger.py:43] Received request cmpl-6f00d35e8d7946f69eaa8914c28c3c14-0: prompt: 'How to handle prompt overflow in generation?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 304, 4386, 9508, 11969, 297, 12623, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:53274 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:31:28 [engine.py:331] Added request cmpl-6f00d35e8d7946f69eaa8914c28c3c14-0.
INFO 08-09 14:31:30 [metrics.py:417] Avg prompt throughput: 1.8 tokens/s, Avg generation throughput: 101.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:31:35 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 121.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:31:36 [logger.py:43] Received request cmpl-93a81eecd1e1400d9cd825d0a8e17e37-0: prompt: 'What is warmup time in inference pipelines?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 14294, 786, 931, 297, 27262, 8450, 24210, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:57000 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:31:36 [engine.py:331] Added request cmpl-93a81eecd1e1400d9cd825d0a8e17e37-0.
INFO 08-09 14:31:40 [metrics.py:417] Avg prompt throughput: 2.2 tokens/s, Avg generation throughput: 99.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:31:45 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 121.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:31:45 [logger.py:43] Received request cmpl-6bcc2b2adc484bef8d3b51de633ced51-0: prompt: 'How to stream JSON tokens using curl?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 304, 4840, 4663, 18897, 773, 11051, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:33086 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:31:45 [engine.py:331] Added request cmpl-6bcc2b2adc484bef8d3b51de633ced51-0.
INFO 08-09 14:31:50 [metrics.py:417] Avg prompt throughput: 1.8 tokens/s, Avg generation throughput: 100.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:31:54 [logger.py:43] Received request cmpl-966a436869a64ee6abce49ada75ec8fc-0: prompt: 'What is a `generate_with_streaming()` wrapper?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 263, 421, 17158, 29918, 2541, 29918, 5461, 292, 2555, 14476, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:37610 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:31:54 [engine.py:331] Added request cmpl-966a436869a64ee6abce49ada75ec8fc-0.
INFO 08-09 14:31:55 [metrics.py:417] Avg prompt throughput: 2.8 tokens/s, Avg generation throughput: 121.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 14:32:00 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 101.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:32:04 [logger.py:43] Received request cmpl-08c2bc894563429abf220ee221c158a0-0: prompt: 'How to evaluate TTFT and ITL for multiple prompts?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 304, 14707, 323, 29911, 7818, 322, 13315, 29931, 363, 2999, 9508, 29879, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:60180 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:32:04 [engine.py:331] Added request cmpl-08c2bc894563429abf220ee221c158a0-0.
INFO 08-09 14:32:05 [logger.py:43] Received request cmpl-18d1433b251c48b89fad688a348c63de-0: prompt: 'What are server cold starts in model serving?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 526, 1923, 11220, 8665, 297, 1904, 16330, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:60182 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:32:05 [engine.py:331] Added request cmpl-18d1433b251c48b89fad688a348c63de-0.
INFO 08-09 14:32:05 [metrics.py:417] Avg prompt throughput: 5.0 tokens/s, Avg generation throughput: 116.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 14:32:10 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 103.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:32:14 [logger.py:43] Received request cmpl-4fd31e0f45da47c593ab0085615809c1-0: prompt: 'Explain how to avoid padding inefficiencies.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 12027, 7420, 920, 304, 4772, 7164, 297, 29872, 2416, 819, 2478, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:59622 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:32:14 [engine.py:331] Added request cmpl-4fd31e0f45da47c593ab0085615809c1-0.
INFO 08-09 14:32:15 [metrics.py:417] Avg prompt throughput: 2.6 tokens/s, Avg generation throughput: 116.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 14:32:20 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 101.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:32:23 [logger.py:43] Received request cmpl-d42d055d1aeb4ecdb41e85d5745ffa43-0: prompt: 'How does tokenizer pre-processing affect TTFT?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 947, 5993, 3950, 758, 29899, 19170, 6602, 323, 29911, 7818, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:41382 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:32:23 [engine.py:331] Added request cmpl-d42d055d1aeb4ecdb41e85d5745ffa43-0.
INFO 08-09 14:32:25 [metrics.py:417] Avg prompt throughput: 2.6 tokens/s, Avg generation throughput: 116.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 14:32:30 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 101.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:32:33 [logger.py:43] Received request cmpl-bc715bbc2a4849659444ba51331af76f-0: prompt: 'How to track per-prompt latency in logs?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 304, 5702, 639, 29899, 14032, 415, 23316, 1270, 297, 10748, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:55768 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:32:33 [engine.py:331] Added request cmpl-bc715bbc2a4849659444ba51331af76f-0.
INFO 08-09 14:32:35 [metrics.py:417] Avg prompt throughput: 2.6 tokens/s, Avg generation throughput: 118.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:32:41 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 106.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:32:41 [logger.py:43] Received request cmpl-689022a239584fc5a2eb32421ba3f58e-0: prompt: 'Explain streaming benchmarks with high concurrency.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 12027, 7420, 24820, 23513, 29879, 411, 1880, 3022, 10880, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:45868 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:32:41 [engine.py:331] Added request cmpl-689022a239584fc5a2eb32421ba3f58e-0.
INFO 08-09 14:32:46 [metrics.py:417] Avg prompt throughput: 2.2 tokens/s, Avg generation throughput: 117.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:32:51 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 104.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:32:51 [logger.py:43] Received request cmpl-ea499f02f4dc49d19c3674b19b1c683c-0: prompt: 'How to calculate power usage per prompt?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 304, 8147, 3081, 8744, 639, 9508, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:45878 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:32:51 [engine.py:331] Added request cmpl-ea499f02f4dc49d19c3674b19b1c683c-0.
INFO 08-09 14:32:56 [metrics.py:417] Avg prompt throughput: 1.8 tokens/s, Avg generation throughput: 113.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:33:00 [logger.py:43] Received request cmpl-d32eb8eb99bf4a82ac576437da5bc007-0: prompt: 'How does the tokenizer handle emojis and symbols?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 947, 278, 5993, 3950, 4386, 953, 3848, 275, 322, 15072, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:53390 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:33:00 [engine.py:331] Added request cmpl-d32eb8eb99bf4a82ac576437da5bc007-0.
INFO 08-09 14:33:01 [metrics.py:417] Avg prompt throughput: 2.6 tokens/s, Avg generation throughput: 105.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 14:33:06 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 122.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:33:08 [logger.py:43] Received request cmpl-262fefb965774cb9987b63a33c75bcae-0: prompt: 'How does model size affect inference latency?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 947, 1904, 2159, 6602, 27262, 23316, 1270, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:33952 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:33:08 [engine.py:331] Added request cmpl-262fefb965774cb9987b63a33c75bcae-0.
INFO 08-09 14:33:11 [metrics.py:417] Avg prompt throughput: 2.0 tokens/s, Avg generation throughput: 108.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 14:33:16 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 110.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:33:18 [logger.py:43] Received request cmpl-778a2c1c62df491ea7a6947979349ab4-0: prompt: 'How to debug slow streaming responses?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 304, 4744, 5232, 24820, 20890, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:46818 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:33:18 [engine.py:331] Added request cmpl-778a2c1c62df491ea7a6947979349ab4-0.
INFO 08-09 14:33:21 [metrics.py:417] Avg prompt throughput: 1.6 tokens/s, Avg generation throughput: 105.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:33:26 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 123.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:33:26 [logger.py:43] Received request cmpl-b26790997c334e158f3a30285d78b79e-0: prompt: 'What causes generation to stall midway?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 9946, 12623, 304, 380, 497, 7145, 1582, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:53470 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:33:26 [engine.py:331] Added request cmpl-b26790997c334e158f3a30285d78b79e-0.
INFO 08-09 14:33:31 [metrics.py:417] Avg prompt throughput: 2.0 tokens/s, Avg generation throughput: 110.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:33:35 [logger.py:43] Received request cmpl-7fc1da1e69b546279721c59c2d0f5b63-0: prompt: 'How to load a model on HPU using HF Transformers?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 304, 2254, 263, 1904, 373, 379, 7056, 773, 379, 29943, 4103, 689, 414, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:40356 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:33:35 [engine.py:331] Added request cmpl-7fc1da1e69b546279721c59c2d0f5b63-0.
INFO 08-09 14:33:36 [metrics.py:417] Avg prompt throughput: 3.2 tokens/s, Avg generation throughput: 115.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 14:33:41 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 98.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:33:44 [logger.py:43] Received request cmpl-f4f2d6989f824f6cbdf723a361efb1fa-0: prompt: 'How to pass prompt + history into a chat model?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 304, 1209, 9508, 718, 4955, 964, 263, 13563, 1904, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:57240 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:33:44 [engine.py:331] Added request cmpl-f4f2d6989f824f6cbdf723a361efb1fa-0.
INFO 08-09 14:33:46 [metrics.py:417] Avg prompt throughput: 2.4 tokens/s, Avg generation throughput: 119.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 14:33:50 [logger.py:43] Received request cmpl-d0d69ae25d024b4785243549321b6338-0: prompt: 'What is an OpenAI-compatible inference API?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 385, 4673, 23869, 29899, 23712, 27262, 3450, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:57242 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:33:50 [engine.py:331] Added request cmpl-d0d69ae25d024b4785243549321b6338-0.
INFO 08-09 14:33:51 [metrics.py:417] Avg prompt throughput: 2.2 tokens/s, Avg generation throughput: 110.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 14:33:56 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 120.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:33:58 [logger.py:43] Received request cmpl-4b76a01b45e14bf98fe8ff441c65a8bf-0: prompt: 'How to handle decoding errors in stream?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 304, 4386, 1602, 3689, 4436, 297, 4840, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:60598 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:33:58 [engine.py:331] Added request cmpl-4b76a01b45e14bf98fe8ff441c65a8bf-0.
INFO 08-09 14:34:01 [metrics.py:417] Avg prompt throughput: 2.0 tokens/s, Avg generation throughput: 103.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 14:34:06 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 115.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:34:08 [logger.py:43] Received request cmpl-06083b967e5841d9abd103ff08edcf32-0: prompt: 'What are temperature vs randomness in sampling?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 526, 10430, 7186, 4036, 2264, 297, 23460, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:59410 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:34:08 [engine.py:331] Added request cmpl-06083b967e5841d9abd103ff08edcf32-0.
INFO 08-09 14:34:11 [metrics.py:417] Avg prompt throughput: 2.0 tokens/s, Avg generation throughput: 100.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:34:16 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 121.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:34:16 [logger.py:43] Received request cmpl-04d30923189d4a3daa78e18050864e86-0: prompt: 'Explain token logprobs in output.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 12027, 7420, 5993, 1480, 771, 5824, 297, 1962, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:45940 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:34:16 [engine.py:331] Added request cmpl-04d30923189d4a3daa78e18050864e86-0.
INFO 08-09 14:34:21 [metrics.py:417] Avg prompt throughput: 2.0 tokens/s, Avg generation throughput: 99.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:34:26 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 116.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:34:26 [logger.py:43] Received request cmpl-8afbe46302b44648843ea5fbbc6557a8-0: prompt: 'How to measure and reduce TTFT on Gaudi?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 304, 5645, 322, 10032, 323, 29911, 7818, 373, 10415, 4749, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:60734 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:34:26 [engine.py:331] Added request cmpl-8afbe46302b44648843ea5fbbc6557a8-0.
INFO 08-09 14:34:31 [metrics.py:417] Avg prompt throughput: 2.6 tokens/s, Avg generation throughput: 102.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:34:35 [logger.py:43] Received request cmpl-ce0a1e5a22a24d70af395d4056b8c928-0: prompt: 'Explain the internals of HF `generate()` loop.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 12027, 7420, 278, 2836, 1338, 310, 379, 29943, 421, 17158, 2555, 2425, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:42240 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:34:35 [engine.py:331] Added request cmpl-ce0a1e5a22a24d70af395d4056b8c928-0.
INFO 08-09 14:34:36 [metrics.py:417] Avg prompt throughput: 2.8 tokens/s, Avg generation throughput: 116.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 14:34:41 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 96.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:34:44 [logger.py:43] Received request cmpl-4b0b886f5f124c70ae05bfbb0f746df9-0: prompt: 'What is speculative token generation?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 1580, 28524, 5993, 12623, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:38294 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:34:44 [engine.py:331] Added request cmpl-4b0b886f5f124c70ae05bfbb0f746df9-0.
INFO 08-09 14:34:46 [metrics.py:417] Avg prompt throughput: 1.6 tokens/s, Avg generation throughput: 113.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 14:34:51 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 100.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:34:54 [logger.py:43] Received request cmpl-5e0b50b54ebc46c1b31c4fd3d7fbfdcc-0: prompt: 'What are possible failure points in vLLM serving?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 526, 1950, 10672, 3291, 297, 325, 2208, 29924, 16330, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:57014 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:34:54 [engine.py:331] Added request cmpl-5e0b50b54ebc46c1b31c4fd3d7fbfdcc-0.
INFO 08-09 14:34:56 [metrics.py:417] Avg prompt throughput: 2.4 tokens/s, Avg generation throughput: 116.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 14:35:01 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 106.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:35:03 [logger.py:43] Received request cmpl-fbc67c328ba44dffba9be489d28dafdf-0: prompt: 'How to catch stream decoding errors gracefully?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 304, 4380, 4840, 1602, 3689, 4436, 17659, 3730, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:43068 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:35:03 [engine.py:331] Added request cmpl-fbc67c328ba44dffba9be489d28dafdf-0.
INFO 08-09 14:35:06 [metrics.py:417] Avg prompt throughput: 2.2 tokens/s, Avg generation throughput: 116.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:35:11 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 108.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:35:12 [logger.py:43] Received request cmpl-876d1d2911334e2eabf5743a07416025-0: prompt: 'How to enable/disable kv caching in transformers?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 304, 9025, 29914, 20472, 10908, 22488, 297, 4327, 414, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:37464 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:35:12 [engine.py:331] Added request cmpl-876d1d2911334e2eabf5743a07416025-0.
INFO 08-09 14:35:13 [logger.py:43] Received request cmpl-ef79d2909c5d4be4ac3f58b30acb865c-0: prompt: 'What are challenges of multilingual streaming inference?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 526, 18066, 267, 310, 1773, 6504, 950, 24820, 27262, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:37466 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:35:13 [engine.py:331] Added request cmpl-ef79d2909c5d4be4ac3f58b30acb865c-0.
INFO 08-09 14:35:16 [metrics.py:417] Avg prompt throughput: 4.8 tokens/s, Avg generation throughput: 114.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:35:21 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 102.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:35:23 [logger.py:43] Received request cmpl-dfd37541ff8145c5994f19a4d53c04b8-0: prompt: 'How to benchmark with different model precisions?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 304, 23513, 411, 1422, 1904, 12132, 1080, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:56094 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:35:23 [engine.py:331] Added request cmpl-dfd37541ff8145c5994f19a4d53c04b8-0.
INFO 08-09 14:35:26 [metrics.py:417] Avg prompt throughput: 2.0 tokens/s, Avg generation throughput: 115.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:35:31 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 106.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:35:32 [logger.py:43] Received request cmpl-edd94f8c7bc4468683ee9d336ab2e4e7-0: prompt: 'What is the tradeoff between token latency and batch size?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 338, 278, 11302, 2696, 1546, 5993, 23316, 1270, 322, 9853, 2159, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:54428 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:35:32 [engine.py:331] Added request cmpl-edd94f8c7bc4468683ee9d336ab2e4e7-0.
INFO 08-09 14:35:36 [metrics.py:417] Avg prompt throughput: 2.8 tokens/s, Avg generation throughput: 116.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:35:41 [logger.py:43] Received request cmpl-3a41430eb6a54196a58520bd58eaf30a-0: prompt: 'Describe the advantages and limitations of transformer-based neural networks.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 20355, 915, 278, 25486, 322, 27028, 310, 4327, 261, 29899, 6707, 19677, 14379, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:54444 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:35:41 [engine.py:331] Added request cmpl-3a41430eb6a54196a58520bd58eaf30a-0.
INFO 08-09 14:35:41 [metrics.py:417] Avg prompt throughput: 3.0 tokens/s, Avg generation throughput: 108.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 14:35:46 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 119.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:35:49 [logger.py:43] Received request cmpl-0364abf07a374df2902c3d24eade9532-0: prompt: 'Explain the concept of overfitting in machine learning and how to prevent it.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 12027, 7420, 278, 6964, 310, 975, 29888, 5367, 297, 4933, 6509, 322, 920, 304, 5557, 372, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:56286 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:35:49 [engine.py:331] Added request cmpl-0364abf07a374df2902c3d24eade9532-0.
INFO 08-09 14:35:51 [metrics.py:417] Avg prompt throughput: 3.6 tokens/s, Avg generation throughput: 106.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 14:35:56 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 117.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:35:58 [logger.py:43] Received request cmpl-8b2e3184148642559ab28a7dbc509b73-0: prompt: 'How does the attention mechanism work in transformer models?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 947, 278, 8570, 13336, 664, 297, 4327, 261, 4733, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:46802 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:35:58 [engine.py:331] Added request cmpl-8b2e3184148642559ab28a7dbc509b73-0.
INFO 08-09 14:36:01 [metrics.py:417] Avg prompt throughput: 2.4 tokens/s, Avg generation throughput: 105.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:36:06 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 117.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:36:07 [logger.py:43] Received request cmpl-082ac31d5c6e409699e89d7cd83bd40b-0: prompt: 'Summarize the main differences between supervised and unsupervised learning.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 6991, 3034, 675, 278, 1667, 12651, 1546, 2428, 11292, 322, 443, 9136, 11292, 6509, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:34568 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:36:07 [engine.py:331] Added request cmpl-082ac31d5c6e409699e89d7cd83bd40b-0.
INFO 08-09 14:36:11 [metrics.py:417] Avg prompt throughput: 3.2 tokens/s, Avg generation throughput: 106.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:36:16 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 113.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:36:16 [logger.py:43] Received request cmpl-a4dfc42f8832439a9daacae282b56f87-0: prompt: 'What are the ethical concerns associated with deploying large language models?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 526, 278, 11314, 936, 21838, 6942, 411, 7246, 292, 2919, 4086, 4733, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:37632 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:36:16 [engine.py:331] Added request cmpl-a4dfc42f8832439a9daacae282b56f87-0.
INFO 08-09 14:36:21 [metrics.py:417] Avg prompt throughput: 3.0 tokens/s, Avg generation throughput: 106.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:36:25 [logger.py:43] Received request cmpl-38ad0f59467e4a4f80c68a00767be412-0: prompt: 'Explain how gradient descent works in training deep neural networks.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 12027, 7420, 920, 16030, 26815, 1736, 297, 6694, 6483, 19677, 14379, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:56842 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:36:25 [engine.py:331] Added request cmpl-38ad0f59467e4a4f80c68a00767be412-0.
INFO 08-09 14:36:26 [metrics.py:417] Avg prompt throughput: 2.6 tokens/s, Avg generation throughput: 115.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 14:36:28 [logger.py:43] Received request cmpl-9719cb51b93a4a35b1c127e3263d1f7a-0: prompt: 'Describe the role of activation functions in deep learning.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 20355, 915, 278, 6297, 310, 26229, 3168, 297, 6483, 6509, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:56856 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:36:28 [engine.py:331] Added request cmpl-9719cb51b93a4a35b1c127e3263d1f7a-0.
INFO 08-09 14:36:31 [metrics.py:417] Avg prompt throughput: 2.4 tokens/s, Avg generation throughput: 102.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:36:36 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 116.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:36:37 [logger.py:43] Received request cmpl-152bf134c26c4298917616817426c925-0: prompt: 'How can transfer learning improve model performance on small datasets?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1128, 508, 6782, 6509, 11157, 1904, 4180, 373, 2319, 20035, 29973], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:36012 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:36:37 [engine.py:331] Added request cmpl-152bf134c26c4298917616817426c925-0.
INFO 08-09 14:36:41 [metrics.py:417] Avg prompt throughput: 2.4 tokens/s, Avg generation throughput: 100.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:36:42 [logger.py:43] Received request cmpl-c8154f5c975d47c59e7d46fcec7371e2-0: prompt: 'Explain the concept of reinforcement learning with an example.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 12027, 7420, 278, 6964, 310, 15561, 1454, 13561, 6509, 411, 385, 1342, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:60876 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:36:42 [engine.py:331] Added request cmpl-c8154f5c975d47c59e7d46fcec7371e2-0.
INFO 08-09 14:36:46 [metrics.py:417] Avg prompt throughput: 2.8 tokens/s, Avg generation throughput: 117.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:36:51 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 103.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:36:52 [logger.py:43] Received request cmpl-023ce85ca0e443f0a777cccd917bd55f-0: prompt: 'What are the main components of a convolutional neural network (CNN)?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 1724, 526, 278, 1667, 7117, 310, 263, 26851, 284, 19677, 3564, 313, 29907, 10262, 6877], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:43980 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:36:52 [engine.py:331] Added request cmpl-023ce85ca0e443f0a777cccd917bd55f-0.
INFO 08-09 14:36:56 [metrics.py:417] Avg prompt throughput: 3.2 tokens/s, Avg generation throughput: 111.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:37:01 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 101.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:37:01 [logger.py:43] Received request cmpl-96efd41d8c1e438b9bf4272dc3b5620d-0: prompt: 'Describe how large language models like GPT or LLaMA are trained and fine-tuned.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 20355, 915, 920, 2919, 4086, 4733, 763, 402, 7982, 470, 365, 5661, 1529, 526, 16370, 322, 2691, 29899, 29873, 348, 287, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:43992 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:37:01 [engine.py:331] Added request cmpl-96efd41d8c1e438b9bf4272dc3b5620d-0.
INFO 08-09 14:37:06 [metrics.py:417] Avg prompt throughput: 4.6 tokens/s, Avg generation throughput: 119.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:37:10 [logger.py:43] Received request cmpl-5a70e5a6699f4a9db8588bba27646c08-0: prompt: 'Compare and contrast BERT and GPT in terms of architecture and use cases.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.9, top_p=0.6, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1, 3831, 598, 322, 12814, 350, 20161, 322, 402, 7982, 297, 4958, 310, 11258, 322, 671, 4251, 29889], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:55560 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 08-09 14:37:10 [engine.py:331] Added request cmpl-5a70e5a6699f4a9db8588bba27646c08-0.
INFO 08-09 14:37:11 [metrics.py:417] Avg prompt throughput: 3.6 tokens/s, Avg generation throughput: 102.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 14:37:16 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 116.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 08-09 14:37:29 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 25.1 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 08-09 14:37:39 [metrics.py:417] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
